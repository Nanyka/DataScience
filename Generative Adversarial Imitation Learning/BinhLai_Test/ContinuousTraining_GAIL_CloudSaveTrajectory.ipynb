{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an Agent using Generative Adversarial Imitation Learning\n",
    "\n",
    "The idea of generative adversarial imitation learning is to train a discriminator network to distinguish between expert trajectories and learner trajectories.\n",
    "The learner is trained using a traditional reinforcement learning algorithm such as PPO and is rewarded for trajectories that make the discriminator think that it was an expert trajectory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import math\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "%matplotlib inline\n",
    "# from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import data_split\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from imitation.data.types import TrajectoryWithRew\n",
    "from pprint import pprint\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TEST_END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "TRAIN_START_DATE = '2000-01-01'\n",
    "TRAIN_END_DATE = '2021-01-01'\n",
    "TEST_START_DATE = '2021-01-01'\n",
    "TEST_END_DATE = datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 99956 entries, 0 to 99955\n",
      "Data columns (total 26 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   date                   99956 non-null  datetime64[ns]\n",
      " 1   tic                    99956 non-null  object        \n",
      " 2   close                  99956 non-null  float64       \n",
      " 3   gross_profit_margin    99956 non-null  float64       \n",
      " 4   sga_ratio              99956 non-null  float64       \n",
      " 5   dep_ratio              99956 non-null  float64       \n",
      " 6   ebit_on_int            99956 non-null  float64       \n",
      " 7   profit_margin          99956 non-null  float64       \n",
      " 8   count_positive_profit  99956 non-null  float64       \n",
      " 9   csti_on_liabilities    99956 non-null  float64       \n",
      " 10  inventory_on_ebit      99956 non-null  float64       \n",
      " 11  receivable_on_rev      99956 non-null  float64       \n",
      " 12  roa                    99956 non-null  float64       \n",
      " 13  roe                    99956 non-null  float64       \n",
      " 14  liabilities_on_equity  99956 non-null  float64       \n",
      " 15  debt_on_min_ebit       99956 non-null  float64       \n",
      " 16  capital_cost_on_ebit   99956 non-null  float64       \n",
      " 17  eps_on_mp              99956 non-null  float64       \n",
      " 18  dividend_on_mp         99956 non-null  float64       \n",
      " 19  mp_on_bv               99956 non-null  float64       \n",
      " 20  trend_gross_margin     99956 non-null  float64       \n",
      " 21  trend_profit_margin    99956 non-null  float64       \n",
      " 22  fluc_sga               99956 non-null  float64       \n",
      " 23  fluc_dep_ratio         99956 non-null  float64       \n",
      " 24  fluc_inv_on_ebit       99956 non-null  float64       \n",
      " 25  fluc_rec_on_rev        99956 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(24), object(1)\n",
      "memory usage: 20.6+ MB\n"
     ]
    }
   ],
   "source": [
    "processed_full = pd.read_csv('./' + DATA_SAVE_DIR + '/dow30_ready_with_filter_data_daily.csv',index_col=0)\n",
    "processed_full['date'] = pd.to_datetime(processed_full.date,format='mixed')\n",
    "processed_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75776\n",
      "24180\n"
     ]
    }
   ],
   "source": [
    "train_data = data_split(processed_full, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "test_data = data_split(processed_full, TEST_START_DATE, TEST_END_DATE)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data = test_data.reset_index(drop=True)\n",
    "# Check the length of the two datasets\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from portfolio import portfolio\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# from stable_baselines3.common import logger\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        hmax,\n",
    "        initial_amount,\n",
    "        buy_cost_pct,\n",
    "        sell_cost_pct,\n",
    "        reward_scaling,\n",
    "        state_space,\n",
    "        action_space,\n",
    "        tech_indicator_list,\n",
    "        stop_loss,\n",
    "        hold_period,\n",
    "        make_plots=False,\n",
    "        print_verbosity=10,\n",
    "        row=0,\n",
    "        initial=True,\n",
    "        previous_state=[],\n",
    "        model_name=\"\",\n",
    "        mode=\"\",\n",
    "        iteration=\"\",\n",
    "    ):\n",
    "        # self.row = row\n",
    "        self.df = df\n",
    "        # self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list        \n",
    "        self.initial_amount = initial_amount\n",
    "        self.hold_period = hold_period\n",
    "        self.buy_cost_pct = buy_cost_pct\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.stop_loss = stop_loss # the game stops when the asset loses more than stop_loss percent\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.state_space,))\n",
    "        self.terminal = False\n",
    "        self.make_plots = make_plots\n",
    "        self.print_verbosity = print_verbosity\n",
    "        # self.turbulence_threshold = turbulence_threshold\n",
    "        # self.risk_indicator_col = risk_indicator_col\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "        self.iteration = iteration\n",
    "        self.tic_list = self.df.tic.unique()\n",
    "        self.original_df = self.df.copy()\n",
    "        self.row = 0\n",
    "        \n",
    "        # initalize state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "        # self.reset()\n",
    "        self._seed()\n",
    "\n",
    "    def _buy_stock(self, action):\n",
    "        def _do_buy():\n",
    "            if self.data.close > 0: # Buy only if the price is > 0 (no missing data in this particular date)\n",
    "                buy_num_shares, buy_fee = self.portfolio.add_buy_stock(self.data.tic,self.data.close,action)\n",
    "                # print(f'Buy amount: {buy_num_shares}')\n",
    "                self.cost += buy_fee\n",
    "                if buy_num_shares == 0:\n",
    "                    self.reward = -5 * self.initial_amount * self.punishment_rate * self.reward_scaling\n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "\n",
    "            return buy_num_shares\n",
    "\n",
    "        buy_num_shares = _do_buy()\n",
    "        return buy_num_shares\n",
    "    \n",
    "    def _sell_stock(self, action):\n",
    "        def _do_sell_normal():\n",
    "            if self.data.close > 0: # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                sell_amount,surplus,sell_fee = self.portfolio.minus_sell_stock(self.data.tic,self.data.close,action)\n",
    "                self.cost += sell_fee\n",
    "                # print(f'Sell amount: {sell_num_shares}')\n",
    "                if sell_amount == 0:\n",
    "                    self.reward = -5 * self.initial_amount * self.punishment_rate * self.reward_scaling\n",
    "                else:\n",
    "                    self.reward += (surplus - sell_amount*self.data.close*self.sell_cost_pct) * self.reward_scaling\n",
    "                    self.win_trade += 1 if surplus > 0 else 0\n",
    "                    self.trades += 1\n",
    "            else:\n",
    "                sell_amount = 0\n",
    "\n",
    "            return sell_amount\n",
    "\n",
    "        sell_amount = _do_sell_normal()\n",
    "        return sell_amount\n",
    "\n",
    "    def step(self, actions):\n",
    "\n",
    "        self.terminal = (self.row >= len(self.df.index.unique()) - 1) | (self.portfolio.get_asset_value() < self.initial_amount*(1-self.stop_loss))\n",
    "        # print(f'Step {self.row}, action: {actions}, current asset: {current_total_asset}, stop loss: {self.initial_amount*(1-self.stop_loss)}, Trade: {self.trades}')\n",
    "\n",
    "        # Reset reward to zero\n",
    "        self.reward = 0\n",
    "        \n",
    "        # --> IN CASE THE STEP IS A TERMINATED STEP\n",
    "        if self.terminal: \n",
    "            \n",
    "            # Summary the training performance after an episode\n",
    "            end_total_asset = self.portfolio.get_asset_value()\n",
    "            tot_reward = end_total_asset - self.initial_amount * (self.df.iloc[-1].close / self.df.iloc[0].close) # compare with buy-and-hold strategy\n",
    "            # tot_reward = end_total_asset - self.initial_amount # compare with initial capital\n",
    "\n",
    "            # Show at each episode\n",
    "            print(f\"Episode: {self.episode}, com: {self.df.iloc[0].tic}, win trade: {self.win_trade}/{self.trades}, Total reward: {self.accumulated_reward}\")\n",
    "\n",
    "            # Print out training results after a certain amount of episodes\n",
    "            if self.episode % self.print_verbosity == 0:\n",
    "                print(f\"Current company: {self.df.iloc[0].tic}\")\n",
    "                print(f\"begin_total_asset: {self.asset_memory[0]:0.2f}\")\n",
    "                print(f\"end_total_asset: {end_total_asset:0.2f}\")\n",
    "                print(f\"surplus from buy-and-hold: {tot_reward:0.2f}\")\n",
    "                print(f\"total_cost: {self.cost:0.2f}\")\n",
    "                print(f\"total_trades: {self.trades}\")\n",
    "                # if df_total_value[\"daily_return\"].std() != 0:\n",
    "                #     print(f\"Sharpe: {sharpe:0.3f}\")\n",
    "                print(\"=================================\")\n",
    "            \n",
    "            truncated = False  # we do not limit the number of steps here\n",
    "            # Optionally we can pass additional info, we are not using that for now\n",
    "            info = {}\n",
    "\n",
    "\n",
    "            return (\n",
    "                np.array(self.state).astype(np.float32),\n",
    "                self.reward,\n",
    "                self.terminal,\n",
    "                truncated,\n",
    "                info,\n",
    "            )\n",
    "\n",
    "        # --> IN A NORMAL STEP\n",
    "        else: \n",
    "\n",
    "            # Act according to actions\n",
    "            action = actions[0]\n",
    "                \n",
    "            if action > 0:\n",
    "                self._buy_stock(action)\n",
    "            elif action < 0:\n",
    "                self._sell_stock(action)\n",
    "\n",
    "            self.current_actions = actions\n",
    "            self.actions_memory.append(actions)\n",
    "\n",
    "            # Set a punishment at each step to push the agent decide an action\n",
    "            self.reward += -1 * self.initial_amount * self.punishment_rate * self.reward_scaling\n",
    "            self.accumulated_reward += self.reward\n",
    "\n",
    "            # Update selected row in the dataset based on state: s -> s+1\n",
    "            self.row += 1\n",
    "            self.data = self.df.loc[self.row]\n",
    "            self.state = self._update_state()\n",
    "\n",
    "            end_total_asset = self.portfolio.get_asset_value()\n",
    "\n",
    "            # Update asset memory\n",
    "            self.current_asset = end_total_asset\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            self.date_memory.append(self._get_date())\n",
    "            \n",
    "            self.rewards_memory.append(self.reward)\n",
    "\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "        \n",
    "        # return self.state, self.reward, self.terminal, {}\n",
    "    \n",
    "        return (\n",
    "            np.array(self.state).astype(np.float32),\n",
    "            self.reward,\n",
    "            self.terminal,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # initiate state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # Reset asset_memory\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "\n",
    "        # Reset support variables\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.win_trade = 0\n",
    "        self.terminal = False\n",
    "        self.accumulated_reward = 0\n",
    "        self.block_remain = 0\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "        self.episode += 1\n",
    "\n",
    "        return np.array(self.state).astype(np.float32), {}\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        return self.state\n",
    "\n",
    "    def _initiate_state(self):\n",
    "        \n",
    "        # Reset portfolio & previous_portfolio\n",
    "        self.portfolio = portfolio(initial_amount=self.initial_amount,hold_period=self.hold_period,\n",
    "                                   buy_cost_pct=self.buy_cost_pct,sell_cost_pct=self.sell_cost_pct)\n",
    "\n",
    "        # Select a random ticker from df\n",
    "        self.df = self.original_df[self.original_df.tic == random.choice(self.tic_list)].reset_index(drop=True)\n",
    "        self.punishment_rate = 1/(len(self.df)*10)\n",
    "        \n",
    "        # Reset data\n",
    "        self.row = 0\n",
    "        self.data = self.df.loc[self.row]\n",
    "        \n",
    "         # Reset state\n",
    "        state = ([self.portfolio.get_remain_capital()] + [self.data.close] \n",
    "                    + [self.portfolio.get_stock_weight(self.data.tic)] \n",
    "                    +[self.portfolio.get_stock_profit(self.data.tic)]\n",
    "                    + sum([[self.data[tech]] for tech in self.tech_indicator_list], []))\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def _update_state(self):\n",
    "\n",
    "        self.portfolio.update_new_state(self.data.tic,self.data.close)\n",
    "        state = ([self.portfolio.get_remain_capital()] + [self.data.close] \n",
    "                    + [self.portfolio.get_stock_amount(self.data.tic)]\n",
    "                    +[self.portfolio.get_stock_profit(self.data.tic)]\n",
    "                    + sum([[self.data[tech]] for tech in self.tech_indicator_list], []))\n",
    "\n",
    "        return state\n",
    "\n",
    "    def _get_date(self):\n",
    "        return self.row\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        asset_list = self.asset_memory\n",
    "        df_account_value = pd.DataFrame({\"date\": date_list, \"account_value\": asset_list})\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        \n",
    "        date_list = self.date_memory[:-1]\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame({\"date\": date_list, \"actions\": action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Dimension: 1, State Space: 24\n"
     ]
    }
   ],
   "source": [
    "features = [ 'gross_profit_margin', 'sga_ratio', 'ebit_on_int', 'profit_margin', 'count_positive_profit',\n",
    "       'csti_on_liabilities', 'roa', 'roe', 'liabilities_on_equity', 'debt_on_min_ebit',\n",
    "       'capital_cost_on_ebit', 'eps_on_mp', 'dividend_on_mp', 'mp_on_bv',\n",
    "       'trend_gross_margin', 'trend_profit_margin', 'fluc_sga', 'fluc_dep_ratio', 'fluc_inv_on_ebit', 'fluc_rec_on_rev']\n",
    "# ratio_list = train_data.columns.drop(['date','tic','close'])\n",
    "\n",
    "action_dimension = 1 # k float in range (-1,1) to decide sell (k<0) or buy (k>0) decisions\n",
    "state_space = 4 + len(features)\n",
    "print(f\"Action Dimension: {action_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the environment\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.001,\n",
    "    \"tech_indicator_list\": features, \n",
    "    \"state_space\": state_space, \n",
    "    \"action_space\": action_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"stop_loss\": 0.8,\n",
    "    \"print_verbosity\":4,\n",
    "    \"hold_period\": 5\n",
    "}\n",
    "\n",
    "#Establish the training environment using StockTradingEnv() class\n",
    "e_train_gym = StockTradingEnv(df = test_data, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test customizing trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_trajectory(number_of_step):\n",
    "\n",
    "    def generate_random_obs():\n",
    "        random_row = processed_full.iloc[np.random.randint(len(processed_full))]\n",
    "        random_port = np.random.uniform(0,env_kwargs[\"initial_amount\"])\n",
    "        random_amount = np.random.randint(1000)\n",
    "        random_profit = np.random.rand(1)\n",
    "        state = ([random_port] + [random_row.close] + [random_amount] + [random_profit[0]]\n",
    "                    + sum([[random_row[tech]] for tech in features], []))\n",
    "    \n",
    "        return state\n",
    "\n",
    "    # Generate random observations\n",
    "    random_obs = np.array([generate_random_obs() for i in range(0,number_of_step + 1)])\n",
    "    \n",
    "    # Generate random actions\n",
    "    random_acts = np.array([np.random.rand(1)*2-1 for i in range(0,number_of_step)])\n",
    "    \n",
    "    # Generate random rewards\n",
    "    random_rews = np.random.rand(number_of_step)*10\n",
    "    \n",
    "    # And put all these components into the same trajectory\n",
    "    random_trajectory = TrajectoryWithRew(acts=random_acts, obs=random_obs,rews=random_rews,terminal=True,infos=None)\n",
    "    return random_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_rollouts = []\n",
    "number_of_step = 10\n",
    "for i in range(0,1000):\n",
    "    random_rollouts.append(generate_random_trajectory(number_of_step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to set up our GAIL trainer.\n",
    "Note, that the `reward_net` is actually the network of the discriminator.\n",
    "We evaluate the learner before and after training so we can see if it made any progress.\n",
    "\n",
    "First we construct a GAIL trainer ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start set up GAIL trainier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnx import load\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "policy_kwargs = dict(net_arch=dict(pi=[128,64, 32, 16], vf=[128,64, 32, 16]))\n",
    "SEED = 42\n",
    "\n",
    "# Load trained model from zip file\n",
    "dir_path=\"./\"+TRAINED_MODEL_DIR+\"/basic_stone\"+\".zip\"\n",
    "if os.path.exists(dir_path):\n",
    "    trained_model = PPO.load(dir_path)\n",
    "    trained_model.env = env_train\n",
    "else:\n",
    "    trained_model = PPO(\"MlpPolicy\", env_train,n_steps=1024,ent_coef=0.01,learning_rate=0.00025,batch_size=2048,clip_range=0.1,\n",
    "                      policy_kwargs=policy_kwargs,tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_ppo\",verbose=1)\n",
    "    current_dir = os.getcwd()\n",
    "    model_name = \"./\"+TRAINED_MODEL_DIR+\"/basic_stone\"\n",
    "    trained_model.save(model_name)\n",
    "learner = trained_model\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    observation_space=env_train.observation_space,\n",
    "    action_space=env_train.action_space,\n",
    "    normalize_input_layer=RunningNorm,\n",
    ")\n",
    "\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=random_rollouts,\n",
    "    demo_batch_size=64,\n",
    "    gen_replay_buffer_capacity=512,\n",
    "    n_disc_updates_per_round=8,\n",
    "    venv=env_train,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... then we evaluate it before training ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 11, com: AAPL, win trade: 40/40, Total reward: -8.703334403296187\n"
     ]
    }
   ],
   "source": [
    "env_train.seed(SEED)\n",
    "learner_rewards_before_training, _ = evaluate_policy(learner, env_train, 1, return_episode_rewards=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and train it ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:   0%|                                              | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 13, com: CRM, win trade: 171/303, Total reward: -7.334860949626051\n",
      "--------------------------------------\n",
      "| raw/                        |      |\n",
      "|    gen/time/fps             | 167  |\n",
      "|    gen/time/iterations      | 1    |\n",
      "|    gen/time/time_elapsed    | 6    |\n",
      "|    gen/time/total_timesteps | 1024 |\n",
      "--------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.531    |\n",
      "|    disc/disc_acc_expert             | 0.0625   |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.691    |\n",
      "|    disc/disc_loss                   | 0.694    |\n",
      "|    disc/disc_proportion_expert_pred | 0.0312   |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.547    |\n",
      "|    disc/disc_acc_expert             | 0.0938   |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.691    |\n",
      "|    disc/disc_loss                   | 0.689    |\n",
      "|    disc/disc_proportion_expert_pred | 0.0469   |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.578    |\n",
      "|    disc/disc_acc_expert             | 0.156    |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.692    |\n",
      "|    disc/disc_loss                   | 0.682    |\n",
      "|    disc/disc_proportion_expert_pred | 0.0781   |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.594    |\n",
      "|    disc/disc_acc_expert             | 0.188    |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.692    |\n",
      "|    disc/disc_loss                   | 0.675    |\n",
      "|    disc/disc_proportion_expert_pred | 0.0938   |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.586    |\n",
      "|    disc/disc_acc_expert             | 0.172    |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.691    |\n",
      "|    disc/disc_loss                   | 0.673    |\n",
      "|    disc/disc_proportion_expert_pred | 0.0859   |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.594    |\n",
      "|    disc/disc_acc_expert             | 0.188    |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.692    |\n",
      "|    disc/disc_loss                   | 0.678    |\n",
      "|    disc/disc_proportion_expert_pred | 0.0938   |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.586    |\n",
      "|    disc/disc_acc_expert             | 0.172    |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.692    |\n",
      "|    disc/disc_loss                   | 0.673    |\n",
      "|    disc/disc_proportion_expert_pred | 0.0859   |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.664    |\n",
      "|    disc/disc_acc_expert             | 0.328    |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.692    |\n",
      "|    disc/disc_loss                   | 0.668    |\n",
      "|    disc/disc_proportion_expert_pred | 0.164    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 1        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                               |           |\n",
      "|    disc/disc_acc                    | 0.585     |\n",
      "|    disc/disc_acc_expert             | 0.17      |\n",
      "|    disc/disc_acc_gen                | 1         |\n",
      "|    disc/disc_entropy                | 0.692     |\n",
      "|    disc/disc_loss                   | 0.679     |\n",
      "|    disc/disc_proportion_expert_pred | 0.085     |\n",
      "|    disc/disc_proportion_expert_true | 0.5       |\n",
      "|    disc/global_step                 | 1         |\n",
      "|    disc/n_expert                    | 64        |\n",
      "|    disc/n_generated                 | 64        |\n",
      "|    gen/time/fps                     | 167       |\n",
      "|    gen/time/iterations              | 1         |\n",
      "|    gen/time/time_elapsed            | 6         |\n",
      "|    gen/time/total_timesteps         | 1.02e+03  |\n",
      "|    gen/train/approx_kl              | 3.08e-07  |\n",
      "|    gen/train/clip_fraction          | 0         |\n",
      "|    gen/train/clip_range             | 0.1       |\n",
      "|    gen/train/entropy_loss           | -1.42     |\n",
      "|    gen/train/explained_variance     | 1.37e-06  |\n",
      "|    gen/train/learning_rate          | 0.00025   |\n",
      "|    gen/train/loss                   | 1.25e+08  |\n",
      "|    gen/train/n_updates              | 10        |\n",
      "|    gen/train/policy_gradient_loss   | -6.94e-05 |\n",
      "|    gen/train/std                    | 1         |\n",
      "|    gen/train/value_loss             | 2.5e+08   |\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round:  50%|███████████████████                   | 1/2 [00:06<00:06,  6.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 14, com: CSCO, win trade: 174/305, Total reward: -2.0901281053809546\n",
      "------------------------------------------------------\n",
      "| raw/                               |               |\n",
      "|    gen/rollout/ep_rew_wrapped_mean | 6.88e+05      |\n",
      "|    gen/time/fps                    | 152           |\n",
      "|    gen/time/iterations             | 1             |\n",
      "|    gen/time/time_elapsed           | 6             |\n",
      "|    gen/time/total_timesteps        | 2048          |\n",
      "|    gen/train/approx_kl             | 3.0820956e-07 |\n",
      "|    gen/train/clip_fraction         | 0             |\n",
      "|    gen/train/clip_range            | 0.1           |\n",
      "|    gen/train/entropy_loss          | -1.42         |\n",
      "|    gen/train/explained_variance    | 1.37e-06      |\n",
      "|    gen/train/learning_rate         | 0.00025       |\n",
      "|    gen/train/loss                  | 1.25e+08      |\n",
      "|    gen/train/n_updates             | 10            |\n",
      "|    gen/train/policy_gradient_loss  | -6.94e-05     |\n",
      "|    gen/train/std                   | 1             |\n",
      "|    gen/train/value_loss            | 2.5e+08       |\n",
      "------------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.664    |\n",
      "|    disc/disc_acc_expert             | 0.344    |\n",
      "|    disc/disc_acc_gen                | 0.984    |\n",
      "|    disc/disc_entropy                | 0.691    |\n",
      "|    disc/disc_loss                   | 0.67     |\n",
      "|    disc/disc_proportion_expert_pred | 0.18     |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 2        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.75     |\n",
      "|    disc/disc_acc_expert             | 0.516    |\n",
      "|    disc/disc_acc_gen                | 0.984    |\n",
      "|    disc/disc_entropy                | 0.692    |\n",
      "|    disc/disc_loss                   | 0.664    |\n",
      "|    disc/disc_proportion_expert_pred | 0.266    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 2        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.742    |\n",
      "|    disc/disc_acc_expert             | 0.516    |\n",
      "|    disc/disc_acc_gen                | 0.969    |\n",
      "|    disc/disc_entropy                | 0.692    |\n",
      "|    disc/disc_loss                   | 0.668    |\n",
      "|    disc/disc_proportion_expert_pred | 0.273    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 2        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.797    |\n",
      "|    disc/disc_acc_expert             | 0.641    |\n",
      "|    disc/disc_acc_gen                | 0.953    |\n",
      "|    disc/disc_entropy                | 0.691    |\n",
      "|    disc/disc_loss                   | 0.661    |\n",
      "|    disc/disc_proportion_expert_pred | 0.344    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 2        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.828    |\n",
      "|    disc/disc_acc_expert             | 0.688    |\n",
      "|    disc/disc_acc_gen                | 0.969    |\n",
      "|    disc/disc_entropy                | 0.69     |\n",
      "|    disc/disc_loss                   | 0.65     |\n",
      "|    disc/disc_proportion_expert_pred | 0.359    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 2        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.875    |\n",
      "|    disc/disc_acc_expert             | 0.781    |\n",
      "|    disc/disc_acc_gen                | 0.969    |\n",
      "|    disc/disc_entropy                | 0.69     |\n",
      "|    disc/disc_loss                   | 0.644    |\n",
      "|    disc/disc_proportion_expert_pred | 0.406    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 2        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.891    |\n",
      "|    disc/disc_acc_expert             | 0.781    |\n",
      "|    disc/disc_acc_gen                | 1        |\n",
      "|    disc/disc_entropy                | 0.69     |\n",
      "|    disc/disc_loss                   | 0.644    |\n",
      "|    disc/disc_proportion_expert_pred | 0.391    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 2        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "| raw/                                |          |\n",
      "|    disc/disc_acc                    | 0.883    |\n",
      "|    disc/disc_acc_expert             | 0.828    |\n",
      "|    disc/disc_acc_gen                | 0.938    |\n",
      "|    disc/disc_entropy                | 0.686    |\n",
      "|    disc/disc_loss                   | 0.633    |\n",
      "|    disc/disc_proportion_expert_pred | 0.445    |\n",
      "|    disc/disc_proportion_expert_true | 0.5      |\n",
      "|    disc/global_step                 | 2        |\n",
      "|    disc/n_expert                    | 64       |\n",
      "|    disc/n_generated                 | 64       |\n",
      "--------------------------------------------------\n",
      "---------------------------------------------------\n",
      "| mean/                               |           |\n",
      "|    disc/disc_acc                    | 0.804     |\n",
      "|    disc/disc_acc_expert             | 0.637     |\n",
      "|    disc/disc_acc_gen                | 0.971     |\n",
      "|    disc/disc_entropy                | 0.69      |\n",
      "|    disc/disc_loss                   | 0.654     |\n",
      "|    disc/disc_proportion_expert_pred | 0.333     |\n",
      "|    disc/disc_proportion_expert_true | 0.5       |\n",
      "|    disc/global_step                 | 2         |\n",
      "|    disc/n_expert                    | 64        |\n",
      "|    disc/n_generated                 | 64        |\n",
      "|    gen/rollout/ep_rew_wrapped_mean  | 6.88e+05  |\n",
      "|    gen/time/fps                     | 152       |\n",
      "|    gen/time/iterations              | 1         |\n",
      "|    gen/time/time_elapsed            | 6         |\n",
      "|    gen/time/total_timesteps         | 2.05e+03  |\n",
      "|    gen/train/approx_kl              | 5.98e-05  |\n",
      "|    gen/train/clip_fraction          | 0         |\n",
      "|    gen/train/clip_range             | 0.1       |\n",
      "|    gen/train/entropy_loss           | -1.42     |\n",
      "|    gen/train/explained_variance     | -0.267    |\n",
      "|    gen/train/learning_rate          | 0.00025   |\n",
      "|    gen/train/loss                   | 52.2      |\n",
      "|    gen/train/n_updates              | 20        |\n",
      "|    gen/train/policy_gradient_loss   | -0.000168 |\n",
      "|    gen/train/std                    | 1         |\n",
      "|    gen/train/value_loss             | 108       |\n",
      "---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "round: 100%|██████████████████████████████████████| 2/2 [00:13<00:00,  6.64s/it]\n"
     ]
    }
   ],
   "source": [
    "gail_trainer.train(2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and finally evaluate it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 16, com: IBM, win trade: 0/0, Total reward: -59.92555831265569\n",
      "Current company: IBM\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "surplus from buy-and-hold: -889089.03\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "env_train.seed(SEED)\n",
    "learner_rewards_after_training, _ = evaluate_policy(learner, env_train, 1, return_episode_rewards=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that an untrained policy performs poorly, while GAIL matches expert returns (500):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards before training: -8.703334500547498 +/- 0.0\n",
      "Rewards after training: -59.92555744946003 +/- 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Rewards before training:\",\n",
    "    np.mean(learner_rewards_before_training),\n",
    "    \"+/-\",\n",
    "    np.std(learner_rewards_before_training),\n",
    ")\n",
    "print(\n",
    "    \"Rewards after training:\",\n",
    "    np.mean(learner_rewards_after_training),\n",
    "    \"+/-\",\n",
    "    np.std(learner_rewards_after_training),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DRL_prediction(model, environment, deterministic=False):\n",
    "        \"\"\"make a prediction and get results\"\"\"\n",
    "        # test_env, test_obs = environment.get_sb_env()\n",
    "        # account_memory = None  # This help avoid unnecessary list creation\n",
    "        # actions_memory = None  # optimize memory consumption\n",
    "\n",
    "        test_obs = environment.reset()[0]\n",
    "        # max_steps = len(environment.df.index.unique()) - 1\n",
    "\n",
    "        for i in range(0,len(environment.df)):\n",
    "            action = model.predict(np.asarray(test_obs), deterministic=deterministic)\n",
    "            test_obs,reward,terminal,truncated,info = environment.step(action[0])\n",
    "\n",
    "            if terminal:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "        return pd.DataFrame(environment.asset_memory, columns=['account_value']), pd.DataFrame(environment.actions_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 18, com: PG, win trade: 181/297, Total reward: -13.57677455666185\n",
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_ppo, df_actions_ppo = DRL_prediction(model=learner, environment = e_train_gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bd378ce8f53beae712f05342da42c6a7612fc68b19bea03b52c7b1cdc8851b5f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
