{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Single Stock Trading Using Ensemble Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "75fcd958-c29f-44f0-85ea-4b4f6ae180ec"
   },
   "outputs": [],
   "source": [
    "# ## install finrl library\n",
    "# !pip install wrds\n",
    "# !pip install swig\n",
    "# !pip install -q condacolab\n",
    "# import condacolab\n",
    "# condacolab.install()\n",
    "# !apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "178c70ab-72e5-4ed7-cfa8-fd6ea7b1e8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ticket = ['AAPL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (3522, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_START_DATE = '2009-04-01'\n",
    "# TRAIN_END_DATE = '2021-01-01'\n",
    "# TEST_START_DATE = '2021-01-01'\n",
    "# TEST_END_DATE = '2022-06-01'\n",
    "\n",
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2021-01-01'\n",
    "TEST_START_DATE = '2021-01-01'\n",
    "TEST_END_DATE = '2024-01-01'\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = test_ticket).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GiRuFOTOtj1Y",
    "outputId": "402874c0-b13f-437b-a67f-a83f88de66eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>7.622500</td>\n",
       "      <td>7.660714</td>\n",
       "      <td>7.585000</td>\n",
       "      <td>6.470741</td>\n",
       "      <td>493729600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>7.664286</td>\n",
       "      <td>7.699643</td>\n",
       "      <td>7.616071</td>\n",
       "      <td>6.481928</td>\n",
       "      <td>601904800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>7.656429</td>\n",
       "      <td>7.686786</td>\n",
       "      <td>7.526786</td>\n",
       "      <td>6.378825</td>\n",
       "      <td>552160000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>7.562500</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466071</td>\n",
       "      <td>6.367033</td>\n",
       "      <td>477131200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>7.510714</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466429</td>\n",
       "      <td>6.409362</td>\n",
       "      <td>447610800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      open      high       low     close     volume   tic  day\n",
       "0  2010-01-04  7.622500  7.660714  7.585000  6.470741  493729600  AAPL    0\n",
       "1  2010-01-05  7.664286  7.699643  7.616071  6.481928  601904800  AAPL    1\n",
       "2  2010-01-06  7.656429  7.686786  7.526786  6.378825  552160000  AAPL    2\n",
       "3  2010-01-07  7.562500  7.571429  7.466071  6.367033  477131200  AAPL    3\n",
       "4  2010-01-08  7.510714  7.571429  7.466429  6.409362  447610800  AAPL    4"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DSw4ZEzVtj1Z",
    "outputId": "94617d16-432c-40eb-f758-16d2fdab09e0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3517</th>\n",
       "      <td>2023-12-22</td>\n",
       "      <td>195.179993</td>\n",
       "      <td>195.410004</td>\n",
       "      <td>192.970001</td>\n",
       "      <td>193.353287</td>\n",
       "      <td>37122800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3518</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>193.610001</td>\n",
       "      <td>193.889999</td>\n",
       "      <td>192.830002</td>\n",
       "      <td>192.803986</td>\n",
       "      <td>28919300</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3519</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>192.490005</td>\n",
       "      <td>193.500000</td>\n",
       "      <td>191.089996</td>\n",
       "      <td>192.903839</td>\n",
       "      <td>48087700</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3520</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>194.139999</td>\n",
       "      <td>194.660004</td>\n",
       "      <td>193.169998</td>\n",
       "      <td>193.333298</td>\n",
       "      <td>34049900</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>193.899994</td>\n",
       "      <td>194.399994</td>\n",
       "      <td>191.729996</td>\n",
       "      <td>192.284637</td>\n",
       "      <td>42628800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        open        high         low       close    volume  \\\n",
       "3517  2023-12-22  195.179993  195.410004  192.970001  193.353287  37122800   \n",
       "3518  2023-12-26  193.610001  193.889999  192.830002  192.803986  28919300   \n",
       "3519  2023-12-27  192.490005  193.500000  191.089996  192.903839  48087700   \n",
       "3520  2023-12-28  194.139999  194.660004  193.169998  193.333298  34049900   \n",
       "3521  2023-12-29  193.899994  194.399994  191.729996  192.284637  42628800   \n",
       "\n",
       "       tic  day  \n",
       "3517  AAPL    4  \n",
       "3518  AAPL    1  \n",
       "3519  AAPL    2  \n",
       "3520  AAPL    3  \n",
       "3521  AAPL    4  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV3HrZHLh1hy",
    "outputId": "73944c23-5a4e-49f8-b9e5-da382b4fc7f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3522, 8)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4hYkeaPiICHS",
    "outputId": "87cca0b1-8d3c-4a61-e061-ea0d9989daa1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>7.622500</td>\n",
       "      <td>7.660714</td>\n",
       "      <td>7.585000</td>\n",
       "      <td>6.470741</td>\n",
       "      <td>493729600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>7.664286</td>\n",
       "      <td>7.699643</td>\n",
       "      <td>7.616071</td>\n",
       "      <td>6.481928</td>\n",
       "      <td>601904800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>7.656429</td>\n",
       "      <td>7.686786</td>\n",
       "      <td>7.526786</td>\n",
       "      <td>6.378825</td>\n",
       "      <td>552160000</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>7.562500</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466071</td>\n",
       "      <td>6.367033</td>\n",
       "      <td>477131200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>7.510714</td>\n",
       "      <td>7.571429</td>\n",
       "      <td>7.466429</td>\n",
       "      <td>6.409362</td>\n",
       "      <td>447610800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      open      high       low     close     volume   tic  day\n",
       "0  2010-01-04  7.622500  7.660714  7.585000  6.470741  493729600  AAPL    0\n",
       "1  2010-01-05  7.664286  7.699643  7.616071  6.481928  601904800  AAPL    1\n",
       "2  2010-01-06  7.656429  7.686786  7.526786  6.378825  552160000  AAPL    2\n",
       "3  2010-01-07  7.562500  7.571429  7.466071  6.367033  477131200  AAPL    3\n",
       "4  2010-01-08  7.510714  7.571429  7.466429  6.409362  447610800  AAPL    4"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(['date','tic']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2vryMsdNL9H",
    "outputId": "6691ba9b-e613-412b-dba5-dee592bb0ff2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.tic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcNyXa7RNPrF",
    "outputId": "edb04575-9b82-4d5e-f13a-55c884214725"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tic\n",
       "AAPL    3522\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "kM5bH9uroCeg"
   },
   "outputs": [],
   "source": [
    "#  INDICATORS = ['macd',\n",
    "#                'rsi_30',\n",
    "#                'cci_30',\n",
    "#                'dx_30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "bd80d5c7-6ab7-4938-e1aa-f60ff642dc02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "grvhGJJII3Xn",
    "outputId": "06a440dc-bb85-4ce9-83ab-53b3a62f0cc6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>tic</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>turbulence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>2011-09-15</td>\n",
       "      <td>13.979643</td>\n",
       "      <td>14.059286</td>\n",
       "      <td>13.925000</td>\n",
       "      <td>11.881422</td>\n",
       "      <td>417818800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.108890</td>\n",
       "      <td>12.073095</td>\n",
       "      <td>10.838246</td>\n",
       "      <td>56.765574</td>\n",
       "      <td>134.465092</td>\n",
       "      <td>0.568691</td>\n",
       "      <td>11.403393</td>\n",
       "      <td>11.214767</td>\n",
       "      <td>0.270231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>2022-07-27</td>\n",
       "      <td>152.580002</td>\n",
       "      <td>157.330002</td>\n",
       "      <td>152.160004</td>\n",
       "      <td>155.246262</td>\n",
       "      <td>78620700</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>2</td>\n",
       "      <td>3.239637</td>\n",
       "      <td>157.321950</td>\n",
       "      <td>134.723985</td>\n",
       "      <td>55.503050</td>\n",
       "      <td>127.880941</td>\n",
       "      <td>23.186162</td>\n",
       "      <td>142.237611</td>\n",
       "      <td>143.991847</td>\n",
       "      <td>3.254549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>2015-04-27</td>\n",
       "      <td>33.077499</td>\n",
       "      <td>33.282501</td>\n",
       "      <td>32.787498</td>\n",
       "      <td>29.764297</td>\n",
       "      <td>387816800</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.282018</td>\n",
       "      <td>29.411167</td>\n",
       "      <td>27.572260</td>\n",
       "      <td>60.547689</td>\n",
       "      <td>290.500583</td>\n",
       "      <td>34.478828</td>\n",
       "      <td>28.408281</td>\n",
       "      <td>28.267057</td>\n",
       "      <td>1.451633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2152</th>\n",
       "      <td>2018-07-23</td>\n",
       "      <td>47.669998</td>\n",
       "      <td>47.990002</td>\n",
       "      <td>47.389999</td>\n",
       "      <td>45.599171</td>\n",
       "      <td>63957600</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.315203</td>\n",
       "      <td>46.330792</td>\n",
       "      <td>43.261180</td>\n",
       "      <td>56.796414</td>\n",
       "      <td>69.347239</td>\n",
       "      <td>10.840701</td>\n",
       "      <td>44.818276</td>\n",
       "      <td>44.450211</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2010-03-04</td>\n",
       "      <td>7.474286</td>\n",
       "      <td>7.532857</td>\n",
       "      <td>7.451071</td>\n",
       "      <td>6.370965</td>\n",
       "      <td>366041200</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>3</td>\n",
       "      <td>0.054877</td>\n",
       "      <td>6.411109</td>\n",
       "      <td>5.759304</td>\n",
       "      <td>54.178601</td>\n",
       "      <td>144.225111</td>\n",
       "      <td>19.789569</td>\n",
       "      <td>6.076428</td>\n",
       "      <td>6.163468</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date        open        high         low       close     volume  \\\n",
       "429   2011-09-15   13.979643   14.059286   13.925000   11.881422  417818800   \n",
       "3162  2022-07-27  152.580002  157.330002  152.160004  155.246262   78620700   \n",
       "1336  2015-04-27   33.077499   33.282501   32.787498   29.764297  387816800   \n",
       "2152  2018-07-23   47.669998   47.990002   47.389999   45.599171   63957600   \n",
       "41    2010-03-04    7.474286    7.532857    7.451071    6.370965  366041200   \n",
       "\n",
       "       tic  day      macd     boll_ub     boll_lb     rsi_30      cci_30  \\\n",
       "429   AAPL    3  0.108890   12.073095   10.838246  56.765574  134.465092   \n",
       "3162  AAPL    2  3.239637  157.321950  134.723985  55.503050  127.880941   \n",
       "1336  AAPL    0  0.282018   29.411167   27.572260  60.547689  290.500583   \n",
       "2152  AAPL    0  0.315203   46.330792   43.261180  56.796414   69.347239   \n",
       "41    AAPL    3  0.054877    6.411109    5.759304  54.178601  144.225111   \n",
       "\n",
       "          dx_30  close_30_sma  close_60_sma  turbulence  \n",
       "429    0.568691     11.403393     11.214767    0.270231  \n",
       "3162  23.186162    142.237611    143.991847    3.254549  \n",
       "1336  34.478828     28.408281     28.267057    1.451633  \n",
       "2152  10.840701     44.818276     44.450211    0.000273  \n",
       "41    19.789569      6.076428      6.163468    0.000000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "e16902dc-86b3-488e-ec15-234a3d6039c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 1, State Space: 11\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension # remain_capital + (prices_of_stocks+amount_of_holdings) + indicators\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "v-gthCxMtj1d"
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "KsfEHa_Etj1d"
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "73e2d3f8-463a-42d5-d49f-c71385a26c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_126_4\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 763          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.5         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -0.00877     |\n",
      "|    reward             | 0.0033626838 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 6.79e-05     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 749        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.5       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.222     |\n",
      "|    reward             | 0.41557628 |\n",
      "|    std                | 1.08       |\n",
      "|    value_loss         | 0.0236     |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 754           |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.51         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | 0.00653       |\n",
      "|    reward             | -9.617228e-05 |\n",
      "|    std                | 1.09          |\n",
      "|    value_loss         | 1.47e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 758           |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 2             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.52         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | 0.000276      |\n",
      "|    reward             | -0.0001888482 |\n",
      "|    std                | 1.11          |\n",
      "|    value_loss         | 4.26e-08      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 762            |\n",
      "|    iterations         | 500            |\n",
      "|    time_elapsed       | 3              |\n",
      "|    total_timesteps    | 2500           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.55          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 499            |\n",
      "|    policy_loss        | 0.00102        |\n",
      "|    reward             | -0.00049387757 |\n",
      "|    std                | 1.14           |\n",
      "|    value_loss         | 7.47e-07       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 761           |\n",
      "|    iterations         | 600           |\n",
      "|    time_elapsed       | 3             |\n",
      "|    total_timesteps    | 3000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.57         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 599           |\n",
      "|    policy_loss        | -0.00121      |\n",
      "|    reward             | -4.878954e-05 |\n",
      "|    std                | 1.16          |\n",
      "|    value_loss         | 1.04e-06      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 760       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.00891   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 6.04e-05  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 757       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.000989 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 4.25e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 756      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000138 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.32     |\n",
      "|    value_loss         | 3.61e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 754       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.75     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.0233    |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.4       |\n",
      "|    value_loss         | 0.000205  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 756      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00384  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 6.31e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 754      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 2.41e-06 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 3.1e-12  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 752       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.000291 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 1.81e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 746       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.97     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 0.000311  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.73      |\n",
      "|    value_loss         | 4.03e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 719      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000225 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.83     |\n",
      "|    value_loss         | 6.27e-08 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 721           |\n",
      "|    iterations         | 1600          |\n",
      "|    time_elapsed       | 11            |\n",
      "|    total_timesteps    | 8000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.09         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1599          |\n",
      "|    policy_loss        | -4.65e-06     |\n",
      "|    reward             | -0.0016961293 |\n",
      "|    std                | 1.96          |\n",
      "|    value_loss         | 6.56e-12      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 721      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.00116 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.07     |\n",
      "|    value_loss         | 2.75e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -0.000643 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 1.34e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.26    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 0.000148 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.31     |\n",
      "|    value_loss         | 8.87e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.31     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.000421 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.43      |\n",
      "|    value_loss         | 3.65e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 719      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -0.0151  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.56     |\n",
      "|    value_loss         | 5.83e-05 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 716            |\n",
      "|    iterations         | 2200           |\n",
      "|    time_elapsed       | 15             |\n",
      "|    total_timesteps    | 11000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.4           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 2199           |\n",
      "|    policy_loss        | -0.144         |\n",
      "|    reward             | -0.00040681273 |\n",
      "|    std                | 2.68           |\n",
      "|    value_loss         | 0.0079         |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 716      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.43    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.00709 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.74     |\n",
      "|    value_loss         | 5.06e-06 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 715           |\n",
      "|    iterations         | 2400          |\n",
      "|    time_elapsed       | 16            |\n",
      "|    total_timesteps    | 12000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.46         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2399          |\n",
      "|    policy_loss        | -0.0012       |\n",
      "|    reward             | -0.0012183082 |\n",
      "|    std                | 2.83          |\n",
      "|    value_loss         | 3.17e-07      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 714           |\n",
      "|    iterations         | 2500          |\n",
      "|    time_elapsed       | 17            |\n",
      "|    total_timesteps    | 12500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.51         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2499          |\n",
      "|    policy_loss        | -0.119        |\n",
      "|    reward             | -0.0014561041 |\n",
      "|    std                | 2.97          |\n",
      "|    value_loss         | 0.00239       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 713           |\n",
      "|    iterations         | 2600          |\n",
      "|    time_elapsed       | 18            |\n",
      "|    total_timesteps    | 13000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.55         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2599          |\n",
      "|    policy_loss        | -0.00295      |\n",
      "|    reward             | -0.0037168937 |\n",
      "|    std                | 3.09          |\n",
      "|    value_loss         | 1.93e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 712      |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 13500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.00433 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.24     |\n",
      "|    value_loss         | 7.02e-06 |\n",
      "------------------------------------\n",
      "day: 2768, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 999380.46\n",
      "total_reward: -619.54\n",
      "total_cost: 4982.26\n",
      "total_trades: 1753\n",
      "Sharpe: -0.013\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 708           |\n",
      "|    iterations         | 2800          |\n",
      "|    time_elapsed       | 19            |\n",
      "|    total_timesteps    | 14000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.64         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2799          |\n",
      "|    policy_loss        | -0.00556      |\n",
      "|    reward             | -4.443711e-05 |\n",
      "|    std                | 3.4           |\n",
      "|    value_loss         | 5.81e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 708           |\n",
      "|    iterations         | 2900          |\n",
      "|    time_elapsed       | 20            |\n",
      "|    total_timesteps    | 14500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.66         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2899          |\n",
      "|    policy_loss        | 0.00181       |\n",
      "|    reward             | -0.0009910528 |\n",
      "|    std                | 3.45          |\n",
      "|    value_loss         | 3.22e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 708           |\n",
      "|    iterations         | 3000          |\n",
      "|    time_elapsed       | 21            |\n",
      "|    total_timesteps    | 15000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.68         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2999          |\n",
      "|    policy_loss        | -0.00847      |\n",
      "|    reward             | -0.0010258898 |\n",
      "|    std                | 3.52          |\n",
      "|    value_loss         | 9.06e-06      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 709         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.69       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | -0.0251     |\n",
      "|    reward             | 0.011818288 |\n",
      "|    std                | 3.58        |\n",
      "|    value_loss         | 0.000106    |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 710            |\n",
      "|    iterations         | 3200           |\n",
      "|    time_elapsed       | 22             |\n",
      "|    total_timesteps    | 16000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.72          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 3199           |\n",
      "|    policy_loss        | -0.00738       |\n",
      "|    reward             | -0.00046363063 |\n",
      "|    std                | 3.67           |\n",
      "|    value_loss         | 9.49e-06       |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 711         |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.73       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | 0.585       |\n",
      "|    reward             | 0.036415033 |\n",
      "|    std                | 3.71        |\n",
      "|    value_loss         | 0.0464      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 711        |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.72      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | 0.291      |\n",
      "|    reward             | 0.06200194 |\n",
      "|    std                | 3.69       |\n",
      "|    value_loss         | 0.0138     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 711          |\n",
      "|    iterations         | 3500         |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 17500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.74        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3499         |\n",
      "|    policy_loss        | 0.206        |\n",
      "|    reward             | -0.006453386 |\n",
      "|    std                | 3.74         |\n",
      "|    value_loss         | 0.0109       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 710        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.75      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -0.516     |\n",
      "|    reward             | -0.3413062 |\n",
      "|    std                | 3.76       |\n",
      "|    value_loss         | 0.0462     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 708        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 1.46       |\n",
      "|    reward             | 0.08440494 |\n",
      "|    std                | 3.79       |\n",
      "|    value_loss         | 0.187      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 706          |\n",
      "|    iterations         | 3800         |\n",
      "|    time_elapsed       | 26           |\n",
      "|    total_timesteps    | 19000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.75        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3799         |\n",
      "|    policy_loss        | 4.55         |\n",
      "|    reward             | -0.030174242 |\n",
      "|    std                | 3.77         |\n",
      "|    value_loss         | 4.69         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 706         |\n",
      "|    iterations         | 3900        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 19500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.74       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3899        |\n",
      "|    policy_loss        | -0.219      |\n",
      "|    reward             | 0.006393946 |\n",
      "|    std                | 3.76        |\n",
      "|    value_loss         | 0.00774     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 705          |\n",
      "|    iterations         | 4000         |\n",
      "|    time_elapsed       | 28           |\n",
      "|    total_timesteps    | 20000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.75        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3999         |\n",
      "|    policy_loss        | 0.0365       |\n",
      "|    reward             | -0.010216578 |\n",
      "|    std                | 3.79         |\n",
      "|    value_loss         | 0.0102       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 703        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.76      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 2.09       |\n",
      "|    reward             | -0.7405807 |\n",
      "|    std                | 3.82       |\n",
      "|    value_loss         | 0.65       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 704       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.77     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 1.67      |\n",
      "|    reward             | 0.352914  |\n",
      "|    std                | 3.84      |\n",
      "|    value_loss         | 2.39      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 704       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.76     |\n",
      "|    explained_variance | -0.403    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | 3.64      |\n",
      "|    reward             | 3.7632523 |\n",
      "|    std                | 3.81      |\n",
      "|    value_loss         | 2.41      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 705        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.77      |\n",
      "|    explained_variance | 0.139      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 29.3       |\n",
      "|    reward             | 0.15815604 |\n",
      "|    std                | 3.88       |\n",
      "|    value_loss         | 77         |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 705         |\n",
      "|    iterations         | 4500        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 22500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.78       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4499        |\n",
      "|    policy_loss        | -0.732      |\n",
      "|    reward             | -0.17846243 |\n",
      "|    std                | 3.9         |\n",
      "|    value_loss         | 0.0722      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 706          |\n",
      "|    iterations         | 4600         |\n",
      "|    time_elapsed       | 32           |\n",
      "|    total_timesteps    | 23000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.78        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4599         |\n",
      "|    policy_loss        | -1.24        |\n",
      "|    reward             | -0.116412066 |\n",
      "|    std                | 3.9          |\n",
      "|    value_loss         | 0.224        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 707        |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.78      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -6.63      |\n",
      "|    reward             | 0.13343205 |\n",
      "|    std                | 3.89       |\n",
      "|    value_loss         | 8.24       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 708       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.77     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | 8.76      |\n",
      "|    reward             | 5.1521177 |\n",
      "|    std                | 3.85      |\n",
      "|    value_loss         | 10.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 709       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.76     |\n",
      "|    explained_variance | 1.73e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -5.33     |\n",
      "|    reward             | 3.3722758 |\n",
      "|    std                | 3.83      |\n",
      "|    value_loss         | 7.08      |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 710           |\n",
      "|    iterations         | 5000          |\n",
      "|    time_elapsed       | 35            |\n",
      "|    total_timesteps    | 25000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.75         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4999          |\n",
      "|    policy_loss        | -0.0742       |\n",
      "|    reward             | -0.0022603418 |\n",
      "|    std                | 3.8           |\n",
      "|    value_loss         | 0.00236       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 711         |\n",
      "|    iterations         | 5100        |\n",
      "|    time_elapsed       | 35          |\n",
      "|    total_timesteps    | 25500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.75       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5099        |\n",
      "|    policy_loss        | -3.76       |\n",
      "|    reward             | -0.74566454 |\n",
      "|    std                | 3.78        |\n",
      "|    value_loss         | 1.75        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 711        |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | -1.52      |\n",
      "|    reward             | 0.51931757 |\n",
      "|    std                | 3.8        |\n",
      "|    value_loss         | 0.602      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 712       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.74     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 1.93      |\n",
      "|    reward             | 1.2369229 |\n",
      "|    std                | 3.76      |\n",
      "|    value_loss         | 0.777     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 713       |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.74     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | -3.76     |\n",
      "|    reward             | 1.6673589 |\n",
      "|    std                | 3.75      |\n",
      "|    value_loss         | 9.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 714       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.74     |\n",
      "|    explained_variance | -0.00337  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 68.8      |\n",
      "|    reward             | 4.3708878 |\n",
      "|    std                | 3.75      |\n",
      "|    value_loss         | 558       |\n",
      "-------------------------------------\n",
      "day: 2768, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5899808.85\n",
      "total_reward: 4899808.85\n",
      "total_cost: 7794.80\n",
      "total_trades: 2762\n",
      "Sharpe: 0.923\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 714          |\n",
      "|    iterations         | 5600         |\n",
      "|    time_elapsed       | 39           |\n",
      "|    total_timesteps    | 28000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.75        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5599         |\n",
      "|    policy_loss        | -0.036       |\n",
      "|    reward             | -0.016795376 |\n",
      "|    std                | 3.78         |\n",
      "|    value_loss         | 0.0246       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 715        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | 2.75       |\n",
      "|    reward             | 0.08117308 |\n",
      "|    std                | 3.79       |\n",
      "|    value_loss         | 1.1        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 716         |\n",
      "|    iterations         | 5800        |\n",
      "|    time_elapsed       | 40          |\n",
      "|    total_timesteps    | 29000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.75       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5799        |\n",
      "|    policy_loss        | -0.896      |\n",
      "|    reward             | -0.25680092 |\n",
      "|    std                | 3.79        |\n",
      "|    value_loss         | 0.452       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 717         |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.75       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | -0.0841     |\n",
      "|    reward             | -0.00643608 |\n",
      "|    std                | 3.79        |\n",
      "|    value_loss         | 0.00179     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 718        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.76      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 0.0696     |\n",
      "|    reward             | 0.12169002 |\n",
      "|    std                | 3.81       |\n",
      "|    value_loss         | 0.00103    |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 719          |\n",
      "|    iterations         | 6100         |\n",
      "|    time_elapsed       | 42           |\n",
      "|    total_timesteps    | 30500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.75        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6099         |\n",
      "|    policy_loss        | -0.03        |\n",
      "|    reward             | 0.0012845227 |\n",
      "|    std                | 3.78         |\n",
      "|    value_loss         | 0.000208     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 718          |\n",
      "|    iterations         | 6200         |\n",
      "|    time_elapsed       | 43           |\n",
      "|    total_timesteps    | 31000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.76        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6199         |\n",
      "|    policy_loss        | -0.0044      |\n",
      "|    reward             | 0.0010878862 |\n",
      "|    std                | 3.83         |\n",
      "|    value_loss         | 3.14e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 719          |\n",
      "|    iterations         | 6300         |\n",
      "|    time_elapsed       | 43           |\n",
      "|    total_timesteps    | 31500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.77        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6299         |\n",
      "|    policy_loss        | 0.00425      |\n",
      "|    reward             | 0.0013106993 |\n",
      "|    std                | 3.88         |\n",
      "|    value_loss         | 2.06e-05     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 720         |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.79       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | -0.193      |\n",
      "|    reward             | -0.13548875 |\n",
      "|    std                | 3.94        |\n",
      "|    value_loss         | 0.00916     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 719       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | -7.38     |\n",
      "|    reward             | 1.4256984 |\n",
      "|    std                | 3.96      |\n",
      "|    value_loss         | 5.27      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 720        |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.8       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -7.94      |\n",
      "|    reward             | -1.2300416 |\n",
      "|    std                | 3.96       |\n",
      "|    value_loss         | 9.35       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 720         |\n",
      "|    iterations         | 6700        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 33500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.79       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6699        |\n",
      "|    policy_loss        | 0.107       |\n",
      "|    reward             | 0.055212434 |\n",
      "|    std                | 3.95        |\n",
      "|    value_loss         | 0.00405     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 721        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.79      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 0.326      |\n",
      "|    reward             | 0.22251058 |\n",
      "|    std                | 3.93       |\n",
      "|    value_loss         | 0.0504     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 722        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.79      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -4.3       |\n",
      "|    reward             | 0.21545167 |\n",
      "|    std                | 3.93       |\n",
      "|    value_loss         | 2.12       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 722          |\n",
      "|    iterations         | 7000         |\n",
      "|    time_elapsed       | 48           |\n",
      "|    total_timesteps    | 35000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.78        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6999         |\n",
      "|    policy_loss        | 2.1          |\n",
      "|    reward             | -0.004600099 |\n",
      "|    std                | 3.91         |\n",
      "|    value_loss         | 0.959        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 723         |\n",
      "|    iterations         | 7100        |\n",
      "|    time_elapsed       | 49          |\n",
      "|    total_timesteps    | 35500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.77       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7099        |\n",
      "|    policy_loss        | -4.98       |\n",
      "|    reward             | -0.99357194 |\n",
      "|    std                | 3.88        |\n",
      "|    value_loss         | 12.5        |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 723            |\n",
      "|    iterations         | 7200           |\n",
      "|    time_elapsed       | 49             |\n",
      "|    total_timesteps    | 36000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.78          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 7199           |\n",
      "|    policy_loss        | 1.26           |\n",
      "|    reward             | -6.3788255e-05 |\n",
      "|    std                | 3.89           |\n",
      "|    value_loss         | 2.7            |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 724       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.78     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | 0.00399   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.91      |\n",
      "|    value_loss         | 3.39e-06  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 725       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.79     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -0.0148   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.94      |\n",
      "|    value_loss         | 2.39e-05  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.8     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | -0.00258 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4        |\n",
      "|    value_loss         | 1.1e-06  |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 725        |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 0.00517    |\n",
      "|    reward             | 0.01252596 |\n",
      "|    std                | 4.09       |\n",
      "|    value_loss         | 5.77e-06   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 726      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | -0.0626  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.11     |\n",
      "|    value_loss         | 0.000495 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 726           |\n",
      "|    iterations         | 7800          |\n",
      "|    time_elapsed       | 53            |\n",
      "|    total_timesteps    | 39000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.85         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7799          |\n",
      "|    policy_loss        | -0.00125      |\n",
      "|    reward             | -9.679957e-05 |\n",
      "|    std                | 4.2           |\n",
      "|    value_loss         | 4.38e-07      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 726          |\n",
      "|    iterations         | 7900         |\n",
      "|    time_elapsed       | 54           |\n",
      "|    total_timesteps    | 39500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.88        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7899         |\n",
      "|    policy_loss        | 0.0717       |\n",
      "|    reward             | 9.902235e-05 |\n",
      "|    std                | 4.32         |\n",
      "|    value_loss         | 0.000459     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 727          |\n",
      "|    iterations         | 8000         |\n",
      "|    time_elapsed       | 54           |\n",
      "|    total_timesteps    | 40000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.91        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7999         |\n",
      "|    policy_loss        | -0.00996     |\n",
      "|    reward             | 0.0059951367 |\n",
      "|    std                | 4.45         |\n",
      "|    value_loss         | 4.09e-05     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 727         |\n",
      "|    iterations         | 8100        |\n",
      "|    time_elapsed       | 55          |\n",
      "|    total_timesteps    | 40500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.94       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8099        |\n",
      "|    policy_loss        | -0.386      |\n",
      "|    reward             | 0.058922768 |\n",
      "|    std                | 4.57        |\n",
      "|    value_loss         | 0.0217      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 728        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.96      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -0.0637    |\n",
      "|    reward             | 0.16007662 |\n",
      "|    std                | 4.64       |\n",
      "|    value_loss         | 0.0585     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 728        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 0.567      |\n",
      "|    reward             | 0.44311035 |\n",
      "|    std                | 4.59       |\n",
      "|    value_loss         | 0.167      |\n",
      "--------------------------------------\n",
      "day: 2768, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1077256.29\n",
      "total_reward: 77256.29\n",
      "total_cost: 7356.97\n",
      "total_trades: 2436\n",
      "Sharpe: 0.480\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 729           |\n",
      "|    iterations         | 8400          |\n",
      "|    time_elapsed       | 57            |\n",
      "|    total_timesteps    | 42000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.95         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8399          |\n",
      "|    policy_loss        | 9.6e-05       |\n",
      "|    reward             | -0.0009775267 |\n",
      "|    std                | 4.62          |\n",
      "|    value_loss         | 3.55e-07      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 729          |\n",
      "|    iterations         | 8500         |\n",
      "|    time_elapsed       | 58           |\n",
      "|    total_timesteps    | 42500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.96        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8499         |\n",
      "|    policy_loss        | 0.0315       |\n",
      "|    reward             | -0.013716743 |\n",
      "|    std                | 4.66         |\n",
      "|    value_loss         | 0.00124      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 729        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 1.08       |\n",
      "|    reward             | 0.98066103 |\n",
      "|    std                | 4.74       |\n",
      "|    value_loss         | 0.181      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 729          |\n",
      "|    iterations         | 8700         |\n",
      "|    time_elapsed       | 59           |\n",
      "|    total_timesteps    | 43500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.99        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8699         |\n",
      "|    policy_loss        | 3.38         |\n",
      "|    reward             | -0.028511757 |\n",
      "|    std                | 4.8          |\n",
      "|    value_loss         | 1.34         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 729       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 11.8      |\n",
      "|    reward             | 0.5477794 |\n",
      "|    std                | 4.84      |\n",
      "|    value_loss         | 19.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 730         |\n",
      "|    iterations         | 8900        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 44500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3          |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8899        |\n",
      "|    policy_loss        | -0.0946     |\n",
      "|    reward             | 0.016297298 |\n",
      "|    std                | 4.85        |\n",
      "|    value_loss         | 0.0032      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 730         |\n",
      "|    iterations         | 9000        |\n",
      "|    time_elapsed       | 61          |\n",
      "|    total_timesteps    | 45000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3          |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8999        |\n",
      "|    policy_loss        | -1.29       |\n",
      "|    reward             | -0.41793528 |\n",
      "|    std                | 4.87        |\n",
      "|    value_loss         | 0.373       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 730         |\n",
      "|    iterations         | 9100        |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 45500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.01       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9099        |\n",
      "|    policy_loss        | 0.167       |\n",
      "|    reward             | -0.13447598 |\n",
      "|    std                | 4.91        |\n",
      "|    value_loss         | 0.609       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 730       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -1.24     |\n",
      "|    reward             | 0.6267871 |\n",
      "|    std                | 4.92      |\n",
      "|    value_loss         | 0.986     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 731       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -11.8     |\n",
      "|    reward             | 2.8740275 |\n",
      "|    std                | 4.88      |\n",
      "|    value_loss         | 26        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 731        |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.01      |\n",
      "|    explained_variance | 0.0261     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | -55.6      |\n",
      "|    reward             | -14.425762 |\n",
      "|    std                | 4.89       |\n",
      "|    value_loss         | 642        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 731        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.01      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | -0.347     |\n",
      "|    reward             | 0.12596205 |\n",
      "|    std                | 4.9        |\n",
      "|    value_loss         | 0.0475     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 731        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.01      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -0.844     |\n",
      "|    reward             | -0.8739717 |\n",
      "|    std                | 4.91       |\n",
      "|    value_loss         | 0.262      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 16.2       |\n",
      "|    reward             | -1.9512757 |\n",
      "|    std                | 4.89       |\n",
      "|    value_loss         | 44.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3        |\n",
      "|    explained_variance | 0.00109   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 1.64      |\n",
      "|    reward             | 1.7979934 |\n",
      "|    std                | 4.88      |\n",
      "|    value_loss         | 6.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.01     |\n",
      "|    explained_variance | -0.00226  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | 24        |\n",
      "|    reward             | 4.9598455 |\n",
      "|    std                | 4.9       |\n",
      "|    value_loss         | 73.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 10000       |\n",
      "|    time_elapsed       | 68          |\n",
      "|    total_timesteps    | 50000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3.01       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9999        |\n",
      "|    policy_loss        | -0.569      |\n",
      "|    reward             | -0.07368668 |\n",
      "|    std                | 4.9         |\n",
      "|    value_loss         | 0.0336      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 10100      |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 50500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -3.01      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10099      |\n",
      "|    policy_loss        | 4.54       |\n",
      "|    reward             | 0.75067997 |\n",
      "|    std                | 4.91       |\n",
      "|    value_loss         | 1.97       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | -6.38     |\n",
      "|    reward             | 1.5410159 |\n",
      "|    std                | 4.85      |\n",
      "|    value_loss         | 8.78      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 10300      |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 51500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.99      |\n",
      "|    explained_variance | -0.000296  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10299      |\n",
      "|    policy_loss        | 21.7       |\n",
      "|    reward             | 0.96815926 |\n",
      "|    std                | 4.82       |\n",
      "|    value_loss         | 65.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 10400     |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10399     |\n",
      "|    policy_loss        | 5.77      |\n",
      "|    reward             | 0.5829379 |\n",
      "|    std                | 4.77      |\n",
      "|    value_loss         | 4.94      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 71        |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.99     |\n",
      "|    explained_variance | 0.00011   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | 30        |\n",
      "|    reward             | 13.845754 |\n",
      "|    std                | 4.8       |\n",
      "|    value_loss         | 129       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 72         |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | 0.527      |\n",
      "|    reward             | 0.62889934 |\n",
      "|    std                | 4.77       |\n",
      "|    value_loss         | 0.049      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 733         |\n",
      "|    iterations         | 10700       |\n",
      "|    time_elapsed       | 72          |\n",
      "|    total_timesteps    | 53500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.98       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10699       |\n",
      "|    policy_loss        | 0.588       |\n",
      "|    reward             | 0.018539844 |\n",
      "|    std                | 4.75        |\n",
      "|    value_loss         | 0.207       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -0.785    |\n",
      "|    reward             | 3.8288217 |\n",
      "|    std                | 4.7       |\n",
      "|    value_loss         | 1.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 10900     |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 54500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.96     |\n",
      "|    explained_variance | 0.00205   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10899     |\n",
      "|    policy_loss        | 0.1       |\n",
      "|    reward             | -2.267885 |\n",
      "|    std                | 4.67      |\n",
      "|    value_loss         | 2.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.96     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -14.9     |\n",
      "|    reward             | 2.0263176 |\n",
      "|    std                | 4.68      |\n",
      "|    value_loss         | 32.2      |\n",
      "-------------------------------------\n",
      "day: 2768, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9261773.71\n",
      "total_reward: 8261773.71\n",
      "total_cost: 6437.57\n",
      "total_trades: 2764\n",
      "Sharpe: 0.964\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 734          |\n",
      "|    iterations         | 11100        |\n",
      "|    time_elapsed       | 75           |\n",
      "|    total_timesteps    | 55500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.96        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 11099        |\n",
      "|    policy_loss        | 0.672        |\n",
      "|    reward             | -0.046056766 |\n",
      "|    std                | 4.66         |\n",
      "|    value_loss         | 0.056        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.96      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | 1.1        |\n",
      "|    reward             | 0.17534418 |\n",
      "|    std                | 4.65       |\n",
      "|    value_loss         | 0.537      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 11300       |\n",
      "|    time_elapsed       | 76          |\n",
      "|    total_timesteps    | 56500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.96       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11299       |\n",
      "|    policy_loss        | 6.96        |\n",
      "|    reward             | -0.16729107 |\n",
      "|    std                | 4.66        |\n",
      "|    value_loss         | 5.8         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.95     |\n",
      "|    explained_variance | -0.000682 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | -7.29     |\n",
      "|    reward             | -1.319969 |\n",
      "|    std                | 4.61      |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.94     |\n",
      "|    explained_variance | 0.00156   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | -4.41     |\n",
      "|    reward             | 1.1205219 |\n",
      "|    std                | 4.56      |\n",
      "|    value_loss         | 2.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 11600     |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.94     |\n",
      "|    explained_variance | -0.000656 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11599     |\n",
      "|    policy_loss        | -0.839    |\n",
      "|    reward             | 2.5595992 |\n",
      "|    std                | 4.56      |\n",
      "|    value_loss         | 14.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -0.214    |\n",
      "|    reward             | 0.3010756 |\n",
      "|    std                | 4.56      |\n",
      "|    value_loss         | 0.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 11800      |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 59000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.93      |\n",
      "|    explained_variance | 0.0954     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11799      |\n",
      "|    policy_loss        | -7.79      |\n",
      "|    reward             | 0.27305502 |\n",
      "|    std                | 4.55       |\n",
      "|    value_loss         | 8.11       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 11900     |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 59500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11899     |\n",
      "|    policy_loss        | -0.606    |\n",
      "|    reward             | 2.3406367 |\n",
      "|    std                | 4.55      |\n",
      "|    value_loss         | 6.07      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 12000     |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 60000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | -0.00213  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | 4.33      |\n",
      "|    reward             | 2.2614896 |\n",
      "|    std                | 4.53      |\n",
      "|    value_loss         | 7.54      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 12100      |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 60500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.93      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12099      |\n",
      "|    policy_loss        | 26.8       |\n",
      "|    reward             | 0.07033467 |\n",
      "|    std                | 4.51       |\n",
      "|    value_loss         | 116        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 12200      |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 61000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.93      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12199      |\n",
      "|    policy_loss        | 0.47       |\n",
      "|    reward             | 0.09589938 |\n",
      "|    std                | 4.52       |\n",
      "|    value_loss         | 0.0663     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -0.881    |\n",
      "|    reward             | 6.3400617 |\n",
      "|    std                | 4.52      |\n",
      "|    value_loss         | 4.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 12400     |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.92     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | -5.42     |\n",
      "|    reward             | -1.691117 |\n",
      "|    std                | 4.51      |\n",
      "|    value_loss         | 4.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 12500      |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 62500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.92      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12499      |\n",
      "|    policy_loss        | -4.73      |\n",
      "|    reward             | -4.1887283 |\n",
      "|    std                | 4.49       |\n",
      "|    value_loss         | 8.55       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 12600    |\n",
      "|    time_elapsed       | 85       |\n",
      "|    total_timesteps    | 63000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.92    |\n",
      "|    explained_variance | 0.0022   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12599    |\n",
      "|    policy_loss        | 23.4     |\n",
      "|    reward             | 3.115197 |\n",
      "|    std                | 4.51     |\n",
      "|    value_loss         | 69.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 12700     |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | 16.9      |\n",
      "|    reward             | -5.730422 |\n",
      "|    std                | 4.51      |\n",
      "|    value_loss         | 125       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 734          |\n",
      "|    iterations         | 12800        |\n",
      "|    time_elapsed       | 87           |\n",
      "|    total_timesteps    | 64000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.93        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12799        |\n",
      "|    policy_loss        | 1.37         |\n",
      "|    reward             | -0.007153054 |\n",
      "|    std                | 4.52         |\n",
      "|    value_loss         | 0.186        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 12900      |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 64500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.93      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12899      |\n",
      "|    policy_loss        | 2.44       |\n",
      "|    reward             | -1.5132549 |\n",
      "|    std                | 4.55       |\n",
      "|    value_loss         | 2.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 88         |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.93      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | -0.8787873 |\n",
      "|    std                | 4.55       |\n",
      "|    value_loss         | 22.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 13100      |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 65500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13099      |\n",
      "|    policy_loss        | -2.87      |\n",
      "|    reward             | -1.2300354 |\n",
      "|    std                | 4.59       |\n",
      "|    value_loss         | 1.59       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.93    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | -14      |\n",
      "|    reward             | 3.654993 |\n",
      "|    std                | 4.55     |\n",
      "|    value_loss         | 24.7     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 733         |\n",
      "|    iterations         | 13300       |\n",
      "|    time_elapsed       | 90          |\n",
      "|    total_timesteps    | 66500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.94       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13299       |\n",
      "|    policy_loss        | 0.105       |\n",
      "|    reward             | 0.038436312 |\n",
      "|    std                | 4.59        |\n",
      "|    value_loss         | 0.0013      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 13400      |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 67000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.93      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13399      |\n",
      "|    policy_loss        | 3.49       |\n",
      "|    reward             | 0.25367546 |\n",
      "|    std                | 4.54       |\n",
      "|    value_loss         | 1.95       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 13500     |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 67500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | -8.22     |\n",
      "|    reward             | 2.5215728 |\n",
      "|    std                | 4.5       |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.92      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | 9.73       |\n",
      "|    reward             | -3.8871813 |\n",
      "|    std                | 4.47       |\n",
      "|    value_loss         | 12.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 13700      |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 68500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13699      |\n",
      "|    policy_loss        | -31.3      |\n",
      "|    reward             | 10.3451185 |\n",
      "|    std                | 4.46       |\n",
      "|    value_loss         | 195        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | 24.8      |\n",
      "|    reward             | 14.362576 |\n",
      "|    std                | 4.44      |\n",
      "|    value_loss         | 226       |\n",
      "-------------------------------------\n",
      "day: 2768, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10109404.25\n",
      "total_reward: 9109404.25\n",
      "total_cost: 3663.42\n",
      "total_trades: 2766\n",
      "Sharpe: 0.977\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 94         |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | -0.414     |\n",
      "|    reward             | 0.21637869 |\n",
      "|    std                | 4.46       |\n",
      "|    value_loss         | 0.22       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 733         |\n",
      "|    iterations         | 14000       |\n",
      "|    time_elapsed       | 95          |\n",
      "|    total_timesteps    | 70000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.91       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13999       |\n",
      "|    policy_loss        | -31.3       |\n",
      "|    reward             | -0.36957794 |\n",
      "|    std                | 4.43        |\n",
      "|    value_loss         | 142         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.89     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | 5.81      |\n",
      "|    reward             | 11.099078 |\n",
      "|    std                | 4.37      |\n",
      "|    value_loss         | 12.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 14200     |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 71000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.9      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14199     |\n",
      "|    policy_loss        | -1.67     |\n",
      "|    reward             | 0.1491067 |\n",
      "|    std                | 4.4       |\n",
      "|    value_loss         | 1.17      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 97        |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.89     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 26.3      |\n",
      "|    reward             | 1.7737767 |\n",
      "|    std                | 4.37      |\n",
      "|    value_loss         | 135       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 733          |\n",
      "|    iterations         | 14400        |\n",
      "|    time_elapsed       | 98           |\n",
      "|    total_timesteps    | 72000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.89        |\n",
      "|    explained_variance | -1.11        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14399        |\n",
      "|    policy_loss        | -108         |\n",
      "|    reward             | -0.004009153 |\n",
      "|    std                | 4.37         |\n",
      "|    value_loss         | 1.87e+03     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 14500      |\n",
      "|    time_elapsed       | 98         |\n",
      "|    total_timesteps    | 72500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.89      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14499      |\n",
      "|    policy_loss        | 0.43       |\n",
      "|    reward             | 0.62492365 |\n",
      "|    std                | 4.34       |\n",
      "|    value_loss         | 0.283      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.89      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | 7.01       |\n",
      "|    reward             | -1.9921138 |\n",
      "|    std                | 4.36       |\n",
      "|    value_loss         | 11.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | -35.1      |\n",
      "|    reward             | -2.2209609 |\n",
      "|    std                | 4.39       |\n",
      "|    value_loss         | 113        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.89      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | 6.31       |\n",
      "|    reward             | -0.3622378 |\n",
      "|    std                | 4.37       |\n",
      "|    value_loss         | 14.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 14900     |\n",
      "|    time_elapsed       | 101       |\n",
      "|    total_timesteps    | 74500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.89     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14899     |\n",
      "|    policy_loss        | 20.1      |\n",
      "|    reward             | 1.0803227 |\n",
      "|    std                | 4.36      |\n",
      "|    value_loss         | 94.4      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 15000       |\n",
      "|    time_elapsed       | 102         |\n",
      "|    total_timesteps    | 75000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.89       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14999       |\n",
      "|    policy_loss        | 0.0833      |\n",
      "|    reward             | 0.050644156 |\n",
      "|    std                | 4.36        |\n",
      "|    value_loss         | 0.0205      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.89     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | 7.38      |\n",
      "|    reward             | 1.6250858 |\n",
      "|    std                | 4.36      |\n",
      "|    value_loss         | 6.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 734        |\n",
      "|    iterations         | 15200      |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 76000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.88      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15199      |\n",
      "|    policy_loss        | 12.4       |\n",
      "|    reward             | -0.7949981 |\n",
      "|    std                | 4.32       |\n",
      "|    value_loss         | 27         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.9      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | 8.38      |\n",
      "|    reward             | 1.0517144 |\n",
      "|    std                | 4.4       |\n",
      "|    value_loss         | 17        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | -59.4     |\n",
      "|    reward             | -17.17951 |\n",
      "|    std                | 4.43      |\n",
      "|    value_loss         | 638       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 15500    |\n",
      "|    time_elapsed       | 105      |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.91    |\n",
      "|    explained_variance | -0.00162 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | 59.9     |\n",
      "|    reward             | 8.247508 |\n",
      "|    std                | 4.46     |\n",
      "|    value_loss         | 1.35e+03 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | -1.36     |\n",
      "|    reward             | -1.148558 |\n",
      "|    std                | 4.46      |\n",
      "|    value_loss         | 0.421     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 734         |\n",
      "|    iterations         | 15700       |\n",
      "|    time_elapsed       | 106         |\n",
      "|    total_timesteps    | 78500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.92       |\n",
      "|    explained_variance | 0.788       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15699       |\n",
      "|    policy_loss        | 2.77        |\n",
      "|    reward             | -0.32212928 |\n",
      "|    std                | 4.47        |\n",
      "|    value_loss         | 1.19        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 0.433     |\n",
      "|    reward             | 2.9284413 |\n",
      "|    std                | 4.5       |\n",
      "|    value_loss         | 15.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 15900    |\n",
      "|    time_elapsed       | 108      |\n",
      "|    total_timesteps    | 79500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15899    |\n",
      "|    policy_loss        | -17.4    |\n",
      "|    reward             | 1.808717 |\n",
      "|    std                | 4.47     |\n",
      "|    value_loss         | 38.2     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 734      |\n",
      "|    iterations         | 16000    |\n",
      "|    time_elapsed       | 108      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.91    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | 4.67     |\n",
      "|    reward             | 6.013596 |\n",
      "|    std                | 4.44     |\n",
      "|    value_loss         | 47.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 109       |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | 1.05      |\n",
      "|    reward             | 0.1680233 |\n",
      "|    std                | 4.43      |\n",
      "|    value_loss         | 0.172     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 734       |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.9      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | 2.34      |\n",
      "|    reward             | 0.9403464 |\n",
      "|    std                | 4.41      |\n",
      "|    value_loss         | 2.57      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 16300     |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 81500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | -6.69     |\n",
      "|    reward             | 3.6623094 |\n",
      "|    std                | 4.44      |\n",
      "|    value_loss         | 12.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 16400      |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 82000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.92      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16399      |\n",
      "|    policy_loss        | -5.64      |\n",
      "|    reward             | -0.9835819 |\n",
      "|    std                | 4.48       |\n",
      "|    value_loss         | 5.91       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 112      |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.91    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | -7.1     |\n",
      "|    reward             | 8.684089 |\n",
      "|    std                | 4.45     |\n",
      "|    value_loss         | 56.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | -58.6     |\n",
      "|    reward             | -36.88668 |\n",
      "|    std                | 4.42      |\n",
      "|    value_loss         | 650       |\n",
      "-------------------------------------\n",
      "day: 2768, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10450313.04\n",
      "total_reward: 9450313.04\n",
      "total_cost: 1955.72\n",
      "total_trades: 2765\n",
      "Sharpe: 0.975\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 16700     |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 83500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | -1.07     |\n",
      "|    reward             | 0.8268567 |\n",
      "|    std                | 4.45      |\n",
      "|    value_loss         | 0.291     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 114        |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | 1.95       |\n",
      "|    reward             | -1.8037344 |\n",
      "|    std                | 4.46       |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 16900      |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 84500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16899      |\n",
      "|    policy_loss        | -16        |\n",
      "|    reward             | -3.9047098 |\n",
      "|    std                | 4.45       |\n",
      "|    value_loss         | 71.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.92      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | 3.3        |\n",
      "|    reward             | 0.09394798 |\n",
      "|    std                | 4.49       |\n",
      "|    value_loss         | 5.18       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 116      |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.91    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | 22.7     |\n",
      "|    reward             | 6.740653 |\n",
      "|    std                | 4.45     |\n",
      "|    value_loss         | 99.8     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 733         |\n",
      "|    iterations         | 17200       |\n",
      "|    time_elapsed       | 117         |\n",
      "|    total_timesteps    | 86000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.92       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17199       |\n",
      "|    policy_loss        | -0.00474    |\n",
      "|    reward             | -0.25225934 |\n",
      "|    std                | 4.47        |\n",
      "|    value_loss         | 0.0311      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | 2.6        |\n",
      "|    reward             | 0.87727964 |\n",
      "|    std                | 4.46       |\n",
      "|    value_loss         | 1.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.92      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -5.31      |\n",
      "|    reward             | 0.45838678 |\n",
      "|    std                | 4.48       |\n",
      "|    value_loss         | 4.31       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.92     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | 5.09      |\n",
      "|    reward             | 0.8055884 |\n",
      "|    std                | 4.46      |\n",
      "|    value_loss         | 35.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 17600     |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | -11.5     |\n",
      "|    reward             | 1.1306663 |\n",
      "|    std                | 4.42      |\n",
      "|    value_loss         | 26.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 733      |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.9     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -85.3    |\n",
      "|    reward             | 8.897514 |\n",
      "|    std                | 4.4      |\n",
      "|    value_loss         | 1.13e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 17800      |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 89000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17799      |\n",
      "|    policy_loss        | 2.67       |\n",
      "|    reward             | 0.36524418 |\n",
      "|    std                | 4.42       |\n",
      "|    value_loss         | 1.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 733        |\n",
      "|    iterations         | 17900      |\n",
      "|    time_elapsed       | 122        |\n",
      "|    total_timesteps    | 89500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17899      |\n",
      "|    policy_loss        | 6.16       |\n",
      "|    reward             | 0.23858555 |\n",
      "|    std                | 4.39       |\n",
      "|    value_loss         | 5.27       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.9      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -27.2     |\n",
      "|    reward             | 2.7836955 |\n",
      "|    std                | 4.39      |\n",
      "|    value_loss         | 100       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 123        |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | -6.8       |\n",
      "|    reward             | 0.80516267 |\n",
      "|    std                | 4.42       |\n",
      "|    value_loss         | 11.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 18200     |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.91     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | -4.51     |\n",
      "|    reward             | -7.433865 |\n",
      "|    std                | 4.44      |\n",
      "|    value_loss         | 10.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 18300       |\n",
      "|    time_elapsed       | 124         |\n",
      "|    total_timesteps    | 91500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.91       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18299       |\n",
      "|    policy_loss        | -0.105      |\n",
      "|    reward             | -0.12162452 |\n",
      "|    std                | 4.46        |\n",
      "|    value_loss         | 0.00467     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | -0.181    |\n",
      "|    reward             | 0.4915368 |\n",
      "|    std                | 4.5       |\n",
      "|    value_loss         | 0.168     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 18500     |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 92500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | -0.852    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | -5.48     |\n",
      "|    reward             | -1.284944 |\n",
      "|    std                | 4.52      |\n",
      "|    value_loss         | 5.25      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 18600       |\n",
      "|    time_elapsed       | 126         |\n",
      "|    total_timesteps    | 93000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.93       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18599       |\n",
      "|    policy_loss        | 0.0638      |\n",
      "|    reward             | -0.51358235 |\n",
      "|    std                | 4.53        |\n",
      "|    value_loss         | 0.376       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | 15.4      |\n",
      "|    reward             | -0.775447 |\n",
      "|    std                | 4.54      |\n",
      "|    value_loss         | 28.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 18800     |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | 0.000824  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | -1.73     |\n",
      "|    reward             | 15.638766 |\n",
      "|    std                | 4.54      |\n",
      "|    value_loss         | 2.2       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 733       |\n",
      "|    iterations         | 18900     |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 94500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.94     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | -0.53     |\n",
      "|    reward             | 0.6663671 |\n",
      "|    std                | 4.57      |\n",
      "|    value_loss         | 0.0682    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 19000      |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 95000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18999      |\n",
      "|    policy_loss        | 0.959      |\n",
      "|    reward             | -0.6063261 |\n",
      "|    std                | 4.59       |\n",
      "|    value_loss         | 2.67       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 130       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.96     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | 10.1      |\n",
      "|    reward             | 2.2111301 |\n",
      "|    std                | 4.66      |\n",
      "|    value_loss         | 14.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 19200       |\n",
      "|    time_elapsed       | 131         |\n",
      "|    total_timesteps    | 96000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.95       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19199       |\n",
      "|    policy_loss        | 25.4        |\n",
      "|    reward             | -0.39757097 |\n",
      "|    std                | 4.64        |\n",
      "|    value_loss         | 80.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 19300     |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 96500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | 2.28      |\n",
      "|    reward             | -20.42869 |\n",
      "|    std                | 4.59      |\n",
      "|    value_loss         | 50.4      |\n",
      "-------------------------------------\n",
      "day: 2768, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9597524.18\n",
      "total_reward: 8597524.18\n",
      "total_cost: 3907.20\n",
      "total_trades: 2766\n",
      "Sharpe: 0.975\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 19400       |\n",
      "|    time_elapsed       | 132         |\n",
      "|    total_timesteps    | 97000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.94       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19399       |\n",
      "|    policy_loss        | -0.0472     |\n",
      "|    reward             | -0.17521307 |\n",
      "|    std                | 4.58        |\n",
      "|    value_loss         | 0.00708     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.94      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | -6.53      |\n",
      "|    reward             | -2.1857052 |\n",
      "|    std                | 4.59       |\n",
      "|    value_loss         | 8.48       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 732      |\n",
      "|    iterations         | 19600    |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | 4.6      |\n",
      "|    reward             | 5.023946 |\n",
      "|    std                | 4.58     |\n",
      "|    value_loss         | 2.29     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 732        |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | 12.5       |\n",
      "|    reward             | -1.9984707 |\n",
      "|    std                | 4.61       |\n",
      "|    value_loss         | 12.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.94     |\n",
      "|    explained_variance | 0.000723  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 11.6      |\n",
      "|    reward             | 4.3199863 |\n",
      "|    std                | 4.58      |\n",
      "|    value_loss         | 28.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 732       |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.93     |\n",
      "|    explained_variance | -0.000422 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | -42       |\n",
      "|    reward             | 9.633098  |\n",
      "|    std                | 4.52      |\n",
      "|    value_loss         | 291       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 732         |\n",
      "|    iterations         | 20000       |\n",
      "|    time_elapsed       | 136         |\n",
      "|    total_timesteps    | 100000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.93       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19999       |\n",
      "|    policy_loss        | 0.833       |\n",
      "|    reward             | -0.14146994 |\n",
      "|    std                | 4.55        |\n",
      "|    value_loss         | 0.149       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  -0.06615529784204349\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_126_4\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1240        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.118551135 |\n",
      "------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 1129           |\n",
      "|    iterations           | 2              |\n",
      "|    time_elapsed         | 3              |\n",
      "|    total_timesteps      | 4096           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0014483993   |\n",
      "|    clip_fraction        | 0.000732       |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.41          |\n",
      "|    explained_variance   | 0.377          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 0.0171         |\n",
      "|    n_updates            | 10             |\n",
      "|    policy_gradient_loss | -0.000184      |\n",
      "|    reward               | -0.00050298445 |\n",
      "|    std                  | 0.989          |\n",
      "|    value_loss           | 0.0638         |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1094         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.002603748  |\n",
      "|    clip_fraction        | 0.00742      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0384       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000665    |\n",
      "|    reward               | 0.0017984525 |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 0.129        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1073         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00421362   |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0163      |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    reward               | -0.034450866 |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 0.0111       |\n",
      "------------------------------------------\n",
      "day: 2768, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1048429.25\n",
      "total_reward: 48429.25\n",
      "total_cost: 5013.69\n",
      "total_trades: 2607\n",
      "Sharpe: 0.867\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1055         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021843698 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0169      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000348    |\n",
      "|    reward               | -0.002373284 |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 0.0108       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1055          |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0021560846  |\n",
      "|    clip_fraction        | 0.00552       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00723      |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.000348     |\n",
      "|    reward               | -0.0013037967 |\n",
      "|    std                  | 0.965         |\n",
      "|    value_loss           | 0.00222       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1051         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028497325 |\n",
      "|    clip_fraction        | 0.00498      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0119      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000445    |\n",
      "|    reward               | -0.014648338 |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 0.00413      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1046        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00433391  |\n",
      "|    clip_fraction        | 0.0187      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.251       |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    reward               | 0.040139724 |\n",
      "|    std                  | 0.951       |\n",
      "|    value_loss           | 0.664       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032589934 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0095      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    reward               | -0.045895576 |\n",
      "|    std                  | 0.95         |\n",
      "|    value_loss           | 0.00472      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1045          |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 19            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0030737033  |\n",
      "|    clip_fraction        | 0.00815       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.0707        |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000582     |\n",
      "|    reward               | 0.00030626563 |\n",
      "|    std                  | 0.947         |\n",
      "|    value_loss           | 0.182         |\n",
      "-------------------------------------------\n",
      "day: 2768, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1886607.52\n",
      "total_reward: 886607.52\n",
      "total_cost: 5289.90\n",
      "total_trades: 2705\n",
      "Sharpe: 0.714\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1043         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042691347 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.29         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000983    |\n",
      "|    reward               | 0.0028705946 |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 2.15         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1042          |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 23            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059395033 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | -0.000146     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.15          |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -1.45e-06     |\n",
      "|    reward               | 0.74231994    |\n",
      "|    std                  | 0.953         |\n",
      "|    value_loss           | 6.51          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1039         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021977718 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.000177     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.08         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000176    |\n",
      "|    reward               | 0.009867442  |\n",
      "|    std                  | 0.955        |\n",
      "|    value_loss           | 2.02         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1040        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002214319 |\n",
      "|    clip_fraction        | 0.00625     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.25        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00137    |\n",
      "|    reward               | 0.01799474  |\n",
      "|    std                  | 0.958       |\n",
      "|    value_loss           | 10.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1040         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051933867 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.011        |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    reward               | 0.006414342  |\n",
      "|    std                  | 0.954        |\n",
      "|    value_loss           | 0.0753       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1041          |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.0077076e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.284         |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | 6.71e-05      |\n",
      "|    reward               | 0.07401931    |\n",
      "|    std                  | 0.953         |\n",
      "|    value_loss           | 0.805         |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1040        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00440756  |\n",
      "|    clip_fraction        | 0.0138      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.394       |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.000714   |\n",
      "|    reward               | -0.14578603 |\n",
      "|    std                  | 0.959       |\n",
      "|    value_loss           | 0.457       |\n",
      "-----------------------------------------\n",
      "day: 2768, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1869833.49\n",
      "total_reward: 869833.49\n",
      "total_cost: 5336.06\n",
      "total_trades: 2749\n",
      "Sharpe: 0.853\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1039        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003819838 |\n",
      "|    clip_fraction        | 0.00713     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.54        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    reward               | -0.03357369 |\n",
      "|    std                  | 0.965       |\n",
      "|    value_loss           | 6.96        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1038         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049037347 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.98         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00257     |\n",
      "|    reward               | 6.834791e-05 |\n",
      "|    std                  | 0.96         |\n",
      "|    value_loss           | 4.12         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1038         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050682584 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0129      |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    reward               | 0.4297593    |\n",
      "|    std                  | 0.96         |\n",
      "|    value_loss           | 0.0292       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1037          |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 41            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042461336 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.271         |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | -1.32e-06     |\n",
      "|    reward               | -0.01217093   |\n",
      "|    std                  | 0.967         |\n",
      "|    value_loss           | 0.685         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1037         |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029710764 |\n",
      "|    clip_fraction        | 0.00552      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.56         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000897    |\n",
      "|    reward               | -0.066775106 |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 8.81         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1037         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.005548016  |\n",
      "|    clip_fraction        | 0.0199       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.372        |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    reward               | 3.058136e-05 |\n",
      "|    std                  | 0.967        |\n",
      "|    value_loss           | 0.855        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1037        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002842295 |\n",
      "|    clip_fraction        | 0.0063      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0429      |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00054    |\n",
      "|    reward               | 0.00563743  |\n",
      "|    std                  | 0.967       |\n",
      "|    value_loss           | 0.0667      |\n",
      "-----------------------------------------\n",
      "day: 2768, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1044649.40\n",
      "total_reward: 44649.40\n",
      "total_cost: 4884.75\n",
      "total_trades: 2536\n",
      "Sharpe: 0.362\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1036         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012794903 |\n",
      "|    clip_fraction        | 0.00596      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0261      |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    reward               | -0.007546566 |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 0.00628      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1036         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044268216 |\n",
      "|    clip_fraction        | 0.00903      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.000503    |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    reward               | 0.059356943  |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 0.0671       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1037          |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 53            |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00036260154 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.46          |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -8.79e-05     |\n",
      "|    reward               | -1.2416222    |\n",
      "|    std                  | 0.977         |\n",
      "|    value_loss           | 4.91          |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 1036           |\n",
      "|    iterations           | 28             |\n",
      "|    time_elapsed         | 55             |\n",
      "|    total_timesteps      | 57344          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.005175217    |\n",
      "|    clip_fraction        | 0.025          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.39          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 0.744          |\n",
      "|    n_updates            | 270            |\n",
      "|    policy_gradient_loss | -0.00256       |\n",
      "|    reward               | -1.1008595e-05 |\n",
      "|    std                  | 0.97           |\n",
      "|    value_loss           | 2.23           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1037         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050729336 |\n",
      "|    clip_fraction        | 0.0231       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00295      |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    reward               | -0.013148423 |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 0.214        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1037         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003752827  |\n",
      "|    clip_fraction        | 0.00684      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00743     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.000383    |\n",
      "|    reward               | 0.0039881165 |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 0.0135       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1037        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005674545 |\n",
      "|    clip_fraction        | 0.0259      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0181     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 0.0107      |\n",
      "-----------------------------------------\n",
      "day: 2768, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 999172.41\n",
      "total_reward: -827.59\n",
      "total_cost: 3253.94\n",
      "total_trades: 1897\n",
      "Sharpe: -0.066\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1035         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028632116 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0214      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    reward               | 0.0066530895 |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 0.00189      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1035         |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00348768   |\n",
      "|    clip_fraction        | 0.0119       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0317      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00078     |\n",
      "|    reward               | 0.0007792202 |\n",
      "|    std                  | 0.964        |\n",
      "|    value_loss           | 0.00407      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1035          |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0006031269  |\n",
      "|    clip_fraction        | 0.00161       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0136       |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.000275     |\n",
      "|    reward               | -9.802757e-05 |\n",
      "|    std                  | 0.978         |\n",
      "|    value_loss           | 0.00264       |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1036        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004141149 |\n",
      "|    clip_fraction        | 0.0217      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0199     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.000972   |\n",
      "|    reward               | 0.023783749 |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 0.00185     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1036         |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043574944 |\n",
      "|    clip_fraction        | 0.0363       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0147      |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 0.000755     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1036         |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035551307 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0178      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00111     |\n",
      "|    reward               | 0.0015352068 |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 0.00207      |\n",
      "------------------------------------------\n",
      "day: 2768, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1388395.91\n",
      "total_reward: 388395.91\n",
      "total_cost: 5196.13\n",
      "total_trades: 2624\n",
      "Sharpe: 0.723\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1035         |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073880167 |\n",
      "|    clip_fraction        | 0.0718       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00105     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00645     |\n",
      "|    reward               | 0.0017651888 |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 0.00681      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1034        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003224416 |\n",
      "|    clip_fraction        | 0.0113      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.399       |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00108    |\n",
      "|    reward               | 0.02623253  |\n",
      "|    std                  | 0.97        |\n",
      "|    value_loss           | 1.08        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1034         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035490128 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0111      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    reward               | 0.07070544   |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 0.00465      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1034         |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036956489 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00894      |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00113     |\n",
      "|    reward               | 0.122320026  |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 0.068        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1035         |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025151619 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000229    |\n",
      "|    reward               | 0.012794298  |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 6.3          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1035          |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 85            |\n",
      "|    total_timesteps      | 88064         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038528323 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.28          |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | 5.9e-05       |\n",
      "|    reward               | -6.52705      |\n",
      "|    std                  | 0.975         |\n",
      "|    value_loss           | 8.63          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1034         |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032317305 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.00733      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.09         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    reward               | 0.49870276   |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 1.94         |\n",
      "------------------------------------------\n",
      "day: 2768, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4556671.43\n",
      "total_reward: 3556671.43\n",
      "total_cost: 5354.02\n",
      "total_trades: 2741\n",
      "Sharpe: 0.858\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1034         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064227385 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0345       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.7          |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00284     |\n",
      "|    reward               | -0.0360377   |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 41.3         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1034          |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 91            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00059912703 |\n",
      "|    clip_fraction        | 0.00225       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.0839        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 40.6          |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.00156      |\n",
      "|    reward               | 0.009465786   |\n",
      "|    std                  | 0.977         |\n",
      "|    value_loss           | 84.8          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1034         |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050527467 |\n",
      "|    clip_fraction        | 0.0213       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.181        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 51.9         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    reward               | -0.52535164  |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 108          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1034         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010279309 |\n",
      "|    clip_fraction        | 0.00493      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.527        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.9          |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | 0.000141     |\n",
      "|    reward               | 1.5907651    |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 11.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1033         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018703277 |\n",
      "|    clip_fraction        | 0.00737      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.291        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 39.6         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0031      |\n",
      "|    reward               | 0.5314523    |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 161          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
      "PPO Sharpe Ratio:  -0.06483409954288334\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_4\n",
      "day: 2768, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1186178.26\n",
      "total_reward: 186178.26\n",
      "total_cost: 795.75\n",
      "total_trades: 700\n",
      "Sharpe: 0.631\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 158       |\n",
      "|    time_elapsed    | 70        |\n",
      "|    total_timesteps | 11076     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+04 |\n",
      "|    critic_loss     | 10.4      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 10975     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 2768, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 155       |\n",
      "|    time_elapsed    | 142       |\n",
      "|    total_timesteps | 22152     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.42e+03 |\n",
      "|    critic_loss     | 3.73e+03  |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 22051     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 2768, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 214       |\n",
      "|    total_timesteps | 33228     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.64e+03 |\n",
      "|    critic_loss     | 23.6      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 33127     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 2768, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 287       |\n",
      "|    total_timesteps | 44304     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.59e+03 |\n",
      "|    critic_loss     | 164       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 44203     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 360       |\n",
      "|    total_timesteps | 55380     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.44e+03 |\n",
      "|    critic_loss     | 139       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 55279     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 2768, episode: 95\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 434      |\n",
      "|    total_timesteps | 66456    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -733     |\n",
      "|    critic_loss     | 0.779    |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 66355    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 2768, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 509      |\n",
      "|    total_timesteps | 77532    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 77431    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 2768, episode: 105\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 585      |\n",
      "|    total_timesteps | 88608    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 0.649    |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 88507    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 2768, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 660      |\n",
      "|    total_timesteps | 99684    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.41    |\n",
      "|    critic_loss     | 0.000121 |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 99583    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2021-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_189_4\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 662       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.52     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -0.00181  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.11      |\n",
      "|    value_loss         | 8.39e-06  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 663        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.0109    |\n",
      "|    reward             | 0.01326932 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 8.33e-05   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 683         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.62       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -0.0201     |\n",
      "|    reward             | -0.02014812 |\n",
      "|    std                | 1.23        |\n",
      "|    value_loss         | 0.000755    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 699           |\n",
      "|    iterations         | 400           |\n",
      "|    time_elapsed       | 2             |\n",
      "|    total_timesteps    | 2000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.64         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 399           |\n",
      "|    policy_loss        | -0.029        |\n",
      "|    reward             | -0.0031787634 |\n",
      "|    std                | 1.25          |\n",
      "|    value_loss         | 0.000228      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 697         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -0.0837     |\n",
      "|    reward             | -0.08280028 |\n",
      "|    std                | 1.28        |\n",
      "|    value_loss         | 0.00239     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.68       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.0972     |\n",
      "|    reward             | 0.017560296 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 0.00377     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 701       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 2.41      |\n",
      "|    reward             | 0.5458784 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 1.56      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 701        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 1.7        |\n",
      "|    reward             | 0.14870694 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 2.18       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.68       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 0.229       |\n",
      "|    reward             | -0.15846561 |\n",
      "|    std                | 1.3         |\n",
      "|    value_loss         | 1.28        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 691       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | 0.000994  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 6.41      |\n",
      "|    reward             | 1.1099001 |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 98.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 692      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0.00174  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 86       |\n",
      "|    reward             | 8.311412 |\n",
      "|    std                | 1.36     |\n",
      "|    value_loss         | 4.08e+03 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 692         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.72       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 0.0726      |\n",
      "|    reward             | 0.056123435 |\n",
      "|    std                | 1.35        |\n",
      "|    value_loss         | 0.0513      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 690         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.72       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 1.43        |\n",
      "|    reward             | -0.36596614 |\n",
      "|    std                | 1.35        |\n",
      "|    value_loss         | 0.908       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 693       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0.0774    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 3.31      |\n",
      "|    reward             | 3.2125041 |\n",
      "|    std                | 1.35      |\n",
      "|    value_loss         | 4.28      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 694      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | -0.00386 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -4.63    |\n",
      "|    reward             | 2.558406 |\n",
      "|    std                | 1.36     |\n",
      "|    value_loss         | 6.21     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 695         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.72       |\n",
      "|    explained_variance | -0.00965    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 6.37        |\n",
      "|    reward             | 0.029861206 |\n",
      "|    std                | 1.35        |\n",
      "|    value_loss         | 27.5        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 696          |\n",
      "|    iterations         | 1700         |\n",
      "|    time_elapsed       | 12           |\n",
      "|    total_timesteps    | 8500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.72        |\n",
      "|    explained_variance | 0.00331      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1699         |\n",
      "|    policy_loss        | 59.7         |\n",
      "|    reward             | 0.0012721462 |\n",
      "|    std                | 1.35         |\n",
      "|    value_loss         | 715          |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 698       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 3.02      |\n",
      "|    reward             | 0.6632761 |\n",
      "|    std                | 1.36      |\n",
      "|    value_loss         | 2.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 699       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.73     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -1.56     |\n",
      "|    reward             | -1.272417 |\n",
      "|    std                | 1.37      |\n",
      "|    value_loss         | 2.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 700       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.73     |\n",
      "|    explained_variance | 0.0213    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -5.66     |\n",
      "|    reward             | 2.3477838 |\n",
      "|    std                | 1.36      |\n",
      "|    value_loss         | 29        |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 698      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | -0.0585  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 7.11     |\n",
      "|    reward             | 4.337762 |\n",
      "|    std                | 1.36     |\n",
      "|    value_loss         | 29.1     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 699      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | -0.00189 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 23       |\n",
      "|    reward             | 6.675755 |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 176      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 694         |\n",
      "|    iterations         | 2300        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 11500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.71       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2299        |\n",
      "|    policy_loss        | 0.53        |\n",
      "|    reward             | 0.006502739 |\n",
      "|    std                | 1.33        |\n",
      "|    value_loss         | 0.0851      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 694        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 0.487      |\n",
      "|    reward             | 0.23753451 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 1.54       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 694       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | 0.00526   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 4.48      |\n",
      "|    reward             | 0.4308933 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 7.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 694        |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.71      |\n",
      "|    explained_variance | -0.000375  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 1.31       |\n",
      "|    reward             | -1.4707693 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 1.83       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 695       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | 0.00258   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 3.38      |\n",
      "|    reward             | -3.919562 |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 7.73      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 694         |\n",
      "|    iterations         | 2800        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 14000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.68       |\n",
      "|    explained_variance | -0.000572   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2799        |\n",
      "|    policy_loss        | 18.3        |\n",
      "|    reward             | -0.77891564 |\n",
      "|    std                | 1.3         |\n",
      "|    value_loss         | 467         |\n",
      "---------------------------------------\n",
      "day: 2831, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9597743.19\n",
      "total_reward: 8597743.19\n",
      "total_cost: 7477.06\n",
      "total_trades: 2822\n",
      "Sharpe: 0.930\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 692        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.69      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | -0.264     |\n",
      "|    reward             | 0.17660962 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 0.0536     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 693        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | -0.4928728 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 46.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 693       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | -0.000874 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 6.52      |\n",
      "|    reward             | 6.6619654 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 26.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 694        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.69      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -3.04      |\n",
      "|    reward             | -1.5493737 |\n",
      "|    std                | 1.31       |\n",
      "|    value_loss         | 3.98       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 695       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | 0.00067   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | -3.02     |\n",
      "|    reward             | 5.5852585 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 3.26      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 694          |\n",
      "|    iterations         | 3400         |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 17000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.7         |\n",
      "|    explained_variance | 0.511        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3399         |\n",
      "|    policy_loss        | 9.44         |\n",
      "|    reward             | -0.003014703 |\n",
      "|    std                | 1.32         |\n",
      "|    value_loss         | 177          |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 695         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.7        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 0.324       |\n",
      "|    reward             | -0.09823178 |\n",
      "|    std                | 1.32        |\n",
      "|    value_loss         | 0.16        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 695        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | -0.00046   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | 0.97       |\n",
      "|    reward             | 0.73555934 |\n",
      "|    std                | 1.33       |\n",
      "|    value_loss         | 5.9        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 695        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.71      |\n",
      "|    explained_variance | -4.48e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -4.57      |\n",
      "|    reward             | -2.5591464 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 24.8       |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 695            |\n",
      "|    iterations         | 3800           |\n",
      "|    time_elapsed       | 27             |\n",
      "|    total_timesteps    | 19000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.71          |\n",
      "|    explained_variance | -0.00125       |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 3799           |\n",
      "|    policy_loss        | 7.81           |\n",
      "|    reward             | -0.00041320262 |\n",
      "|    std                | 1.33           |\n",
      "|    value_loss         | 18.8           |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 694        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.71      |\n",
      "|    explained_variance | -8e-05     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 7.73       |\n",
      "|    reward             | 0.54224515 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 41.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 695        |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.69      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | 0.648      |\n",
      "|    reward             | 0.10891116 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 0.297      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 694       |\n",
      "|    iterations         | 4100      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 20500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4099      |\n",
      "|    policy_loss        | -2.64     |\n",
      "|    reward             | 0.7403534 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 2.57      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 694       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | 2.56      |\n",
      "|    reward             | 1.5092999 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 2         |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 694       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -1.37     |\n",
      "|    reward             | -1.509464 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 695       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 18.9      |\n",
      "|    reward             | 1.2822771 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 204       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 695       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 2.98e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 27.8      |\n",
      "|    reward             | 20.174038 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 671       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 696        |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | -0.0378    |\n",
      "|    reward             | -0.5262356 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 0.00851    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 696       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | 6.12      |\n",
      "|    reward             | 0.3648156 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 695       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -14.7     |\n",
      "|    reward             | 1.3517389 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 95.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 695       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 7.61      |\n",
      "|    reward             | 1.6910043 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 17.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 695       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 8.03      |\n",
      "|    reward             | 0.5810595 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 52.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 696         |\n",
      "|    iterations         | 5100        |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 25500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.69       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5099        |\n",
      "|    policy_loss        | -0.298      |\n",
      "|    reward             | -0.01259117 |\n",
      "|    std                | 1.31        |\n",
      "|    value_loss         | 0.0535      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 694         |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 37          |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.68       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | 1.54        |\n",
      "|    reward             | -0.16780871 |\n",
      "|    std                | 1.3         |\n",
      "|    value_loss         | 1.5         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 694         |\n",
      "|    iterations         | 5300        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 26500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.7        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5299        |\n",
      "|    policy_loss        | -8.13       |\n",
      "|    reward             | -0.79762304 |\n",
      "|    std                | 1.32        |\n",
      "|    value_loss         | 19.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 695        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | 1.99       |\n",
      "|    reward             | -3.5195057 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 3.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 694        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | 0.0012     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | 1.16       |\n",
      "|    reward             | -3.3609824 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 5.81       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 694       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 16.4      |\n",
      "|    reward             | 10.566138 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 105       |\n",
      "-------------------------------------\n",
      "day: 2831, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9529121.93\n",
      "total_reward: 8529121.93\n",
      "total_cost: 2962.03\n",
      "total_trades: 2822\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 694         |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.72       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | 0.405       |\n",
      "|    reward             | 0.024420017 |\n",
      "|    std                | 1.35        |\n",
      "|    value_loss         | 0.0502      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 694       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0.00623   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 1.23      |\n",
      "|    reward             | 2.4192028 |\n",
      "|    std                | 1.35      |\n",
      "|    value_loss         | 1.31      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 693       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -12.4     |\n",
      "|    reward             | 0.7699567 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 59        |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 693          |\n",
      "|    iterations         | 6000         |\n",
      "|    time_elapsed       | 43           |\n",
      "|    total_timesteps    | 30000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.71        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5999         |\n",
      "|    policy_loss        | -4.88        |\n",
      "|    reward             | -0.055827804 |\n",
      "|    std                | 1.33         |\n",
      "|    value_loss         | 9.84         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 691       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -0.363    |\n",
      "|    reward             | 3.9388587 |\n",
      "|    std                | 1.35      |\n",
      "|    value_loss         | 6.92      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 691       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | 0.000123  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 10.4      |\n",
      "|    reward             | 13.375607 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 103       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 691         |\n",
      "|    iterations         | 6300        |\n",
      "|    time_elapsed       | 45          |\n",
      "|    total_timesteps    | 31500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.71       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6299        |\n",
      "|    policy_loss        | -0.886      |\n",
      "|    reward             | -0.43623996 |\n",
      "|    std                | 1.33        |\n",
      "|    value_loss         | 0.383       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 692         |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.71       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | -1.46       |\n",
      "|    reward             | -0.26469812 |\n",
      "|    std                | 1.33        |\n",
      "|    value_loss         | 1.37        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 692        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | -0.000356  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | -8.06      |\n",
      "|    reward             | 0.24421401 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 31.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 692       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | -0.000437 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 7.04      |\n",
      "|    reward             | 7.3292565 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 18        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 692       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | 0.00054   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -4.1      |\n",
      "|    reward             | 4.8236427 |\n",
      "|    std                | 1.34      |\n",
      "|    value_loss         | 16.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 692         |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 49          |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.7        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | -0.144      |\n",
      "|    reward             | 0.006665874 |\n",
      "|    std                | 1.32        |\n",
      "|    value_loss         | 0.00438     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 692        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -0.21      |\n",
      "|    reward             | -0.8308459 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 0.0409     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 693       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | 0.00874   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -3.47     |\n",
      "|    reward             | -0.727162 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 7.91      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 693       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | -5.81e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -13.3     |\n",
      "|    reward             | 2.475022  |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 61.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 694       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -6.91     |\n",
      "|    reward             | 3.5466514 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 34.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 695       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.68     |\n",
      "|    explained_variance | 0.00193   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | 8.94      |\n",
      "|    reward             | 12.391078 |\n",
      "|    std                | 1.3       |\n",
      "|    value_loss         | 41        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 696        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.67      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 1.01       |\n",
      "|    reward             | -0.0591987 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 0.344      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 697         |\n",
      "|    iterations         | 7500        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 37500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7499        |\n",
      "|    policy_loss        | -0.321      |\n",
      "|    reward             | 0.035736542 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 2.82        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 698       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 5.56      |\n",
      "|    reward             | 1.2704408 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 14.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 698      |\n",
      "|    iterations         | 7700     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 38500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0.002    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7699     |\n",
      "|    policy_loss        | 2.25     |\n",
      "|    reward             | 4.261061 |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 3.08     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 699       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 14.9      |\n",
      "|    reward             | -7.216232 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 125       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 700       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 43.8      |\n",
      "|    reward             | 40.344063 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 2.5e+03   |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 700         |\n",
      "|    iterations         | 8000        |\n",
      "|    time_elapsed       | 57          |\n",
      "|    total_timesteps    | 40000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.68       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7999        |\n",
      "|    policy_loss        | 0.103       |\n",
      "|    reward             | -0.11209374 |\n",
      "|    std                | 1.3         |\n",
      "|    value_loss         | 0.0878      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 701       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -2.2      |\n",
      "|    reward             | 0.1060641 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 5.29      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 702         |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 58          |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.69       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -0.503      |\n",
      "|    reward             | -0.18376198 |\n",
      "|    std                | 1.31        |\n",
      "|    value_loss         | 2.49        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 702       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 5.58      |\n",
      "|    reward             | 3.4343042 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 15.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 702        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.69      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | 7.95       |\n",
      "|    reward             | -3.5773816 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 59.9       |\n",
      "--------------------------------------\n",
      "day: 2831, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8421732.75\n",
      "total_reward: 7421732.75\n",
      "total_cost: 5911.19\n",
      "total_trades: 2811\n",
      "Sharpe: 0.919\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 703         |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.69       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -0.0733     |\n",
      "|    reward             | 0.005107072 |\n",
      "|    std                | 1.31        |\n",
      "|    value_loss         | 0.00244     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 703         |\n",
      "|    iterations         | 8600        |\n",
      "|    time_elapsed       | 61          |\n",
      "|    total_timesteps    | 43000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.68       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8599        |\n",
      "|    policy_loss        | 0.968       |\n",
      "|    reward             | -0.24324293 |\n",
      "|    std                | 1.3         |\n",
      "|    value_loss         | 0.308       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 704       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 5.51      |\n",
      "|    reward             | 1.1204883 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 14.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 705       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0.000246  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -7.28     |\n",
      "|    reward             | -0.820932 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 56        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 705       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | -0.000128 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 4.14      |\n",
      "|    reward             | 1.7952572 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 10.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 706      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 13.4     |\n",
      "|    reward             | 9.000093 |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 127      |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 707          |\n",
      "|    iterations         | 9100         |\n",
      "|    time_elapsed       | 64           |\n",
      "|    total_timesteps    | 45500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.67        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9099         |\n",
      "|    policy_loss        | 0.727        |\n",
      "|    reward             | -0.059340317 |\n",
      "|    std                | 1.28         |\n",
      "|    value_loss         | 0.187        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 707       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | 5.36      |\n",
      "|    reward             | -3.306761 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 18.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 707       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.69     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -1.5      |\n",
      "|    reward             | 2.8246694 |\n",
      "|    std                | 1.31      |\n",
      "|    value_loss         | 5.23      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 708        |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | -20.1      |\n",
      "|    reward             | -1.2071209 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 70.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 708        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.71      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 4.39       |\n",
      "|    reward             | -5.3331323 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 43.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 709       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.71     |\n",
      "|    explained_variance | 0.000478  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 36        |\n",
      "|    reward             | -63.85015 |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 770       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 709        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.71      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 0.0642     |\n",
      "|    reward             | 0.06119568 |\n",
      "|    std                | 1.34       |\n",
      "|    value_loss         | 0.138      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 710       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 0.845     |\n",
      "|    reward             | 1.6369714 |\n",
      "|    std                | 1.35      |\n",
      "|    value_loss         | 1.14      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 710        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.72      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | 7.22       |\n",
      "|    reward             | -5.2438064 |\n",
      "|    std                | 1.36       |\n",
      "|    value_loss         | 25.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 711       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.73     |\n",
      "|    explained_variance | 0.000199  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 9.48      |\n",
      "|    reward             | 4.2965155 |\n",
      "|    std                | 1.36      |\n",
      "|    value_loss         | 56.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 711      |\n",
      "|    iterations         | 10100    |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 50500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10099    |\n",
      "|    policy_loss        | -18      |\n",
      "|    reward             | 4.381183 |\n",
      "|    std                | 1.37     |\n",
      "|    value_loss         | 171      |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 712          |\n",
      "|    iterations         | 10200        |\n",
      "|    time_elapsed       | 71           |\n",
      "|    total_timesteps    | 51000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.73        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 10199        |\n",
      "|    policy_loss        | -0.0612      |\n",
      "|    reward             | -0.009049365 |\n",
      "|    std                | 1.37         |\n",
      "|    value_loss         | 0.00332      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 711          |\n",
      "|    iterations         | 10300        |\n",
      "|    time_elapsed       | 72           |\n",
      "|    total_timesteps    | 51500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.74        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 10299        |\n",
      "|    policy_loss        | 6.15         |\n",
      "|    reward             | -0.044295374 |\n",
      "|    std                | 1.38         |\n",
      "|    value_loss         | 9.32         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 711        |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.74      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | -0.65      |\n",
      "|    reward             | -1.4404924 |\n",
      "|    std                | 1.38       |\n",
      "|    value_loss         | 2.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 712        |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.74      |\n",
      "|    explained_variance | -0.00403   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | -4.22      |\n",
      "|    reward             | -3.6171372 |\n",
      "|    std                | 1.38       |\n",
      "|    value_loss         | 14.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 712       |\n",
      "|    iterations         | 10600     |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 53000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.75     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10599     |\n",
      "|    policy_loss        | 1.11      |\n",
      "|    reward             | 0.3027653 |\n",
      "|    std                | 1.39      |\n",
      "|    value_loss         | 2.65      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 712       |\n",
      "|    iterations         | 10700     |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 53500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.76     |\n",
      "|    explained_variance | -5.26e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10699     |\n",
      "|    policy_loss        | 4.41      |\n",
      "|    reward             | -8.363048 |\n",
      "|    std                | 1.4       |\n",
      "|    value_loss         | 48.8      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 713          |\n",
      "|    iterations         | 10800        |\n",
      "|    time_elapsed       | 75           |\n",
      "|    total_timesteps    | 54000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.76        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 10799        |\n",
      "|    policy_loss        | -0.664       |\n",
      "|    reward             | 0.0015021056 |\n",
      "|    std                | 1.41         |\n",
      "|    value_loss         | 0.169        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 714        |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.75      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | -3.84      |\n",
      "|    reward             | -1.4013268 |\n",
      "|    std                | 1.4        |\n",
      "|    value_loss         | 4.41       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 714        |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 77         |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | -1.85      |\n",
      "|    reward             | -1.1497878 |\n",
      "|    std                | 1.4        |\n",
      "|    value_loss         | 2.61       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 714       |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.76     |\n",
      "|    explained_variance | -0.000229 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | 22.3      |\n",
      "|    reward             | 1.9886434 |\n",
      "|    std                | 1.41      |\n",
      "|    value_loss         | 258       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 714        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.76      |\n",
      "|    explained_variance | -0.000119  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | -3.16      |\n",
      "|    reward             | 0.69180554 |\n",
      "|    std                | 1.4        |\n",
      "|    value_loss         | 55.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 715      |\n",
      "|    iterations         | 11300    |\n",
      "|    time_elapsed       | 78       |\n",
      "|    total_timesteps    | 56500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.75    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11299    |\n",
      "|    policy_loss        | -72.8    |\n",
      "|    reward             | 26.47285 |\n",
      "|    std                | 1.39     |\n",
      "|    value_loss         | 4.51e+03 |\n",
      "------------------------------------\n",
      "day: 2831, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9950581.24\n",
      "total_reward: 8950581.24\n",
      "total_cost: 5118.66\n",
      "total_trades: 2820\n",
      "Sharpe: 0.936\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 714        |\n",
      "|    iterations         | 11400      |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 57000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11399      |\n",
      "|    policy_loss        | 1.26       |\n",
      "|    reward             | 0.01795633 |\n",
      "|    std                | 1.39       |\n",
      "|    value_loss         | 0.54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 714        |\n",
      "|    iterations         | 11500      |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 57500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.75      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11499      |\n",
      "|    policy_loss        | 0.499      |\n",
      "|    reward             | -0.8341732 |\n",
      "|    std                | 1.4        |\n",
      "|    value_loss         | 0.232      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 714       |\n",
      "|    iterations         | 11600     |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.76     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11599     |\n",
      "|    policy_loss        | 5.46      |\n",
      "|    reward             | 0.4611874 |\n",
      "|    std                | 1.41      |\n",
      "|    value_loss         | 12.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 715        |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.76      |\n",
      "|    explained_variance | -0.000358  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | -16.8      |\n",
      "|    reward             | -0.8468123 |\n",
      "|    std                | 1.4        |\n",
      "|    value_loss         | 85         |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 715      |\n",
      "|    iterations         | 11800    |\n",
      "|    time_elapsed       | 82       |\n",
      "|    total_timesteps    | 59000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.76    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11799    |\n",
      "|    policy_loss        | -30      |\n",
      "|    reward             | 6.672974 |\n",
      "|    std                | 1.41     |\n",
      "|    value_loss         | 439      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 715        |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.76      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -0.00374   |\n",
      "|    reward             | 0.00694178 |\n",
      "|    std                | 1.41       |\n",
      "|    value_loss         | 9.79e-05   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 715        |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.76      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | 2.59       |\n",
      "|    reward             | 0.41805434 |\n",
      "|    std                | 1.41       |\n",
      "|    value_loss         | 2.11       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 716       |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.77     |\n",
      "|    explained_variance | -1.43e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -11.1     |\n",
      "|    reward             | 1.2392272 |\n",
      "|    std                | 1.42      |\n",
      "|    value_loss         | 57.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 716       |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.78     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | 9.4       |\n",
      "|    reward             | 5.0761023 |\n",
      "|    std                | 1.43      |\n",
      "|    value_loss         | 34.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 716       |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.77     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | 7.81      |\n",
      "|    reward             | -4.573154 |\n",
      "|    std                | 1.42      |\n",
      "|    value_loss         | 26.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 716        |\n",
      "|    iterations         | 12400      |\n",
      "|    time_elapsed       | 86         |\n",
      "|    total_timesteps    | 62000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.76      |\n",
      "|    explained_variance | 1.01e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12399      |\n",
      "|    policy_loss        | 34.5       |\n",
      "|    reward             | -3.6084955 |\n",
      "|    std                | 1.41       |\n",
      "|    value_loss         | 458        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 717        |\n",
      "|    iterations         | 12500      |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 62500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.77      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12499      |\n",
      "|    policy_loss        | 0.294      |\n",
      "|    reward             | 0.06477294 |\n",
      "|    std                | 1.42       |\n",
      "|    value_loss         | 0.0448     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 716        |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.77      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | -3.9       |\n",
      "|    reward             | -1.8754909 |\n",
      "|    std                | 1.42       |\n",
      "|    value_loss         | 6.97       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 717         |\n",
      "|    iterations         | 12700       |\n",
      "|    time_elapsed       | 88          |\n",
      "|    total_timesteps    | 63500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.77       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12699       |\n",
      "|    policy_loss        | 0.619       |\n",
      "|    reward             | -0.38578025 |\n",
      "|    std                | 1.42        |\n",
      "|    value_loss         | 5.27        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 717       |\n",
      "|    iterations         | 12800     |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 64000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.78     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12799     |\n",
      "|    policy_loss        | -1.97     |\n",
      "|    reward             | 1.3714321 |\n",
      "|    std                | 1.43      |\n",
      "|    value_loss         | 6.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 717       |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | -15.9     |\n",
      "|    reward             | 5.1726303 |\n",
      "|    std                | 1.46      |\n",
      "|    value_loss         | 93.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 717       |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.78     |\n",
      "|    explained_variance | -0.000262 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | -60.4     |\n",
      "|    reward             | -23.60068 |\n",
      "|    std                | 1.44      |\n",
      "|    value_loss         | 1.77e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 717        |\n",
      "|    iterations         | 13100      |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 65500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.79      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13099      |\n",
      "|    policy_loss        | -0.775     |\n",
      "|    reward             | 0.23266181 |\n",
      "|    std                | 1.45       |\n",
      "|    value_loss         | 0.263      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 717          |\n",
      "|    iterations         | 13200        |\n",
      "|    time_elapsed       | 91           |\n",
      "|    total_timesteps    | 66000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.78        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13199        |\n",
      "|    policy_loss        | 0.133        |\n",
      "|    reward             | -0.058370702 |\n",
      "|    std                | 1.44         |\n",
      "|    value_loss         | 0.0254       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 717         |\n",
      "|    iterations         | 13300       |\n",
      "|    time_elapsed       | 92          |\n",
      "|    total_timesteps    | 66500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.79       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13299       |\n",
      "|    policy_loss        | -0.79       |\n",
      "|    reward             | -0.24976963 |\n",
      "|    std                | 1.44        |\n",
      "|    value_loss         | 0.376       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 718         |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 93          |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.79       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | 0.213       |\n",
      "|    reward             | -0.43445182 |\n",
      "|    std                | 1.45        |\n",
      "|    value_loss         | 0.734       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 718         |\n",
      "|    iterations         | 13500       |\n",
      "|    time_elapsed       | 93          |\n",
      "|    total_timesteps    | 67500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.77       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13499       |\n",
      "|    policy_loss        | -3.08       |\n",
      "|    reward             | -0.24896981 |\n",
      "|    std                | 1.43        |\n",
      "|    value_loss         | 4.91        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 718          |\n",
      "|    iterations         | 13600        |\n",
      "|    time_elapsed       | 94           |\n",
      "|    total_timesteps    | 68000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.77        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13599        |\n",
      "|    policy_loss        | -0.659       |\n",
      "|    reward             | -6.13573e-05 |\n",
      "|    std                | 1.43         |\n",
      "|    value_loss         | 0.142        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 719           |\n",
      "|    iterations         | 13700         |\n",
      "|    time_elapsed       | 95            |\n",
      "|    total_timesteps    | 68500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.78         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 13699         |\n",
      "|    policy_loss        | 0.000332      |\n",
      "|    reward             | -9.697281e-05 |\n",
      "|    std                | 1.44          |\n",
      "|    value_loss         | 1.51e-07      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 719           |\n",
      "|    iterations         | 13800         |\n",
      "|    time_elapsed       | 95            |\n",
      "|    total_timesteps    | 69000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.8          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 13799         |\n",
      "|    policy_loss        | 0.000912      |\n",
      "|    reward             | -6.702652e-05 |\n",
      "|    std                | 1.46          |\n",
      "|    value_loss         | 5.09e-07      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 719           |\n",
      "|    iterations         | 13900         |\n",
      "|    time_elapsed       | 96            |\n",
      "|    total_timesteps    | 69500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.82         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 13899         |\n",
      "|    policy_loss        | -0.0041       |\n",
      "|    reward             | 0.00037932576 |\n",
      "|    std                | 1.49          |\n",
      "|    value_loss         | 3e-05         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 719          |\n",
      "|    iterations         | 14000        |\n",
      "|    time_elapsed       | 97           |\n",
      "|    total_timesteps    | 70000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.85        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13999        |\n",
      "|    policy_loss        | -0.0631      |\n",
      "|    reward             | -0.023795394 |\n",
      "|    std                | 1.54         |\n",
      "|    value_loss         | 0.00301      |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 719       |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 97        |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | 1.51      |\n",
      "|    reward             | -2.158037 |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 1.38      |\n",
      "-------------------------------------\n",
      "day: 2831, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1823821.72\n",
      "total_reward: 823821.72\n",
      "total_cost: 6406.78\n",
      "total_trades: 2461\n",
      "Sharpe: 0.548\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 719         |\n",
      "|    iterations         | 14200       |\n",
      "|    time_elapsed       | 98          |\n",
      "|    total_timesteps    | 71000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.83       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14199       |\n",
      "|    policy_loss        | 0.135       |\n",
      "|    reward             | -0.13021778 |\n",
      "|    std                | 1.51        |\n",
      "|    value_loss         | 0.0074      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 719        |\n",
      "|    iterations         | 14300      |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 71500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14299      |\n",
      "|    policy_loss        | -0.313     |\n",
      "|    reward             | -1.0451918 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 0.481      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 719        |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | -0.68      |\n",
      "|    reward             | 0.21432295 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 1.08       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 719       |\n",
      "|    iterations         | 14500     |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 72500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | -2.12     |\n",
      "|    reward             | 0.6542382 |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 3.3       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 720       |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 101       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | -0.000577 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | -11.3     |\n",
      "|    reward             | 1.1104084 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 24.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 720      |\n",
      "|    iterations         | 14700    |\n",
      "|    time_elapsed       | 102      |\n",
      "|    total_timesteps    | 73500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 1.93e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | -44.3    |\n",
      "|    reward             | 6.341608 |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 477      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 720        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 102        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | -0.61      |\n",
      "|    reward             | -0.3063891 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 0.181      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 720       |\n",
      "|    iterations         | 14900     |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 74500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14899     |\n",
      "|    policy_loss        | -1.37     |\n",
      "|    reward             | 0.2810254 |\n",
      "|    std                | 1.52      |\n",
      "|    value_loss         | 1.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 720        |\n",
      "|    iterations         | 15000      |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 75000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0.000442   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14999      |\n",
      "|    policy_loss        | -5.56      |\n",
      "|    reward             | -0.4523456 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 9.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 720        |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | -0.00183   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | -1.59      |\n",
      "|    reward             | 0.84933156 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 1.36       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 721        |\n",
      "|    iterations         | 15200      |\n",
      "|    time_elapsed       | 105        |\n",
      "|    total_timesteps    | 76000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0.000284   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15199      |\n",
      "|    policy_loss        | -16.3      |\n",
      "|    reward             | -2.9843683 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 125        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 720         |\n",
      "|    iterations         | 15300       |\n",
      "|    time_elapsed       | 106         |\n",
      "|    total_timesteps    | 76500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.85       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15299       |\n",
      "|    policy_loss        | -0.0261     |\n",
      "|    reward             | 0.010400165 |\n",
      "|    std                | 1.54        |\n",
      "|    value_loss         | 0.000252    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 720          |\n",
      "|    iterations         | 15400        |\n",
      "|    time_elapsed       | 106          |\n",
      "|    total_timesteps    | 77000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.86        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15399        |\n",
      "|    policy_loss        | 3.69         |\n",
      "|    reward             | -0.009186469 |\n",
      "|    std                | 1.55         |\n",
      "|    value_loss         | 5.2          |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 720          |\n",
      "|    iterations         | 15500        |\n",
      "|    time_elapsed       | 107          |\n",
      "|    total_timesteps    | 77500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.87        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15499        |\n",
      "|    policy_loss        | 4.92         |\n",
      "|    reward             | -0.080901235 |\n",
      "|    std                | 1.56         |\n",
      "|    value_loss         | 7.1          |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 721        |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 3.55       |\n",
      "|    reward             | -1.1647094 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 6.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 721        |\n",
      "|    iterations         | 15700      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 78500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15699      |\n",
      "|    policy_loss        | -22.9      |\n",
      "|    reward             | -6.7031565 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 144        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 721       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 109       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | -6.39e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 0.374     |\n",
      "|    reward             | -24.64958 |\n",
      "|    std                | 1.57      |\n",
      "|    value_loss         | 5.2       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 721          |\n",
      "|    iterations         | 15900        |\n",
      "|    time_elapsed       | 110          |\n",
      "|    total_timesteps    | 79500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.88        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15899        |\n",
      "|    policy_loss        | 0.23         |\n",
      "|    reward             | 0.0101274615 |\n",
      "|    std                | 1.58         |\n",
      "|    value_loss         | 0.0116       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 721           |\n",
      "|    iterations         | 16000         |\n",
      "|    time_elapsed       | 110           |\n",
      "|    total_timesteps    | 80000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.89         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 15999         |\n",
      "|    policy_loss        | -0.00611      |\n",
      "|    reward             | -0.0017703025 |\n",
      "|    std                | 1.6           |\n",
      "|    value_loss         | 1.13e-05      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 721       |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | -0.000701 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 4.25e-07  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 722         |\n",
      "|    iterations         | 16200       |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 81000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.92       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16199       |\n",
      "|    policy_loss        | -0.0213     |\n",
      "|    reward             | 0.036953356 |\n",
      "|    std                | 1.64        |\n",
      "|    value_loss         | 0.000389    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 721        |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 112        |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.93      |\n",
      "|    explained_variance | -0.177     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | 4.07       |\n",
      "|    reward             | -2.1993957 |\n",
      "|    std                | 1.67       |\n",
      "|    value_loss         | 6.4        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 721       |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | 0.00371   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -18       |\n",
      "|    reward             | 4.1392717 |\n",
      "|    std                | 1.64      |\n",
      "|    value_loss         | 89.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 721         |\n",
      "|    iterations         | 16500       |\n",
      "|    time_elapsed       | 114         |\n",
      "|    total_timesteps    | 82500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.93       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16499       |\n",
      "|    policy_loss        | -1.23       |\n",
      "|    reward             | -0.46618864 |\n",
      "|    std                | 1.67        |\n",
      "|    value_loss         | 0.59        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 722        |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 114        |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.92      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | -5.65      |\n",
      "|    reward             | -0.7670324 |\n",
      "|    std                | 1.64       |\n",
      "|    value_loss         | 7.13       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 722        |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.91      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -0.721     |\n",
      "|    reward             | -0.5405116 |\n",
      "|    std                | 1.63       |\n",
      "|    value_loss         | 5.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 722       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -0.838    |\n",
      "|    reward             | -6.653299 |\n",
      "|    std                | 1.64      |\n",
      "|    value_loss         | 2.72      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 722      |\n",
      "|    iterations         | 16900    |\n",
      "|    time_elapsed       | 116      |\n",
      "|    total_timesteps    | 84500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.91    |\n",
      "|    explained_variance | -0.00021 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | -14.7    |\n",
      "|    reward             | 9.50296  |\n",
      "|    std                | 1.64     |\n",
      "|    value_loss         | 58       |\n",
      "------------------------------------\n",
      "day: 2831, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9886277.76\n",
      "total_reward: 8886277.76\n",
      "total_cost: 4051.85\n",
      "total_trades: 2818\n",
      "Sharpe: 0.934\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 722         |\n",
      "|    iterations         | 17000       |\n",
      "|    time_elapsed       | 117         |\n",
      "|    total_timesteps    | 85000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.92       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16999       |\n",
      "|    policy_loss        | -0.108      |\n",
      "|    reward             | 0.005400226 |\n",
      "|    std                | 1.65        |\n",
      "|    value_loss         | 0.00376     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 722        |\n",
      "|    iterations         | 17100      |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 85500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.91      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17099      |\n",
      "|    policy_loss        | 0.61       |\n",
      "|    reward             | 0.85241735 |\n",
      "|    std                | 1.64       |\n",
      "|    value_loss         | 0.843      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 723        |\n",
      "|    iterations         | 17200      |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 86000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17199      |\n",
      "|    policy_loss        | 5.21       |\n",
      "|    reward             | -1.4973552 |\n",
      "|    std                | 1.64       |\n",
      "|    value_loss         | 16.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -1.54     |\n",
      "|    reward             | 2.7288992 |\n",
      "|    std                | 1.65      |\n",
      "|    value_loss         | 4.87      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 17400     |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 87000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17399     |\n",
      "|    policy_loss        | -35.8     |\n",
      "|    reward             | 3.6147754 |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 253       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | 22.7      |\n",
      "|    reward             | 7.3780503 |\n",
      "|    std                | 1.64      |\n",
      "|    value_loss         | 214       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 723         |\n",
      "|    iterations         | 17600       |\n",
      "|    time_elapsed       | 121         |\n",
      "|    total_timesteps    | 88000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.92       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17599       |\n",
      "|    policy_loss        | -0.575      |\n",
      "|    reward             | -0.24475046 |\n",
      "|    std                | 1.65        |\n",
      "|    value_loss         | 0.137       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | 4.74      |\n",
      "|    reward             | -4.213742 |\n",
      "|    std                | 1.66      |\n",
      "|    value_loss         | 9.19      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 723      |\n",
      "|    iterations         | 17800    |\n",
      "|    time_elapsed       | 123      |\n",
      "|    total_timesteps    | 89000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17799    |\n",
      "|    policy_loss        | -6.14    |\n",
      "|    reward             | 4.893048 |\n",
      "|    std                | 1.65     |\n",
      "|    value_loss         | 14.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | 2.31      |\n",
      "|    reward             | 1.2193989 |\n",
      "|    std                | 1.64      |\n",
      "|    value_loss         | 1.12      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | 25.4      |\n",
      "|    reward             | -20.34277 |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 161       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 723        |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | 66.8       |\n",
      "|    reward             | -26.552076 |\n",
      "|    std                | 1.63       |\n",
      "|    value_loss         | 1.09e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 723       |\n",
      "|    iterations         | 18200     |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | 2.26      |\n",
      "|    reward             | 0.1760756 |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 723        |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.9       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -8.6       |\n",
      "|    reward             | -1.0721269 |\n",
      "|    std                | 1.62       |\n",
      "|    value_loss         | 20.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 723        |\n",
      "|    iterations         | 18400      |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 92000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18399      |\n",
      "|    policy_loss        | -2.57      |\n",
      "|    reward             | -1.0574217 |\n",
      "|    std                | 1.63       |\n",
      "|    value_loss         | 4.54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 723        |\n",
      "|    iterations         | 18500      |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 92500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18499      |\n",
      "|    policy_loss        | -28.4      |\n",
      "|    reward             | -3.8302722 |\n",
      "|    std                | 1.64       |\n",
      "|    value_loss         | 141        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 723          |\n",
      "|    iterations         | 18600        |\n",
      "|    time_elapsed       | 128          |\n",
      "|    total_timesteps    | 93000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.91        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 18599        |\n",
      "|    policy_loss        | 25           |\n",
      "|    reward             | -0.078226656 |\n",
      "|    std                | 1.64         |\n",
      "|    value_loss         | 217          |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 723         |\n",
      "|    iterations         | 18700       |\n",
      "|    time_elapsed       | 129         |\n",
      "|    total_timesteps    | 93500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.91       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18699       |\n",
      "|    policy_loss        | 0.0401      |\n",
      "|    reward             | 0.052100427 |\n",
      "|    std                | 1.63        |\n",
      "|    value_loss         | 0.000797    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 724       |\n",
      "|    iterations         | 18800     |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | 3.03      |\n",
      "|    reward             | 0.3294609 |\n",
      "|    std                | 1.64      |\n",
      "|    value_loss         | 3.25      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 724       |\n",
      "|    iterations         | 18900     |\n",
      "|    time_elapsed       | 130       |\n",
      "|    total_timesteps    | 94500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | -5.83     |\n",
      "|    reward             | 2.6415951 |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 13.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 724        |\n",
      "|    iterations         | 19000      |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 95000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18999      |\n",
      "|    policy_loss        | 8.54       |\n",
      "|    reward             | -4.0724664 |\n",
      "|    std                | 1.62       |\n",
      "|    value_loss         | 12         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 724       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -24.2     |\n",
      "|    reward             | 10.840737 |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 222       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 724       |\n",
      "|    iterations         | 19200     |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 96000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.89     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19199     |\n",
      "|    policy_loss        | 28.6      |\n",
      "|    reward             | 15.058034 |\n",
      "|    std                | 1.61      |\n",
      "|    value_loss         | 247       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 724       |\n",
      "|    iterations         | 19300     |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 96500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | -0.603    |\n",
      "|    reward             | 0.3505282 |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 0.0788    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 724        |\n",
      "|    iterations         | 19400      |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 97000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19399      |\n",
      "|    policy_loss        | 3.43       |\n",
      "|    reward             | -1.8768345 |\n",
      "|    std                | 1.61       |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 724        |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.9       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | 4.15       |\n",
      "|    reward             | -0.1995341 |\n",
      "|    std                | 1.61       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 724         |\n",
      "|    iterations         | 19600       |\n",
      "|    time_elapsed       | 135         |\n",
      "|    total_timesteps    | 98000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.89       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19599       |\n",
      "|    policy_loss        | 3.97        |\n",
      "|    reward             | -0.11311545 |\n",
      "|    std                | 1.61        |\n",
      "|    value_loss         | 7.63        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 724      |\n",
      "|    iterations         | 19700    |\n",
      "|    time_elapsed       | 135      |\n",
      "|    total_timesteps    | 98500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.89    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | -3.67    |\n",
      "|    reward             | 9.325129 |\n",
      "|    std                | 1.6      |\n",
      "|    value_loss         | 46.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 725       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | 27.5      |\n",
      "|    reward             | 59.339104 |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 240       |\n",
      "-------------------------------------\n",
      "day: 2831, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10087339.49\n",
      "total_reward: 9087339.49\n",
      "total_cost: 998.99\n",
      "total_trades: 2831\n",
      "Sharpe: 0.928\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 725       |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | 0.753     |\n",
      "|    reward             | 0.6249148 |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 0.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 724       |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | -13.5     |\n",
      "|    reward             | 0.5814359 |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 24.6      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
      "A2C Sharpe Ratio:  0.3698754453719308\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_189_4\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1175        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.059666052 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1110        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004380843 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00984    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00243    |\n",
      "|    reward               | -0.6618273  |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.00945     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1086         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058562546 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.22         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    reward               | 0.08211913   |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.581        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1080         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048194337 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.19         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.001       |\n",
      "|    reward               | -0.0764035   |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 6.84         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1060         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004811817  |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0104      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000554    |\n",
      "|    reward               | 0.0055001993 |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.0341       |\n",
      "------------------------------------------\n",
      "day: 2831, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1302911.21\n",
      "total_reward: 302911.21\n",
      "total_cost: 5609.31\n",
      "total_trades: 2719\n",
      "Sharpe: 0.501\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1056         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038886531 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.113        |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    reward               | -0.07020896  |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 0.193        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1051         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032931487 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.641        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00134     |\n",
      "|    reward               | 0.0012042733 |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 1.48         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1050         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014525508 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.306        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -1.1e-05     |\n",
      "|    reward               | 0.7007871    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.623        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 1052          |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 17            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043808704 |\n",
      "|    clip_fraction        | 0.00278       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.171         |\n",
      "|    n_updates            | 80            |\n",
      "|    policy_gradient_loss | 1.21e-05      |\n",
      "|    reward               | 0.13920845    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 0.449         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1050         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020455746 |\n",
      "|    clip_fraction        | 0.000293     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0139      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.01         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000497    |\n",
      "|    reward               | 0.08050125   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 14.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1052         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030501257 |\n",
      "|    clip_fraction        | 0.00103      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.00381     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 15.2         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    reward               | 6.1149497    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 35.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1050        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00436003  |\n",
      "|    clip_fraction        | 0.0142      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.0584     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    reward               | -0.35761115 |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 86.5        |\n",
      "-----------------------------------------\n",
      "day: 2831, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5682378.94\n",
      "total_reward: 4682378.94\n",
      "total_cost: 6099.03\n",
      "total_trades: 2805\n",
      "Sharpe: 0.855\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1048        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002609462 |\n",
      "|    clip_fraction        | 0.013       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.0818      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.96        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    reward               | -0.15021461 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 29.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1050         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008196809 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 66.9         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    reward               | -0.035992373 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 180          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1049         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028610784 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.194        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 64.5         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    reward               | 2.4541178    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 138          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1047        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004861462 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    reward               | 0.4604376   |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1046         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006268211 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.283        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 65           |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000491    |\n",
      "|    reward               | 0.12020618   |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1047         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018299462 |\n",
      "|    clip_fraction        | 0.00313      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.289        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 55.5         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    reward               | -0.007985962 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 122          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1047         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032686503 |\n",
      "|    clip_fraction        | 0.0188       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.345        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 54.2         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    reward               | 3.362335     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 129          |\n",
      "------------------------------------------\n",
      "day: 2831, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3119666.54\n",
      "total_reward: 2119666.54\n",
      "total_cost: 6034.39\n",
      "total_trades: 2801\n",
      "Sharpe: 0.774\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1044         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003773533 |\n",
      "|    clip_fraction        | 0.00962      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.694        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | 4.84e-05     |\n",
      "|    reward               | 0.37085286   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.78         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034038597 |\n",
      "|    clip_fraction        | 0.00503      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.00233      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000463    |\n",
      "|    reward               | 0.12664773   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 40.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1044        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005084307 |\n",
      "|    clip_fraction        | 0.0301      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.383       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.6        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0046     |\n",
      "|    reward               | 4.7813354   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 67.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015274211 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.34         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000393    |\n",
      "|    reward               | 0.3390257    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 22.7         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1045       |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 47         |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00392221 |\n",
      "|    clip_fraction        | 0.0203     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.211      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 22.8       |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.00192   |\n",
      "|    reward               | 0.10486806 |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 45.8       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1046         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028942723 |\n",
      "|    clip_fraction        | 0.0102       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.351        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 75.3         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000585    |\n",
      "|    reward               | 0.099331744  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1046         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024126582 |\n",
      "|    clip_fraction        | 0.0111       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.475        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 61.4         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000853    |\n",
      "|    reward               | -1.940589    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 79.6         |\n",
      "------------------------------------------\n",
      "day: 2831, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4575698.05\n",
      "total_reward: 3575698.05\n",
      "total_cost: 6239.01\n",
      "total_trades: 2817\n",
      "Sharpe: 0.818\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030938475 |\n",
      "|    clip_fraction        | 0.00937      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.493        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.15         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000747    |\n",
      "|    reward               | -0.37015525  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 15           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1044         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030408292 |\n",
      "|    clip_fraction        | 0.00854      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.491        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 37.5         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0016      |\n",
      "|    reward               | -0.18047774  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 92.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019997796 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.551        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 48.4         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00135     |\n",
      "|    reward               | 2.5767426    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 77.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020449439 |\n",
      "|    clip_fraction        | 0.00762      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.517        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 59.6         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | 0.000607     |\n",
      "|    reward               | 0.7702898    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 128          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015446664 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.401        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.25         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    reward               | 0.40131044   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 28.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028362873 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 57.5         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    reward               | -0.4290009   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1044         |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062135486 |\n",
      "|    clip_fraction        | 0.0247       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.455        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 45.5         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000461    |\n",
      "|    reward               | 2.7270863    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 145          |\n",
      "------------------------------------------\n",
      "day: 2831, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4450280.10\n",
      "total_reward: 3450280.10\n",
      "total_cost: 6064.67\n",
      "total_trades: 2802\n",
      "Sharpe: 0.829\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1044        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004574728 |\n",
      "|    clip_fraction        | 0.0215      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.448       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.49        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    reward               | 0.20303412  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 17.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1044         |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017644162 |\n",
      "|    clip_fraction        | 0.00288      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.531        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 38.1         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.000274    |\n",
      "|    reward               | 0.13343018   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 81           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1043         |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014672717 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.546        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 79           |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000321    |\n",
      "|    reward               | 0.022691049  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 144          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1044        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001899898 |\n",
      "|    clip_fraction        | 0.00693     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.534       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 71.5        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | 0.00102     |\n",
      "|    reward               | -2.5988357  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 133         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1042         |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022487203 |\n",
      "|    clip_fraction        | 0.00381      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.9          |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.57         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    reward               | 0.21699497   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1043         |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041258708 |\n",
      "|    clip_fraction        | 0.0157       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.527        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 75.6         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00296     |\n",
      "|    reward               | 1.6357074    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 171          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1043         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022283886 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.454        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 129          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    reward               | 2.5598757    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 211          |\n",
      "------------------------------------------\n",
      "day: 2831, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6684495.23\n",
      "total_reward: 5684495.23\n",
      "total_cost: 6025.27\n",
      "total_trades: 2818\n",
      "Sharpe: 0.892\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1042         |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038388171 |\n",
      "|    clip_fraction        | 0.0122       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.557        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 59           |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    reward               | -1.1139916   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 110          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1043         |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011309902 |\n",
      "|    clip_fraction        | 0.00776      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 35.9         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    reward               | 0.24113849   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 140          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1043         |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013049501 |\n",
      "|    clip_fraction        | 0.0084       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 117          |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000331    |\n",
      "|    reward               | 0.15998988   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 257          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1043        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003100277 |\n",
      "|    clip_fraction        | 0.0157      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.377       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 102         |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00303    |\n",
      "|    reward               | -3.146934   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 251         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1043         |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022635865 |\n",
      "|    clip_fraction        | 0.0281       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.743        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 29.2         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | 0.000872     |\n",
      "|    reward               | -0.9418486   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 52.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1044         |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039916015 |\n",
      "|    clip_fraction        | 0.0393       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.469        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 127          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.000783    |\n",
      "|    reward               | -0.6026731   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 235          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1044        |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 92          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001985941 |\n",
      "|    clip_fraction        | 0.00723     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 133         |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.000793    |\n",
      "|    reward               | -6.6435366  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 302         |\n",
      "-----------------------------------------\n",
      "day: 2831, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7499058.89\n",
      "total_reward: 6499058.89\n",
      "total_cost: 6094.02\n",
      "total_trades: 2811\n",
      "Sharpe: 0.904\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1044         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025119502 |\n",
      "|    clip_fraction        | 0.0514       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.238        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 96.5         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | 0.00162      |\n",
      "|    reward               | 2.739604     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 299          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1043        |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007250201 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.2        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.000976    |\n",
      "|    reward               | 2.6996417   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 27.4        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
      "PPO Sharpe Ratio:  0.3566047326661636\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_4\n",
      "day: 2831, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10087339.49\n",
      "total_reward: 9087339.49\n",
      "total_cost: 998.99\n",
      "total_trades: 2831\n",
      "Sharpe: 0.928\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 73        |\n",
      "|    total_timesteps | 11328     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -442      |\n",
      "|    critic_loss     | 77.2      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 11227     |\n",
      "|    reward          | 23.235329 |\n",
      "----------------------------------\n",
      "day: 2831, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10087339.49\n",
      "total_reward: 9087339.49\n",
      "total_cost: 998.99\n",
      "total_trades: 2831\n",
      "Sharpe: 0.928\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 151       |\n",
      "|    time_elapsed    | 149       |\n",
      "|    total_timesteps | 22656     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -435      |\n",
      "|    critic_loss     | 52.7      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 22555     |\n",
      "|    reward          | 23.235329 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 150       |\n",
      "|    time_elapsed    | 225       |\n",
      "|    total_timesteps | 33984     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -269      |\n",
      "|    critic_loss     | 50.6      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 33883     |\n",
      "|    reward          | 23.235329 |\n",
      "----------------------------------\n",
      "day: 2831, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10087339.49\n",
      "total_reward: 9087339.49\n",
      "total_cost: 998.99\n",
      "total_trades: 2831\n",
      "Sharpe: 0.928\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 148       |\n",
      "|    time_elapsed    | 305       |\n",
      "|    total_timesteps | 45312     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -165      |\n",
      "|    critic_loss     | 23.3      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 45211     |\n",
      "|    reward          | 23.235329 |\n",
      "----------------------------------\n",
      "day: 2831, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10087339.49\n",
      "total_reward: 9087339.49\n",
      "total_cost: 998.99\n",
      "total_trades: 2831\n",
      "Sharpe: 0.928\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 147       |\n",
      "|    time_elapsed    | 383       |\n",
      "|    total_timesteps | 56640     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -104      |\n",
      "|    critic_loss     | 122       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 56539     |\n",
      "|    reward          | 23.235329 |\n",
      "----------------------------------\n",
      "day: 2831, episode: 95\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10087339.49\n",
      "total_reward: 9087339.49\n",
      "total_cost: 998.99\n",
      "total_trades: 2831\n",
      "Sharpe: 0.928\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 147       |\n",
      "|    time_elapsed    | 461       |\n",
      "|    total_timesteps | 67968     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -52.4     |\n",
      "|    critic_loss     | 166       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 67867     |\n",
      "|    reward          | 23.235329 |\n",
      "----------------------------------\n",
      "day: 2831, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10087339.49\n",
      "total_reward: 9087339.49\n",
      "total_cost: 998.99\n",
      "total_trades: 2831\n",
      "Sharpe: 0.928\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 146       |\n",
      "|    time_elapsed    | 540       |\n",
      "|    total_timesteps | 79296     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -46.4     |\n",
      "|    critic_loss     | 123       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 79195     |\n",
      "|    reward          | 23.235329 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 146       |\n",
      "|    time_elapsed    | 618       |\n",
      "|    total_timesteps | 90624     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -37.7     |\n",
      "|    critic_loss     | 149       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 90523     |\n",
      "|    reward          | 23.235329 |\n",
      "----------------------------------\n",
      "day: 2831, episode: 105\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10087339.49\n",
      "total_reward: 9087339.49\n",
      "total_cost: 998.99\n",
      "total_trades: 2831\n",
      "Sharpe: 0.928\n",
      "=================================\n",
      "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2021-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_252_4\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 528        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -0.133     |\n",
      "|    reward             | 0.19585627 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.115      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 558      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.93    |\n",
      "|    reward             | 2.386855 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.552    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 592        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | -0.0285    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -1.64      |\n",
      "|    reward             | -3.0071917 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 598        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | -0.00457   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.64      |\n",
      "|    reward             | -1.3217118 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 609        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | -0.0108    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -3.11      |\n",
      "|    reward             | -5.2829204 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 13.4       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 614         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 0.244       |\n",
      "|    reward             | -0.18972127 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.0319      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 617       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 3.08      |\n",
      "|    reward             | 1.0690508 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.81      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 619       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 1.11      |\n",
      "|    reward             | 1.1322255 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.01      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 615         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -0.96       |\n",
      "|    reward             | -0.05792671 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.284       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 618          |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 8            |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.44        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | 0.0173       |\n",
      "|    reward             | -0.004426819 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.000209     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 622         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.46       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 0.0137      |\n",
      "|    reward             | 0.020478822 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 0.000164    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 625          |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 9            |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.47        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | -0.000304    |\n",
      "|    reward             | 0.0014742197 |\n",
      "|    std                | 1.06         |\n",
      "|    value_loss         | 2.11e-08     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 623            |\n",
      "|    iterations         | 1300           |\n",
      "|    time_elapsed       | 10             |\n",
      "|    total_timesteps    | 6500           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.5           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1299           |\n",
      "|    policy_loss        | 0.00091        |\n",
      "|    reward             | -0.00078498904 |\n",
      "|    std                | 1.08           |\n",
      "|    value_loss         | 1.09e-06       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 625          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 11           |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.53        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | -0.000759    |\n",
      "|    reward             | 0.0010823206 |\n",
      "|    std                | 1.12         |\n",
      "|    value_loss         | 4.01e-07     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 626          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 11           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.57        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | 0.00605      |\n",
      "|    reward             | -0.000195371 |\n",
      "|    std                | 1.16         |\n",
      "|    value_loss         | 2.72e-05     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 626      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.00784  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 3.64e-05 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 625            |\n",
      "|    iterations         | 1700           |\n",
      "|    time_elapsed       | 13             |\n",
      "|    total_timesteps    | 8500           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.65          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1699           |\n",
      "|    policy_loss        | 0.00503        |\n",
      "|    reward             | -0.00052989786 |\n",
      "|    std                | 1.26           |\n",
      "|    value_loss         | 0.000223       |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 622      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.7     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.0014   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.32     |\n",
      "|    value_loss         | 9.95e-07 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 624            |\n",
      "|    iterations         | 1900           |\n",
      "|    time_elapsed       | 15             |\n",
      "|    total_timesteps    | 9500           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.76          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1899           |\n",
      "|    policy_loss        | 0.00456        |\n",
      "|    reward             | -1.8204326e-05 |\n",
      "|    std                | 1.41           |\n",
      "|    value_loss         | 9.64e-06       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 625          |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 15           |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.82        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | 0.00799      |\n",
      "|    reward             | 0.0062650437 |\n",
      "|    std                | 1.5          |\n",
      "|    value_loss         | 2.14e-05     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 624            |\n",
      "|    iterations         | 2100           |\n",
      "|    time_elapsed       | 16             |\n",
      "|    total_timesteps    | 10500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.88          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 2099           |\n",
      "|    policy_loss        | -0.00862       |\n",
      "|    reward             | -0.00030969054 |\n",
      "|    std                | 1.59           |\n",
      "|    value_loss         | 2.99e-05       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 625          |\n",
      "|    iterations         | 2200         |\n",
      "|    time_elapsed       | 17           |\n",
      "|    total_timesteps    | 11000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.92        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2199         |\n",
      "|    policy_loss        | 0.00313      |\n",
      "|    reward             | 0.0041249245 |\n",
      "|    std                | 1.65         |\n",
      "|    value_loss         | 4.1e-06      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 627         |\n",
      "|    iterations         | 2300        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 11500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.96       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2299        |\n",
      "|    policy_loss        | -0.182      |\n",
      "|    reward             | 0.054672252 |\n",
      "|    std                | 1.71        |\n",
      "|    value_loss         | 0.0308      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 628           |\n",
      "|    iterations         | 2400          |\n",
      "|    time_elapsed       | 19            |\n",
      "|    total_timesteps    | 12000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.97         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2399          |\n",
      "|    policy_loss        | 0.0197        |\n",
      "|    reward             | -0.0020876178 |\n",
      "|    std                | 1.73          |\n",
      "|    value_loss         | 0.000275      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 627           |\n",
      "|    iterations         | 2500          |\n",
      "|    time_elapsed       | 19            |\n",
      "|    total_timesteps    | 12500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.99         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2499          |\n",
      "|    policy_loss        | -0.000468     |\n",
      "|    reward             | 1.3459675e-05 |\n",
      "|    std                | 1.78          |\n",
      "|    value_loss         | 1.5e-06       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 627         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | 0.0342      |\n",
      "|    reward             | 0.009559374 |\n",
      "|    std                | 1.84        |\n",
      "|    value_loss         | 0.00064     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 628          |\n",
      "|    iterations         | 2700         |\n",
      "|    time_elapsed       | 21           |\n",
      "|    total_timesteps    | 13500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.05        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2699         |\n",
      "|    policy_loss        | -0.195       |\n",
      "|    reward             | -0.063742116 |\n",
      "|    std                | 1.87         |\n",
      "|    value_loss         | 0.132        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 630        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 1.41       |\n",
      "|    reward             | -0.3475878 |\n",
      "|    std                | 1.89       |\n",
      "|    value_loss         | 4.38       |\n",
      "--------------------------------------\n",
      "day: 2894, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2634576.97\n",
      "total_reward: 1634576.97\n",
      "total_cost: 7484.70\n",
      "total_trades: 2709\n",
      "Sharpe: 0.705\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 628          |\n",
      "|    iterations         | 2900         |\n",
      "|    time_elapsed       | 23           |\n",
      "|    total_timesteps    | 14500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.06        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2899         |\n",
      "|    policy_loss        | -0.395       |\n",
      "|    reward             | 0.0036654857 |\n",
      "|    std                | 1.89         |\n",
      "|    value_loss         | 0.0584       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 628          |\n",
      "|    iterations         | 3000         |\n",
      "|    time_elapsed       | 23           |\n",
      "|    total_timesteps    | 15000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.06        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2999         |\n",
      "|    policy_loss        | 0.188        |\n",
      "|    reward             | -0.008118575 |\n",
      "|    std                | 1.9          |\n",
      "|    value_loss         | 0.0151       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 626            |\n",
      "|    iterations         | 3100           |\n",
      "|    time_elapsed       | 24             |\n",
      "|    total_timesteps    | 15500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.07          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 3099           |\n",
      "|    policy_loss        | -0.00752       |\n",
      "|    reward             | -0.00045993182 |\n",
      "|    std                | 1.91           |\n",
      "|    value_loss         | 1.51e-05       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 3200         |\n",
      "|    time_elapsed       | 25           |\n",
      "|    total_timesteps    | 16000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.08        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3199         |\n",
      "|    policy_loss        | -0.0275      |\n",
      "|    reward             | 0.0066789687 |\n",
      "|    std                | 1.94         |\n",
      "|    value_loss         | 0.00038      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 626          |\n",
      "|    iterations         | 3300         |\n",
      "|    time_elapsed       | 26           |\n",
      "|    total_timesteps    | 16500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.1         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3299         |\n",
      "|    policy_loss        | 0.0403       |\n",
      "|    reward             | -0.026805764 |\n",
      "|    std                | 1.98         |\n",
      "|    value_loss         | 0.000775     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 3400         |\n",
      "|    time_elapsed       | 27           |\n",
      "|    total_timesteps    | 17000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.1         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3399         |\n",
      "|    policy_loss        | 0.248        |\n",
      "|    reward             | -0.054967444 |\n",
      "|    std                | 1.98         |\n",
      "|    value_loss         | 0.027        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 3500         |\n",
      "|    time_elapsed       | 27           |\n",
      "|    total_timesteps    | 17500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.1         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3499         |\n",
      "|    policy_loss        | -0.329       |\n",
      "|    reward             | -0.032063816 |\n",
      "|    std                | 1.98         |\n",
      "|    value_loss         | 0.0437       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 628        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -1.33      |\n",
      "|    reward             | 0.42460787 |\n",
      "|    std                | 1.99       |\n",
      "|    value_loss         | 0.476      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 627        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -1.74      |\n",
      "|    reward             | 0.41748163 |\n",
      "|    std                | 2.01       |\n",
      "|    value_loss         | 1.17       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 627        |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | -5.37      |\n",
      "|    reward             | -2.4892745 |\n",
      "|    std                | 2.02       |\n",
      "|    value_loss         | 5.36       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 627        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | 0.0014     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | -4.34      |\n",
      "|    reward             | 0.93273884 |\n",
      "|    std                | 2          |\n",
      "|    value_loss         | 6.7        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | -0.00911 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 40.8     |\n",
      "|    reward             | 11.09418 |\n",
      "|    std                | 2        |\n",
      "|    value_loss         | 428      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 627        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 0.0382     |\n",
      "|    reward             | 0.04950695 |\n",
      "|    std                | 1.97       |\n",
      "|    value_loss         | 0.00269    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 627         |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | 1.19        |\n",
      "|    reward             | -0.40719697 |\n",
      "|    std                | 1.99        |\n",
      "|    value_loss         | 0.449       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 628          |\n",
      "|    iterations         | 4300         |\n",
      "|    time_elapsed       | 34           |\n",
      "|    total_timesteps    | 21500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.11        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4299         |\n",
      "|    policy_loss        | 2.31         |\n",
      "|    reward             | -0.043962646 |\n",
      "|    std                | 1.99         |\n",
      "|    value_loss         | 1.54         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 628       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -3.28     |\n",
      "|    reward             | 0.0662769 |\n",
      "|    std                | 2         |\n",
      "|    value_loss         | 6.91      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 22500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 0.0518   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -2.45    |\n",
      "|    reward             | 2.431895 |\n",
      "|    std                | 1.99     |\n",
      "|    value_loss         | 14.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 628        |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | 0.000475   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 80.3       |\n",
      "|    reward             | -1.3193849 |\n",
      "|    std                | 2          |\n",
      "|    value_loss         | 1.3e+03    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 629        |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -0.0583    |\n",
      "|    reward             | 0.03312368 |\n",
      "|    std                | 2          |\n",
      "|    value_loss         | 0.00294    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 629         |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | 3.74        |\n",
      "|    reward             | -0.09162526 |\n",
      "|    std                | 2           |\n",
      "|    value_loss         | 1.72        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 628       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 1.7       |\n",
      "|    reward             | 1.3254308 |\n",
      "|    std                | 2         |\n",
      "|    value_loss         | 0.848     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 629         |\n",
      "|    iterations         | 5000        |\n",
      "|    time_elapsed       | 39          |\n",
      "|    total_timesteps    | 25000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4999        |\n",
      "|    policy_loss        | -1.49       |\n",
      "|    reward             | -0.34209895 |\n",
      "|    std                | 2           |\n",
      "|    value_loss         | 0.227       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 629       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | -0.686    |\n",
      "|    reward             | 1.0537797 |\n",
      "|    std                | 2.01      |\n",
      "|    value_loss         | 0.171     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 629         |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | 10.8        |\n",
      "|    reward             | -0.36203486 |\n",
      "|    std                | 2.03        |\n",
      "|    value_loss         | 44.5        |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 628           |\n",
      "|    iterations         | 5300          |\n",
      "|    time_elapsed       | 42            |\n",
      "|    total_timesteps    | 26500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.13         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5299          |\n",
      "|    policy_loss        | -0.0738       |\n",
      "|    reward             | -0.0107114175 |\n",
      "|    std                | 2.03          |\n",
      "|    value_loss         | 0.00152       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 629           |\n",
      "|    iterations         | 5400          |\n",
      "|    time_elapsed       | 42            |\n",
      "|    total_timesteps    | 27000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.14         |\n",
      "|    explained_variance | -2.38e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5399          |\n",
      "|    policy_loss        | -0.00162      |\n",
      "|    reward             | -0.0008942763 |\n",
      "|    std                | 2.05          |\n",
      "|    value_loss         | 0.000419      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 629        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -0.064     |\n",
      "|    reward             | 0.10660066 |\n",
      "|    std                | 2.08       |\n",
      "|    value_loss         | 0.00966    |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 629         |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.16       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | -0.113      |\n",
      "|    reward             | -0.06725023 |\n",
      "|    std                | 2.09        |\n",
      "|    value_loss         | 0.00622     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 628         |\n",
      "|    iterations         | 5700        |\n",
      "|    time_elapsed       | 45          |\n",
      "|    total_timesteps    | 28500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.17       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5699        |\n",
      "|    policy_loss        | -0.0194     |\n",
      "|    reward             | -0.21541034 |\n",
      "|    std                | 2.13        |\n",
      "|    value_loss         | 0.0721      |\n",
      "---------------------------------------\n",
      "day: 2894, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1546694.34\n",
      "total_reward: 546694.34\n",
      "total_cost: 8065.18\n",
      "total_trades: 2856\n",
      "Sharpe: 0.533\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 628          |\n",
      "|    iterations         | 5800         |\n",
      "|    time_elapsed       | 46           |\n",
      "|    total_timesteps    | 29000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.17        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5799         |\n",
      "|    policy_loss        | -0.328       |\n",
      "|    reward             | -0.002552103 |\n",
      "|    std                | 2.11         |\n",
      "|    value_loss         | 0.018        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 629        |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 1          |\n",
      "|    reward             | 0.14446124 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 0.345      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 629          |\n",
      "|    iterations         | 6000         |\n",
      "|    time_elapsed       | 47           |\n",
      "|    total_timesteps    | 30000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.18        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5999         |\n",
      "|    policy_loss        | 0.148        |\n",
      "|    reward             | -0.027606448 |\n",
      "|    std                | 2.13         |\n",
      "|    value_loss         | 0.143        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 628        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | -1.06      |\n",
      "|    reward             | 0.22873962 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 31000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | -8.8e-05 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 16.8     |\n",
      "|    reward             | 4.444371 |\n",
      "|    std                | 2.11     |\n",
      "|    value_loss         | 133      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 628        |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0.00212    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 4.72       |\n",
      "|    reward             | -9.5367565 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 20.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 628         |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | -0.0545     |\n",
      "|    reward             | -0.02732874 |\n",
      "|    std                | 2.14        |\n",
      "|    value_loss         | 0.00105     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 627         |\n",
      "|    iterations         | 6500        |\n",
      "|    time_elapsed       | 51          |\n",
      "|    total_timesteps    | 32500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6499        |\n",
      "|    policy_loss        | 2.3         |\n",
      "|    reward             | -0.06638989 |\n",
      "|    std                | 2.15        |\n",
      "|    value_loss         | 1.91        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 628         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 52          |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | 0.0937      |\n",
      "|    reward             | -0.13482608 |\n",
      "|    std                | 2.13        |\n",
      "|    value_loss         | 0.316       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 53       |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | -0.146   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -7.41    |\n",
      "|    reward             | 2.417137 |\n",
      "|    std                | 2.1      |\n",
      "|    value_loss         | 9.71     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 628        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | -0.000112  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | 1.6        |\n",
      "|    reward             | -0.8442315 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 0.967      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 628       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 4.15e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | 17        |\n",
      "|    reward             | 1.5433329 |\n",
      "|    std                | 2.11      |\n",
      "|    value_loss         | 104       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 628        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 0.102      |\n",
      "|    reward             | 0.01918678 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 0.00649    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 628       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 1.86      |\n",
      "|    reward             | 0.2722693 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 0.917     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 628          |\n",
      "|    iterations         | 7200         |\n",
      "|    time_elapsed       | 57           |\n",
      "|    total_timesteps    | 36000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.14        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7199         |\n",
      "|    policy_loss        | 0.4          |\n",
      "|    reward             | 0.0020649047 |\n",
      "|    std                | 2.07         |\n",
      "|    value_loss         | 0.0423       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 628          |\n",
      "|    iterations         | 7300         |\n",
      "|    time_elapsed       | 58           |\n",
      "|    total_timesteps    | 36500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.15        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7299         |\n",
      "|    policy_loss        | 0.259        |\n",
      "|    reward             | -0.007346229 |\n",
      "|    std                | 2.07         |\n",
      "|    value_loss         | 0.0346       |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 628       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -1.2      |\n",
      "|    reward             | 2.8022108 |\n",
      "|    std                | 2.08      |\n",
      "|    value_loss         | 0.702     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -1.53     |\n",
      "|    reward             | -1.120193 |\n",
      "|    std                | 2.08      |\n",
      "|    value_loss         | 0.779     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 7600         |\n",
      "|    time_elapsed       | 60           |\n",
      "|    total_timesteps    | 38000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.15        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7599         |\n",
      "|    policy_loss        | -0.0446      |\n",
      "|    reward             | -0.013107387 |\n",
      "|    std                | 2.08         |\n",
      "|    value_loss         | 0.000392     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 626           |\n",
      "|    iterations         | 7700          |\n",
      "|    time_elapsed       | 61            |\n",
      "|    total_timesteps    | 38500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.16         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7699          |\n",
      "|    policy_loss        | 0.000846      |\n",
      "|    reward             | -0.0002688239 |\n",
      "|    std                | 2.09          |\n",
      "|    value_loss         | 3.88e-07      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 627           |\n",
      "|    iterations         | 7800          |\n",
      "|    time_elapsed       | 62            |\n",
      "|    total_timesteps    | 39000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.17         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7799          |\n",
      "|    policy_loss        | -0.0124       |\n",
      "|    reward             | -0.0041202875 |\n",
      "|    std                | 2.13          |\n",
      "|    value_loss         | 7.44e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 7900     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 39500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.19    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7899     |\n",
      "|    policy_loss        | 0.0048   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.17     |\n",
      "|    value_loss         | 1.02e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -0.0141  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.23     |\n",
      "|    value_loss         | 4.79e-05 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 8100         |\n",
      "|    time_elapsed       | 64           |\n",
      "|    total_timesteps    | 40500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.25        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8099         |\n",
      "|    policy_loss        | -0.0568      |\n",
      "|    reward             | -0.010411519 |\n",
      "|    std                | 2.29         |\n",
      "|    value_loss         | 0.00606      |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.27     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -0.000733 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.34      |\n",
      "|    value_loss         | 1.56e-07  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 628         |\n",
      "|    iterations         | 8300        |\n",
      "|    time_elapsed       | 66          |\n",
      "|    total_timesteps    | 41500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.29       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8299        |\n",
      "|    policy_loss        | -0.0347     |\n",
      "|    reward             | 0.014047361 |\n",
      "|    std                | 2.39        |\n",
      "|    value_loss         | 0.000115    |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 628            |\n",
      "|    iterations         | 8400           |\n",
      "|    time_elapsed       | 66             |\n",
      "|    total_timesteps    | 42000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.32          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 8399           |\n",
      "|    policy_loss        | 0.0224         |\n",
      "|    reward             | -0.00027742411 |\n",
      "|    std                | 2.47           |\n",
      "|    value_loss         | 0.00012        |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 8500     |\n",
      "|    time_elapsed       | 67       |\n",
      "|    total_timesteps    | 42500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.36    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8499     |\n",
      "|    policy_loss        | -0.00093 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.57     |\n",
      "|    value_loss         | 5.14e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 8600     |\n",
      "|    time_elapsed       | 68       |\n",
      "|    total_timesteps    | 43000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8599     |\n",
      "|    policy_loss        | -0.0596  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.64     |\n",
      "|    value_loss         | 0.000611 |\n",
      "------------------------------------\n",
      "day: 2894, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1011742.74\n",
      "total_reward: 11742.74\n",
      "total_cost: 6597.08\n",
      "total_trades: 2215\n",
      "Sharpe: 0.150\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 8700     |\n",
      "|    time_elapsed       | 69       |\n",
      "|    total_timesteps    | 43500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.39    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8699     |\n",
      "|    policy_loss        | -0.00523 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.65     |\n",
      "|    value_loss         | 1.18e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 628      |\n",
      "|    iterations         | 8800     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 44000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.41    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8799     |\n",
      "|    policy_loss        | 0.00914  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.69     |\n",
      "|    value_loss         | 4.32e-05 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 627           |\n",
      "|    iterations         | 8900          |\n",
      "|    time_elapsed       | 70            |\n",
      "|    total_timesteps    | 44500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.43         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 8899          |\n",
      "|    policy_loss        | -0.00674      |\n",
      "|    reward             | -0.0029405286 |\n",
      "|    std                | 2.75          |\n",
      "|    value_loss         | 9.51e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 9000         |\n",
      "|    time_elapsed       | 71           |\n",
      "|    total_timesteps    | 45000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.46        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8999         |\n",
      "|    policy_loss        | 0.0554       |\n",
      "|    reward             | -0.028571071 |\n",
      "|    std                | 2.84         |\n",
      "|    value_loss         | 0.000775     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 627         |\n",
      "|    iterations         | 9100        |\n",
      "|    time_elapsed       | 72          |\n",
      "|    total_timesteps    | 45500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.47       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9099        |\n",
      "|    policy_loss        | 0.118       |\n",
      "|    reward             | 0.025954576 |\n",
      "|    std                | 2.87        |\n",
      "|    value_loss         | 0.0139      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 627         |\n",
      "|    iterations         | 9200        |\n",
      "|    time_elapsed       | 73          |\n",
      "|    total_timesteps    | 46000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.49       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9199        |\n",
      "|    policy_loss        | -0.655      |\n",
      "|    reward             | -0.11177034 |\n",
      "|    std                | 2.91        |\n",
      "|    value_loss         | 0.112       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 9300         |\n",
      "|    time_elapsed       | 74           |\n",
      "|    total_timesteps    | 46500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.5         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9299         |\n",
      "|    policy_loss        | 0.0308       |\n",
      "|    reward             | 0.0023662348 |\n",
      "|    std                | 2.95         |\n",
      "|    value_loss         | 0.000177     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 627        |\n",
      "|    iterations         | 9400       |\n",
      "|    time_elapsed       | 74         |\n",
      "|    total_timesteps    | 47000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.51      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9399       |\n",
      "|    policy_loss        | 0.267      |\n",
      "|    reward             | 0.27764022 |\n",
      "|    std                | 2.97       |\n",
      "|    value_loss         | 0.021      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 627        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 75         |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.52      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | -4.18      |\n",
      "|    reward             | 0.17387931 |\n",
      "|    std                | 3          |\n",
      "|    value_loss         | 2.71       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 9600         |\n",
      "|    time_elapsed       | 76           |\n",
      "|    total_timesteps    | 48000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.53        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9599         |\n",
      "|    policy_loss        | -3.09        |\n",
      "|    reward             | -0.023200203 |\n",
      "|    std                | 3.03         |\n",
      "|    value_loss         | 1.23         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.52     |\n",
      "|    explained_variance | 3.03e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | -0.371    |\n",
      "|    reward             | 2.1263354 |\n",
      "|    std                | 3         |\n",
      "|    value_loss         | 2.06      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 627      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 78       |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | 8.94     |\n",
      "|    reward             | 7.220089 |\n",
      "|    std                | 3.04     |\n",
      "|    value_loss         | 28.9     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 627         |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.53       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | 0.255       |\n",
      "|    reward             | -0.38920146 |\n",
      "|    std                | 3.04        |\n",
      "|    value_loss         | 0.0402      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 627        |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.54      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 5.63       |\n",
      "|    reward             | -0.8026737 |\n",
      "|    std                | 3.07       |\n",
      "|    value_loss         | 7.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.53     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | 5.08      |\n",
      "|    reward             | 3.9535084 |\n",
      "|    std                | 3.05      |\n",
      "|    value_loss         | 7.34      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.53     |\n",
      "|    explained_variance | 0.000158  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | 0.996     |\n",
      "|    reward             | 1.8565897 |\n",
      "|    std                | 3.03      |\n",
      "|    value_loss         | 1.58      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 627        |\n",
      "|    iterations         | 10300      |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 51500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.53      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10299      |\n",
      "|    policy_loss        | 11.7       |\n",
      "|    reward             | 0.12669235 |\n",
      "|    std                | 3.02       |\n",
      "|    value_loss         | 28         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 10400     |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.53     |\n",
      "|    explained_variance | -3.52e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10399     |\n",
      "|    policy_loss        | 47.1      |\n",
      "|    reward             | -7.083338 |\n",
      "|    std                | 3.04      |\n",
      "|    value_loss         | 489       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 627         |\n",
      "|    iterations         | 10500       |\n",
      "|    time_elapsed       | 83          |\n",
      "|    total_timesteps    | 52500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.52       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10499       |\n",
      "|    policy_loss        | 0.0289      |\n",
      "|    reward             | 0.016809708 |\n",
      "|    std                | 3.01        |\n",
      "|    value_loss         | 0.0111      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 627       |\n",
      "|    iterations         | 10600     |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 53000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.53     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10599     |\n",
      "|    policy_loss        | 1.83      |\n",
      "|    reward             | 0.1672359 |\n",
      "|    std                | 3.03      |\n",
      "|    value_loss         | 0.837     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 627          |\n",
      "|    iterations         | 10700        |\n",
      "|    time_elapsed       | 85           |\n",
      "|    total_timesteps    | 53500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.54        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 10699        |\n",
      "|    policy_loss        | -1.2         |\n",
      "|    reward             | -0.060041778 |\n",
      "|    std                | 3.08         |\n",
      "|    value_loss         | 0.928        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 626       |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | -0.000221 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -7.2      |\n",
      "|    reward             | 2.043129  |\n",
      "|    std                | 3.06      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 626       |\n",
      "|    iterations         | 10900     |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 54500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.53     |\n",
      "|    explained_variance | 0.0012    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10899     |\n",
      "|    policy_loss        | 5.09      |\n",
      "|    reward             | 2.4402447 |\n",
      "|    std                | 3.05      |\n",
      "|    value_loss         | 8.36      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 626      |\n",
      "|    iterations         | 11000    |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 55000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | -0.0839  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10999    |\n",
      "|    policy_loss        | 24.3     |\n",
      "|    reward             | 9.790681 |\n",
      "|    std                | 3.07     |\n",
      "|    value_loss         | 104      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 626         |\n",
      "|    iterations         | 11100       |\n",
      "|    time_elapsed       | 88          |\n",
      "|    total_timesteps    | 55500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.55       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11099       |\n",
      "|    policy_loss        | -2.33e-05   |\n",
      "|    reward             | 0.040457543 |\n",
      "|    std                | 3.09        |\n",
      "|    value_loss         | 0.00323     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 626        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.54      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | 0.574      |\n",
      "|    reward             | 0.20716476 |\n",
      "|    std                | 3.07       |\n",
      "|    value_loss         | 0.227      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 625        |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 90         |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.54      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | -3.67      |\n",
      "|    reward             | 0.31866294 |\n",
      "|    std                | 3.07       |\n",
      "|    value_loss         | 2.53       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 625         |\n",
      "|    iterations         | 11400       |\n",
      "|    time_elapsed       | 91          |\n",
      "|    total_timesteps    | 57000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.54       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11399       |\n",
      "|    policy_loss        | -0.955      |\n",
      "|    reward             | -0.11241118 |\n",
      "|    std                | 3.08        |\n",
      "|    value_loss         | 1.69        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 625        |\n",
      "|    iterations         | 11500      |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 57500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.54      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11499      |\n",
      "|    policy_loss        | -7.76      |\n",
      "|    reward             | -3.0245562 |\n",
      "|    std                | 3.07       |\n",
      "|    value_loss         | 17.1       |\n",
      "--------------------------------------\n",
      "day: 2894, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5518786.36\n",
      "total_reward: 4518786.36\n",
      "total_cost: 7698.27\n",
      "total_trades: 2890\n",
      "Sharpe: 0.857\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 625        |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.53      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | -0.363     |\n",
      "|    reward             | 0.06826273 |\n",
      "|    std                | 3.05       |\n",
      "|    value_loss         | 0.0283     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 625       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -1.37     |\n",
      "|    reward             | 1.7508787 |\n",
      "|    std                | 3.06      |\n",
      "|    value_loss         | 0.383     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 625        |\n",
      "|    iterations         | 11800      |\n",
      "|    time_elapsed       | 94         |\n",
      "|    total_timesteps    | 59000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.53      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11799      |\n",
      "|    policy_loss        | 0.36       |\n",
      "|    reward             | 0.85588634 |\n",
      "|    std                | 3.04       |\n",
      "|    value_loss         | 0.335      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 625        |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.53      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -1.05      |\n",
      "|    reward             | -2.6249688 |\n",
      "|    std                | 3.04       |\n",
      "|    value_loss         | 0.712      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 625       |\n",
      "|    iterations         | 12000     |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 60000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.51     |\n",
      "|    explained_variance | 0.000359  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | 20.8      |\n",
      "|    reward             | 1.2254944 |\n",
      "|    std                | 2.99      |\n",
      "|    value_loss         | 46.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 625       |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.51     |\n",
      "|    explained_variance | 0.000175  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | 23.4      |\n",
      "|    reward             | 5.8017144 |\n",
      "|    std                | 2.98      |\n",
      "|    value_loss         | 132       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 625          |\n",
      "|    iterations         | 12200        |\n",
      "|    time_elapsed       | 97           |\n",
      "|    total_timesteps    | 61000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.52        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12199        |\n",
      "|    policy_loss        | 0.0235       |\n",
      "|    reward             | -0.016670637 |\n",
      "|    std                | 3.02         |\n",
      "|    value_loss         | 0.0215       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 625         |\n",
      "|    iterations         | 12300       |\n",
      "|    time_elapsed       | 98          |\n",
      "|    total_timesteps    | 61500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.53       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12299       |\n",
      "|    policy_loss        | -3.05       |\n",
      "|    reward             | -0.87529856 |\n",
      "|    std                | 3.05        |\n",
      "|    value_loss         | 2.65        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 625       |\n",
      "|    iterations         | 12400     |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.55     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | 2.18      |\n",
      "|    reward             | -1.016788 |\n",
      "|    std                | 3.1       |\n",
      "|    value_loss         | 0.996     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 625        |\n",
      "|    iterations         | 12500      |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 62500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.56      |\n",
      "|    explained_variance | 0.149      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12499      |\n",
      "|    policy_loss        | 0.477      |\n",
      "|    reward             | 0.31283382 |\n",
      "|    std                | 3.12       |\n",
      "|    value_loss         | 0.756      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 625        |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | 13.3       |\n",
      "|    reward             | -4.9573207 |\n",
      "|    std                | 3.12       |\n",
      "|    value_loss         | 36.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 624       |\n",
      "|    iterations         | 12700     |\n",
      "|    time_elapsed       | 101       |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.56     |\n",
      "|    explained_variance | -0.000668 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | -16.5     |\n",
      "|    reward             | 5.381159  |\n",
      "|    std                | 3.12      |\n",
      "|    value_loss         | 152       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 624         |\n",
      "|    iterations         | 12800       |\n",
      "|    time_elapsed       | 102         |\n",
      "|    total_timesteps    | 64000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.57       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12799       |\n",
      "|    policy_loss        | -0.0799     |\n",
      "|    reward             | -0.03370589 |\n",
      "|    std                | 3.15        |\n",
      "|    value_loss         | 0.0817      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 624        |\n",
      "|    iterations         | 12900      |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 64500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.57      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12899      |\n",
      "|    policy_loss        | 4.61       |\n",
      "|    reward             | 0.15623172 |\n",
      "|    std                | 3.16       |\n",
      "|    value_loss         | 3.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 624        |\n",
      "|    iterations         | 13000      |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 65000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.57      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12999      |\n",
      "|    policy_loss        | -8.39      |\n",
      "|    reward             | -1.2246542 |\n",
      "|    std                | 3.16       |\n",
      "|    value_loss         | 9.46       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 624         |\n",
      "|    iterations         | 13100       |\n",
      "|    time_elapsed       | 104         |\n",
      "|    total_timesteps    | 65500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13099       |\n",
      "|    policy_loss        | 0.186       |\n",
      "|    reward             | -0.29675952 |\n",
      "|    std                | 3.13        |\n",
      "|    value_loss         | 1.67        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 624      |\n",
      "|    iterations         | 13200    |\n",
      "|    time_elapsed       | 105      |\n",
      "|    total_timesteps    | 66000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 0.000376 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13199    |\n",
      "|    policy_loss        | 3.51     |\n",
      "|    reward             | 8.725614 |\n",
      "|    std                | 3.15     |\n",
      "|    value_loss         | 3.29     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 624         |\n",
      "|    iterations         | 13300       |\n",
      "|    time_elapsed       | 106         |\n",
      "|    total_timesteps    | 66500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.57       |\n",
      "|    explained_variance | -4.01e-05   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13299       |\n",
      "|    policy_loss        | -98.9       |\n",
      "|    reward             | -11.4627495 |\n",
      "|    std                | 3.16        |\n",
      "|    value_loss         | 1.57e+03    |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 624        |\n",
      "|    iterations         | 13400      |\n",
      "|    time_elapsed       | 107        |\n",
      "|    total_timesteps    | 67000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.58      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13399      |\n",
      "|    policy_loss        | -1.9       |\n",
      "|    reward             | -0.1486636 |\n",
      "|    std                | 3.2        |\n",
      "|    value_loss         | 0.669      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 624        |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.58      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 7.09       |\n",
      "|    reward             | 0.07634894 |\n",
      "|    std                | 3.2        |\n",
      "|    value_loss         | 13         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 624        |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.57      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -9.85      |\n",
      "|    reward             | -0.9556282 |\n",
      "|    std                | 3.17       |\n",
      "|    value_loss         | 23.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 624      |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | 14.6     |\n",
      "|    reward             | 3.248819 |\n",
      "|    std                | 3.17     |\n",
      "|    value_loss         | 53.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 624       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.58     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -6.51     |\n",
      "|    reward             | 6.7753096 |\n",
      "|    std                | 3.18      |\n",
      "|    value_loss         | 12.6      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 624          |\n",
      "|    iterations         | 13900        |\n",
      "|    time_elapsed       | 111          |\n",
      "|    total_timesteps    | 69500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.57        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13899        |\n",
      "|    policy_loss        | -0.189       |\n",
      "|    reward             | 0.0057846014 |\n",
      "|    std                | 3.15         |\n",
      "|    value_loss         | 0.00966      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 624         |\n",
      "|    iterations         | 14000       |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 70000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.57       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13999       |\n",
      "|    policy_loss        | 1.29        |\n",
      "|    reward             | -0.21928665 |\n",
      "|    std                | 3.16        |\n",
      "|    value_loss         | 0.257       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 624       |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 112       |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.58     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | 8.09      |\n",
      "|    reward             | 0.9516955 |\n",
      "|    std                | 3.19      |\n",
      "|    value_loss         | 11        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 623        |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 113        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.57      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | -10.3      |\n",
      "|    reward             | -0.8013862 |\n",
      "|    std                | 3.15       |\n",
      "|    value_loss         | 53.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 623       |\n",
      "|    iterations         | 14300     |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 71500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.57     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14299     |\n",
      "|    policy_loss        | 6.28      |\n",
      "|    reward             | 1.7547361 |\n",
      "|    std                | 3.15      |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 624       |\n",
      "|    iterations         | 14400     |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.57     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | 23.1      |\n",
      "|    reward             | 8.767413  |\n",
      "|    std                | 3.17      |\n",
      "|    value_loss         | 120       |\n",
      "-------------------------------------\n",
      "day: 2894, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10340895.62\n",
      "total_reward: 9340895.62\n",
      "total_cost: 4079.01\n",
      "total_trades: 2891\n",
      "Sharpe: 0.959\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 624          |\n",
      "|    iterations         | 14500        |\n",
      "|    time_elapsed       | 116          |\n",
      "|    total_timesteps    | 72500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.58        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14499        |\n",
      "|    policy_loss        | -0.466       |\n",
      "|    reward             | -0.037581746 |\n",
      "|    std                | 3.18         |\n",
      "|    value_loss         | 0.0263       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 623        |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.59      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | 3.23       |\n",
      "|    reward             | 0.14722715 |\n",
      "|    std                | 3.21       |\n",
      "|    value_loss         | 1.67       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 623         |\n",
      "|    iterations         | 14700       |\n",
      "|    time_elapsed       | 117         |\n",
      "|    total_timesteps    | 73500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.58       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14699       |\n",
      "|    policy_loss        | -4.61       |\n",
      "|    reward             | -0.87617075 |\n",
      "|    std                | 3.2         |\n",
      "|    value_loss         | 7.9         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 623        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.59      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | -1.63      |\n",
      "|    reward             | -3.7600765 |\n",
      "|    std                | 3.23       |\n",
      "|    value_loss         | 1.01       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 624        |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.6       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | 16.2       |\n",
      "|    reward             | -2.7687001 |\n",
      "|    std                | 3.25       |\n",
      "|    value_loss         | 53.3       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 15000    |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 75000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.61    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14999    |\n",
      "|    policy_loss        | 17.8     |\n",
      "|    reward             | 18.95034 |\n",
      "|    std                | 3.29     |\n",
      "|    value_loss         | 65.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 623        |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.61      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 0.163      |\n",
      "|    reward             | 0.23862207 |\n",
      "|    std                | 3.28       |\n",
      "|    value_loss         | 0.0435     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 623        |\n",
      "|    iterations         | 15200      |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 76000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.62      |\n",
      "|    explained_variance | 0.165      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15199      |\n",
      "|    policy_loss        | 15.5       |\n",
      "|    reward             | -0.6500851 |\n",
      "|    std                | 3.33       |\n",
      "|    value_loss         | 36.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 623      |\n",
      "|    iterations         | 15300    |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 76500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | 12.6     |\n",
      "|    reward             | 2.697961 |\n",
      "|    std                | 3.32     |\n",
      "|    value_loss         | 21.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 623       |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.63     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | 1.3       |\n",
      "|    reward             | 2.3942893 |\n",
      "|    std                | 3.34      |\n",
      "|    value_loss         | 3.06      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 623        |\n",
      "|    iterations         | 15500      |\n",
      "|    time_elapsed       | 124        |\n",
      "|    total_timesteps    | 77500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.63      |\n",
      "|    explained_variance | -0.000268  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15499      |\n",
      "|    policy_loss        | -53.2      |\n",
      "|    reward             | -7.0900893 |\n",
      "|    std                | 3.37       |\n",
      "|    value_loss         | 311        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 622       |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.63     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | 7.45      |\n",
      "|    reward             | 29.363224 |\n",
      "|    std                | 3.37      |\n",
      "|    value_loss         | 198       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 622        |\n",
      "|    iterations         | 15700      |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 78500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.63      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15699      |\n",
      "|    policy_loss        | 0.934      |\n",
      "|    reward             | 0.11985035 |\n",
      "|    std                | 3.37       |\n",
      "|    value_loss         | 0.359      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 622       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.64     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 0.341     |\n",
      "|    reward             | 1.8829592 |\n",
      "|    std                | 3.37      |\n",
      "|    value_loss         | 3.98      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 622       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.63     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | -10.3     |\n",
      "|    reward             | 0.9904691 |\n",
      "|    std                | 3.37      |\n",
      "|    value_loss         | 16.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 622        |\n",
      "|    iterations         | 16000      |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 80000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.64      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15999      |\n",
      "|    policy_loss        | -11.5      |\n",
      "|    reward             | -0.8784145 |\n",
      "|    std                | 3.38       |\n",
      "|    value_loss         | 21.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 622       |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.65     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | 21.7      |\n",
      "|    reward             | 0.6265415 |\n",
      "|    std                | 3.43      |\n",
      "|    value_loss         | 80        |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 622      |\n",
      "|    iterations         | 16200    |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 81000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.65    |\n",
      "|    explained_variance | 0.000177 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16199    |\n",
      "|    policy_loss        | 34.6     |\n",
      "|    reward             | 18.67552 |\n",
      "|    std                | 3.43     |\n",
      "|    value_loss         | 188      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 622        |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 130        |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.66      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | -0.536     |\n",
      "|    reward             | -0.5578361 |\n",
      "|    std                | 3.45       |\n",
      "|    value_loss         | 0.173      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 622        |\n",
      "|    iterations         | 16400      |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 82000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.67      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16399      |\n",
      "|    policy_loss        | 1.87       |\n",
      "|    reward             | -0.5392894 |\n",
      "|    std                | 3.48       |\n",
      "|    value_loss         | 1.89       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 622       |\n",
      "|    iterations         | 16500     |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 82500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.67     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | 9.59      |\n",
      "|    reward             | 1.4117159 |\n",
      "|    std                | 3.5       |\n",
      "|    value_loss         | 18.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 622        |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.67      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | -1.95      |\n",
      "|    reward             | -1.9336616 |\n",
      "|    std                | 3.5        |\n",
      "|    value_loss         | 11.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 622       |\n",
      "|    iterations         | 16700     |\n",
      "|    time_elapsed       | 134       |\n",
      "|    total_timesteps    | 83500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.67     |\n",
      "|    explained_variance | -1.31e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | 14.6      |\n",
      "|    reward             | -7.139064 |\n",
      "|    std                | 3.49      |\n",
      "|    value_loss         | 46.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 622         |\n",
      "|    iterations         | 16800       |\n",
      "|    time_elapsed       | 134         |\n",
      "|    total_timesteps    | 84000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.66       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16799       |\n",
      "|    policy_loss        | 0.0126      |\n",
      "|    reward             | 0.018868018 |\n",
      "|    std                | 3.48        |\n",
      "|    value_loss         | 0.0004      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 621        |\n",
      "|    iterations         | 16900      |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 84500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.67      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16899      |\n",
      "|    policy_loss        | 2.31       |\n",
      "|    reward             | 0.04845658 |\n",
      "|    std                | 3.49       |\n",
      "|    value_loss         | 0.785      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 621         |\n",
      "|    iterations         | 17000       |\n",
      "|    time_elapsed       | 136         |\n",
      "|    total_timesteps    | 85000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.68       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16999       |\n",
      "|    policy_loss        | -6.56       |\n",
      "|    reward             | -0.21478981 |\n",
      "|    std                | 3.52        |\n",
      "|    value_loss         | 7.04        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 621      |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 137      |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | 9.96     |\n",
      "|    reward             | 2.063247 |\n",
      "|    std                | 3.51     |\n",
      "|    value_loss         | 17.7     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 621         |\n",
      "|    iterations         | 17200       |\n",
      "|    time_elapsed       | 138         |\n",
      "|    total_timesteps    | 86000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.67       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17199       |\n",
      "|    policy_loss        | -8.13       |\n",
      "|    reward             | -0.85052544 |\n",
      "|    std                | 3.5         |\n",
      "|    value_loss         | 33.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 621        |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 139        |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.67      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | 28.2       |\n",
      "|    reward             | -3.6363273 |\n",
      "|    std                | 3.48       |\n",
      "|    value_loss         | 249        |\n",
      "--------------------------------------\n",
      "day: 2894, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8823780.18\n",
      "total_reward: 7823780.18\n",
      "total_cost: 4628.98\n",
      "total_trades: 2890\n",
      "Sharpe: 0.929\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 621         |\n",
      "|    iterations         | 17400       |\n",
      "|    time_elapsed       | 139         |\n",
      "|    total_timesteps    | 87000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.67       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17399       |\n",
      "|    policy_loss        | 0.0374      |\n",
      "|    reward             | 0.053621143 |\n",
      "|    std                | 3.49        |\n",
      "|    value_loss         | 0.00852     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 621         |\n",
      "|    iterations         | 17500       |\n",
      "|    time_elapsed       | 140         |\n",
      "|    total_timesteps    | 87500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.68       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17499       |\n",
      "|    policy_loss        | -9.56       |\n",
      "|    reward             | -0.51612175 |\n",
      "|    std                | 3.51        |\n",
      "|    value_loss         | 16.5        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 621         |\n",
      "|    iterations         | 17600       |\n",
      "|    time_elapsed       | 141         |\n",
      "|    total_timesteps    | 88000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.69       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17599       |\n",
      "|    policy_loss        | 11.3        |\n",
      "|    reward             | -0.38684568 |\n",
      "|    std                | 3.55        |\n",
      "|    value_loss         | 17          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 621        |\n",
      "|    iterations         | 17700      |\n",
      "|    time_elapsed       | 142        |\n",
      "|    total_timesteps    | 88500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17699      |\n",
      "|    policy_loss        | 6.42       |\n",
      "|    reward             | -2.3051476 |\n",
      "|    std                | 3.54       |\n",
      "|    value_loss         | 7.98       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 620       |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.68     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | 9.35      |\n",
      "|    reward             | 2.6747317 |\n",
      "|    std                | 3.53      |\n",
      "|    value_loss         | 23.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 17900    |\n",
      "|    time_elapsed       | 144      |\n",
      "|    total_timesteps    | 89500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17899    |\n",
      "|    policy_loss        | 57.6     |\n",
      "|    reward             | 4.956091 |\n",
      "|    std                | 3.51     |\n",
      "|    value_loss         | 614      |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 620          |\n",
      "|    iterations         | 18000        |\n",
      "|    time_elapsed       | 144          |\n",
      "|    total_timesteps    | 90000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.69        |\n",
      "|    explained_variance | 1.79e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 17999        |\n",
      "|    policy_loss        | -0.39        |\n",
      "|    reward             | -0.017061256 |\n",
      "|    std                | 3.55         |\n",
      "|    value_loss         | 0.0195       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 620        |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 145        |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.68      |\n",
      "|    explained_variance | -0.374     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | -0.304     |\n",
      "|    reward             | -1.2966785 |\n",
      "|    std                | 3.51       |\n",
      "|    value_loss         | 2.37       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 620         |\n",
      "|    iterations         | 18200       |\n",
      "|    time_elapsed       | 146         |\n",
      "|    total_timesteps    | 91000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.68       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18199       |\n",
      "|    policy_loss        | 7.1         |\n",
      "|    reward             | -0.14140649 |\n",
      "|    std                | 3.52        |\n",
      "|    value_loss         | 20.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 620        |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.69      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | 7.56       |\n",
      "|    reward             | 0.20212686 |\n",
      "|    std                | 3.57       |\n",
      "|    value_loss         | 12.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 620      |\n",
      "|    iterations         | 18400    |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.69    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | -32.7    |\n",
      "|    reward             | 4.036585 |\n",
      "|    std                | 3.58     |\n",
      "|    value_loss         | 291      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 620       |\n",
      "|    iterations         | 18500     |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 92500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.69     |\n",
      "|    explained_variance | -0.000226 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | 52        |\n",
      "|    reward             | -6.438044 |\n",
      "|    std                | 3.58      |\n",
      "|    value_loss         | 487       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 620         |\n",
      "|    iterations         | 18600       |\n",
      "|    time_elapsed       | 149         |\n",
      "|    total_timesteps    | 93000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.7        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18599       |\n",
      "|    policy_loss        | 1.33        |\n",
      "|    reward             | 0.014326382 |\n",
      "|    std                | 3.62        |\n",
      "|    value_loss         | 0.372       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 619         |\n",
      "|    iterations         | 18700       |\n",
      "|    time_elapsed       | 150         |\n",
      "|    total_timesteps    | 93500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.7        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18699       |\n",
      "|    policy_loss        | 0.0323      |\n",
      "|    reward             | -0.77181876 |\n",
      "|    std                | 3.59        |\n",
      "|    value_loss         | 0.076       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 619       |\n",
      "|    iterations         | 18800     |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.7      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | 7.34      |\n",
      "|    reward             | 0.4410911 |\n",
      "|    std                | 3.6       |\n",
      "|    value_loss         | 10.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 619        |\n",
      "|    iterations         | 18900      |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 94500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.7       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18899      |\n",
      "|    policy_loss        | -20.7      |\n",
      "|    reward             | -0.8139157 |\n",
      "|    std                | 3.59       |\n",
      "|    value_loss         | 82         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 619       |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.71     |\n",
      "|    explained_variance | 4.01e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -47.6     |\n",
      "|    reward             | 6.4020896 |\n",
      "|    std                | 3.62      |\n",
      "|    value_loss         | 412       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 619       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.7      |\n",
      "|    explained_variance | -1.51e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | 14.2      |\n",
      "|    reward             | 18.380344 |\n",
      "|    std                | 3.6       |\n",
      "|    value_loss         | 202       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 619         |\n",
      "|    iterations         | 19200       |\n",
      "|    time_elapsed       | 154         |\n",
      "|    total_timesteps    | 96000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.71       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19199       |\n",
      "|    policy_loss        | 0.858       |\n",
      "|    reward             | -0.25300235 |\n",
      "|    std                | 3.63        |\n",
      "|    value_loss         | 0.295       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 618        |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 155        |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.72      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | 1.14       |\n",
      "|    reward             | -0.5291597 |\n",
      "|    std                | 3.66       |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 619       |\n",
      "|    iterations         | 19400     |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 97000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.72     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19399     |\n",
      "|    policy_loss        | 5.44      |\n",
      "|    reward             | 7.9384956 |\n",
      "|    std                | 3.66      |\n",
      "|    value_loss         | 7.85      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 619        |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 157        |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.72      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | 15.2       |\n",
      "|    reward             | -0.1399702 |\n",
      "|    std                | 3.67       |\n",
      "|    value_loss         | 26.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 618       |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 158       |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.72     |\n",
      "|    explained_variance | 0.000127  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 39        |\n",
      "|    reward             | 2.0285318 |\n",
      "|    std                | 3.68      |\n",
      "|    value_loss         | 234       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 618         |\n",
      "|    iterations         | 19700       |\n",
      "|    time_elapsed       | 159         |\n",
      "|    total_timesteps    | 98500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.72       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19699       |\n",
      "|    policy_loss        | -0.0546     |\n",
      "|    reward             | 0.038932096 |\n",
      "|    std                | 3.68        |\n",
      "|    value_loss         | 0.000416    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 618       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.73     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | -3.09     |\n",
      "|    reward             | 0.2510438 |\n",
      "|    std                | 3.71      |\n",
      "|    value_loss         | 1.45      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 618        |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.73      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -2.11      |\n",
      "|    reward             | -1.4610662 |\n",
      "|    std                | 3.7        |\n",
      "|    value_loss         | 1.27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 618        |\n",
      "|    iterations         | 20000      |\n",
      "|    time_elapsed       | 161        |\n",
      "|    total_timesteps    | 100000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.72      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19999      |\n",
      "|    policy_loss        | -2.3       |\n",
      "|    reward             | -0.9427677 |\n",
      "|    std                | 3.69       |\n",
      "|    value_loss         | 1.11       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
      "A2C Sharpe Ratio:  -0.1324347253051089\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_252_4\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 1099      |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 1         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.0786503 |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1045         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043780226 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.000508     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    reward               | -0.046335466 |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 0.058        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1028         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016509828 |\n",
      "|    clip_fraction        | 0.00103      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.000312     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.52         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000383    |\n",
      "|    reward               | 0.09094516   |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 4.02         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1023         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039315713 |\n",
      "|    clip_fraction        | 0.0159       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.25         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00136     |\n",
      "|    reward               | 1.8403658    |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 4.81         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1012         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036033858 |\n",
      "|    clip_fraction        | 0.0082       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.22         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000251    |\n",
      "|    reward               | 0.006258925  |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 3.7          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1010        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002846067 |\n",
      "|    clip_fraction        | 0.00244     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.0043      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | 3.17e-05    |\n",
      "|    reward               | -0.4154814  |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1008         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035371091 |\n",
      "|    clip_fraction        | 0.0104       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.73         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000339    |\n",
      "|    reward               | -1.9111902   |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 11           |\n",
      "------------------------------------------\n",
      "day: 2894, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4294880.67\n",
      "total_reward: 3294880.67\n",
      "total_cost: 6536.80\n",
      "total_trades: 2870\n",
      "Sharpe: 0.825\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 999           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 16            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085888046 |\n",
      "|    clip_fraction        | 0.00254       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -0.00578      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 15.5          |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000933     |\n",
      "|    reward               | -0.99138707   |\n",
      "|    std                  | 0.992         |\n",
      "|    value_loss           | 61.8          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1000         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031126053 |\n",
      "|    clip_fraction        | 0.0109       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0208       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.33         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    reward               | 0.38697347   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 19.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 994          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015976059 |\n",
      "|    clip_fraction        | 0.00596      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.107        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 45.1         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00219     |\n",
      "|    reward               | -0.047844417 |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 115          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 990          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010123849 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0855       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 67.8         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | 2.74e-05     |\n",
      "|    reward               | 1.4260141    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 181          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 991          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024145632 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0682       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.03         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00203     |\n",
      "|    reward               | -1.1108539   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 20.8         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 990           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00031799346 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.214         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 99.9          |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -4.09e-05     |\n",
      "|    reward               | 0.3474613     |\n",
      "|    std                  | 0.987         |\n",
      "|    value_loss           | 151           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 991          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032680228 |\n",
      "|    clip_fraction        | 0.00596      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.182        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 125          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000677    |\n",
      "|    reward               | 0.13963464   |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 193          |\n",
      "------------------------------------------\n",
      "day: 2894, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5728926.94\n",
      "total_reward: 4728926.94\n",
      "total_cost: 6405.31\n",
      "total_trades: 2873\n",
      "Sharpe: 0.877\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 986          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039810035 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.382        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 24.3         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    reward               | -0.22427462  |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 58.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 986         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007167874 |\n",
      "|    clip_fraction        | 0.0699      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.354       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.6        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00469    |\n",
      "|    reward               | 0.48911083  |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 92          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 986          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021069304 |\n",
      "|    clip_fraction        | 0.0125       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.358        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 112          |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -5.36e-05    |\n",
      "|    reward               | 0.018254936  |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 198          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 986          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050529456 |\n",
      "|    clip_fraction        | 0.0154       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.437        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 71.2         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    reward               | 0.43459177   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 134          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 987          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053424984 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.793        |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00195     |\n",
      "|    reward               | -0.6026612   |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 1.26         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 987           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 41            |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020147176 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.478         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 25.2          |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.000473     |\n",
      "|    reward               | 0.11950602    |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 57.8          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 986          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042380258 |\n",
      "|    clip_fraction        | 0.00703      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.49         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 26.2         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    reward               | 0.1125539    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 92.9         |\n",
      "------------------------------------------\n",
      "day: 2894, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5950787.01\n",
      "total_reward: 4950787.01\n",
      "total_cost: 6360.31\n",
      "total_trades: 2870\n",
      "Sharpe: 0.868\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 985          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013955007 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.701        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000717    |\n",
      "|    reward               | -1.3309895   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 35.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 985          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018483428 |\n",
      "|    clip_fraction        | 0.00327      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.451        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 52.6         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    reward               | -0.028464088 |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 133          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 985          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010213505 |\n",
      "|    clip_fraction        | 0.00117      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.417        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 92.8         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000414    |\n",
      "|    reward               | 1.5537962    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 232          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012843064 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.412        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 78.2         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000618    |\n",
      "|    reward               | -0.2007646   |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 224          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037488448 |\n",
      "|    clip_fraction        | 0.0084       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.814        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.55         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000881    |\n",
      "|    reward               | 0.5194148    |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 20.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005835731 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.519        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 105          |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000418    |\n",
      "|    reward               | -0.10022162  |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 220          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015073649 |\n",
      "|    clip_fraction        | 0.00317      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.569        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 63           |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00133     |\n",
      "|    reward               | 0.8328533    |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 155          |\n",
      "------------------------------------------\n",
      "day: 2894, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7124554.79\n",
      "total_reward: 6124554.79\n",
      "total_cost: 6172.13\n",
      "total_trades: 2877\n",
      "Sharpe: 0.911\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055191284 |\n",
      "|    clip_fraction        | 0.0629       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.759        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 24.8         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00344     |\n",
      "|    reward               | -0.83537936  |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 41.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011223047 |\n",
      "|    clip_fraction        | 0.00435      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.555        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 150          |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    reward               | -1.2450845   |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 203          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018120579 |\n",
      "|    clip_fraction        | 0.004        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.477        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 206          |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    reward               | 0.8920568    |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 283          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032381166 |\n",
      "|    clip_fraction        | 0.00464      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.622        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 64           |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00143     |\n",
      "|    reward               | -0.7562432   |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 172          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 984         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004196142 |\n",
      "|    clip_fraction        | 0.0275      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.735       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 39          |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00327    |\n",
      "|    reward               | 0.6168028   |\n",
      "|    std                  | 0.968       |\n",
      "|    value_loss           | 74          |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 984           |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 70            |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00073981483 |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0.523         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 102           |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -0.00043      |\n",
      "|    reward               | -0.19588947   |\n",
      "|    std                  | 0.964         |\n",
      "|    value_loss           | 335           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 985           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 72            |\n",
      "|    total_timesteps      | 71680         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046565925 |\n",
      "|    clip_fraction        | 0.000732      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0.309         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 118           |\n",
      "|    n_updates            | 340           |\n",
      "|    policy_gradient_loss | -0.000128     |\n",
      "|    reward               | 1.1895914     |\n",
      "|    std                  | 0.966         |\n",
      "|    value_loss           | 363           |\n",
      "-------------------------------------------\n",
      "day: 2894, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8702829.73\n",
      "total_reward: 7702829.73\n",
      "total_cost: 5473.10\n",
      "total_trades: 2883\n",
      "Sharpe: 0.927\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 983           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 74            |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005684532   |\n",
      "|    clip_fraction        | 0.0272        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0.862         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 18.4          |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -0.00289      |\n",
      "|    reward               | -0.0148222055 |\n",
      "|    std                  | 0.967         |\n",
      "|    value_loss           | 33.9          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026410534 |\n",
      "|    clip_fraction        | 0.00566      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.488        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 185          |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    reward               | 0.3952247    |\n",
      "|    std                  | 0.969        |\n",
      "|    value_loss           | 340          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 984         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003402024 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.428       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 150         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0032     |\n",
      "|    reward               | -31.783249  |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 394         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040942635 |\n",
      "|    clip_fraction        | 0.0151       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.541        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 73.3         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00153     |\n",
      "|    reward               | -0.56568104  |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 146          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028313082 |\n",
      "|    clip_fraction        | 0.00571      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.491        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 176          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | 0.000213     |\n",
      "|    reward               | -0.65438694  |\n",
      "|    std                  | 0.963        |\n",
      "|    value_loss           | 330          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007292684 |\n",
      "|    clip_fraction        | 0.00576      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.483        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 153          |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    reward               | -0.021156257 |\n",
      "|    std                  | 0.964        |\n",
      "|    value_loss           | 416          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013759292 |\n",
      "|    clip_fraction        | 0.0259       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 204          |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00253     |\n",
      "|    reward               | -3.0312173   |\n",
      "|    std                  | 0.964        |\n",
      "|    value_loss           | 490          |\n",
      "------------------------------------------\n",
      "day: 2894, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10060369.06\n",
      "total_reward: 9060369.06\n",
      "total_cost: 5625.40\n",
      "total_trades: 2876\n",
      "Sharpe: 0.952\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037426339 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.853        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13.8         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    reward               | 0.96785015   |\n",
      "|    std                  | 0.96         |\n",
      "|    value_loss           | 38.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014846777 |\n",
      "|    clip_fraction        | 0.00718      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.424        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 226          |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.000362    |\n",
      "|    reward               | -0.33500102  |\n",
      "|    std                  | 0.961        |\n",
      "|    value_loss           | 476          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006818919 |\n",
      "|    clip_fraction        | 0.00444      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.327        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 168          |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    reward               | 7.8035345    |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 505          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038942224 |\n",
      "|    clip_fraction        | 0.0204       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.664        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 28.5         |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00205     |\n",
      "|    reward               | 4.298961     |\n",
      "|    std                  | 0.972        |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021085786 |\n",
      "|    clip_fraction        | 0.0042       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 265          |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | 0.000573     |\n",
      "|    reward               | 0.009831332  |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 436          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 983          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037305476 |\n",
      "|    clip_fraction        | 0.021        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.448        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 292          |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    reward               | -23.53681    |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 512          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 984        |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 101        |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00418426 |\n",
      "|    clip_fraction        | 0.0434     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | 0.243      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 189        |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.00249   |\n",
      "|    reward               | 4.483776   |\n",
      "|    std                  | 0.974      |\n",
      "|    value_loss           | 434        |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
      "PPO Sharpe Ratio:  -0.13904030996744202\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_4\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 157       |\n",
      "|    time_elapsed    | 73        |\n",
      "|    total_timesteps | 11580     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.32e+03  |\n",
      "|    critic_loss     | 127       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 11479     |\n",
      "|    reward          | 21.589424 |\n",
      "----------------------------------\n",
      "day: 2894, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11232898.75\n",
      "total_reward: 10232898.75\n",
      "total_cost: 998.99\n",
      "total_trades: 2894\n",
      "Sharpe: 0.950\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 151       |\n",
      "|    total_timesteps | 23160     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.06e+03  |\n",
      "|    critic_loss     | 84.2      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 23059     |\n",
      "|    reward          | 21.589424 |\n",
      "----------------------------------\n",
      "day: 2894, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11232898.75\n",
      "total_reward: 10232898.75\n",
      "total_cost: 998.99\n",
      "total_trades: 2894\n",
      "Sharpe: 0.950\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 225       |\n",
      "|    total_timesteps | 34740     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 899       |\n",
      "|    critic_loss     | 163       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 34639     |\n",
      "|    reward          | 21.589424 |\n",
      "----------------------------------\n",
      "day: 2894, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11232898.75\n",
      "total_reward: 10232898.75\n",
      "total_cost: 998.99\n",
      "total_trades: 2894\n",
      "Sharpe: 0.950\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 153       |\n",
      "|    time_elapsed    | 302       |\n",
      "|    total_timesteps | 46320     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 516       |\n",
      "|    critic_loss     | 53.5      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 46219     |\n",
      "|    reward          | 21.589424 |\n",
      "----------------------------------\n",
      "day: 2894, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11232898.75\n",
      "total_reward: 10232898.75\n",
      "total_cost: 998.99\n",
      "total_trades: 2894\n",
      "Sharpe: 0.950\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 151       |\n",
      "|    time_elapsed    | 381       |\n",
      "|    total_timesteps | 57900     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 194       |\n",
      "|    critic_loss     | 97.1      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 57799     |\n",
      "|    reward          | 21.589424 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 151       |\n",
      "|    time_elapsed    | 459       |\n",
      "|    total_timesteps | 69480     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 75.3      |\n",
      "|    critic_loss     | 77.1      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 69379     |\n",
      "|    reward          | 21.589424 |\n",
      "----------------------------------\n",
      "day: 2894, episode: 95\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11232898.75\n",
      "total_reward: 10232898.75\n",
      "total_cost: 998.99\n",
      "total_trades: 2894\n",
      "Sharpe: 0.950\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 151       |\n",
      "|    time_elapsed    | 535       |\n",
      "|    total_timesteps | 81060     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 30        |\n",
      "|    critic_loss     | 36.6      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 80959     |\n",
      "|    reward          | 21.589424 |\n",
      "----------------------------------\n",
      "day: 2894, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11232898.75\n",
      "total_reward: 10232898.75\n",
      "total_cost: 998.99\n",
      "total_trades: 2894\n",
      "Sharpe: 0.950\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 32        |\n",
      "|    fps             | 150       |\n",
      "|    time_elapsed    | 613       |\n",
      "|    total_timesteps | 92640     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.53     |\n",
      "|    critic_loss     | 47        |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 92539     |\n",
      "|    reward          | 21.589424 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2021-07-06 to  2021-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-04\n",
      "======Trading from:  2021-10-04 to  2022-01-03\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2021-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_315_4\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 508         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -0.131      |\n",
      "|    reward             | 0.088259414 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.027       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.42     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.203    |\n",
      "|    reward             | 1.0083646 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.111     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.42      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -1.1       |\n",
      "|    reward             | -2.4520779 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 3.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.42      |\n",
      "|    explained_variance | 0.0498     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -1.85      |\n",
      "|    reward             | -1.1293584 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.95       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 560        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -2.83      |\n",
      "|    reward             | -4.5391073 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 10.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 561        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.0623    |\n",
      "|    reward             | 0.08230206 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.00223    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.328     |\n",
      "|    reward             | 0.2929941 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.1       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 560      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.44    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.000517 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 1.88e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 560      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.45    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.000376 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 6.82e-08 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 564      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.47    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -0.00382 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 7.82e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 564       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.5      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.000254 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 4.82e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 561       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -3.05e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 5.75e-10  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 563      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.59    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.00336  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.19     |\n",
      "|    value_loss         | 7.55e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 559       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -0.000838 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 4.97e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 562      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 13       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.000787 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.34     |\n",
      "|    value_loss         | 1.94e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 564      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.0108  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.44     |\n",
      "|    value_loss         | 6.17e-05 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -2.45      |\n",
      "|    reward             | 0.19901282 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 1.07       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 565         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.8        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -0.0718     |\n",
      "|    reward             | 0.008322592 |\n",
      "|    std                | 1.46        |\n",
      "|    value_loss         | 0.00323     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 567        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 0.299      |\n",
      "|    reward             | 0.07933034 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 0.0828     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 566         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.81       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -0.0726     |\n",
      "|    reward             | 0.010954427 |\n",
      "|    std                | 1.49        |\n",
      "|    value_loss         | 0.0219      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 568           |\n",
      "|    iterations         | 2100          |\n",
      "|    time_elapsed       | 18            |\n",
      "|    total_timesteps    | 10500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.82         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2099          |\n",
      "|    policy_loss        | -0.0175       |\n",
      "|    reward             | -0.0010667125 |\n",
      "|    std                | 1.49          |\n",
      "|    value_loss         | 7.89e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 569            |\n",
      "|    iterations         | 2200           |\n",
      "|    time_elapsed       | 19             |\n",
      "|    total_timesteps    | 11000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.83          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 2199           |\n",
      "|    policy_loss        | -0.00369       |\n",
      "|    reward             | -5.8820628e-05 |\n",
      "|    std                | 1.51           |\n",
      "|    value_loss         | 5.3e-06        |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 568      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.512    |\n",
      "|    reward             | 0.788666 |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 0.112    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 569         |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.84       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | -0.0762     |\n",
      "|    reward             | 0.011722884 |\n",
      "|    std                | 1.52        |\n",
      "|    value_loss         | 0.00282     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 1.08      |\n",
      "|    reward             | 0.3100119 |\n",
      "|    std                | 1.54      |\n",
      "|    value_loss         | 0.447     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 570        |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 3.3        |\n",
      "|    reward             | 0.12189451 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0.00606    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -0.215     |\n",
      "|    reward             | -0.1574573 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 1.18       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | 0.00626   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 5.07      |\n",
      "|    reward             | 1.0198348 |\n",
      "|    std                | 1.52      |\n",
      "|    value_loss         | 82.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 571      |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 172      |\n",
      "|    reward             | 7.71444  |\n",
      "|    std                | 1.53     |\n",
      "|    value_loss         | 3.53e+03 |\n",
      "------------------------------------\n",
      "day: 2957, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6857693.90\n",
      "total_reward: 5857693.90\n",
      "total_cost: 5666.67\n",
      "total_trades: 2943\n",
      "Sharpe: 0.862\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | -0.144     |\n",
      "|    reward             | 0.19065055 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 0.0131     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 570         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.87       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | -0.867      |\n",
      "|    reward             | -0.83130074 |\n",
      "|    std                | 1.57        |\n",
      "|    value_loss         | 1.24        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | 0.0786    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -8.41     |\n",
      "|    reward             | 2.8108628 |\n",
      "|    std                | 1.56      |\n",
      "|    value_loss         | 28.7      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 571         |\n",
      "|    iterations         | 3300        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 16500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.86       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3299        |\n",
      "|    policy_loss        | 4.97        |\n",
      "|    reward             | -0.12993501 |\n",
      "|    std                | 1.55        |\n",
      "|    value_loss         | 14.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -10.8     |\n",
      "|    reward             | 12.849319 |\n",
      "|    std                | 1.56      |\n",
      "|    value_loss         | 63.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 570         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.87       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 40.9        |\n",
      "|    reward             | -0.75693166 |\n",
      "|    std                | 1.58        |\n",
      "|    value_loss         | 955         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.88      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -0.025     |\n",
      "|    reward             | 0.37717232 |\n",
      "|    std                | 1.59       |\n",
      "|    value_loss         | 0.00332    |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -0.785     |\n",
      "|    reward             | -1.0146958 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 2.71       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.88      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | -14        |\n",
      "|    reward             | -0.6943639 |\n",
      "|    std                | 1.58       |\n",
      "|    value_loss         | 74.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 14.4     |\n",
      "|    reward             | 0.2716   |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 38.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 571       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.89     |\n",
      "|    explained_variance | 0.00338   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -15.3     |\n",
      "|    reward             | 3.4371965 |\n",
      "|    std                | 1.6       |\n",
      "|    value_loss         | 130       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.87    |\n",
      "|    explained_variance | -0.00131 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 72.5     |\n",
      "|    reward             | 4.811117 |\n",
      "|    std                | 1.57     |\n",
      "|    value_loss         | 2.56e+03 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 570         |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.87       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | 0.511       |\n",
      "|    reward             | 0.025304753 |\n",
      "|    std                | 1.57        |\n",
      "|    value_loss         | 0.168       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 570        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | -2.14      |\n",
      "|    reward             | -1.7699369 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 2.8        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 570        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 7.2        |\n",
      "|    reward             | -5.7033153 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 32.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 571       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | 4.39      |\n",
      "|    reward             | 0.649102  |\n",
      "|    std                | 1.54      |\n",
      "|    value_loss         | 9.68      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | 0.000634  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | -0.167    |\n",
      "|    reward             | 1.1164323 |\n",
      "|    std                | 1.53      |\n",
      "|    value_loss         | 37.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 570        |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | -0.000239  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -44.2      |\n",
      "|    reward             | -6.6326985 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 1.44e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 570        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | 0.328      |\n",
      "|    reward             | 0.08931012 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 0.144      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 569        |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | 2.01       |\n",
      "|    reward             | -0.5901276 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 2.37       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 2.96      |\n",
      "|    reward             | 3.9655485 |\n",
      "|    std                | 1.52      |\n",
      "|    value_loss         | 5.7       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 571      |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 25500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | -4.78    |\n",
      "|    reward             | 3.039059 |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 10.3     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 571         |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 45          |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | -0.0147     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | 7.64        |\n",
      "|    reward             | 0.035258222 |\n",
      "|    std                | 1.49        |\n",
      "|    value_loss         | 37          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 5300       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 26500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5299       |\n",
      "|    policy_loss        | 43.7       |\n",
      "|    reward             | -12.821083 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 1e+03      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | 1.16       |\n",
      "|    reward             | 0.63002914 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 0.805      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -10.9     |\n",
      "|    reward             | 3.022946  |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 29        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 570        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | 0.258      |\n",
      "|    reward             | -1.1057996 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 1.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -16.5      |\n",
      "|    reward             | -2.5059388 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 164        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 27.9      |\n",
      "|    reward             | 2.2364147 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 489       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 571      |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 29500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.83    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -28.9    |\n",
      "|    reward             | -8.0724  |\n",
      "|    std                | 1.51     |\n",
      "|    value_loss         | 237      |\n",
      "------------------------------------\n",
      "day: 2957, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11319245.87\n",
      "total_reward: 10319245.87\n",
      "total_cost: 3082.16\n",
      "total_trades: 2947\n",
      "Sharpe: 0.948\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 570         |\n",
      "|    iterations         | 6000        |\n",
      "|    time_elapsed       | 52          |\n",
      "|    total_timesteps    | 30000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.83       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5999        |\n",
      "|    policy_loss        | 1.12        |\n",
      "|    reward             | -0.43670768 |\n",
      "|    std                | 1.51        |\n",
      "|    value_loss         | 1.44        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 570       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -0.606    |\n",
      "|    reward             | 0.2007315 |\n",
      "|    std                | 1.52      |\n",
      "|    value_loss         | 0.709     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 571       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 2.23      |\n",
      "|    reward             | 1.1325449 |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 5.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 570        |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | -1.7       |\n",
      "|    reward             | -0.6833505 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 16         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 7.79       |\n",
      "|    reward             | -1.9761037 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 140        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 19.5       |\n",
      "|    reward             | -3.8975527 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 164        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -1.15      |\n",
      "|    reward             | -1.0776067 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 2.67       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | 3.81       |\n",
      "|    reward             | -3.3094535 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 9.7        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 571       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -3.05     |\n",
      "|    reward             | 6.5341735 |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 3.99      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 3.13       |\n",
      "|    reward             | -1.3470082 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 5.87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 1.75       |\n",
      "|    reward             | -1.0755086 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 40.1       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 571          |\n",
      "|    iterations         | 7100         |\n",
      "|    time_elapsed       | 62           |\n",
      "|    total_timesteps    | 35500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.84        |\n",
      "|    explained_variance | 2.38e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7099         |\n",
      "|    policy_loss        | -16          |\n",
      "|    reward             | 0.0016295213 |\n",
      "|    std                | 1.52         |\n",
      "|    value_loss         | 368          |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | -0.928     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | 3.95       |\n",
      "|    reward             | 0.94842744 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 6.3        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -2.74      |\n",
      "|    reward             | -1.4195228 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 4.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | -10.4     |\n",
      "|    reward             | 2.5564342 |\n",
      "|    std                | 1.53      |\n",
      "|    value_loss         | 39.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 571      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 65       |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.85    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | 9.74     |\n",
      "|    reward             | 4.714281 |\n",
      "|    std                | 1.53     |\n",
      "|    value_loss         | 30.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 571       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 19.8      |\n",
      "|    reward             | 7.31497   |\n",
      "|    std                | 1.53      |\n",
      "|    value_loss         | 203       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 571         |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 67          |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.85       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | 0.0413      |\n",
      "|    reward             | 0.009110733 |\n",
      "|    std                | 1.54        |\n",
      "|    value_loss         | 0.00149     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 2.68       |\n",
      "|    reward             | -1.9845552 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 3.62       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -4.24      |\n",
      "|    reward             | 0.38984305 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 14.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | 2.35      |\n",
      "|    reward             | 1.2300004 |\n",
      "|    std                | 1.54      |\n",
      "|    value_loss         | 9.95      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | 19.3       |\n",
      "|    reward             | -1.1211429 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 111        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 11.1       |\n",
      "|    reward             | 0.15926562 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 215        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 72         |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -0.262     |\n",
      "|    reward             | 0.06724357 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 0.0257     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 570         |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 73          |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.85       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | 8.26        |\n",
      "|    reward             | -0.73971933 |\n",
      "|    std                | 1.55        |\n",
      "|    value_loss         | 28.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 570        |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 74         |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | 14.9       |\n",
      "|    reward             | 0.35640308 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 95.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 75         |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | -10.8      |\n",
      "|    reward             | -1.3601063 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 61.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 571        |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 8.47       |\n",
      "|    reward             | -13.682351 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 41.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 571       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 45        |\n",
      "|    reward             | 4.492028  |\n",
      "|    std                | 1.57      |\n",
      "|    value_loss         | 1.29e+03  |\n",
      "-------------------------------------\n",
      "day: 2957, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 572          |\n",
      "|    iterations         | 8900         |\n",
      "|    time_elapsed       | 77           |\n",
      "|    total_timesteps    | 44500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.89        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8899         |\n",
      "|    policy_loss        | -0.758       |\n",
      "|    reward             | -0.091662034 |\n",
      "|    std                | 1.6          |\n",
      "|    value_loss         | 0.343        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.89     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | -2.97     |\n",
      "|    reward             | 1.3117087 |\n",
      "|    std                | 1.6       |\n",
      "|    value_loss         | 4.91      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 571       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.88     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -5.3      |\n",
      "|    reward             | 1.0649302 |\n",
      "|    std                | 1.59      |\n",
      "|    value_loss         | 10.9      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.87    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -10      |\n",
      "|    reward             | -5.06056 |\n",
      "|    std                | 1.57     |\n",
      "|    value_loss         | 26.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.88     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -7.11     |\n",
      "|    reward             | 1.5713298 |\n",
      "|    std                | 1.59      |\n",
      "|    value_loss         | 22.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.88     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 86.2      |\n",
      "|    reward             | 18.115583 |\n",
      "|    std                | 1.59      |\n",
      "|    value_loss         | 1.11e+03  |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 573          |\n",
      "|    iterations         | 9500         |\n",
      "|    time_elapsed       | 82           |\n",
      "|    total_timesteps    | 47500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.88        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9499         |\n",
      "|    policy_loss        | 0.646        |\n",
      "|    reward             | 0.0077112033 |\n",
      "|    std                | 1.59         |\n",
      "|    value_loss         | 0.133        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 0.134      |\n",
      "|    reward             | 0.27940962 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 1.9        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 3.25       |\n",
      "|    reward             | 0.45353463 |\n",
      "|    std                | 1.58       |\n",
      "|    value_loss         | 6.61       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | 0.647      |\n",
      "|    reward             | -1.5453203 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 1.18       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | 5.1       |\n",
      "|    reward             | -4.127229 |\n",
      "|    std                | 1.57      |\n",
      "|    value_loss         | 6.94      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 573         |\n",
      "|    iterations         | 10000       |\n",
      "|    time_elapsed       | 87          |\n",
      "|    total_timesteps    | 50000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.86       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9999        |\n",
      "|    policy_loss        | 15.7        |\n",
      "|    reward             | -0.81866044 |\n",
      "|    std                | 1.56        |\n",
      "|    value_loss         | 502         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 10100      |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 50500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.87      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10099      |\n",
      "|    policy_loss        | -0.203     |\n",
      "|    reward             | 0.09632061 |\n",
      "|    std                | 1.57       |\n",
      "|    value_loss         | 0.0565     |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 573      |\n",
      "|    iterations         | 10200    |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | -0.00197 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -3.72    |\n",
      "|    reward             | -4.2776  |\n",
      "|    std                | 1.56     |\n",
      "|    value_loss         | 15.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | 13        |\n",
      "|    reward             | 1.0883359 |\n",
      "|    std                | 1.54      |\n",
      "|    value_loss         | 59.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 10400     |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 52000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10399     |\n",
      "|    policy_loss        | 0.755     |\n",
      "|    reward             | 1.9785876 |\n",
      "|    std                | 1.53      |\n",
      "|    value_loss         | 0.955     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 10500      |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 52500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10499      |\n",
      "|    policy_loss        | -38.6      |\n",
      "|    reward             | -10.063846 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 418        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 573      |\n",
      "|    iterations         | 10600    |\n",
      "|    time_elapsed       | 92       |\n",
      "|    total_timesteps    | 53000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | -4.86    |\n",
      "|    reward             | 0.718644 |\n",
      "|    std                | 1.55     |\n",
      "|    value_loss         | 178      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 10700      |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 53500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10699      |\n",
      "|    policy_loss        | -0.492     |\n",
      "|    reward             | 0.18470642 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 0.0503     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -3.44     |\n",
      "|    reward             | -3.481037 |\n",
      "|    std                | 1.55      |\n",
      "|    value_loss         | 3.29      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 7.67       |\n",
      "|    reward             | -2.5322936 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 37.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | 4.85       |\n",
      "|    reward             | -1.4584032 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 96         |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | -9.79      |\n",
      "|    reward             | -10.118727 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 80.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | 36         |\n",
      "|    reward             | -5.0396495 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 305        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 574         |\n",
      "|    iterations         | 11300       |\n",
      "|    time_elapsed       | 98          |\n",
      "|    total_timesteps    | 56500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.85       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11299       |\n",
      "|    policy_loss        | 1.12        |\n",
      "|    reward             | -0.52271265 |\n",
      "|    std                | 1.54        |\n",
      "|    value_loss         | 0.583       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | 0.282     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | 1.49      |\n",
      "|    reward             | 1.2044843 |\n",
      "|    std                | 1.54      |\n",
      "|    value_loss         | 1.58      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 11500     |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 57500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11499     |\n",
      "|    policy_loss        | 8.03      |\n",
      "|    reward             | 0.4938675 |\n",
      "|    std                | 1.55      |\n",
      "|    value_loss         | 24.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | 4.11       |\n",
      "|    reward             | 0.24729913 |\n",
      "|    std                | 1.56       |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | -4.73      |\n",
      "|    reward             | -1.9031383 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 13.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 11800      |\n",
      "|    time_elapsed       | 102        |\n",
      "|    total_timesteps    | 59000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11799      |\n",
      "|    policy_loss        | 18         |\n",
      "|    reward             | -17.466455 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 268        |\n",
      "--------------------------------------\n",
      "day: 2957, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 11900     |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 59500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11899     |\n",
      "|    policy_loss        | -0.367    |\n",
      "|    reward             | 0.1900671 |\n",
      "|    std                | 1.53      |\n",
      "|    value_loss         | 0.0636    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 572         |\n",
      "|    iterations         | 12000       |\n",
      "|    time_elapsed       | 104         |\n",
      "|    total_timesteps    | 60000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.85       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11999       |\n",
      "|    policy_loss        | 10.8        |\n",
      "|    reward             | -0.51279783 |\n",
      "|    std                | 1.53        |\n",
      "|    value_loss         | 44.3        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 12100    |\n",
      "|    time_elapsed       | 105      |\n",
      "|    total_timesteps    | 60500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12099    |\n",
      "|    policy_loss        | 7.58     |\n",
      "|    reward             | 6.950777 |\n",
      "|    std                | 1.53     |\n",
      "|    value_loss         | 24.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 12200      |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 61000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12199      |\n",
      "|    policy_loss        | -3.73      |\n",
      "|    reward             | -1.6171389 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 6.29       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 107       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -2.38     |\n",
      "|    reward             | 5.7878103 |\n",
      "|    std                | 1.53      |\n",
      "|    value_loss         | 5.02      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 12400      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 62000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12399      |\n",
      "|    policy_loss        | 94.3       |\n",
      "|    reward             | -2.7239754 |\n",
      "|    std                | 1.53       |\n",
      "|    value_loss         | 2.75e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 12500      |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 62500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.85      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12499      |\n",
      "|    policy_loss        | 1.81       |\n",
      "|    reward             | -0.6596039 |\n",
      "|    std                | 1.54       |\n",
      "|    value_loss         | 1.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 109        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | -3.97      |\n",
      "|    reward             | -0.5967054 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 9.29       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 12700     |\n",
      "|    time_elapsed       | 110       |\n",
      "|    total_timesteps    | 63500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12699     |\n",
      "|    policy_loss        | 0.883     |\n",
      "|    reward             | 2.1490226 |\n",
      "|    std                | 1.52      |\n",
      "|    value_loss         | 1.42      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 111        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | 2.9        |\n",
      "|    reward             | -0.8787088 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 14.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 12900      |\n",
      "|    time_elapsed       | 112        |\n",
      "|    total_timesteps    | 64500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12899      |\n",
      "|    policy_loss        | -2.44      |\n",
      "|    reward             | -1.3392869 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | 23.7      |\n",
      "|    reward             | 4.0931153 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 228       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 572         |\n",
      "|    iterations         | 13100       |\n",
      "|    time_elapsed       | 114         |\n",
      "|    total_timesteps    | 65500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13099       |\n",
      "|    policy_loss        | 0.212       |\n",
      "|    reward             | 0.026797978 |\n",
      "|    std                | 1.5         |\n",
      "|    value_loss         | 0.719       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 572        |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -5.56      |\n",
      "|    reward             | -0.8667675 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 12.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 573         |\n",
      "|    iterations         | 13300       |\n",
      "|    time_elapsed       | 116         |\n",
      "|    total_timesteps    | 66500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.81       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13299       |\n",
      "|    policy_loss        | -39.7       |\n",
      "|    reward             | -0.97797054 |\n",
      "|    std                | 1.48        |\n",
      "|    value_loss         | 444         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 13400      |\n",
      "|    time_elapsed       | 116        |\n",
      "|    total_timesteps    | 67000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13399      |\n",
      "|    policy_loss        | -4.74      |\n",
      "|    reward             | -1.3615835 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 26.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 13500     |\n",
      "|    time_elapsed       | 117       |\n",
      "|    total_timesteps    | 67500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | 0.617     |\n",
      "|    reward             | 4.5074525 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 47.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 13600     |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 68000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13599     |\n",
      "|    policy_loss        | -3.57     |\n",
      "|    reward             | 16.236382 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 39.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 573         |\n",
      "|    iterations         | 13700       |\n",
      "|    time_elapsed       | 119         |\n",
      "|    total_timesteps    | 68500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.83       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13699       |\n",
      "|    policy_loss        | 0.283       |\n",
      "|    reward             | -0.07198247 |\n",
      "|    std                | 1.51        |\n",
      "|    value_loss         | 0.216       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 572       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 120       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.84     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | 0.895     |\n",
      "|    reward             | 1.7029822 |\n",
      "|    std                | 1.52      |\n",
      "|    value_loss         | 0.797     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 13900    |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 69500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13899    |\n",
      "|    policy_loss        | 8.41     |\n",
      "|    reward             | 2.324845 |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 24.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 14000     |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 70000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13999     |\n",
      "|    policy_loss        | 10.4      |\n",
      "|    reward             | 1.7836324 |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 36.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 14100    |\n",
      "|    time_elapsed       | 123      |\n",
      "|    total_timesteps    | 70500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14099    |\n",
      "|    policy_loss        | 23.5     |\n",
      "|    reward             | 8.105659 |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 215      |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 573          |\n",
      "|    iterations         | 14200        |\n",
      "|    time_elapsed       | 123          |\n",
      "|    total_timesteps    | 71000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.83        |\n",
      "|    explained_variance | -1.87        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14199        |\n",
      "|    policy_loss        | -11          |\n",
      "|    reward             | -0.003014703 |\n",
      "|    std                | 1.52         |\n",
      "|    value_loss         | 137          |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 573         |\n",
      "|    iterations         | 14300       |\n",
      "|    time_elapsed       | 124         |\n",
      "|    total_timesteps    | 71500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.83       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14299       |\n",
      "|    policy_loss        | 0.4         |\n",
      "|    reward             | -0.10302841 |\n",
      "|    std                | 1.51        |\n",
      "|    value_loss         | 0.196       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | 0.0687     |\n",
      "|    reward             | 0.75197285 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 5.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 14500     |\n",
      "|    time_elapsed       | 126       |\n",
      "|    total_timesteps    | 72500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | -7.6      |\n",
      "|    reward             | -2.631322 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 30.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | 5.57      |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 16.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | 7.95       |\n",
      "|    reward             | 0.55533075 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 38.5       |\n",
      "--------------------------------------\n",
      "day: 2957, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 573          |\n",
      "|    iterations         | 14800        |\n",
      "|    time_elapsed       | 129          |\n",
      "|    total_timesteps    | 74000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.81        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14799        |\n",
      "|    policy_loss        | 0.159        |\n",
      "|    reward             | -0.005056296 |\n",
      "|    std                | 1.48         |\n",
      "|    value_loss         | 0.00997      |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 14900     |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 74500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14899     |\n",
      "|    policy_loss        | 2.52      |\n",
      "|    reward             | 0.5286476 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 4.06      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 573         |\n",
      "|    iterations         | 15000       |\n",
      "|    time_elapsed       | 130         |\n",
      "|    total_timesteps    | 75000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14999       |\n",
      "|    policy_loss        | -0.672      |\n",
      "|    reward             | -0.07951511 |\n",
      "|    std                | 1.49        |\n",
      "|    value_loss         | 1.5         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 573        |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | -2.1       |\n",
      "|    reward             | 0.40988997 |\n",
      "|    std                | 1.49       |\n",
      "|    value_loss         | 8.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | 25.4      |\n",
      "|    reward             | 6.7059193 |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 288       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | 7.11       |\n",
      "|    reward             | -14.410426 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 44.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 134       |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | 0.00211   |\n",
      "|    reward             | 0.011048  |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 0.0893    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 15500      |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 77500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15499      |\n",
      "|    policy_loss        | -4.11      |\n",
      "|    reward             | 0.17877597 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 11.2       |\n",
      "|    reward             | -0.5334074 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 53         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 15700      |\n",
      "|    time_elapsed       | 136        |\n",
      "|    total_timesteps    | 78500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15699      |\n",
      "|    policy_loss        | -6.72      |\n",
      "|    reward             | -0.1863023 |\n",
      "|    std                | 1.49       |\n",
      "|    value_loss         | 17.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | -9.8      |\n",
      "|    reward             | 1.1016123 |\n",
      "|    std                | 1.46      |\n",
      "|    value_loss         | 60.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 45.1      |\n",
      "|    reward             | 15.364359 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 987       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 574         |\n",
      "|    iterations         | 16000       |\n",
      "|    time_elapsed       | 139         |\n",
      "|    total_timesteps    | 80000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.81       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15999       |\n",
      "|    policy_loss        | 0.495       |\n",
      "|    reward             | -0.06287383 |\n",
      "|    std                | 1.48        |\n",
      "|    value_loss         | 0.0985      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 16100      |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 80500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16099      |\n",
      "|    policy_loss        | 5.45       |\n",
      "|    reward             | -1.0889903 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 10         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 16200      |\n",
      "|    time_elapsed       | 141        |\n",
      "|    total_timesteps    | 81000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16199      |\n",
      "|    policy_loss        | 4.34       |\n",
      "|    reward             | -1.1192287 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 8.09       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 16300     |\n",
      "|    time_elapsed       | 141       |\n",
      "|    total_timesteps    | 81500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16299     |\n",
      "|    policy_loss        | -1.38     |\n",
      "|    reward             | 2.2492948 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 4         |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -19.2     |\n",
      "|    reward             | 4.3843703 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 102       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 574      |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 143      |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | 5.78     |\n",
      "|    reward             | 18.23623 |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 198      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 144        |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | 0.95       |\n",
      "|    reward             | 0.11484693 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 0.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 145        |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -2.62      |\n",
      "|    reward             | 0.85584456 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 3.56       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 146       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | 1.32      |\n",
      "|    reward             | 1.5419774 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 1.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 16900      |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 84500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16899      |\n",
      "|    policy_loss        | -2.51      |\n",
      "|    reward             | -1.5453048 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 2.97       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | 27.9      |\n",
      "|    reward             | 1.3043338 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 197       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | 46.5      |\n",
      "|    reward             | 20.505259 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 675       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 17200      |\n",
      "|    time_elapsed       | 149        |\n",
      "|    total_timesteps    | 86000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17199      |\n",
      "|    policy_loss        | 0.853      |\n",
      "|    reward             | -0.5682767 |\n",
      "|    std                | 1.49       |\n",
      "|    value_loss         | 0.399      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 574      |\n",
      "|    iterations         | 17300    |\n",
      "|    time_elapsed       | 150      |\n",
      "|    total_timesteps    | 86500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.82    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | -4.34    |\n",
      "|    reward             | -4.43537 |\n",
      "|    std                | 1.5      |\n",
      "|    value_loss         | 8.65     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 151        |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | 6.08       |\n",
      "|    reward             | -1.4512674 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 16.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -5.25     |\n",
      "|    reward             | -1.432236 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 6.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 574       |\n",
      "|    iterations         | 17600     |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | 5.55      |\n",
      "|    reward             | -14.87343 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 51.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 153      |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | 42.7     |\n",
      "|    reward             | 12.21933 |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 1.98e+03 |\n",
      "------------------------------------\n",
      "day: 2957, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 17800      |\n",
      "|    time_elapsed       | 154        |\n",
      "|    total_timesteps    | 89000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17799      |\n",
      "|    policy_loss        | 0.652      |\n",
      "|    reward             | 0.09894905 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 0.205      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | -0.501    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | 11.7      |\n",
      "|    reward             | 1.4858553 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 70.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 575         |\n",
      "|    iterations         | 18000       |\n",
      "|    time_elapsed       | 156         |\n",
      "|    total_timesteps    | 90000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17999       |\n",
      "|    policy_loss        | 2.19        |\n",
      "|    reward             | 0.018253958 |\n",
      "|    std                | 1.49        |\n",
      "|    value_loss         | 3.04        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 575         |\n",
      "|    iterations         | 18100       |\n",
      "|    time_elapsed       | 157         |\n",
      "|    total_timesteps    | 90500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18099       |\n",
      "|    policy_loss        | 1.32        |\n",
      "|    reward             | -0.05686092 |\n",
      "|    std                | 1.5         |\n",
      "|    value_loss         | 1.17        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 18200     |\n",
      "|    time_elapsed       | 158       |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | -7.34     |\n",
      "|    reward             | 20.200008 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 43.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 159        |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -21.5      |\n",
      "|    reward             | -16.320282 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 98.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 18400      |\n",
      "|    time_elapsed       | 159        |\n",
      "|    total_timesteps    | 92000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18399      |\n",
      "|    policy_loss        | -0.823     |\n",
      "|    reward             | -1.4081433 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 0.373      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 575         |\n",
      "|    iterations         | 18500       |\n",
      "|    time_elapsed       | 160         |\n",
      "|    total_timesteps    | 92500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | 0.846       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18499       |\n",
      "|    policy_loss        | -1.25       |\n",
      "|    reward             | -0.01975742 |\n",
      "|    std                | 1.49        |\n",
      "|    value_loss         | 1.12        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 18600      |\n",
      "|    time_elapsed       | 161        |\n",
      "|    total_timesteps    | 93000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18599      |\n",
      "|    policy_loss        | -8.23      |\n",
      "|    reward             | 0.98775053 |\n",
      "|    std                | 1.49       |\n",
      "|    value_loss         | 27         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 18700      |\n",
      "|    time_elapsed       | 162        |\n",
      "|    total_timesteps    | 93500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18699      |\n",
      "|    policy_loss        | 0.255      |\n",
      "|    reward             | -1.5790762 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 0.865      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 18800     |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | 2.47      |\n",
      "|    reward             | 1.0594448 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 3.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 18900      |\n",
      "|    time_elapsed       | 164        |\n",
      "|    total_timesteps    | 94500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18899      |\n",
      "|    policy_loss        | -11.9      |\n",
      "|    reward             | -31.007097 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 77.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 19000      |\n",
      "|    time_elapsed       | 165        |\n",
      "|    total_timesteps    | 95000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18999      |\n",
      "|    policy_loss        | -0.0322    |\n",
      "|    reward             | -0.7489647 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 0.0169     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 19100      |\n",
      "|    time_elapsed       | 166        |\n",
      "|    total_timesteps    | 95500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19099      |\n",
      "|    policy_loss        | 6.88       |\n",
      "|    reward             | 0.44565153 |\n",
      "|    std                | 1.49       |\n",
      "|    value_loss         | 15.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 19200     |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 96000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19199     |\n",
      "|    policy_loss        | -17.6     |\n",
      "|    reward             | 1.4143552 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 113       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 19300     |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 96500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19299     |\n",
      "|    policy_loss        | 6.02      |\n",
      "|    reward             | 1.7693589 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 16.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 19400      |\n",
      "|    time_elapsed       | 168        |\n",
      "|    total_timesteps    | 97000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19399      |\n",
      "|    policy_loss        | 19.9       |\n",
      "|    reward             | 0.60822684 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 51.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 19500      |\n",
      "|    time_elapsed       | 169        |\n",
      "|    total_timesteps    | 97500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19499      |\n",
      "|    policy_loss        | 35.9       |\n",
      "|    reward             | -12.499265 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 412        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 576       |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 170       |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 1.86      |\n",
      "|    reward             | 0.8343681 |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 2.35      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 575         |\n",
      "|    iterations         | 19700       |\n",
      "|    time_elapsed       | 171         |\n",
      "|    total_timesteps    | 98500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.8        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19699       |\n",
      "|    policy_loss        | 3.84        |\n",
      "|    reward             | -0.19638552 |\n",
      "|    std                | 1.47        |\n",
      "|    value_loss         | 7.39        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | -4.81     |\n",
      "|    reward             | -5.730747 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 18.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | 2.34      |\n",
      "|    reward             | 0.6496481 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 8.23      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 575         |\n",
      "|    iterations         | 20000       |\n",
      "|    time_elapsed       | 173         |\n",
      "|    total_timesteps    | 100000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.81       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19999       |\n",
      "|    policy_loss        | 11          |\n",
      "|    reward             | -0.11823091 |\n",
      "|    std                | 1.48        |\n",
      "|    value_loss         | 64          |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2021-10-04 to  2022-01-03\n",
      "A2C Sharpe Ratio:  0.3868219861168162\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_315_4\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1024        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.003944106 |\n",
      "------------------------------------\n",
      "day: 2957, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1424997.59\n",
      "total_reward: 424997.59\n",
      "total_cost: 6744.77\n",
      "total_trades: 2809\n",
      "Sharpe: 0.539\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 984          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040402142 |\n",
      "|    clip_fraction        | 0.0162       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00715     |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    reward               | 0.11903163   |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 0.00158      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 961          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062934104 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.22         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    reward               | 0.0029785086 |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 2.33         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 964          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028690754 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.143        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    reward               | 0.14978985   |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 0.301        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 967          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027903207 |\n",
      "|    clip_fraction        | 0.0131       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.133        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000605    |\n",
      "|    reward               | -0.13646743  |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 0.397        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 965         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006667533 |\n",
      "|    clip_fraction        | 0.0524      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.87        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    reward               | 0.047752276 |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 10.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 968          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037385023 |\n",
      "|    clip_fraction        | 0.0147       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.000341     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 10           |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    reward               | 0.0033261734 |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 24.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 965          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037730704 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00151     |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.000484    |\n",
      "|    reward               | -0.024449073 |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 0.0295       |\n",
      "------------------------------------------\n",
      "day: 2957, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1609580.34\n",
      "total_reward: 609580.34\n",
      "total_cost: 7110.09\n",
      "total_trades: 2906\n",
      "Sharpe: 0.600\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 963          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047351876 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0619       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    reward               | 0.0029229883 |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 0.245        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 961          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.232244e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.000132    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.91         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | 0.000104     |\n",
      "|    reward               | -3.6490312   |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 4.27         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 960         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002754216 |\n",
      "|    clip_fraction        | 0.00303     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.15        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00073    |\n",
      "|    reward               | -0.04593843 |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 15.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 962          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045276014 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0032      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.91         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00044     |\n",
      "|    reward               | 0.2298981    |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 8.29         |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 958            |\n",
      "|    iterations           | 13             |\n",
      "|    time_elapsed         | 27             |\n",
      "|    total_timesteps      | 26624          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.004350623    |\n",
      "|    clip_fraction        | 0.011          |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | -0.0511        |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 23.7           |\n",
      "|    n_updates            | 120            |\n",
      "|    policy_gradient_loss | -0.00132       |\n",
      "|    reward               | -0.00010958461 |\n",
      "|    std                  | 0.997          |\n",
      "|    value_loss           | 38.4           |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 961           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 29            |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018802966 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.0207        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 30.3          |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.00015      |\n",
      "|    reward               | 1.2474991     |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 69.1          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 960         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002764008 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.43        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000479   |\n",
      "|    reward               | -0.22829624 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.958       |\n",
      "-----------------------------------------\n",
      "day: 2957, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4737336.79\n",
      "total_reward: 3737336.79\n",
      "total_cost: 6955.76\n",
      "total_trades: 2928\n",
      "Sharpe: 0.851\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 959           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00071533397 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.0724        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 15.3          |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000111     |\n",
      "|    reward               | 0.058798365   |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 46.4          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 959          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035551698 |\n",
      "|    clip_fraction        | 0.00747      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.0938       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 44.8         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    reward               | 0.5908115    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 960          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028363473 |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.235        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.45         |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00141     |\n",
      "|    reward               | -0.1975316   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 18.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 959          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013191504 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.164        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 61.3         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    reward               | 0.070991315  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 960           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 42            |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055828725 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.123         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 79.8          |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.00047      |\n",
      "|    reward               | 0.54555213    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 214           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 960          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043440815 |\n",
      "|    clip_fraction        | 0.0332       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.391        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 20.3         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00362     |\n",
      "|    reward               | -0.26592675  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 32.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 960          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017311875 |\n",
      "|    clip_fraction        | 0.00776      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.249        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 55.6         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00209     |\n",
      "|    reward               | 0.050720513  |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 133          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 959         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003788801 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.388       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 40.7        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00275    |\n",
      "|    reward               | 14.246089   |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 107         |\n",
      "-----------------------------------------\n",
      "day: 2957, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5961928.98\n",
      "total_reward: 4961928.98\n",
      "total_cost: 7025.48\n",
      "total_trades: 2940\n",
      "Sharpe: 0.872\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 957          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018805934 |\n",
      "|    clip_fraction        | 0.0207       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.369        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 58.2         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000112    |\n",
      "|    reward               | 0.034167252  |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 958          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018755817 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.414        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 19.9         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00028     |\n",
      "|    reward               | 0.08853187   |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 59.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 958          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034243013 |\n",
      "|    clip_fraction        | 0.0291       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.397        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 59.9         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000959    |\n",
      "|    reward               | 0.0002517393 |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 177          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 959          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015199627 |\n",
      "|    clip_fraction        | 0.0211       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.221        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 89           |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | 0.000742     |\n",
      "|    reward               | -0.2697884   |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 197          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 958          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032844949 |\n",
      "|    clip_fraction        | 0.0244       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.87         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.47         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000177    |\n",
      "|    reward               | 0.48627493   |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 10.8         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 959           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 61            |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009706393   |\n",
      "|    clip_fraction        | 0.0964        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.405         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 96.2          |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | 0.00258       |\n",
      "|    reward               | -0.0019853092 |\n",
      "|    std                  | 0.989         |\n",
      "|    value_loss           | 169           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 957           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 64            |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015238306 |\n",
      "|    clip_fraction        | 0.0216        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.0652        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 26.6          |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000387     |\n",
      "|    reward               | 0.6972476     |\n",
      "|    std                  | 0.991         |\n",
      "|    value_loss           | 56.8          |\n",
      "-------------------------------------------\n",
      "day: 2957, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1648045.69\n",
      "total_reward: 648045.69\n",
      "total_cost: 7051.96\n",
      "total_trades: 2922\n",
      "Sharpe: 0.710\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 956           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 66            |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00080709584 |\n",
      "|    clip_fraction        | 0.0181        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.174         |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000509     |\n",
      "|    reward               | -0.0093242945 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 0.394         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 956           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 68            |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0030113787  |\n",
      "|    clip_fraction        | 0.00659       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.96          |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000191     |\n",
      "|    reward               | 0.00022501711 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 3.04          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 956         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007579547 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0551      |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    reward               | 0.05284359  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0762      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 957         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003216473 |\n",
      "|    clip_fraction        | 0.00703     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.426       |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.000182   |\n",
      "|    reward               | 0.18922867  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.978       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 956          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.892623e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5            |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -2.2e-05     |\n",
      "|    reward               | -0.35230935  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 7.5          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 956         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 77          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004074894 |\n",
      "|    clip_fraction        | 0.00488     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.375       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.001      |\n",
      "|    reward               | 0.16131744  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 47.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 956          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011239199 |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.57         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 59.9         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | 0.00218      |\n",
      "|    reward               | -0.52239007  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 75.4         |\n",
      "------------------------------------------\n",
      "day: 2957, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5255873.81\n",
      "total_reward: 4255873.81\n",
      "total_cost: 7276.37\n",
      "total_trades: 2933\n",
      "Sharpe: 0.861\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 956          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004058226 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.548        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 17.5         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00124     |\n",
      "|    reward               | -0.0754753   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 41.5         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 955           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 83            |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0025319594  |\n",
      "|    clip_fraction        | 0.00996       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.494         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 46.6          |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.00116      |\n",
      "|    reward               | -0.0018556803 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 132           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 955          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046470207 |\n",
      "|    clip_fraction        | 0.0228       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.499        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 59.4         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0011      |\n",
      "|    reward               | 1.1215727    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 955         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002387147 |\n",
      "|    clip_fraction        | 0.0111      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.07        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00019    |\n",
      "|    reward               | 0.25503471  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 10          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 956          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.001565739  |\n",
      "|    clip_fraction        | 0.00894      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.508        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 90.4         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    reward               | -0.019361459 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 160          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 957          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035682083 |\n",
      "|    clip_fraction        | 0.00918      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.521        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 40.7         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    reward               | -1.1258502   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 128          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 956          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 94           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012009881 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.55         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.64         |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | 4.63e-05     |\n",
      "|    reward               | -0.08668536  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "day: 2957, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4659383.08\n",
      "total_reward: 3659383.08\n",
      "total_cost: 7280.00\n",
      "total_trades: 2932\n",
      "Sharpe: 0.830\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 956         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004306216 |\n",
      "|    clip_fraction        | 0.00977     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.619       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 66.9        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.00109     |\n",
      "|    reward               | -0.27630743 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 89.3        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 955           |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 98            |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052561495 |\n",
      "|    clip_fraction        | 0.00181       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.627         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 73.2          |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -0.000154     |\n",
      "|    reward               | 4.4700294     |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 99.3          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 955          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042755352 |\n",
      "|    clip_fraction        | 0.0352       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.722        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 20.4         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    reward               | -1.48601     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 36.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 955          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021494282 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.53         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 48           |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00057     |\n",
      "|    reward               | 0.7746463    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 147          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 956          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037282459 |\n",
      "|    clip_fraction        | 0.00586      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.516        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 51.9         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000799    |\n",
      "|    reward               | -3.527217    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 130          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-10-04 to  2022-01-03\n",
      "PPO Sharpe Ratio:  0.37905814995857795\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_315_4\n",
      "day: 2957, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 76       |\n",
      "|    total_timesteps | 11832    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -223     |\n",
      "|    critic_loss     | 36       |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 11731    |\n",
      "|    reward          | 9.243157 |\n",
      "---------------------------------\n",
      "day: 2957, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 156      |\n",
      "|    total_timesteps | 23664    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -127     |\n",
      "|    critic_loss     | 90.6     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 23563    |\n",
      "|    reward          | 9.243157 |\n",
      "---------------------------------\n",
      "day: 2957, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 35496    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -81.2    |\n",
      "|    critic_loss     | 87.3     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 35395    |\n",
      "|    reward          | 9.243157 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 315      |\n",
      "|    total_timesteps | 47328    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -70.5    |\n",
      "|    critic_loss     | 36.1     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 47227    |\n",
      "|    reward          | 9.243157 |\n",
      "---------------------------------\n",
      "day: 2957, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 397      |\n",
      "|    total_timesteps | 59160    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.3    |\n",
      "|    critic_loss     | 84.4     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 59059    |\n",
      "|    reward          | 9.243157 |\n",
      "---------------------------------\n",
      "day: 2957, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 478      |\n",
      "|    total_timesteps | 70992    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -48.4    |\n",
      "|    critic_loss     | 64.8     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 70891    |\n",
      "|    reward          | 9.243157 |\n",
      "---------------------------------\n",
      "day: 2957, episode: 95\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 559      |\n",
      "|    total_timesteps | 82824    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -36.5    |\n",
      "|    critic_loss     | 53.4     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 82723    |\n",
      "|    reward          | 9.243157 |\n",
      "---------------------------------\n",
      "day: 2957, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11465943.63\n",
      "total_reward: 10465943.63\n",
      "total_cost: 998.99\n",
      "total_trades: 2957\n",
      "Sharpe: 0.942\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 640      |\n",
      "|    total_timesteps | 94656    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.8    |\n",
      "|    critic_loss     | 56.3     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 94555    |\n",
      "|    reward          | 9.243157 |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2021-10-04 to  2022-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-03\n",
      "======Trading from:  2022-01-03 to  2022-04-04\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2022-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_378_3\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 489         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.47       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -0.00584    |\n",
      "|    reward             | 0.039802812 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.00302     |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 553            |\n",
      "|    iterations         | 200            |\n",
      "|    time_elapsed       | 1              |\n",
      "|    total_timesteps    | 1000           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.49          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 199            |\n",
      "|    policy_loss        | -0.00148       |\n",
      "|    reward             | -2.5752204e-05 |\n",
      "|    std                | 1.07           |\n",
      "|    value_loss         | 2.03e-06       |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 582         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.5        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -0.0392     |\n",
      "|    reward             | -0.06048526 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.00149     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 578          |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 3            |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.51        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -0.00378     |\n",
      "|    reward             | -0.003062454 |\n",
      "|    std                | 1.1          |\n",
      "|    value_loss         | 3.5e-05      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 584         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.54       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -0.158      |\n",
      "|    reward             | -0.12732174 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.0153      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 586        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | -0.05      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 4.36       |\n",
      "|    reward             | -1.1388034 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 22.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 582        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -0.835     |\n",
      "|    reward             | -0.2742214 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 0.447      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 586      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 1.97     |\n",
      "|    reward             | 1.007974 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 2.32     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 588        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -11.2      |\n",
      "|    reward             | -0.7577023 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 47.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 582        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 10.6       |\n",
      "|    reward             | -0.9314306 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 89.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 585      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.00926  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 9.15     |\n",
      "|    reward             | 4.894378 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 46       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | -0.0253  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 2.41     |\n",
      "|    reward             | 7.102266 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 10.2     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 574         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.55       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -0.672      |\n",
      "|    reward             | 0.069576934 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 0.212       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 574      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.553    |\n",
      "|    reward             | 0.253907 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 0.166    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 577       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 0.347     |\n",
      "|    reward             | 0.3367503 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.241     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.53      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 1.21       |\n",
      "|    reward             | -0.3448914 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 0.72       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 576       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -6.27     |\n",
      "|    reward             | 2.4566958 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 575        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.52      |\n",
      "|    explained_variance | -0.0745    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -1.74      |\n",
      "|    reward             | -13.235807 |\n",
      "|    std                | 1.11       |\n",
      "|    value_loss         | 42.8       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 572          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 16           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.54        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 0.322        |\n",
      "|    reward             | -0.048758138 |\n",
      "|    std                | 1.13         |\n",
      "|    value_loss         | 0.0463       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 574          |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 17           |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.54        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | -0.272       |\n",
      "|    reward             | -0.017991444 |\n",
      "|    std                | 1.13         |\n",
      "|    value_loss         | 0.0488       |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 572      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.00466  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 2.88e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 573       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | -0.000511 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 8.43e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 572            |\n",
      "|    iterations         | 2300           |\n",
      "|    time_elapsed       | 20             |\n",
      "|    total_timesteps    | 11500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.58          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 2299           |\n",
      "|    policy_loss        | -0.0123        |\n",
      "|    reward             | -0.00038955297 |\n",
      "|    std                | 1.18           |\n",
      "|    value_loss         | 7.89e-05       |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 574        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.61      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | -0.593     |\n",
      "|    reward             | 0.24290271 |\n",
      "|    std                | 1.21       |\n",
      "|    value_loss         | 0.113      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 576        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.62      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -1.18      |\n",
      "|    reward             | 0.42421275 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 0.927      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 574         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.62       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | 3.94        |\n",
      "|    reward             | -0.24948497 |\n",
      "|    std                | 1.22        |\n",
      "|    value_loss         | 11.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 576        |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.63      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | -1.84      |\n",
      "|    reward             | -2.2148838 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 10.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 14000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 11.3     |\n",
      "|    reward             | 3.661527 |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 85.9     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.63     |\n",
      "|    explained_variance | 0.00238   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -14.9     |\n",
      "|    reward             | -2.659409 |\n",
      "|    std                | 1.24      |\n",
      "|    value_loss         | 87        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 576        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.62      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | -18.5      |\n",
      "|    reward             | -2.6495771 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 298        |\n",
      "--------------------------------------\n",
      "day: 3020, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11508082.76\n",
      "total_reward: 10508082.76\n",
      "total_cost: 3513.50\n",
      "total_trades: 3008\n",
      "Sharpe: 0.989\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 575         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.63       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 1.64        |\n",
      "|    reward             | -0.07277511 |\n",
      "|    std                | 1.23        |\n",
      "|    value_loss         | 2.6         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 576         |\n",
      "|    iterations         | 3200        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 16000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.63       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3199        |\n",
      "|    policy_loss        | 0.716       |\n",
      "|    reward             | -0.36340624 |\n",
      "|    std                | 1.23        |\n",
      "|    value_loss         | 0.287       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 577        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.63      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 3.22       |\n",
      "|    reward             | -1.7714069 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 14.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 576       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.63     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | 0.445     |\n",
      "|    reward             | 1.0830057 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 0.308     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 17500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -0.253   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 0.0355   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 18000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | 0.0314   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.23     |\n",
      "|    value_loss         | 0.000251 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 577           |\n",
      "|    iterations         | 3700          |\n",
      "|    time_elapsed       | 32            |\n",
      "|    total_timesteps    | 18500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.64         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3699          |\n",
      "|    policy_loss        | 7.98e-05      |\n",
      "|    reward             | -8.232356e-05 |\n",
      "|    std                | 1.25          |\n",
      "|    value_loss         | 3.73e-09      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 32       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.000529 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 5.27e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 19500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.00489  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 1.06e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 578       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.73     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | -0.000274 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.37      |\n",
      "|    value_loss         | 3.66e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 20500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.78    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | -0.00306 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.44     |\n",
      "|    value_loss         | 4.22e-06 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 577           |\n",
      "|    iterations         | 4200          |\n",
      "|    time_elapsed       | 36            |\n",
      "|    total_timesteps    | 21000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.81         |\n",
      "|    explained_variance | 1.79e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4199          |\n",
      "|    policy_loss        | 0.00388       |\n",
      "|    reward             | -0.0004986779 |\n",
      "|    std                | 1.48          |\n",
      "|    value_loss         | 8.47e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.84    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | 0.000234 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.52     |\n",
      "|    value_loss         | 5.87e-08 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 576            |\n",
      "|    iterations         | 4400           |\n",
      "|    time_elapsed       | 38             |\n",
      "|    total_timesteps    | 22000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.87          |\n",
      "|    explained_variance | 5.96e-08       |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 4399           |\n",
      "|    policy_loss        | -0.000412      |\n",
      "|    reward             | -5.4622098e-05 |\n",
      "|    std                | 1.57           |\n",
      "|    value_loss         | 7.76e-08       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 577            |\n",
      "|    iterations         | 4500           |\n",
      "|    time_elapsed       | 38             |\n",
      "|    total_timesteps    | 22500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.92          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 4499           |\n",
      "|    policy_loss        | 0.000307       |\n",
      "|    reward             | -2.9307303e-06 |\n",
      "|    std                | 1.66           |\n",
      "|    value_loss         | 3.49e-08       |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.96    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.00905 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.73     |\n",
      "|    value_loss         | 2.87e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.00075  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.8      |\n",
      "|    value_loss         | 2.17e-07 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 4800        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 24000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4799        |\n",
      "|    policy_loss        | -0.405      |\n",
      "|    reward             | 0.066112034 |\n",
      "|    std                | 1.82        |\n",
      "|    value_loss         | 0.0901      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 579           |\n",
      "|    iterations         | 4900          |\n",
      "|    time_elapsed       | 42            |\n",
      "|    total_timesteps    | 24500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.02         |\n",
      "|    explained_variance | 1.79e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4899          |\n",
      "|    policy_loss        | 0.0254        |\n",
      "|    reward             | -0.0071170507 |\n",
      "|    std                | 1.82          |\n",
      "|    value_loss         | 0.000262      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 578        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | -0.188     |\n",
      "|    reward             | 0.01367366 |\n",
      "|    std                | 1.84       |\n",
      "|    value_loss         | 0.0117     |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 579          |\n",
      "|    iterations         | 5100         |\n",
      "|    time_elapsed       | 44           |\n",
      "|    total_timesteps    | 25500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.03        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5099         |\n",
      "|    policy_loss        | -0.0881      |\n",
      "|    reward             | -0.039116476 |\n",
      "|    std                | 1.85         |\n",
      "|    value_loss         | 0.00224      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 578           |\n",
      "|    iterations         | 5200          |\n",
      "|    time_elapsed       | 44            |\n",
      "|    total_timesteps    | 26000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.04         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5199          |\n",
      "|    policy_loss        | -0.00771      |\n",
      "|    reward             | -0.0008741126 |\n",
      "|    std                | 1.87          |\n",
      "|    value_loss         | 2.1e-05       |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 578       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 0.00701   |\n",
      "|    reward             | 0.0046416 |\n",
      "|    std                | 1.9       |\n",
      "|    value_loss         | 2.35e-05  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 578         |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | -0.0146     |\n",
      "|    reward             | 0.023015704 |\n",
      "|    std                | 1.9         |\n",
      "|    value_loss         | 0.00152     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 578          |\n",
      "|    iterations         | 5500         |\n",
      "|    time_elapsed       | 47           |\n",
      "|    total_timesteps    | 27500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.07        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5499         |\n",
      "|    policy_loss        | 0.0324       |\n",
      "|    reward             | 0.0017914904 |\n",
      "|    std                | 1.91         |\n",
      "|    value_loss         | 0.000925     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 48          |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.08       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | 2.2         |\n",
      "|    reward             | -0.08530296 |\n",
      "|    std                | 1.94        |\n",
      "|    value_loss         | 0.495       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | -1.48      |\n",
      "|    reward             | -0.9818758 |\n",
      "|    std                | 1.95       |\n",
      "|    value_loss         | 1.89       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -0.614    |\n",
      "|    reward             | 1.0063859 |\n",
      "|    std                | 1.94      |\n",
      "|    value_loss         | 0.154     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | 0.964     |\n",
      "|    reward             | 1.5670102 |\n",
      "|    std                | 1.94      |\n",
      "|    value_loss         | 0.813     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -25.5     |\n",
      "|    reward             | 4.3206177 |\n",
      "|    std                | 1.95      |\n",
      "|    value_loss         | 309       |\n",
      "-------------------------------------\n",
      "day: 3020, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6249611.01\n",
      "total_reward: 5249611.01\n",
      "total_cost: 9698.84\n",
      "total_trades: 3002\n",
      "Sharpe: 0.916\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.09      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | -0.286     |\n",
      "|    reward             | 0.06623975 |\n",
      "|    std                | 1.96       |\n",
      "|    value_loss         | 0.0229     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | -0.628      |\n",
      "|    reward             | -0.24108513 |\n",
      "|    std                | 1.99        |\n",
      "|    value_loss         | 0.153       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 4.22       |\n",
      "|    reward             | -0.1233375 |\n",
      "|    std                | 1.99       |\n",
      "|    value_loss         | 4.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 4.93      |\n",
      "|    reward             | 0.7842028 |\n",
      "|    std                | 2         |\n",
      "|    value_loss         | 12.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 1.88e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 15.2       |\n",
      "|    reward             | 0.19529147 |\n",
      "|    std                | 2.01       |\n",
      "|    value_loss         | 54.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 20.8      |\n",
      "|    reward             | -4.170594 |\n",
      "|    std                | 2.02      |\n",
      "|    value_loss         | 418       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 579          |\n",
      "|    iterations         | 6700         |\n",
      "|    time_elapsed       | 57           |\n",
      "|    total_timesteps    | 33500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.13        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6699         |\n",
      "|    policy_loss        | -0.999       |\n",
      "|    reward             | -0.022122625 |\n",
      "|    std                | 2.03         |\n",
      "|    value_loss         | 0.184        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -1.19     |\n",
      "|    reward             | -8.623373 |\n",
      "|    std                | 2.03      |\n",
      "|    value_loss         | 4.35      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -0.737    |\n",
      "|    reward             | 4.3631444 |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 16        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 0.101      |\n",
      "|    reward             | -0.8308991 |\n",
      "|    std                | 2.02       |\n",
      "|    value_loss         | 0.649      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 9.01      |\n",
      "|    reward             | 4.3872585 |\n",
      "|    std                | 2         |\n",
      "|    value_loss         | 69.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 7200       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 36000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7199       |\n",
      "|    policy_loss        | 31.2       |\n",
      "|    reward             | -23.017576 |\n",
      "|    std                | 2          |\n",
      "|    value_loss         | 497        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 7300        |\n",
      "|    time_elapsed       | 62          |\n",
      "|    total_timesteps    | 36500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7299        |\n",
      "|    policy_loss        | 0.114       |\n",
      "|    reward             | 0.055223204 |\n",
      "|    std                | 2           |\n",
      "|    value_loss         | 0.00549     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | -0.5032624 |\n",
      "|    std                | 2          |\n",
      "|    value_loss         | 30.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -8.65     |\n",
      "|    reward             | 4.8477774 |\n",
      "|    std                | 2.02      |\n",
      "|    value_loss         | 33.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 65       |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -5.7     |\n",
      "|    reward             | -1.23139 |\n",
      "|    std                | 2        |\n",
      "|    value_loss         | 13.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | 13.9       |\n",
      "|    reward             | -12.172791 |\n",
      "|    std                | 2.01       |\n",
      "|    value_loss         | 77.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.1      |\n",
      "|    explained_variance | 7.7e-05   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | -66.8     |\n",
      "|    reward             | 2.8805857 |\n",
      "|    std                | 1.97      |\n",
      "|    value_loss         | 585       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 7900        |\n",
      "|    time_elapsed       | 68          |\n",
      "|    total_timesteps    | 39500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.09       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7899        |\n",
      "|    policy_loss        | 0.0171      |\n",
      "|    reward             | 0.013478519 |\n",
      "|    std                | 1.95        |\n",
      "|    value_loss         | 0.181       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -4.44      |\n",
      "|    reward             | 0.17001511 |\n",
      "|    std                | 1.96       |\n",
      "|    value_loss         | 7.28       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 8100        |\n",
      "|    time_elapsed       | 69          |\n",
      "|    total_timesteps    | 40500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.1        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8099        |\n",
      "|    policy_loss        | -0.236      |\n",
      "|    reward             | -0.34668314 |\n",
      "|    std                | 1.98        |\n",
      "|    value_loss         | 0.357       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.1       |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -12.1      |\n",
      "|    reward             | -5.8612432 |\n",
      "|    std                | 1.97       |\n",
      "|    value_loss         | 34         |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 581      |\n",
      "|    iterations         | 8300     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 41500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8299     |\n",
      "|    policy_loss        | -15      |\n",
      "|    reward             | 4.249633 |\n",
      "|    std                | 1.99     |\n",
      "|    value_loss         | 119      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 72          |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | -17.9       |\n",
      "|    reward             | -0.71988887 |\n",
      "|    std                | 2           |\n",
      "|    value_loss         | 147         |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 579          |\n",
      "|    iterations         | 8500         |\n",
      "|    time_elapsed       | 73           |\n",
      "|    total_timesteps    | 42500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.11        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 8499         |\n",
      "|    policy_loss        | 0.0973       |\n",
      "|    reward             | -0.012846114 |\n",
      "|    std                | 1.99         |\n",
      "|    value_loss         | 0.135        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.12     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | -5.3      |\n",
      "|    reward             | -4.343948 |\n",
      "|    std                | 2.01      |\n",
      "|    value_loss         | 7         |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 1.46      |\n",
      "|    reward             | 2.5354204 |\n",
      "|    std                | 2         |\n",
      "|    value_loss         | 2.45      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -1.45     |\n",
      "|    reward             | 1.9426367 |\n",
      "|    std                | 2         |\n",
      "|    value_loss         | 1.67      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 578        |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | 14.1       |\n",
      "|    reward             | -1.0034039 |\n",
      "|    std                | 2          |\n",
      "|    value_loss         | 77.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 77         |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 50.7       |\n",
      "|    reward             | -29.843391 |\n",
      "|    std                | 2          |\n",
      "|    value_loss         | 915        |\n",
      "--------------------------------------\n",
      "day: 3020, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14167791.20\n",
      "total_reward: 13167791.20\n",
      "total_cost: 1666.41\n",
      "total_trades: 3015\n",
      "Sharpe: 1.006\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 9100        |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 45500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9099        |\n",
      "|    policy_loss        | 0.736       |\n",
      "|    reward             | -0.21561961 |\n",
      "|    std                | 2.02        |\n",
      "|    value_loss         | 0.19        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 3.88       |\n",
      "|    reward             | -0.6424737 |\n",
      "|    std                | 2.02       |\n",
      "|    value_loss         | 6.49       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 578        |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 80         |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | 7.16       |\n",
      "|    reward             | 0.37755397 |\n",
      "|    std                | 2.01       |\n",
      "|    value_loss         | 18.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 4.02      |\n",
      "|    reward             | 4.6883755 |\n",
      "|    std                | 2.03      |\n",
      "|    value_loss         | 2.62      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 26.2       |\n",
      "|    reward             | -3.4729369 |\n",
      "|    std                | 2.05       |\n",
      "|    value_loss         | 164        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 81.4      |\n",
      "|    reward             | -22.00225 |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 2.31e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 9700        |\n",
      "|    time_elapsed       | 83          |\n",
      "|    total_timesteps    | 48500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9699        |\n",
      "|    policy_loss        | -0.092      |\n",
      "|    reward             | 0.061289843 |\n",
      "|    std                | 2.05        |\n",
      "|    value_loss         | 0.00877     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 9800       |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 49000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9799       |\n",
      "|    policy_loss        | 2.31       |\n",
      "|    reward             | -1.6497757 |\n",
      "|    std                | 2.02       |\n",
      "|    value_loss         | 2.11       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | 2.25      |\n",
      "|    reward             | 2.4758227 |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 2.33      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 10000      |\n",
      "|    time_elapsed       | 86         |\n",
      "|    total_timesteps    | 50000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.13      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9999       |\n",
      "|    policy_loss        | 13.4       |\n",
      "|    reward             | 0.46937305 |\n",
      "|    std                | 2.04       |\n",
      "|    value_loss         | 19.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 10100      |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 50500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10099      |\n",
      "|    policy_loss        | 38.5       |\n",
      "|    reward             | 0.27054864 |\n",
      "|    std                | 2.07       |\n",
      "|    value_loss         | 316        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 10200    |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 51000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10199    |\n",
      "|    policy_loss        | -27.8    |\n",
      "|    reward             | 5.77739  |\n",
      "|    std                | 2.06     |\n",
      "|    value_loss         | 441      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 10300       |\n",
      "|    time_elapsed       | 88          |\n",
      "|    total_timesteps    | 51500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.15       |\n",
      "|    explained_variance | -3.58e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10299       |\n",
      "|    policy_loss        | 0.734       |\n",
      "|    reward             | -0.12086768 |\n",
      "|    std                | 2.08        |\n",
      "|    value_loss         | 0.121       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 10400       |\n",
      "|    time_elapsed       | 89          |\n",
      "|    total_timesteps    | 52000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.16       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10399       |\n",
      "|    policy_loss        | -0.181      |\n",
      "|    reward             | -0.08772186 |\n",
      "|    std                | 2.1         |\n",
      "|    value_loss         | 1.1         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 10500       |\n",
      "|    time_elapsed       | 90          |\n",
      "|    total_timesteps    | 52500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.16       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10499       |\n",
      "|    policy_loss        | 2.13        |\n",
      "|    reward             | -0.89058584 |\n",
      "|    std                | 2.09        |\n",
      "|    value_loss         | 2.12        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 580      |\n",
      "|    iterations         | 10600    |\n",
      "|    time_elapsed       | 91       |\n",
      "|    total_timesteps    | 53000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10599    |\n",
      "|    policy_loss        | 1.3      |\n",
      "|    reward             | 3.623522 |\n",
      "|    std                | 2.09     |\n",
      "|    value_loss         | 1.08     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 10700       |\n",
      "|    time_elapsed       | 92          |\n",
      "|    total_timesteps    | 53500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.15       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10699       |\n",
      "|    policy_loss        | 8.36        |\n",
      "|    reward             | -0.44927108 |\n",
      "|    std                | 2.08        |\n",
      "|    value_loss         | 27.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -27.1     |\n",
      "|    reward             | 19.545187 |\n",
      "|    std                | 2.11      |\n",
      "|    value_loss         | 311       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | 0.267      |\n",
      "|    reward             | -0.4332911 |\n",
      "|    std                | 2.09       |\n",
      "|    value_loss         | 0.0431     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 94         |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | -2.97      |\n",
      "|    reward             | -1.4809495 |\n",
      "|    std                | 2.1        |\n",
      "|    value_loss         | 1.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.15      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | -5.98      |\n",
      "|    reward             | -0.5773963 |\n",
      "|    std                | 2.09       |\n",
      "|    value_loss         | 7.75       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 96         |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.15      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | 0.534      |\n",
      "|    reward             | 0.22480719 |\n",
      "|    std                | 2.07       |\n",
      "|    value_loss         | 2.72       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 11300     |\n",
      "|    time_elapsed       | 97        |\n",
      "|    total_timesteps    | 56500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11299     |\n",
      "|    policy_loss        | 7.56      |\n",
      "|    reward             | -3.412811 |\n",
      "|    std                | 2.05      |\n",
      "|    value_loss         | 17.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 581        |\n",
      "|    iterations         | 11400      |\n",
      "|    time_elapsed       | 98         |\n",
      "|    total_timesteps    | 57000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11399      |\n",
      "|    policy_loss        | -0.33      |\n",
      "|    reward             | -5.5778913 |\n",
      "|    std                | 2.05       |\n",
      "|    value_loss         | 4.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 11500      |\n",
      "|    time_elapsed       | 98         |\n",
      "|    total_timesteps    | 57500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11499      |\n",
      "|    policy_loss        | -0.537     |\n",
      "|    reward             | 0.10723356 |\n",
      "|    std                | 2.03       |\n",
      "|    value_loss         | 0.084      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 11600       |\n",
      "|    time_elapsed       | 99          |\n",
      "|    total_timesteps    | 58000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11599       |\n",
      "|    policy_loss        | -5.29       |\n",
      "|    reward             | -0.78250116 |\n",
      "|    std                | 2.02        |\n",
      "|    value_loss         | 7.86        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | 0.578     |\n",
      "|    reward             | 0.0309294 |\n",
      "|    std                | 2.05      |\n",
      "|    value_loss         | 0.694     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 101       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.14     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | -4.11     |\n",
      "|    reward             | 0.3372548 |\n",
      "|    std                | 2.05      |\n",
      "|    value_loss         | 6.21      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 11900     |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 59500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11899     |\n",
      "|    policy_loss        | 50.3      |\n",
      "|    reward             | 2.5306776 |\n",
      "|    std                | 2.04      |\n",
      "|    value_loss         | 472       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 12000     |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 60000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.13     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11999     |\n",
      "|    policy_loss        | 1.82      |\n",
      "|    reward             | 8.742047  |\n",
      "|    std                | 2.02      |\n",
      "|    value_loss         | 118       |\n",
      "-------------------------------------\n",
      "day: 3020, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14287966.32\n",
      "total_reward: 13287966.32\n",
      "total_cost: 998.99\n",
      "total_trades: 3020\n",
      "Sharpe: 0.998\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 12100      |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 60500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12099      |\n",
      "|    policy_loss        | 0.697      |\n",
      "|    reward             | 0.17020556 |\n",
      "|    std                | 2.04       |\n",
      "|    value_loss         | 0.114      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 12200       |\n",
      "|    time_elapsed       | 105         |\n",
      "|    total_timesteps    | 61000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.14       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12199       |\n",
      "|    policy_loss        | -17.3       |\n",
      "|    reward             | -0.22463384 |\n",
      "|    std                | 2.06        |\n",
      "|    value_loss         | 64.5        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 106      |\n",
      "|    total_timesteps    | 61500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | -4.81    |\n",
      "|    reward             | 1.597641 |\n",
      "|    std                | 2.08     |\n",
      "|    value_loss         | 7.81     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 12400      |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 62000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12399      |\n",
      "|    policy_loss        | -7.57      |\n",
      "|    reward             | 0.11174718 |\n",
      "|    std                | 2.08       |\n",
      "|    value_loss         | 15.9       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -3.27    |\n",
      "|    reward             | 6.18408  |\n",
      "|    std                | 2.07     |\n",
      "|    value_loss         | 23.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | 21.3       |\n",
      "|    reward             | -6.9963965 |\n",
      "|    std                | 2.08       |\n",
      "|    value_loss         | 756        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 12700       |\n",
      "|    time_elapsed       | 109         |\n",
      "|    total_timesteps    | 63500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.15       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12699       |\n",
      "|    policy_loss        | 0.104       |\n",
      "|    reward             | 0.061652992 |\n",
      "|    std                | 2.08        |\n",
      "|    value_loss         | 0.00234     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 12800       |\n",
      "|    time_elapsed       | 110         |\n",
      "|    total_timesteps    | 64000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.14       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12799       |\n",
      "|    policy_loss        | 10.1        |\n",
      "|    reward             | -0.53425634 |\n",
      "|    std                | 2.07        |\n",
      "|    value_loss         | 21.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 579         |\n",
      "|    iterations         | 12900       |\n",
      "|    time_elapsed       | 111         |\n",
      "|    total_timesteps    | 64500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.15       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12899       |\n",
      "|    policy_loss        | -2.63       |\n",
      "|    reward             | -0.65644276 |\n",
      "|    std                | 2.07        |\n",
      "|    value_loss         | 2.18        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 112       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | -3.52     |\n",
      "|    reward             | 3.8379023 |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 4.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 13100     |\n",
      "|    time_elapsed       | 112       |\n",
      "|    total_timesteps    | 65500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13099     |\n",
      "|    policy_loss        | -4.73     |\n",
      "|    reward             | 5.8748198 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 17.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 113        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -112       |\n",
      "|    reward             | -19.518486 |\n",
      "|    std                | 2.11       |\n",
      "|    value_loss         | 3.89e+03   |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 13300       |\n",
      "|    time_elapsed       | 114         |\n",
      "|    total_timesteps    | 66500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.17       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13299       |\n",
      "|    policy_loss        | -0.162      |\n",
      "|    reward             | 0.050147407 |\n",
      "|    std                | 2.11        |\n",
      "|    value_loss         | 0.00318     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 580         |\n",
      "|    iterations         | 13400       |\n",
      "|    time_elapsed       | 115         |\n",
      "|    total_timesteps    | 67000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.17       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13399       |\n",
      "|    policy_loss        | 7.57        |\n",
      "|    reward             | -0.29429308 |\n",
      "|    std                | 2.13        |\n",
      "|    value_loss         | 13.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 116        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 9.18       |\n",
      "|    reward             | -2.2126591 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 21.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -10.1      |\n",
      "|    reward             | 0.54083455 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 21.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 13700      |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 68500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13699      |\n",
      "|    policy_loss        | -11        |\n",
      "|    reward             | -6.7236857 |\n",
      "|    std                | 2.1        |\n",
      "|    value_loss         | 70.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | 4.23      |\n",
      "|    reward             | 20.225561 |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 62.7      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 580          |\n",
      "|    iterations         | 13900        |\n",
      "|    time_elapsed       | 119          |\n",
      "|    total_timesteps    | 69500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.17        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13899        |\n",
      "|    policy_loss        | -0.16        |\n",
      "|    reward             | -0.044086095 |\n",
      "|    std                | 2.11         |\n",
      "|    value_loss         | 0.00856      |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 580      |\n",
      "|    iterations         | 14000    |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 70000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13999    |\n",
      "|    policy_loss        | -1.28    |\n",
      "|    reward             | 1.074948 |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 0.399    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | -5.03      |\n",
      "|    reward             | -3.4744654 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 9.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 122        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | -26.1      |\n",
      "|    reward             | -4.7796507 |\n",
      "|    std                | 2.11       |\n",
      "|    value_loss         | 124        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 14300      |\n",
      "|    time_elapsed       | 123        |\n",
      "|    total_timesteps    | 71500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14299      |\n",
      "|    policy_loss        | -23.3      |\n",
      "|    reward             | -1.2556221 |\n",
      "|    std                | 2.11       |\n",
      "|    value_loss         | 68         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 124        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | 34.8       |\n",
      "|    reward             | -5.8114424 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 258        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 14500     |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 72500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14499     |\n",
      "|    policy_loss        | -69.1     |\n",
      "|    reward             | -8.402275 |\n",
      "|    std                | 2.11      |\n",
      "|    value_loss         | 2.07e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 580       |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | -0.884    |\n",
      "|    reward             | 1.8997182 |\n",
      "|    std                | 2.15      |\n",
      "|    value_loss         | 0.222     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 14700      |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 73500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14699      |\n",
      "|    policy_loss        | -1.51      |\n",
      "|    reward             | -0.6405348 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | 5.21       |\n",
      "|    reward             | -5.5457873 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 14.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | -18.2      |\n",
      "|    reward             | -1.2136482 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 93.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 581       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | 6.98      |\n",
      "|    reward             | -9.348609 |\n",
      "|    std                | 2.13      |\n",
      "|    value_loss         | 36        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 129        |\n",
      "|    reward             | -41.334263 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 3.51e+03   |\n",
      "--------------------------------------\n",
      "day: 3020, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14307425.82\n",
      "total_reward: 13307425.82\n",
      "total_cost: 1056.39\n",
      "total_trades: 3019\n",
      "Sharpe: 1.006\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 579          |\n",
      "|    iterations         | 15200        |\n",
      "|    time_elapsed       | 131          |\n",
      "|    total_timesteps    | 76000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.19        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15199        |\n",
      "|    policy_loss        | -1.31        |\n",
      "|    reward             | -0.081461415 |\n",
      "|    std                | 2.16         |\n",
      "|    value_loss         | 0.24         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | -0.000805 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 1.53e-07  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 579           |\n",
      "|    iterations         | 15400         |\n",
      "|    time_elapsed       | 132           |\n",
      "|    total_timesteps    | 77000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.21         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 15399         |\n",
      "|    policy_loss        | 0.00161       |\n",
      "|    reward             | -0.0017657101 |\n",
      "|    std                | 2.21          |\n",
      "|    value_loss         | 1.1e-06       |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 15500    |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 77500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.23    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15499    |\n",
      "|    policy_loss        | 0.0114   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.25     |\n",
      "|    value_loss         | 3.02e-05 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 579        |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 134        |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.26      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 0.00482    |\n",
      "|    reward             | 0.00579129 |\n",
      "|    std                | 2.31       |\n",
      "|    value_loss         | 5.94e-06   |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 579      |\n",
      "|    iterations         | 15700    |\n",
      "|    time_elapsed       | 135      |\n",
      "|    total_timesteps    | 78500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.27    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15699    |\n",
      "|    policy_loss        | 9.6e-05  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.35     |\n",
      "|    value_loss         | 2.29e-09 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.3      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | -5.31e-06 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.42      |\n",
      "|    value_loss         | 7.88e-12  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 579       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.34     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 2.22e-07  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.51      |\n",
      "|    value_loss         | 2.54e-15  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 578       |\n",
      "|    iterations         | 16000     |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.39     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15999     |\n",
      "|    policy_loss        | -0.000729 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.64      |\n",
      "|    value_loss         | 8.53e-08  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 16100    |\n",
      "|    time_elapsed       | 139      |\n",
      "|    total_timesteps    | 80500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.45    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16099    |\n",
      "|    policy_loss        | -0.00305 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.8      |\n",
      "|    value_loss         | 1.97e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 578       |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.51     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -6.64e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 2.98      |\n",
      "|    value_loss         | 1.11e-13  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 16300    |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 81500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.54    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | 2.03e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.07     |\n",
      "|    value_loss         | 8.47e-14 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 578       |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 141       |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.58     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -1.66e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.18      |\n",
      "|    value_loss         | 4.38e-15  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 16500    |\n",
      "|    time_elapsed       | 142      |\n",
      "|    total_timesteps    | 82500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16499    |\n",
      "|    policy_loss        | 2.35e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.33     |\n",
      "|    value_loss         | 1.11e-13 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 578       |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.68     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | -9.3e-07  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.53      |\n",
      "|    value_loss         | 2.03e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 16700    |\n",
      "|    time_elapsed       | 144      |\n",
      "|    total_timesteps    | 83500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | 0.00218  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 3.76     |\n",
      "|    value_loss         | 7.8e-07  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 578       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.81     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -0.00109  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.02      |\n",
      "|    value_loss         | 2.2e-07   |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 16900    |\n",
      "|    time_elapsed       | 146      |\n",
      "|    total_timesteps    | 84500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16899    |\n",
      "|    policy_loss        | 1.09e-08 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.31     |\n",
      "|    value_loss         | 1.52e-17 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 578      |\n",
      "|    iterations         | 17000    |\n",
      "|    time_elapsed       | 147      |\n",
      "|    total_timesteps    | 85000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | -1.3e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.56     |\n",
      "|    value_loss         | 2.8e-11  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 577       |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3        |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -0.00014  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 4.86      |\n",
      "|    value_loss         | 2.98e-09  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 17200    |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.07    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | 8.88e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 5.2      |\n",
      "|    value_loss         | 1.15e-09 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 17300    |\n",
      "|    time_elapsed       | 149      |\n",
      "|    total_timesteps    | 86500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17299    |\n",
      "|    policy_loss        | 0.000818 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 5.56     |\n",
      "|    value_loss         | 2.16e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 17400    |\n",
      "|    time_elapsed       | 150      |\n",
      "|    total_timesteps    | 87000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.2     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17399    |\n",
      "|    policy_loss        | 8.65e-07 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 5.94     |\n",
      "|    value_loss         | 8.41e-14 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 577       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.26     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -0.000245 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 6.28      |\n",
      "|    value_loss         | 8.73e-09  |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 577          |\n",
      "|    iterations         | 17600        |\n",
      "|    time_elapsed       | 152          |\n",
      "|    total_timesteps    | 88000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -3.31        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 17599        |\n",
      "|    policy_loss        | -0.00126     |\n",
      "|    reward             | 0.0017000884 |\n",
      "|    std                | 6.64         |\n",
      "|    value_loss         | 1.86e-07     |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 577       |\n",
      "|    iterations         | 17700     |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 88500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.37     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17699     |\n",
      "|    policy_loss        | -0.000978 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 7.07      |\n",
      "|    value_loss         | 8.17e-08  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 577       |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.44     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | -3.59e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 7.56      |\n",
      "|    value_loss         | 1.19e-14  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 577      |\n",
      "|    iterations         | 17900    |\n",
      "|    time_elapsed       | 155      |\n",
      "|    total_timesteps    | 89500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17899    |\n",
      "|    policy_loss        | -8.8e-05 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 8.04     |\n",
      "|    value_loss         | 9.47e-10 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 577       |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -6.34e-07 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 8.55      |\n",
      "|    value_loss         | 6.25e-14  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 577       |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.62     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | 0.000548  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 9.06      |\n",
      "|    value_loss         | 1.73e-06  |\n",
      "-------------------------------------\n",
      "day: 3020, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 997653.00\n",
      "total_reward: -2347.00\n",
      "total_cost: 1227.74\n",
      "total_trades: 278\n",
      "Sharpe: -0.511\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 576            |\n",
      "|    iterations         | 18200          |\n",
      "|    time_elapsed       | 157            |\n",
      "|    total_timesteps    | 91000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -3.66          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 18199          |\n",
      "|    policy_loss        | 0.000537       |\n",
      "|    reward             | -0.00050143903 |\n",
      "|    std                | 9.41           |\n",
      "|    value_loss         | 3.04e-08       |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 18300    |\n",
      "|    time_elapsed       | 158      |\n",
      "|    total_timesteps    | 91500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.71    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18299    |\n",
      "|    policy_loss        | 0.000802 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 9.92     |\n",
      "|    value_loss         | 1.33e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 18400    |\n",
      "|    time_elapsed       | 159      |\n",
      "|    total_timesteps    | 92000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18399    |\n",
      "|    policy_loss        | 0.00103  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 10.5     |\n",
      "|    value_loss         | 1.06e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 576      |\n",
      "|    iterations         | 18500    |\n",
      "|    time_elapsed       | 160      |\n",
      "|    total_timesteps    | 92500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.83    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18499    |\n",
      "|    policy_loss        | -0.0118  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 11.2     |\n",
      "|    value_loss         | 9.18e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 18600    |\n",
      "|    time_elapsed       | 161      |\n",
      "|    total_timesteps    | 93000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.89    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18599    |\n",
      "|    policy_loss        | 0.00428  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 11.8     |\n",
      "|    value_loss         | 1.6e-06  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 18700    |\n",
      "|    time_elapsed       | 162      |\n",
      "|    total_timesteps    | 93500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -3.93    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | 0.0769   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 12.3     |\n",
      "|    value_loss         | 0.000466 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 575            |\n",
      "|    iterations         | 18800          |\n",
      "|    time_elapsed       | 163            |\n",
      "|    total_timesteps    | 94000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -3.95          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 18799          |\n",
      "|    policy_loss        | 0.000502       |\n",
      "|    reward             | -0.00034929236 |\n",
      "|    std                | 12.6           |\n",
      "|    value_loss         | 1.51e-08       |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 575       |\n",
      "|    iterations         | 18900     |\n",
      "|    time_elapsed       | 164       |\n",
      "|    total_timesteps    | 94500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -3.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | -0.000361 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 13        |\n",
      "|    value_loss         | 1.63e-07  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 19000    |\n",
      "|    time_elapsed       | 164      |\n",
      "|    total_timesteps    | 95000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.03    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18999    |\n",
      "|    policy_loss        | -0.00845 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 13.7     |\n",
      "|    value_loss         | 6.21e-06 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 575            |\n",
      "|    iterations         | 19100          |\n",
      "|    time_elapsed       | 165            |\n",
      "|    total_timesteps    | 95500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -4.09          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 19099          |\n",
      "|    policy_loss        | 0.0167         |\n",
      "|    reward             | -0.00035708412 |\n",
      "|    std                | 14.5           |\n",
      "|    value_loss         | 2.47e-05       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 575          |\n",
      "|    iterations         | 19200        |\n",
      "|    time_elapsed       | 166          |\n",
      "|    total_timesteps    | 96000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -4.15        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 19199        |\n",
      "|    policy_loss        | -0.0338      |\n",
      "|    reward             | -0.006619987 |\n",
      "|    std                | 15.4         |\n",
      "|    value_loss         | 7.82e-05     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 19300    |\n",
      "|    time_elapsed       | 167      |\n",
      "|    total_timesteps    | 96500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.2     |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19299    |\n",
      "|    policy_loss        | -0.0309  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 16.2     |\n",
      "|    value_loss         | 7.5e-05  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 19400    |\n",
      "|    time_elapsed       | 168      |\n",
      "|    total_timesteps    | 97000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.22    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19399    |\n",
      "|    policy_loss        | -0.00363 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 16.5     |\n",
      "|    value_loss         | 7.63e-07 |\n",
      "------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                 |                 |\n",
      "|    fps                | 575             |\n",
      "|    iterations         | 19500           |\n",
      "|    time_elapsed       | 169             |\n",
      "|    total_timesteps    | 97500           |\n",
      "| train/                |                 |\n",
      "|    entropy_loss       | -4.25           |\n",
      "|    explained_variance | -1.19e-07       |\n",
      "|    learning_rate      | 0.0007          |\n",
      "|    n_updates          | 19499           |\n",
      "|    policy_loss        | 0.000956        |\n",
      "|    reward             | -0.000120288736 |\n",
      "|    std                | 17              |\n",
      "|    value_loss         | 3.28e-07        |\n",
      "-------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 19600    |\n",
      "|    time_elapsed       | 170      |\n",
      "|    total_timesteps    | 98000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19599    |\n",
      "|    policy_loss        | -0.0114  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 17.8     |\n",
      "|    value_loss         | 1.02e-05 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 575           |\n",
      "|    iterations         | 19700         |\n",
      "|    time_elapsed       | 171           |\n",
      "|    total_timesteps    | 98500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -4.35         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 19699         |\n",
      "|    policy_loss        | 0.0053        |\n",
      "|    reward             | -0.0014553543 |\n",
      "|    std                | 18.7          |\n",
      "|    value_loss         | 1.09e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 575      |\n",
      "|    iterations         | 19800    |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 99000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -4.39    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19799    |\n",
      "|    policy_loss        | -0.0201  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 19.5     |\n",
      "|    value_loss         | 3.31e-05 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 574          |\n",
      "|    iterations         | 19900        |\n",
      "|    time_elapsed       | 173          |\n",
      "|    total_timesteps    | 99500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -4.44        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 19899        |\n",
      "|    policy_loss        | -0.167       |\n",
      "|    reward             | -0.015837869 |\n",
      "|    std                | 20.5         |\n",
      "|    value_loss         | 0.00378      |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 574            |\n",
      "|    iterations         | 20000          |\n",
      "|    time_elapsed       | 173            |\n",
      "|    total_timesteps    | 100000         |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -4.46          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 19999          |\n",
      "|    policy_loss        | -0.000118      |\n",
      "|    reward             | -0.00071089313 |\n",
      "|    std                | 20.9           |\n",
      "|    value_loss         | 4.67e-07       |\n",
      "------------------------------------------\n",
      "======A2C Validation from:  2022-01-03 to  2022-04-04\n",
      "A2C Sharpe Ratio:  -0.3908405640141606\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_378_3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 938       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 2         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.1980816 |\n",
      "----------------------------------\n",
      "day: 3020, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1790757.54\n",
      "total_reward: 790757.54\n",
      "total_cost: 7876.28\n",
      "total_trades: 2996\n",
      "Sharpe: 0.870\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 881           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 4             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0032878742  |\n",
      "|    clip_fraction        | 0.00913       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.0581        |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.000227     |\n",
      "|    reward               | -0.0056195757 |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 0.179         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028127672 |\n",
      "|    clip_fraction        | 0.00239      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.23         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00032     |\n",
      "|    reward               | 0.009426228  |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 3.07         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009766456 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.275        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000138    |\n",
      "|    reward               | 0.34341335   |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.74         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 866         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001774838 |\n",
      "|    clip_fraction        | 0.00654     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.205       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.000127   |\n",
      "|    reward               | -0.00226995 |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 0.484       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 861          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053418176 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.99         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    reward               | 0.013458318  |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 11           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 866          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037717153 |\n",
      "|    clip_fraction        | 0.0123       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.443        |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000724    |\n",
      "|    reward               | 0.0816661    |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 0.821        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 859           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 19            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005589926   |\n",
      "|    clip_fraction        | 0.0363        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.0235        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00335      |\n",
      "|    reward               | -4.138446e-05 |\n",
      "|    std                  | 0.983         |\n",
      "|    value_loss           | 0.107         |\n",
      "-------------------------------------------\n",
      "day: 3020, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1044609.21\n",
      "total_reward: 44609.21\n",
      "total_cost: 7229.62\n",
      "total_trades: 2886\n",
      "Sharpe: 0.353\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 845        |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 18432      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00466387 |\n",
      "|    clip_fraction        | 0.0147     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.4       |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 0.185      |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.00139   |\n",
      "|    reward               | 0.01498384 |\n",
      "|    std                  | 0.975      |\n",
      "|    value_loss           | 0.354      |\n",
      "----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 840           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 24            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043080654 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.0395        |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | 6.35e-05      |\n",
      "|    reward               | -0.31991962   |\n",
      "|    std                  | 0.982         |\n",
      "|    value_loss           | 0.0646        |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 841         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005050486 |\n",
      "|    clip_fraction        | 0.0197      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.00285    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    reward               | 0.24107698  |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 0.0237      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 844         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004543462 |\n",
      "|    clip_fraction        | 0.0241      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.789       |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00221    |\n",
      "|    reward               | -0.03883128 |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 2.05        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002296656 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.0293      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 29.9         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -4.7e-06     |\n",
      "|    reward               | 0.04409886   |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 852          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010940329 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.116        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000251    |\n",
      "|    reward               | -0.4334355   |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 19.3         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 854           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014116103 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.157         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 85.2          |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.00032      |\n",
      "|    reward               | -0.008481449  |\n",
      "|    std                  | 0.981         |\n",
      "|    value_loss           | 150           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 859           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 38            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085061917 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.309         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 58.2          |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.00057      |\n",
      "|    reward               | 10.724815     |\n",
      "|    std                  | 0.982         |\n",
      "|    value_loss           | 113           |\n",
      "-------------------------------------------\n",
      "day: 3020, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5726446.61\n",
      "total_reward: 4726446.61\n",
      "total_cost: 6334.98\n",
      "total_trades: 2997\n",
      "Sharpe: 0.883\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 858          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006308836 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.579        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.66         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    reward               | -0.49964032  |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 16.8         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 857           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 42            |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051569124 |\n",
      "|    clip_fraction        | 0.000684      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.485         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 29.8          |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -0.000263     |\n",
      "|    reward               | 0.007504058   |\n",
      "|    std                  | 0.978         |\n",
      "|    value_loss           | 95.1          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 860          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005350958 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.476        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 38.8         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    reward               | 5.724401     |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 123          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 862          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 47           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032328991 |\n",
      "|    clip_fraction        | 0.00884      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.528        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13.6         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00145     |\n",
      "|    reward               | 2.7184289    |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 31.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004538574 |\n",
      "|    clip_fraction        | 0.0283      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.601       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 38.3        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    reward               | -0.9177171  |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 67.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001909317 |\n",
      "|    clip_fraction        | 0.00635     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 111         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.000409   |\n",
      "|    reward               | 17.44389    |\n",
      "|    std                  | 0.98        |\n",
      "|    value_loss           | 228         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 862          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 54           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001210504 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.53         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 98.9         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | 9.55e-05     |\n",
      "|    reward               | -0.33575612  |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 180          |\n",
      "------------------------------------------\n",
      "day: 3020, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8966721.21\n",
      "total_reward: 7966721.21\n",
      "total_cost: 7555.84\n",
      "total_trades: 3000\n",
      "Sharpe: 0.967\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002411081 |\n",
      "|    clip_fraction        | 0.00112     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.58        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 37.4        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.000927   |\n",
      "|    reward               | 0.18453431  |\n",
      "|    std                  | 0.985       |\n",
      "|    value_loss           | 110         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 862          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025777302 |\n",
      "|    clip_fraction        | 0.0325       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.481        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 220          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.0039      |\n",
      "|    reward               | 14.08188     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 304          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006603027 |\n",
      "|    clip_fraction        | 0.000439     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.424        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 173          |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00101     |\n",
      "|    reward               | 1.1255761    |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 272          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 865          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018567368 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.677        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 64.5         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    reward               | -1.4364096   |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 868           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 66            |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00091333117 |\n",
      "|    clip_fraction        | 0.00371       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.515         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 126           |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.00115      |\n",
      "|    reward               | 16.143732     |\n",
      "|    std                  | 0.982         |\n",
      "|    value_loss           | 348           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 869          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016452537 |\n",
      "|    clip_fraction        | 0.00479      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.357        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 169          |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000471    |\n",
      "|    reward               | -1.0500373   |\n",
      "|    std                  | 0.982        |\n",
      "|    value_loss           | 349          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 872         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006635091 |\n",
      "|    clip_fraction        | 0.0248      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.751       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.01        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    reward               | 0.6115278   |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 69          |\n",
      "-----------------------------------------\n",
      "day: 3020, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10488837.81\n",
      "total_reward: 9488837.81\n",
      "total_cost: 7430.91\n",
      "total_trades: 3000\n",
      "Sharpe: 0.983\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050756466 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 232          |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    reward               | 0.002223522  |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 350          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022383705 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.337        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 242          |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    reward               | 5.076279     |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 433          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013430545 |\n",
      "|    clip_fraction        | 0.00225      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.906        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.61         |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000596    |\n",
      "|    reward               | -0.2602992   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 21           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003369504  |\n",
      "|    clip_fraction        | 0.00469      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.548        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 231          |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.000573    |\n",
      "|    reward               | -0.024873216 |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 344          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036938735 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.419        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 171          |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.0026      |\n",
      "|    reward               | 1.979095     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 370          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003725545 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.894       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00504    |\n",
      "|    reward               | 1.7372279   |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 27.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009591315 |\n",
      "|    clip_fraction        | 0.000928     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.542        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 187          |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000424    |\n",
      "|    reward               | -0.033178743 |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 379          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 88          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001534693 |\n",
      "|    clip_fraction        | 0.0141      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 9.8e-05     |\n",
      "|    reward               | 2.903777    |\n",
      "|    std                  | 0.986       |\n",
      "|    value_loss           | 429         |\n",
      "-----------------------------------------\n",
      "day: 3020, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11082944.03\n",
      "total_reward: 10082944.03\n",
      "total_cost: 7140.00\n",
      "total_trades: 2999\n",
      "Sharpe: 0.982\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 91           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010164119 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.784        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 15.3         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000601    |\n",
      "|    reward               | -0.35253528  |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 74.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 875         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003460839 |\n",
      "|    clip_fraction        | 0.00557     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.544       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 244         |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -1.59e-05   |\n",
      "|    reward               | 0.13244797  |\n",
      "|    std                  | 0.981       |\n",
      "|    value_loss           | 412         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026945416 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.35         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 216          |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00168     |\n",
      "|    reward               | -5.004453    |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 494          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 98           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023479788 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.737        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 28.3         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000449    |\n",
      "|    reward               | -1.7574688   |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 93.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005486248 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.491        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 264          |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000164    |\n",
      "|    reward               | -0.1700189   |\n",
      "|    std                  | 0.986        |\n",
      "|    value_loss           | 482          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026866873 |\n",
      "|    clip_fraction        | 0.0166       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.369        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 325          |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00116     |\n",
      "|    reward               | 1.1410123    |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 602          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011665623 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.715        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 88.5         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00166     |\n",
      "|    reward               | -3.0814314   |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 109          |\n",
      "------------------------------------------\n",
      "day: 3020, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 12393868.78\n",
      "total_reward: 11393868.78\n",
      "total_cost: 7484.69\n",
      "total_trades: 3006\n",
      "Sharpe: 1.002\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 107          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048883664 |\n",
      "|    clip_fraction        | 0.032        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.497        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 258          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00315     |\n",
      "|    reward               | 0.4345329    |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 483          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002685688 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 224         |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.000366   |\n",
      "|    reward               | 8.069423    |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 592         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034504123 |\n",
      "|    clip_fraction        | 0.0085       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.612        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 84.4         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000986    |\n",
      "|    reward               | 2.203668     |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 186          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010997725 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.572        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 197          |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    reward               | 0.13590784   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 394          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-01-03 to  2022-04-04\n",
      "PPO Sharpe Ratio:  0.09700385819856999\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_378_3\n",
      "day: 3020, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 12084    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 7.63e+03 |\n",
      "|    critic_loss     | 2.37e+04 |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 11983    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3020, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 158      |\n",
      "|    total_timesteps | 24168    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 4.81e+03 |\n",
      "|    critic_loss     | 147      |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 24067    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3020, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 235      |\n",
      "|    total_timesteps | 36252    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.5e+03  |\n",
      "|    critic_loss     | 0.772    |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 36151    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 312      |\n",
      "|    total_timesteps | 48336    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.32e+03 |\n",
      "|    critic_loss     | 0.64     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 48235    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3020, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 390      |\n",
      "|    total_timesteps | 60420    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 704      |\n",
      "|    critic_loss     | 40.6     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 60319    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3020, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 468      |\n",
      "|    total_timesteps | 72504    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 331      |\n",
      "|    critic_loss     | 0.119    |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 72403    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3020, episode: 95\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 545      |\n",
      "|    total_timesteps | 84588    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 165      |\n",
      "|    critic_loss     | 0.0512   |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 84487    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3020, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 623      |\n",
      "|    total_timesteps | 96672    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 91       |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 96571    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2022-01-03 to  2022-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-04\n",
      "======Trading from:  2022-04-04 to  2022-07-06\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2022-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_441_3\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 546          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.51        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -0.000555    |\n",
      "|    reward             | 0.0027331545 |\n",
      "|    std                | 1.1          |\n",
      "|    value_loss         | 1.01e-06     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 522         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.57       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | -0.00119    |\n",
      "|    reward             | 0.010615365 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 1.33e-06    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 552           |\n",
      "|    iterations         | 300           |\n",
      "|    time_elapsed       | 2             |\n",
      "|    total_timesteps    | 1500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.61         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 299           |\n",
      "|    policy_loss        | -0.000544     |\n",
      "|    reward             | -5.567869e-05 |\n",
      "|    std                | 1.21          |\n",
      "|    value_loss         | 1.79e-07      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -0.00609  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 2.3e-05   |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 554            |\n",
      "|    iterations         | 500            |\n",
      "|    time_elapsed       | 4              |\n",
      "|    total_timesteps    | 2500           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.68          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 499            |\n",
      "|    policy_loss        | -0.00174       |\n",
      "|    reward             | -0.00060581596 |\n",
      "|    std                | 1.3            |\n",
      "|    value_loss         | 1.58e-05       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 550           |\n",
      "|    iterations         | 600           |\n",
      "|    time_elapsed       | 5             |\n",
      "|    total_timesteps    | 3000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.67         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 599           |\n",
      "|    policy_loss        | 0.145         |\n",
      "|    reward             | -0.0062099174 |\n",
      "|    std                | 1.29          |\n",
      "|    value_loss         | 0.0204        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 554         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.69       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -0.083      |\n",
      "|    reward             | 0.018728701 |\n",
      "|    std                | 1.31        |\n",
      "|    value_loss         | 0.00193     |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.71    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00663  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.33     |\n",
      "|    value_loss         | 1.57e-05 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 555           |\n",
      "|    iterations         | 900           |\n",
      "|    time_elapsed       | 8             |\n",
      "|    total_timesteps    | 4500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.73         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 899           |\n",
      "|    policy_loss        | 0.000137      |\n",
      "|    reward             | -0.0057270387 |\n",
      "|    std                | 1.36          |\n",
      "|    value_loss         | 0.000219      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 554          |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 9            |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.76        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | 0.0123       |\n",
      "|    reward             | 0.0046845716 |\n",
      "|    std                | 1.4          |\n",
      "|    value_loss         | 6.1e-05      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 554           |\n",
      "|    iterations         | 1100          |\n",
      "|    time_elapsed       | 9             |\n",
      "|    total_timesteps    | 5500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.8          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1099          |\n",
      "|    policy_loss        | 0.00597       |\n",
      "|    reward             | -0.0014729447 |\n",
      "|    std                | 1.46          |\n",
      "|    value_loss         | 2.93e-05      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 559          |\n",
      "|    iterations         | 1200         |\n",
      "|    time_elapsed       | 10           |\n",
      "|    total_timesteps    | 6000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.82        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1199         |\n",
      "|    policy_loss        | -0.00501     |\n",
      "|    reward             | -0.005487515 |\n",
      "|    std                | 1.5          |\n",
      "|    value_loss         | 9.09e-06     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 558          |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 11           |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.84        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | 0.0254       |\n",
      "|    reward             | -0.003661467 |\n",
      "|    std                | 1.52         |\n",
      "|    value_loss         | 0.000276     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 560          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 12           |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.86        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | -0.0253      |\n",
      "|    reward             | 0.0021792387 |\n",
      "|    std                | 1.56         |\n",
      "|    value_loss         | 0.00024      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 558           |\n",
      "|    iterations         | 1500          |\n",
      "|    time_elapsed       | 13            |\n",
      "|    total_timesteps    | 7500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.9          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1499          |\n",
      "|    policy_loss        | -0.0086       |\n",
      "|    reward             | -0.0002863117 |\n",
      "|    std                | 1.62          |\n",
      "|    value_loss         | 1.6e-05       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 554           |\n",
      "|    iterations         | 1600          |\n",
      "|    time_elapsed       | 14            |\n",
      "|    total_timesteps    | 8000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.94         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1599          |\n",
      "|    policy_loss        | -0.0101       |\n",
      "|    reward             | -0.0020812205 |\n",
      "|    std                | 1.69          |\n",
      "|    value_loss         | 3.27e-05      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 554        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.97      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.562      |\n",
      "|    reward             | 0.06308342 |\n",
      "|    std                | 1.75       |\n",
      "|    value_loss         | 0.0638     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.99      |\n",
      "|    explained_variance | 0.00103    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 0.118      |\n",
      "|    reward             | 0.45643368 |\n",
      "|    std                | 1.77       |\n",
      "|    value_loss         | 3.38       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.99       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 0.00207     |\n",
      "|    reward             | 0.026842142 |\n",
      "|    std                | 1.77        |\n",
      "|    value_loss         | 0.00088     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 559         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2          |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -5.47       |\n",
      "|    reward             | -0.29203942 |\n",
      "|    std                | 1.79        |\n",
      "|    value_loss         | 9.32        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 559       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | -0.0138   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -7.76     |\n",
      "|    reward             | 3.9293137 |\n",
      "|    std                | 1.77      |\n",
      "|    value_loss         | 20.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 561        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.99      |\n",
      "|    explained_variance | 0.0769     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | -5.12      |\n",
      "|    reward             | -1.0242432 |\n",
      "|    std                | 1.77       |\n",
      "|    value_loss         | 7.55       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 561      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2       |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 9.78     |\n",
      "|    reward             | -10.1317 |\n",
      "|    std                | 1.78     |\n",
      "|    value_loss         | 56.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 2400      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 12000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.98     |\n",
      "|    explained_variance | -0.00248  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2399      |\n",
      "|    policy_loss        | -27.6     |\n",
      "|    reward             | 2.3959012 |\n",
      "|    std                | 1.76      |\n",
      "|    value_loss         | 397       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 561         |\n",
      "|    iterations         | 2500        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 12500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.98       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2499        |\n",
      "|    policy_loss        | -0.0678     |\n",
      "|    reward             | 0.030624801 |\n",
      "|    std                | 1.75        |\n",
      "|    value_loss         | 0.00343     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.97       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | 1.46        |\n",
      "|    reward             | -0.65100366 |\n",
      "|    std                | 1.74        |\n",
      "|    value_loss         | 0.423       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 1.43      |\n",
      "|    reward             | 1.3465786 |\n",
      "|    std                | 1.75      |\n",
      "|    value_loss         | 1.12      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 561        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.97      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 6.02       |\n",
      "|    reward             | 0.33459914 |\n",
      "|    std                | 1.73       |\n",
      "|    value_loss         | 12.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 561        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.96      |\n",
      "|    explained_variance | -0.0009    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 26.9       |\n",
      "|    reward             | 0.19998558 |\n",
      "|    std                | 1.71       |\n",
      "|    value_loss         | 179        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 562      |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 15000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.96    |\n",
      "|    explained_variance | 0.0025   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -17.6    |\n",
      "|    reward             | 4.271845 |\n",
      "|    std                | 1.71     |\n",
      "|    value_loss         | 236      |\n",
      "------------------------------------\n",
      "day: 3083, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10372743.19\n",
      "total_reward: 9372743.19\n",
      "total_cost: 7610.79\n",
      "total_trades: 3072\n",
      "Sharpe: 0.957\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 561        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.95      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | 0.161      |\n",
      "|    reward             | 0.06101052 |\n",
      "|    std                | 1.7        |\n",
      "|    value_loss         | 0.0121     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 562         |\n",
      "|    iterations         | 3200        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 16000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.96       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3199        |\n",
      "|    policy_loss        | -5.64       |\n",
      "|    reward             | -0.10282372 |\n",
      "|    std                | 1.71        |\n",
      "|    value_loss         | 13          |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | -2.2      |\n",
      "|    reward             | 1.0895444 |\n",
      "|    std                | 1.71      |\n",
      "|    value_loss         | 2.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 3400      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 17000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3399      |\n",
      "|    policy_loss        | -4.22     |\n",
      "|    reward             | 0.0896774 |\n",
      "|    std                | 1.71      |\n",
      "|    value_loss         | 8.71      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.96     |\n",
      "|    explained_variance | -0.00392  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -0.815    |\n",
      "|    reward             | 4.9510603 |\n",
      "|    std                | 1.71      |\n",
      "|    value_loss         | 13.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | -5.36e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 23.6      |\n",
      "|    reward             | -5.574967 |\n",
      "|    std                | 1.69      |\n",
      "|    value_loss         | 491       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 564       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | 0.000204  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 86.4      |\n",
      "|    reward             | 21.569813 |\n",
      "|    std                | 1.69      |\n",
      "|    value_loss         | 2.98e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.96     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -0.714    |\n",
      "|    reward             | 1.2387077 |\n",
      "|    std                | 1.71      |\n",
      "|    value_loss         | 0.0784    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 564         |\n",
      "|    iterations         | 3900        |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 19500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.94       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3899        |\n",
      "|    policy_loss        | -0.0523     |\n",
      "|    reward             | -0.37952784 |\n",
      "|    std                | 1.69        |\n",
      "|    value_loss         | 0.184       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | 0.000347  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | 4.47      |\n",
      "|    reward             | -4.477865 |\n",
      "|    std                | 1.7       |\n",
      "|    value_loss         | 10.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 565        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.96      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -12.5      |\n",
      "|    reward             | -0.9664404 |\n",
      "|    std                | 1.71       |\n",
      "|    value_loss         | 54.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 565        |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.96      |\n",
      "|    explained_variance | -0.000918  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 7.2        |\n",
      "|    reward             | -7.4651337 |\n",
      "|    std                | 1.72       |\n",
      "|    value_loss         | 23.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | 0.000745  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | 76.2      |\n",
      "|    reward             | -32.90924 |\n",
      "|    std                | 1.7       |\n",
      "|    value_loss         | 2.26e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 566         |\n",
      "|    iterations         | 4400        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 22000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.96       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4399        |\n",
      "|    policy_loss        | 0.728       |\n",
      "|    reward             | 0.011652816 |\n",
      "|    std                | 1.71        |\n",
      "|    value_loss         | 0.209       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 565        |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.97      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 0.381      |\n",
      "|    reward             | 0.18296807 |\n",
      "|    std                | 1.73       |\n",
      "|    value_loss         | 0.0526     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 566         |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 40          |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.97       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | -2.57       |\n",
      "|    reward             | -0.05342349 |\n",
      "|    std                | 1.74        |\n",
      "|    value_loss         | 6.18        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 565      |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 23500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.98    |\n",
      "|    explained_variance | 0.00307  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -2.16    |\n",
      "|    reward             | 1.415882 |\n",
      "|    std                | 1.76     |\n",
      "|    value_loss         | 7.17     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 566        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.99      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | 2.71       |\n",
      "|    reward             | -12.091758 |\n",
      "|    std                | 1.77       |\n",
      "|    value_loss         | 3.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | 0.0322    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 18.4      |\n",
      "|    reward             | 3.0365257 |\n",
      "|    std                | 1.77      |\n",
      "|    value_loss         | 97.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 564       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -0.386    |\n",
      "|    reward             | 0.5138226 |\n",
      "|    std                | 1.78      |\n",
      "|    value_loss         | 0.0902    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | -0.0349    |\n",
      "|    reward             | -0.2440859 |\n",
      "|    std                | 1.78       |\n",
      "|    value_loss         | 0.498      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -1.16     |\n",
      "|    reward             | 0.7487154 |\n",
      "|    std                | 1.78      |\n",
      "|    value_loss         | 2.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.99     |\n",
      "|    explained_variance | 0.0362    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 4.18      |\n",
      "|    reward             | -0.518631 |\n",
      "|    std                | 1.77      |\n",
      "|    value_loss         | 6.23      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 565        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.99      |\n",
      "|    explained_variance | -0.0319    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -5.47      |\n",
      "|    reward             | 0.55813676 |\n",
      "|    std                | 1.78       |\n",
      "|    value_loss         | 32.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 5500       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 27500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.99      |\n",
      "|    explained_variance | 0.00347    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5499       |\n",
      "|    policy_loss        | -4.29      |\n",
      "|    reward             | -10.015583 |\n",
      "|    std                | 1.78       |\n",
      "|    value_loss         | 236        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 565      |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 28000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -0.0978  |\n",
      "|    reward             | 0.107051 |\n",
      "|    std                | 1.77     |\n",
      "|    value_loss         | 0.0043   |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 564       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | -7.5      |\n",
      "|    reward             | 1.4465802 |\n",
      "|    std                | 1.76      |\n",
      "|    value_loss         | 30.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 565        |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | -5.79      |\n",
      "|    reward             | -0.5389262 |\n",
      "|    std                | 1.77       |\n",
      "|    value_loss         | 17.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 5900      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 29500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2        |\n",
      "|    explained_variance | -0.00141  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5899      |\n",
      "|    policy_loss        | -1.56     |\n",
      "|    reward             | 1.4539032 |\n",
      "|    std                | 1.78      |\n",
      "|    value_loss         | 0.894     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 565        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -39.4      |\n",
      "|    reward             | -1.7451882 |\n",
      "|    std                | 1.82       |\n",
      "|    value_loss         | 684        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 565       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.02     |\n",
      "|    explained_variance | 1.76e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | -24.6     |\n",
      "|    reward             | 6.3319373 |\n",
      "|    std                | 1.82      |\n",
      "|    value_loss         | 138       |\n",
      "-------------------------------------\n",
      "day: 3083, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 12925303.82\n",
      "total_reward: 11925303.82\n",
      "total_cost: 6765.16\n",
      "total_trades: 3076\n",
      "Sharpe: 0.977\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 565         |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 54          |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.01       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | -0.428      |\n",
      "|    reward             | -0.12117672 |\n",
      "|    std                | 1.8         |\n",
      "|    value_loss         | 0.0819      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 564          |\n",
      "|    iterations         | 6300         |\n",
      "|    time_elapsed       | 55           |\n",
      "|    total_timesteps    | 31500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.01        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6299         |\n",
      "|    policy_loss        | 1.13         |\n",
      "|    reward             | -0.122118875 |\n",
      "|    std                | 1.81         |\n",
      "|    value_loss         | 1.1          |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | -4.96     |\n",
      "|    reward             | 2.1590977 |\n",
      "|    std                | 1.83      |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 15.8      |\n",
      "|    reward             | 1.5738662 |\n",
      "|    std                | 1.83      |\n",
      "|    value_loss         | 95.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 3.66      |\n",
      "|    reward             | 10.715002 |\n",
      "|    std                | 1.86      |\n",
      "|    value_loss         | 9.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.04     |\n",
      "|    explained_variance | -0.000131 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -0.874    |\n",
      "|    reward             | 13.294359 |\n",
      "|    std                | 1.86      |\n",
      "|    value_loss         | 32.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | 0.0377      |\n",
      "|    reward             | 0.070578896 |\n",
      "|    std                | 1.87        |\n",
      "|    value_loss         | 0.00116     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 6900      |\n",
      "|    time_elapsed       | 61        |\n",
      "|    total_timesteps    | 34500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6899      |\n",
      "|    policy_loss        | -0.307    |\n",
      "|    reward             | 3.5639439 |\n",
      "|    std                | 1.88      |\n",
      "|    value_loss         | 0.521     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 62         |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | -9.06      |\n",
      "|    reward             | 0.48061067 |\n",
      "|    std                | 1.89       |\n",
      "|    value_loss         | 17.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 63         |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | 0.000111   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | 6.63       |\n",
      "|    reward             | 0.20796503 |\n",
      "|    std                | 1.9        |\n",
      "|    value_loss         | 21.8       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 563      |\n",
      "|    iterations         | 7200     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 36000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7199     |\n",
      "|    policy_loss        | -13.3    |\n",
      "|    reward             | 5.805991 |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 64.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -57.1     |\n",
      "|    reward             | 13.114133 |\n",
      "|    std                | 1.89      |\n",
      "|    value_loss         | 1.43e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 564       |\n",
      "|    iterations         | 7400      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 37000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | 0.000175  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7399      |\n",
      "|    policy_loss        | 39        |\n",
      "|    reward             | 10.448195 |\n",
      "|    std                | 1.91      |\n",
      "|    value_loss         | 1.52e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 66         |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -0.126     |\n",
      "|    reward             | -1.0581775 |\n",
      "|    std                | 1.92       |\n",
      "|    value_loss         | 0.166      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | 4.7        |\n",
      "|    reward             | -1.0417912 |\n",
      "|    std                | 1.93       |\n",
      "|    value_loss         | 13.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 7700       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 38500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7699       |\n",
      "|    policy_loss        | -2.14      |\n",
      "|    reward             | -1.3535125 |\n",
      "|    std                | 1.94       |\n",
      "|    value_loss         | 2.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 564       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.07     |\n",
      "|    explained_variance | -0.000709 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 15.5      |\n",
      "|    reward             | 4.422129  |\n",
      "|    std                | 1.91      |\n",
      "|    value_loss         | 56.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | 4.91       |\n",
      "|    reward             | -4.0208297 |\n",
      "|    std                | 1.9        |\n",
      "|    value_loss         | 20.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 564       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | 6.54      |\n",
      "|    reward             | 20.970861 |\n",
      "|    std                | 1.91      |\n",
      "|    value_loss         | 145       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | -1.29      |\n",
      "|    reward             | -0.2896694 |\n",
      "|    std                | 1.91       |\n",
      "|    value_loss         | 2.05       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 563      |\n",
      "|    iterations         | 8200     |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 41000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8199     |\n",
      "|    policy_loss        | 2.34     |\n",
      "|    reward             | 4.687551 |\n",
      "|    std                | 1.91     |\n",
      "|    value_loss         | 1.92     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -7.4       |\n",
      "|    reward             | 0.69017303 |\n",
      "|    std                | 1.9        |\n",
      "|    value_loss         | 18.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | 5.31      |\n",
      "|    reward             | 12.983988 |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 12.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 75         |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.08      |\n",
      "|    explained_variance | -0.00277   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | 17.8       |\n",
      "|    reward             | -1.6902041 |\n",
      "|    std                | 1.93       |\n",
      "|    value_loss         | 57.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.08      |\n",
      "|    explained_variance | -0.000767  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | -4.73      |\n",
      "|    reward             | 13.4814415 |\n",
      "|    std                | 1.94       |\n",
      "|    value_loss         | 204        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 8700        |\n",
      "|    time_elapsed       | 77          |\n",
      "|    total_timesteps    | 43500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8699        |\n",
      "|    policy_loss        | -0.806      |\n",
      "|    reward             | -0.34131455 |\n",
      "|    std                | 1.94        |\n",
      "|    value_loss         | 0.172       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 564       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -0.98     |\n",
      "|    reward             | -1.565549 |\n",
      "|    std                | 1.94      |\n",
      "|    value_loss         | 0.529     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -2.47     |\n",
      "|    reward             | 1.5875229 |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 3.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | -1.58      |\n",
      "|    reward             | 0.06879107 |\n",
      "|    std                | 1.91       |\n",
      "|    value_loss         | 1.78       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | 6.53      |\n",
      "|    reward             | 2.1811368 |\n",
      "|    std                | 1.91      |\n",
      "|    value_loss         | 40.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 564        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | 26.5       |\n",
      "|    reward             | -17.743439 |\n",
      "|    std                | 1.92       |\n",
      "|    value_loss         | 438        |\n",
      "--------------------------------------\n",
      "day: 3083, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 12698407.15\n",
      "total_reward: 11698407.15\n",
      "total_cost: 3623.93\n",
      "total_trades: 3074\n",
      "Sharpe: 0.977\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 82          |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.07       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | 0.159       |\n",
      "|    reward             | 0.002955567 |\n",
      "|    std                | 1.92        |\n",
      "|    value_loss         | 0.0067      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | -1.91     |\n",
      "|    reward             | 1.0372819 |\n",
      "|    std                | 1.93      |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.08      |\n",
      "|    explained_variance | -1.24      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 3.96       |\n",
      "|    reward             | -0.5431319 |\n",
      "|    std                | 1.93       |\n",
      "|    value_loss         | 7.27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 5.02       |\n",
      "|    reward             | -1.3541604 |\n",
      "|    std                | 1.93       |\n",
      "|    value_loss         | 7.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | -0.0784   |\n",
      "|    reward             | 3.4601786 |\n",
      "|    std                | 1.92      |\n",
      "|    value_loss         | 83        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 563       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | -0.000506 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 14.4      |\n",
      "|    reward             | 3.7011206 |\n",
      "|    std                | 1.9       |\n",
      "|    value_loss         | 129       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -0.2       |\n",
      "|    reward             | 0.11126187 |\n",
      "|    std                | 1.91       |\n",
      "|    value_loss         | 0.00996    |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 88        |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 2.09      |\n",
      "|    reward             | 0.3460818 |\n",
      "|    std                | 1.91      |\n",
      "|    value_loss         | 3.06      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 562          |\n",
      "|    iterations         | 10100        |\n",
      "|    time_elapsed       | 89           |\n",
      "|    total_timesteps    | 50500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.06        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 10099        |\n",
      "|    policy_loss        | -7.56        |\n",
      "|    reward             | -0.009962989 |\n",
      "|    std                | 1.89         |\n",
      "|    value_loss         | 6.18         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 10200      |\n",
      "|    time_elapsed       | 90         |\n",
      "|    total_timesteps    | 51000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.04      |\n",
      "|    explained_variance | -0.00604   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10199      |\n",
      "|    policy_loss        | -7.29      |\n",
      "|    reward             | -2.1266923 |\n",
      "|    std                | 1.87       |\n",
      "|    value_loss         | 17.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.05     |\n",
      "|    explained_variance | 0.0249    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -1.25     |\n",
      "|    reward             | -4.659512 |\n",
      "|    std                | 1.87      |\n",
      "|    value_loss         | 0.887     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 92         |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.05      |\n",
      "|    explained_variance | -0.00169   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | -1.39      |\n",
      "|    reward             | -26.677328 |\n",
      "|    std                | 1.88       |\n",
      "|    value_loss         | 23.8       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 562           |\n",
      "|    iterations         | 10500         |\n",
      "|    time_elapsed       | 93            |\n",
      "|    total_timesteps    | 52500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.05         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10499         |\n",
      "|    policy_loss        | 0.00105       |\n",
      "|    reward             | -0.0032557473 |\n",
      "|    std                | 1.89          |\n",
      "|    value_loss         | 1.8e-05       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 562         |\n",
      "|    iterations         | 10600       |\n",
      "|    time_elapsed       | 94          |\n",
      "|    total_timesteps    | 53000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10599       |\n",
      "|    policy_loss        | 1.31        |\n",
      "|    reward             | -0.09276352 |\n",
      "|    std                | 1.9         |\n",
      "|    value_loss         | 0.463       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 562           |\n",
      "|    iterations         | 10700         |\n",
      "|    time_elapsed       | 95            |\n",
      "|    total_timesteps    | 53500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.06         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10699         |\n",
      "|    policy_loss        | 0.108         |\n",
      "|    reward             | -0.0011200444 |\n",
      "|    std                | 1.9           |\n",
      "|    value_loss         | 0.0148        |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 96        |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -0.366    |\n",
      "|    reward             | 0.2092113 |\n",
      "|    std                | 1.91      |\n",
      "|    value_loss         | 0.0393    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 96         |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | -4.76      |\n",
      "|    reward             | -0.6936964 |\n",
      "|    std                | 1.91       |\n",
      "|    value_loss         | 5.29       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 97        |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -11.2     |\n",
      "|    reward             | 12.306863 |\n",
      "|    std                | 1.91      |\n",
      "|    value_loss         | 139       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 562       |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.06     |\n",
      "|    explained_variance | -0.00616  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | -13.9     |\n",
      "|    reward             | 2.6687045 |\n",
      "|    std                | 1.91      |\n",
      "|    value_loss         | 143       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | 0.594      |\n",
      "|    reward             | 0.02304441 |\n",
      "|    std                | 1.91       |\n",
      "|    value_loss         | 0.0575     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 1.06       |\n",
      "|    reward             | 0.07898216 |\n",
      "|    std                | 1.92       |\n",
      "|    value_loss         | 0.388      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 562        |\n",
      "|    iterations         | 11400      |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 57000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11399      |\n",
      "|    policy_loss        | 0.23       |\n",
      "|    reward             | -0.2129891 |\n",
      "|    std                | 1.9        |\n",
      "|    value_loss         | 0.281      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 11500       |\n",
      "|    time_elapsed       | 102         |\n",
      "|    total_timesteps    | 57500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11499       |\n",
      "|    policy_loss        | -0.787      |\n",
      "|    reward             | 0.001552349 |\n",
      "|    std                | 1.9         |\n",
      "|    value_loss         | 0.471       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 563        |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | 3.29       |\n",
      "|    reward             | -1.0312954 |\n",
      "|    std                | 1.88       |\n",
      "|    value_loss         | 2.08       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 11700       |\n",
      "|    time_elapsed       | 103         |\n",
      "|    total_timesteps    | 58500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.05       |\n",
      "|    explained_variance | -0.00925    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11699       |\n",
      "|    policy_loss        | -1.16       |\n",
      "|    reward             | 0.018688114 |\n",
      "|    std                | 1.89        |\n",
      "|    value_loss         | 2.94        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 11800       |\n",
      "|    time_elapsed       | 104         |\n",
      "|    total_timesteps    | 59000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11799       |\n",
      "|    policy_loss        | -0.0539     |\n",
      "|    reward             | -0.21439329 |\n",
      "|    std                | 1.9         |\n",
      "|    value_loss         | 0.00406     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 563          |\n",
      "|    iterations         | 11900        |\n",
      "|    time_elapsed       | 105          |\n",
      "|    total_timesteps    | 59500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.06        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 11899        |\n",
      "|    policy_loss        | 0.792        |\n",
      "|    reward             | -0.015982471 |\n",
      "|    std                | 1.9          |\n",
      "|    value_loss         | 0.26         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 563         |\n",
      "|    iterations         | 12000       |\n",
      "|    time_elapsed       | 106         |\n",
      "|    total_timesteps    | 60000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.07       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11999       |\n",
      "|    policy_loss        | -1.74       |\n",
      "|    reward             | -0.47819325 |\n",
      "|    std                | 1.92        |\n",
      "|    value_loss         | 1.15        |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 563           |\n",
      "|    iterations         | 12100         |\n",
      "|    time_elapsed       | 107           |\n",
      "|    total_timesteps    | 60500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.08         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 12099         |\n",
      "|    policy_loss        | -0.00276      |\n",
      "|    reward             | -0.0019906063 |\n",
      "|    std                | 1.93          |\n",
      "|    value_loss         | 5.98e-06      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 562           |\n",
      "|    iterations         | 12200         |\n",
      "|    time_elapsed       | 108           |\n",
      "|    total_timesteps    | 61000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.09         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 12199         |\n",
      "|    policy_loss        | 0.000758      |\n",
      "|    reward             | -0.0004343115 |\n",
      "|    std                | 1.95          |\n",
      "|    value_loss         | 4.93e-07      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 563      |\n",
      "|    iterations         | 12300    |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 61500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12299    |\n",
      "|    policy_loss        | -0.00293 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.97     |\n",
      "|    value_loss         | 7.19e-05 |\n",
      "------------------------------------\n",
      "day: 3083, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1205496.71\n",
      "total_reward: 205496.71\n",
      "total_cost: 7774.80\n",
      "total_trades: 2703\n",
      "Sharpe: 0.430\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 562      |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 110      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.11    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | -0.0025  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2        |\n",
      "|    value_loss         | 1.55e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 561      |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 111      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.05     |\n",
      "|    value_loss         | 4.33e-05 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 561          |\n",
      "|    iterations         | 12600        |\n",
      "|    time_elapsed       | 112          |\n",
      "|    total_timesteps    | 63000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.17        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12599        |\n",
      "|    policy_loss        | 0.000235     |\n",
      "|    reward             | 0.0021279545 |\n",
      "|    std                | 2.13         |\n",
      "|    value_loss         | 5.33e-07     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 12700      |\n",
      "|    time_elapsed       | 113        |\n",
      "|    total_timesteps    | 63500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12699      |\n",
      "|    policy_loss        | -0.00349   |\n",
      "|    reward             | 0.00750781 |\n",
      "|    std                | 2.21       |\n",
      "|    value_loss         | 2.85e-06   |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 557           |\n",
      "|    iterations         | 12800         |\n",
      "|    time_elapsed       | 114           |\n",
      "|    total_timesteps    | 64000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.25         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 12799         |\n",
      "|    policy_loss        | 0.0144        |\n",
      "|    reward             | -0.0060193487 |\n",
      "|    std                | 2.31          |\n",
      "|    value_loss         | 5.23e-05      |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 12900      |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 64500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.27      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12899      |\n",
      "|    policy_loss        | 2.36       |\n",
      "|    reward             | -2.5605793 |\n",
      "|    std                | 2.33       |\n",
      "|    value_loss         | 2.29       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 557          |\n",
      "|    iterations         | 13000        |\n",
      "|    time_elapsed       | 116          |\n",
      "|    total_timesteps    | 65000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.26        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12999        |\n",
      "|    policy_loss        | 0.115        |\n",
      "|    reward             | -0.038368344 |\n",
      "|    std                | 2.32         |\n",
      "|    value_loss         | 0.00559      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 13100      |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 65500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.27      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13099      |\n",
      "|    policy_loss        | 1.62       |\n",
      "|    reward             | -3.9125073 |\n",
      "|    std                | 2.33       |\n",
      "|    value_loss         | 2.2        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 13200     |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 66000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.27     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13199     |\n",
      "|    policy_loss        | 3.96      |\n",
      "|    reward             | -4.070191 |\n",
      "|    std                | 2.34      |\n",
      "|    value_loss         | 3.07      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 13300      |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 66500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.27      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13299      |\n",
      "|    policy_loss        | 3.97       |\n",
      "|    reward             | -0.8005699 |\n",
      "|    std                | 2.34       |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 13400      |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 67000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.26      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13399      |\n",
      "|    policy_loss        | -31.8      |\n",
      "|    reward             | -11.083194 |\n",
      "|    std                | 2.33       |\n",
      "|    value_loss         | 290        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 13500      |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 67500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.27      |\n",
      "|    explained_variance | 0.00529    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13499      |\n",
      "|    policy_loss        | 110        |\n",
      "|    reward             | 0.29517776 |\n",
      "|    std                | 2.33       |\n",
      "|    value_loss         | 1.85e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 121        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.27      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | 0.0771     |\n",
      "|    reward             | -0.1572235 |\n",
      "|    std                | 2.34       |\n",
      "|    value_loss         | 0.0014     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 558       |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.27     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | -1.27     |\n",
      "|    reward             | 0.6744247 |\n",
      "|    std                | 2.33      |\n",
      "|    value_loss         | 1.83      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 558       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | 4.94      |\n",
      "|    reward             | 0.6279445 |\n",
      "|    std                | 2.37      |\n",
      "|    value_loss         | 7.79      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 558       |\n",
      "|    iterations         | 13900     |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 69500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | -8.21e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13899     |\n",
      "|    policy_loss        | 1.5       |\n",
      "|    reward             | 9.068233  |\n",
      "|    std                | 2.37      |\n",
      "|    value_loss         | 1.98      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 125        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.28      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | 0.572      |\n",
      "|    reward             | 0.25125453 |\n",
      "|    std                | 2.36       |\n",
      "|    value_loss         | 3.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.27      |\n",
      "|    explained_variance | -0.000962  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | 29.6       |\n",
      "|    reward             | -1.1759701 |\n",
      "|    std                | 2.35       |\n",
      "|    value_loss         | 243        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 558          |\n",
      "|    iterations         | 14200        |\n",
      "|    time_elapsed       | 127          |\n",
      "|    total_timesteps    | 71000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.28        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14199        |\n",
      "|    policy_loss        | -0.000944    |\n",
      "|    reward             | 0.0012786571 |\n",
      "|    std                | 2.36         |\n",
      "|    value_loss         | 7.09e-05     |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 14300       |\n",
      "|    time_elapsed       | 128         |\n",
      "|    total_timesteps    | 71500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.29       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14299       |\n",
      "|    policy_loss        | 1.81        |\n",
      "|    reward             | -0.33932307 |\n",
      "|    std                | 2.39        |\n",
      "|    value_loss         | 0.891       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.3       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | 1.07       |\n",
      "|    reward             | 0.12644963 |\n",
      "|    std                | 2.4        |\n",
      "|    value_loss         | 0.915      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 14500    |\n",
      "|    time_elapsed       | 130      |\n",
      "|    total_timesteps    | 72500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.31    |\n",
      "|    explained_variance | 0.0126   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14499    |\n",
      "|    policy_loss        | 0.404    |\n",
      "|    reward             | 3.339391 |\n",
      "|    std                | 2.43     |\n",
      "|    value_loss         | 2.16     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 130        |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.3       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | -3.48      |\n",
      "|    reward             | -3.2411296 |\n",
      "|    std                | 2.42       |\n",
      "|    value_loss         | 7.42       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | -37.7     |\n",
      "|    reward             | -8.451374 |\n",
      "|    std                | 2.37      |\n",
      "|    value_loss         | 559       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 132        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.28      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | -29        |\n",
      "|    reward             | -21.129194 |\n",
      "|    std                | 2.37       |\n",
      "|    value_loss         | 185        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 133        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.28      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | -1.67      |\n",
      "|    reward             | 0.09849932 |\n",
      "|    std                | 2.36       |\n",
      "|    value_loss         | 0.369      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 134       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | -0.454    |\n",
      "|    reward             | 1.2070218 |\n",
      "|    std                | 2.36      |\n",
      "|    value_loss         | 0.387     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 15100     |\n",
      "|    time_elapsed       | 135       |\n",
      "|    total_timesteps    | 75500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.27     |\n",
      "|    explained_variance | -0.336    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15099     |\n",
      "|    policy_loss        | -6.37     |\n",
      "|    reward             | 1.3803383 |\n",
      "|    std                | 2.33      |\n",
      "|    value_loss         | 15.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 15200      |\n",
      "|    time_elapsed       | 136        |\n",
      "|    total_timesteps    | 76000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.27      |\n",
      "|    explained_variance | 0.356      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15199      |\n",
      "|    policy_loss        | -1.01      |\n",
      "|    reward             | -1.2449247 |\n",
      "|    std                | 2.33       |\n",
      "|    value_loss         | 1.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 15300      |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 76500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.28      |\n",
      "|    explained_variance | -0.00253   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15299      |\n",
      "|    policy_loss        | 14.3       |\n",
      "|    reward             | -2.5216944 |\n",
      "|    std                | 2.36       |\n",
      "|    value_loss         | 51         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | -0.00261  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | -1.54     |\n",
      "|    reward             | 2.0600252 |\n",
      "|    std                | 2.36      |\n",
      "|    value_loss         | 59.9      |\n",
      "-------------------------------------\n",
      "day: 3083, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9770780.30\n",
      "total_reward: 8770780.30\n",
      "total_cost: 11146.87\n",
      "total_trades: 3076\n",
      "Sharpe: 0.941\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 15500      |\n",
      "|    time_elapsed       | 139        |\n",
      "|    total_timesteps    | 77500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.27      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15499      |\n",
      "|    policy_loss        | -0.472     |\n",
      "|    reward             | -0.8934089 |\n",
      "|    std                | 2.35       |\n",
      "|    value_loss         | 0.146      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.28      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 2.78       |\n",
      "|    reward             | 0.49842754 |\n",
      "|    std                | 2.35       |\n",
      "|    value_loss         | 1.97       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 15700    |\n",
      "|    time_elapsed       | 141      |\n",
      "|    total_timesteps    | 78500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.26    |\n",
      "|    explained_variance | -0.0126  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15699    |\n",
      "|    policy_loss        | 4.22     |\n",
      "|    reward             | 0.850442 |\n",
      "|    std                | 2.33     |\n",
      "|    value_loss         | 12.1     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 556          |\n",
      "|    iterations         | 15800        |\n",
      "|    time_elapsed       | 142          |\n",
      "|    total_timesteps    | 79000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.27        |\n",
      "|    explained_variance | 0.00526      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15799        |\n",
      "|    policy_loss        | 12           |\n",
      "|    reward             | -0.103529125 |\n",
      "|    std                | 2.35         |\n",
      "|    value_loss         | 18           |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.27     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | -5.62     |\n",
      "|    reward             | 3.5010464 |\n",
      "|    std                | 2.34      |\n",
      "|    value_loss         | 12.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 16000     |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.28     |\n",
      "|    explained_variance | -2.8e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15999     |\n",
      "|    policy_loss        | 55.7      |\n",
      "|    reward             | 7.0145597 |\n",
      "|    std                | 2.36      |\n",
      "|    value_loss         | 947       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 16100       |\n",
      "|    time_elapsed       | 144         |\n",
      "|    total_timesteps    | 80500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.28       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16099       |\n",
      "|    policy_loss        | 0.419       |\n",
      "|    reward             | -0.11343025 |\n",
      "|    std                | 2.36        |\n",
      "|    value_loss         | 0.0845      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 16200      |\n",
      "|    time_elapsed       | 145        |\n",
      "|    total_timesteps    | 81000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.28      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16199      |\n",
      "|    policy_loss        | 0.768      |\n",
      "|    reward             | 0.23824479 |\n",
      "|    std                | 2.37       |\n",
      "|    value_loss         | 0.425      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 146        |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.29      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | -1.23      |\n",
      "|    reward             | -0.9414166 |\n",
      "|    std                | 2.38       |\n",
      "|    value_loss         | 2.97       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 147      |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.29    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | 3.98     |\n",
      "|    reward             | 1.626159 |\n",
      "|    std                | 2.4      |\n",
      "|    value_loss         | 2.05     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 16500      |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 82500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.3       |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16499      |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | -1.7134072 |\n",
      "|    std                | 2.42       |\n",
      "|    value_loss         | 50.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.3      |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | -18.6     |\n",
      "|    reward             | -4.383507 |\n",
      "|    std                | 2.41      |\n",
      "|    value_loss         | 260       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 16700    |\n",
      "|    time_elapsed       | 149      |\n",
      "|    total_timesteps    | 83500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.3     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16699    |\n",
      "|    policy_loss        | -0.0743  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.42     |\n",
      "|    value_loss         | 0.0016   |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 16800       |\n",
      "|    time_elapsed       | 150         |\n",
      "|    total_timesteps    | 84000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.31       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16799       |\n",
      "|    policy_loss        | 0.0011      |\n",
      "|    reward             | 0.004091218 |\n",
      "|    std                | 2.44        |\n",
      "|    value_loss         | 7.46e-07    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 16900       |\n",
      "|    time_elapsed       | 151         |\n",
      "|    total_timesteps    | 84500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.33       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16899       |\n",
      "|    policy_loss        | 0.0335      |\n",
      "|    reward             | 0.034015883 |\n",
      "|    std                | 2.49        |\n",
      "|    value_loss         | 0.000548    |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 556            |\n",
      "|    iterations         | 17000          |\n",
      "|    time_elapsed       | 152            |\n",
      "|    total_timesteps    | 85000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.35          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 16999          |\n",
      "|    policy_loss        | -0.0811        |\n",
      "|    reward             | -0.00069436984 |\n",
      "|    std                | 2.54           |\n",
      "|    value_loss         | 0.000995       |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 17100    |\n",
      "|    time_elapsed       | 153      |\n",
      "|    total_timesteps    | 85500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.37    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17099    |\n",
      "|    policy_loss        | -0.101   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.58     |\n",
      "|    value_loss         | 0.0027   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 17200    |\n",
      "|    time_elapsed       | 154      |\n",
      "|    total_timesteps    | 86000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.4     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17199    |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.68     |\n",
      "|    value_loss         | 0.000642 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 17300         |\n",
      "|    time_elapsed       | 155           |\n",
      "|    total_timesteps    | 86500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.43         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 17299         |\n",
      "|    policy_loss        | -0.00595      |\n",
      "|    reward             | -0.0017127789 |\n",
      "|    std                | 2.74          |\n",
      "|    value_loss         | 3.75e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 557          |\n",
      "|    iterations         | 17400        |\n",
      "|    time_elapsed       | 156          |\n",
      "|    total_timesteps    | 87000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.46        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 17399        |\n",
      "|    policy_loss        | -0.00188     |\n",
      "|    reward             | 0.0032627718 |\n",
      "|    std                | 2.82         |\n",
      "|    value_loss         | 1.03e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 556            |\n",
      "|    iterations         | 17500          |\n",
      "|    time_elapsed       | 157            |\n",
      "|    total_timesteps    | 87500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.5           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 17499          |\n",
      "|    policy_loss        | -0.000505      |\n",
      "|    reward             | -0.00018815868 |\n",
      "|    std                | 2.93           |\n",
      "|    value_loss         | 6.77e-06       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 17600         |\n",
      "|    time_elapsed       | 158           |\n",
      "|    total_timesteps    | 88000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.54         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 17599         |\n",
      "|    policy_loss        | 0.0187        |\n",
      "|    reward             | -0.0045194654 |\n",
      "|    std                | 3.06          |\n",
      "|    value_loss         | 5.86e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 17700         |\n",
      "|    time_elapsed       | 159           |\n",
      "|    total_timesteps    | 88500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.58         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 17699         |\n",
      "|    policy_loss        | 0.00257       |\n",
      "|    reward             | 0.00041943512 |\n",
      "|    std                | 3.18          |\n",
      "|    value_loss         | 1.22e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 17800         |\n",
      "|    time_elapsed       | 159           |\n",
      "|    total_timesteps    | 89000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.63         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 17799         |\n",
      "|    policy_loss        | -0.0238       |\n",
      "|    reward             | -0.0052652797 |\n",
      "|    std                | 3.35          |\n",
      "|    value_loss         | 0.000113      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 17900         |\n",
      "|    time_elapsed       | 160           |\n",
      "|    total_timesteps    | 89500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.64         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 17899         |\n",
      "|    policy_loss        | 0.00244       |\n",
      "|    reward             | 0.00073024206 |\n",
      "|    std                | 3.39          |\n",
      "|    value_loss         | 1.26e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 556          |\n",
      "|    iterations         | 18000        |\n",
      "|    time_elapsed       | 161          |\n",
      "|    total_timesteps    | 90000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.66        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 17999        |\n",
      "|    policy_loss        | 0.00255      |\n",
      "|    reward             | -0.005184495 |\n",
      "|    std                | 3.47         |\n",
      "|    value_loss         | 1.06e-05     |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 18100         |\n",
      "|    time_elapsed       | 162           |\n",
      "|    total_timesteps    | 90500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.7          |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 18099         |\n",
      "|    policy_loss        | 0.000304      |\n",
      "|    reward             | -0.0008996111 |\n",
      "|    std                | 3.58          |\n",
      "|    value_loss         | 4.4e-08       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 556          |\n",
      "|    iterations         | 18200        |\n",
      "|    time_elapsed       | 163          |\n",
      "|    total_timesteps    | 91000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.72        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 18199        |\n",
      "|    policy_loss        | -4.53e-05    |\n",
      "|    reward             | 0.0024051077 |\n",
      "|    std                | 3.69         |\n",
      "|    value_loss         | 4.16e-07     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 556            |\n",
      "|    iterations         | 18300          |\n",
      "|    time_elapsed       | 164            |\n",
      "|    total_timesteps    | 91500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.76          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 18299          |\n",
      "|    policy_loss        | 0.0117         |\n",
      "|    reward             | -0.00042351638 |\n",
      "|    std                | 3.83           |\n",
      "|    value_loss         | 2.23e-05       |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 18400      |\n",
      "|    time_elapsed       | 165        |\n",
      "|    total_timesteps    | 92000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.79      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18399      |\n",
      "|    policy_loss        | -0.019     |\n",
      "|    reward             | -1.1884696 |\n",
      "|    std                | 3.91       |\n",
      "|    value_loss         | 0.154      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 18500         |\n",
      "|    time_elapsed       | 166           |\n",
      "|    total_timesteps    | 92500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.79         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 18499         |\n",
      "|    policy_loss        | 0.00844       |\n",
      "|    reward             | -0.0007259688 |\n",
      "|    std                | 3.95          |\n",
      "|    value_loss         | 1.29e-05      |\n",
      "-----------------------------------------\n",
      "day: 3083, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 995013.33\n",
      "total_reward: -4986.67\n",
      "total_cost: 8464.01\n",
      "total_trades: 2376\n",
      "Sharpe: -0.031\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.8      |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | 0.00182   |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 3.99      |\n",
      "|    value_loss         | 6.86e-07  |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 18700         |\n",
      "|    time_elapsed       | 168           |\n",
      "|    total_timesteps    | 93500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.82         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 18699         |\n",
      "|    policy_loss        | 0.0104        |\n",
      "|    reward             | -0.0003305733 |\n",
      "|    std                | 4.08          |\n",
      "|    value_loss         | 2.46e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 18800    |\n",
      "|    time_elapsed       | 168      |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | -0.00809 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.2      |\n",
      "|    value_loss         | 1.15e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 18900    |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 94500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.88    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18899    |\n",
      "|    policy_loss        | 0.00282  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.3      |\n",
      "|    value_loss         | 1.54e-06 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 557          |\n",
      "|    iterations         | 19000        |\n",
      "|    time_elapsed       | 170          |\n",
      "|    total_timesteps    | 95000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.91        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 18999        |\n",
      "|    policy_loss        | -0.0204      |\n",
      "|    reward             | 0.0045502554 |\n",
      "|    std                | 4.45         |\n",
      "|    value_loss         | 0.000175     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 556            |\n",
      "|    iterations         | 19100          |\n",
      "|    time_elapsed       | 171            |\n",
      "|    total_timesteps    | 95500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.9           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 19099          |\n",
      "|    policy_loss        | -0.00617       |\n",
      "|    reward             | -0.00060955354 |\n",
      "|    std                | 4.39           |\n",
      "|    value_loss         | 1.5e-05        |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 19200    |\n",
      "|    time_elapsed       | 172      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.91    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.44     |\n",
      "|    value_loss         | 0.000121 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 557          |\n",
      "|    iterations         | 19300        |\n",
      "|    time_elapsed       | 173          |\n",
      "|    total_timesteps    | 96500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.93        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 19299        |\n",
      "|    policy_loss        | 0.00955      |\n",
      "|    reward             | 0.0006336215 |\n",
      "|    std                | 4.53         |\n",
      "|    value_loss         | 2.07e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 557          |\n",
      "|    iterations         | 19400        |\n",
      "|    time_elapsed       | 174          |\n",
      "|    total_timesteps    | 97000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.94        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 19399        |\n",
      "|    policy_loss        | 0.0801       |\n",
      "|    reward             | -0.020230977 |\n",
      "|    std                | 4.6          |\n",
      "|    value_loss         | 0.00115      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 19500       |\n",
      "|    time_elapsed       | 174         |\n",
      "|    total_timesteps    | 97500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.95       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19499       |\n",
      "|    policy_loss        | 0.104       |\n",
      "|    reward             | 0.017917063 |\n",
      "|    std                | 4.61        |\n",
      "|    value_loss         | 0.00217     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 19600       |\n",
      "|    time_elapsed       | 175         |\n",
      "|    total_timesteps    | 98000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.96       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19599       |\n",
      "|    policy_loss        | 0.0933      |\n",
      "|    reward             | 0.016102087 |\n",
      "|    std                | 4.65        |\n",
      "|    value_loss         | 0.00231     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 176        |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.96      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | 1.33       |\n",
      "|    reward             | 0.39727855 |\n",
      "|    std                | 4.69       |\n",
      "|    value_loss         | 0.151      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 557           |\n",
      "|    iterations         | 19800         |\n",
      "|    time_elapsed       | 177           |\n",
      "|    total_timesteps    | 99000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.97         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 19799         |\n",
      "|    policy_loss        | 0.00162       |\n",
      "|    reward             | -0.0018390871 |\n",
      "|    std                | 4.71          |\n",
      "|    value_loss         | 2.55e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 19900    |\n",
      "|    time_elapsed       | 178      |\n",
      "|    total_timesteps    | 99500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.98    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | 0.00177  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 4.76     |\n",
      "|    value_loss         | 9.41e-07 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 20000       |\n",
      "|    time_elapsed       | 179         |\n",
      "|    total_timesteps    | 100000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -3          |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19999       |\n",
      "|    policy_loss        | -0.00924    |\n",
      "|    reward             | 0.005596409 |\n",
      "|    std                | 4.85        |\n",
      "|    value_loss         | 2.27e-05    |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2022-04-04 to  2022-07-06\n",
      "A2C Sharpe Ratio:  -0.1754681839701614\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_441_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 964        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 2          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.15169215 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 907         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002115566 |\n",
      "|    clip_fraction        | 0.0127      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0324      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.000754   |\n",
      "|    reward               | -0.09126307 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0948      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 900          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050708745 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.39         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00312     |\n",
      "|    reward               | -1.2897277   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 6.68         |\n",
      "------------------------------------------\n",
      "day: 3083, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2296797.42\n",
      "total_reward: 1296797.42\n",
      "total_cost: 8309.38\n",
      "total_trades: 3052\n",
      "Sharpe: 0.839\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 904           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 9             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.002008303   |\n",
      "|    clip_fraction        | 0.0082        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.95          |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000791     |\n",
      "|    reward               | 0.00021749114 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.97          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 903          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003457194 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00201     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 0.000155     |\n",
      "|    reward               | 0.004656826  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.331        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 899          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054890793 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.000534    |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00372     |\n",
      "|    reward               | -0.62660486  |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.0155       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 906          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011420611 |\n",
      "|    clip_fraction        | 0.00669      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.38         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000829    |\n",
      "|    reward               | -0.024248175 |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 2.83         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 908           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 18            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0027048162  |\n",
      "|    clip_fraction        | 0.0182        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.0268        |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000857     |\n",
      "|    reward               | -0.0016268115 |\n",
      "|    std                  | 0.987         |\n",
      "|    value_loss           | 0.183         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 913          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032275494 |\n",
      "|    clip_fraction        | 0.0198       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.000727     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000634    |\n",
      "|    reward               | -0.01567684  |\n",
      "|    std                  | 0.983        |\n",
      "|    value_loss           | 0.0426       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 918          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028730207 |\n",
      "|    clip_fraction        | 0.00791      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00107      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000871    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.987        |\n",
      "|    value_loss           | 0.0426       |\n",
      "------------------------------------------\n",
      "day: 3083, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1016410.70\n",
      "total_reward: 16410.70\n",
      "total_cost: 7003.27\n",
      "total_trades: 2748\n",
      "Sharpe: 0.258\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026262593 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0119      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000134    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 0.00944      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 918           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.004363849   |\n",
      "|    clip_fraction        | 0.0318        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0173       |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.00201      |\n",
      "|    reward               | -0.0012451063 |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 0.0112        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 916          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003205317  |\n",
      "|    clip_fraction        | 0.0127       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0109      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    reward               | -0.028560419 |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 0.0137       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 918           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 31            |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0030844202  |\n",
      "|    clip_fraction        | 0.00503       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00757      |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.00091      |\n",
      "|    reward               | -0.0005725462 |\n",
      "|    std                  | 0.986         |\n",
      "|    value_loss           | 0.0245        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 918          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055146017 |\n",
      "|    clip_fraction        | 0.0428       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0128       |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00275     |\n",
      "|    reward               | -0.70294523  |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 0.0313       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 919           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 35            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.003875961   |\n",
      "|    clip_fraction        | 0.0143        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.12          |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.00194      |\n",
      "|    reward               | -0.0002694019 |\n",
      "|    std                  | 0.983         |\n",
      "|    value_loss           | 2.41          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 920          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018089279 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0792       |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000613    |\n",
      "|    reward               | 0.0034084436 |\n",
      "|    std                  | 0.966        |\n",
      "|    value_loss           | 0.975        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030551068 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0428       |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000342    |\n",
      "|    reward               | -0.9070534   |\n",
      "|    std                  | 0.958        |\n",
      "|    value_loss           | 0.0887       |\n",
      "------------------------------------------\n",
      "day: 3083, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1901305.44\n",
      "total_reward: 901305.44\n",
      "total_cost: 8114.12\n",
      "total_trades: 3025\n",
      "Sharpe: 0.763\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013742764 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.38         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000156    |\n",
      "|    reward               | -0.28598738  |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 3.91         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 918           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 44            |\n",
      "|    total_timesteps      | 40960         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0025296984  |\n",
      "|    clip_fraction        | 0.00317       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.38         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.773         |\n",
      "|    n_updates            | 190           |\n",
      "|    policy_gradient_loss | -0.000228     |\n",
      "|    reward               | 5.1836974e-05 |\n",
      "|    std                  | 0.97          |\n",
      "|    value_loss           | 1.38          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 920          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024963212 |\n",
      "|    clip_fraction        | 0.00493      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.324        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000565    |\n",
      "|    reward               | -0.03585931  |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 0.512        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 919           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 48            |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00436137    |\n",
      "|    clip_fraction        | 0.0126        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.0267        |\n",
      "|    n_updates            | 210           |\n",
      "|    policy_gradient_loss | -0.000772     |\n",
      "|    reward               | -0.0045531285 |\n",
      "|    std                  | 0.986         |\n",
      "|    value_loss           | 0.0789        |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 920          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046769343 |\n",
      "|    clip_fraction        | 0.0241       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0265       |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    reward               | 0.004141951  |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.121        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 921          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033564498 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.3          |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    reward               | 0.7184886    |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 2.98         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012420996 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.99         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -1.81e-05    |\n",
      "|    reward               | 0.062164266  |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 10.1         |\n",
      "------------------------------------------\n",
      "day: 3083, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2256540.81\n",
      "total_reward: 1256540.81\n",
      "total_cost: 8187.23\n",
      "total_trades: 2992\n",
      "Sharpe: 0.674\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 922         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005032938 |\n",
      "|    clip_fraction        | 0.0161      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -1.79e-06   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.36        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00153    |\n",
      "|    reward               | 0.020209236 |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 5.36        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006608635 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0583       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.47         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000685    |\n",
      "|    reward               | -0.11635103  |\n",
      "|    std                  | 0.989        |\n",
      "|    value_loss           | 13.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 920          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026765345 |\n",
      "|    clip_fraction        | 0.00898      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.00431      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 19.3         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00148     |\n",
      "|    reward               | -0.12099225  |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 49.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 921          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 64           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009287406 |\n",
      "|    clip_fraction        | 0.0178       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.0503       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 11           |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.000386    |\n",
      "|    reward               | 0.09845957   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 23.9         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 922           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 66            |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00057925854 |\n",
      "|    clip_fraction        | 0.00898       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.05          |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.00103      |\n",
      "|    reward               | -0.17272209   |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 5.71          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 921          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 68           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025271324 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0538       |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    reward               | 0.0034284808 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 0.0738       |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 923           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 70            |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00051564304 |\n",
      "|    clip_fraction        | 0.00483       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.00595      |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000109     |\n",
      "|    reward               | 0.0029518576  |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 0.039         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 73           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001488794 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0182       |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | 8.77e-05     |\n",
      "|    reward               | -0.009416253 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.0472       |\n",
      "------------------------------------------\n",
      "day: 3083, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1026722.24\n",
      "total_reward: 26722.24\n",
      "total_cost: 7395.91\n",
      "total_trades: 2772\n",
      "Sharpe: 0.295\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0052399226 |\n",
      "|    clip_fraction        | 0.0233       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0241      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    reward               | 0.019126939  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00967      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020588404 |\n",
      "|    clip_fraction        | 0.00195      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00362     |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | 2.83e-05     |\n",
      "|    reward               | -0.015323199 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0337       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 922         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003500873 |\n",
      "|    clip_fraction        | 0.0117      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.0284      |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0012     |\n",
      "|    reward               | -0.19641851 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.0727      |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 922           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 82            |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0038624075  |\n",
      "|    clip_fraction        | 0.0137        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.22          |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -0.00138      |\n",
      "|    reward               | -0.0101670185 |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 3.28          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035070905 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.67         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    reward               | 0.011655195  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 4.09         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047468636 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.6          |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00126     |\n",
      "|    reward               | 0.3215745    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 19.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053737606 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0126      |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    reward               | -0.034590356 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0188       |\n",
      "------------------------------------------\n",
      "day: 3083, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2411900.72\n",
      "total_reward: 1411900.72\n",
      "total_cost: 8489.95\n",
      "total_trades: 3015\n",
      "Sharpe: 0.716\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 924          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029310635 |\n",
      "|    clip_fraction        | 0.00894      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00193      |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.000882    |\n",
      "|    reward               | -0.22891536  |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0802       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064530894 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.103        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.88         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    reward               | 0.09684925   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 15.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004542213  |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0151       |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000806    |\n",
      "|    reward               | -0.015369323 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 0.117        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023345337 |\n",
      "|    clip_fraction        | 0.00405      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.622        |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.00063     |\n",
      "|    reward               | 0.12349543   |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.17         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008102949 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.104        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 26.8         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    reward               | -18.410435   |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 73.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 923         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003435477 |\n",
      "|    clip_fraction        | 0.00225     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.11        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 37.4        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00156    |\n",
      "|    reward               | -0.5355659  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 85.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008610374 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.191        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 23.6         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.000181    |\n",
      "|    reward               | 0.41109636   |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 83.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 923          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 106          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014824255 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.162        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 110          |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000147    |\n",
      "|    reward               | 5.0253963    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 266          |\n",
      "------------------------------------------\n",
      "day: 3083, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8143178.31\n",
      "total_reward: 7143178.31\n",
      "total_cost: 8601.14\n",
      "total_trades: 3065\n",
      "Sharpe: 0.925\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 922          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013050443 |\n",
      "|    clip_fraction        | 0.000732     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.265        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 58.2         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000544    |\n",
      "|    reward               | 0.24879669   |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-04-04 to  2022-07-06\n",
      "PPO Sharpe Ratio:  -0.10681605443749803\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_441_3\n",
      "day: 3083, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14049040.87\n",
      "total_reward: 13049040.87\n",
      "total_cost: 998.99\n",
      "total_trades: 3083\n",
      "Sharpe: 0.972\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 154        |\n",
      "|    time_elapsed    | 79         |\n",
      "|    total_timesteps | 12336      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -744       |\n",
      "|    critic_loss     | 182        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 12235      |\n",
      "|    reward          | -2.4178562 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 154        |\n",
      "|    time_elapsed    | 159        |\n",
      "|    total_timesteps | 24672      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -286       |\n",
      "|    critic_loss     | 157        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 24571      |\n",
      "|    reward          | -2.4178562 |\n",
      "-----------------------------------\n",
      "day: 3083, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14049040.87\n",
      "total_reward: 13049040.87\n",
      "total_cost: 998.99\n",
      "total_trades: 3083\n",
      "Sharpe: 0.972\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 12         |\n",
      "|    fps             | 152        |\n",
      "|    time_elapsed    | 242        |\n",
      "|    total_timesteps | 37008      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -258       |\n",
      "|    critic_loss     | 53         |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 36907      |\n",
      "|    reward          | -2.4178562 |\n",
      "-----------------------------------\n",
      "day: 3083, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14049040.87\n",
      "total_reward: 13049040.87\n",
      "total_cost: 998.99\n",
      "total_trades: 3083\n",
      "Sharpe: 0.972\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 16         |\n",
      "|    fps             | 150        |\n",
      "|    time_elapsed    | 326        |\n",
      "|    total_timesteps | 49344      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -115       |\n",
      "|    critic_loss     | 90.7       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 49243      |\n",
      "|    reward          | -2.4178562 |\n",
      "-----------------------------------\n",
      "day: 3083, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14049040.87\n",
      "total_reward: 13049040.87\n",
      "total_cost: 998.99\n",
      "total_trades: 3083\n",
      "Sharpe: 0.972\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 20         |\n",
      "|    fps             | 150        |\n",
      "|    time_elapsed    | 410        |\n",
      "|    total_timesteps | 61680      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -90.2      |\n",
      "|    critic_loss     | 110        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 61579      |\n",
      "|    reward          | -2.4178562 |\n",
      "-----------------------------------\n",
      "day: 3083, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14049040.87\n",
      "total_reward: 13049040.87\n",
      "total_cost: 998.99\n",
      "total_trades: 3083\n",
      "Sharpe: 0.972\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 24         |\n",
      "|    fps             | 151        |\n",
      "|    time_elapsed    | 489        |\n",
      "|    total_timesteps | 74016      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -45.6      |\n",
      "|    critic_loss     | 87.3       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 73915      |\n",
      "|    reward          | -2.4178562 |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 28         |\n",
      "|    fps             | 151        |\n",
      "|    time_elapsed    | 570        |\n",
      "|    total_timesteps | 86352      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -39.5      |\n",
      "|    critic_loss     | 79.6       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 86251      |\n",
      "|    reward          | -2.4178562 |\n",
      "-----------------------------------\n",
      "day: 3083, episode: 95\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14049040.87\n",
      "total_reward: 13049040.87\n",
      "total_cost: 998.99\n",
      "total_trades: 3083\n",
      "Sharpe: 0.972\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 32         |\n",
      "|    fps             | 151        |\n",
      "|    time_elapsed    | 651        |\n",
      "|    total_timesteps | 98688      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -38        |\n",
      "|    critic_loss     | 231        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 98587      |\n",
      "|    reward          | -2.4178562 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2022-04-04 to  2022-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-06\n",
      "======Trading from:  2022-07-06 to  2022-10-04\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2022-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_504_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 603        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -0.0838    |\n",
      "|    reward             | 0.13045748 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.0504     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.43     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.522    |\n",
      "|    reward             | 1.8202933 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.319     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 559        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.43      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -0.96      |\n",
      "|    reward             | -1.9816959 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.77       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.47      |\n",
      "|    reward             | -1.0147438 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.5        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -2.25      |\n",
      "|    reward             | -4.1962705 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.44       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.44     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 14        |\n",
      "|    reward             | -4.357166 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 328       |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 545           |\n",
      "|    iterations         | 700           |\n",
      "|    time_elapsed       | 6             |\n",
      "|    total_timesteps    | 3500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.44         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 699           |\n",
      "|    policy_loss        | -0.0255       |\n",
      "|    reward             | 0.00081225164 |\n",
      "|    std                | 1.03          |\n",
      "|    value_loss         | 0.00034       |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.45      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -0.393     |\n",
      "|    reward             | 0.09346619 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.183      |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 546           |\n",
      "|    iterations         | 900           |\n",
      "|    time_elapsed       | 8             |\n",
      "|    total_timesteps    | 4500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.46         |\n",
      "|    explained_variance | 5.96e-08      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 899           |\n",
      "|    policy_loss        | -0.108        |\n",
      "|    reward             | -0.0027179741 |\n",
      "|    std                | 1.04          |\n",
      "|    value_loss         | 0.0621        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.47       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 3.52        |\n",
      "|    reward             | -0.15056561 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 4.13        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 551         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.48       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 0.691       |\n",
      "|    reward             | -0.32394835 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.55        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 555      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.49    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00227 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 1.18e-05 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 559            |\n",
      "|    iterations         | 1300           |\n",
      "|    time_elapsed       | 11             |\n",
      "|    total_timesteps    | 6500           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.5           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1299           |\n",
      "|    policy_loss        | -1.48e-05      |\n",
      "|    reward             | -2.1422447e-05 |\n",
      "|    std                | 1.09           |\n",
      "|    value_loss         | 2.93e-09       |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.52    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.00215 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.11     |\n",
      "|    value_loss         | 2.2e-06  |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 560            |\n",
      "|    iterations         | 1500           |\n",
      "|    time_elapsed       | 13             |\n",
      "|    total_timesteps    | 7500           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.55          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1499           |\n",
      "|    policy_loss        | -0.000638      |\n",
      "|    reward             | -0.00017779625 |\n",
      "|    std                | 1.14           |\n",
      "|    value_loss         | 1.88e-07       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 562            |\n",
      "|    iterations         | 1600           |\n",
      "|    time_elapsed       | 14             |\n",
      "|    total_timesteps    | 8000           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.59          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1599           |\n",
      "|    policy_loss        | 0.00129        |\n",
      "|    reward             | -0.00018282539 |\n",
      "|    std                | 1.19           |\n",
      "|    value_loss         | 9.02e-07       |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 559      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.64    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00017  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 8.28e-09 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 561           |\n",
      "|    iterations         | 1800          |\n",
      "|    time_elapsed       | 16            |\n",
      "|    total_timesteps    | 9000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.68         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1799          |\n",
      "|    policy_loss        | -0.0187       |\n",
      "|    reward             | -0.0004004683 |\n",
      "|    std                | 1.3           |\n",
      "|    value_loss         | 0.000293      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 559          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 16           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.68        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | -0.0201      |\n",
      "|    reward             | 0.0074202185 |\n",
      "|    std                | 1.29         |\n",
      "|    value_loss         | 0.000429     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 557          |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 17           |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.68        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | 0.262        |\n",
      "|    reward             | -0.011574222 |\n",
      "|    std                | 1.3          |\n",
      "|    value_loss         | 0.015        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 2100          |\n",
      "|    time_elapsed       | 18            |\n",
      "|    total_timesteps    | 10500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.69         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2099          |\n",
      "|    policy_loss        | -0.0054       |\n",
      "|    reward             | -0.0041129133 |\n",
      "|    std                | 1.31          |\n",
      "|    value_loss         | 1.32e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.69    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.00493 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.31     |\n",
      "|    value_loss         | 9.61e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -4.22e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.33      |\n",
      "|    value_loss         | 1.03e-06  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.72      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | -5.24      |\n",
      "|    reward             | -1.0837018 |\n",
      "|    std                | 1.35       |\n",
      "|    value_loss         | 13.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.7       |\n",
      "|    explained_variance | 0.00574    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -3.86      |\n",
      "|    reward             | 0.31863287 |\n",
      "|    std                | 1.32       |\n",
      "|    value_loss         | 10.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.71       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | 1.01        |\n",
      "|    reward             | 0.021350259 |\n",
      "|    std                | 1.34        |\n",
      "|    value_loss         | 0.686       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 2700       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 13500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.72      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2699       |\n",
      "|    policy_loss        | 0.952      |\n",
      "|    reward             | 0.49382105 |\n",
      "|    std                | 1.35       |\n",
      "|    value_loss         | 0.402      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 2800        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 14000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.73       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2799        |\n",
      "|    policy_loss        | -4.89       |\n",
      "|    reward             | -0.08920447 |\n",
      "|    std                | 1.36        |\n",
      "|    value_loss         | 17.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 558       |\n",
      "|    iterations         | 2900      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 14500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.73     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2899      |\n",
      "|    policy_loss        | -3.43     |\n",
      "|    reward             | 1.3672605 |\n",
      "|    std                | 1.37      |\n",
      "|    value_loss         | 5.55      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 560        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.73      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 2.26       |\n",
      "|    reward             | -10.865086 |\n",
      "|    std                | 1.37       |\n",
      "|    value_loss         | 3.23       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.72    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 4.46     |\n",
      "|    reward             | 2.321858 |\n",
      "|    std                | 1.35     |\n",
      "|    value_loss         | 61.2     |\n",
      "------------------------------------\n",
      "day: 3146, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6224357.12\n",
      "total_reward: 5224357.12\n",
      "total_cost: 10084.66\n",
      "total_trades: 3118\n",
      "Sharpe: 0.848\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 558           |\n",
      "|    iterations         | 3200          |\n",
      "|    time_elapsed       | 28            |\n",
      "|    total_timesteps    | 16000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.72         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3199          |\n",
      "|    policy_loss        | 0.00464       |\n",
      "|    reward             | -0.0082699265 |\n",
      "|    std                | 1.35          |\n",
      "|    value_loss         | 1.65e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 554      |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 16500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | -0.00242 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.36     |\n",
      "|    value_loss         | 3.27e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 17000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.74    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.00797 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.38     |\n",
      "|    value_loss         | 2.94e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 552       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.77     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -0.0199   |\n",
      "|    reward             | 0.2557052 |\n",
      "|    std                | 1.42      |\n",
      "|    value_loss         | 0.00503   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.77      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | 0.579      |\n",
      "|    reward             | -4.0817327 |\n",
      "|    std                | 1.43       |\n",
      "|    value_loss         | 3.5        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 552       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.78     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | 1.36      |\n",
      "|    reward             | 2.9910977 |\n",
      "|    std                | 1.43      |\n",
      "|    value_loss         | 3.44      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 3800        |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 19000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.79       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3799        |\n",
      "|    policy_loss        | -0.00531    |\n",
      "|    reward             | -0.00794578 |\n",
      "|    std                | 1.44        |\n",
      "|    value_loss         | 5.85e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 553         |\n",
      "|    iterations         | 3900        |\n",
      "|    time_elapsed       | 35          |\n",
      "|    total_timesteps    | 19500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.79       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3899        |\n",
      "|    policy_loss        | 0.029       |\n",
      "|    reward             | 0.004709736 |\n",
      "|    std                | 1.45        |\n",
      "|    value_loss         | 0.000259    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 553          |\n",
      "|    iterations         | 4000         |\n",
      "|    time_elapsed       | 36           |\n",
      "|    total_timesteps    | 20000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.8         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3999         |\n",
      "|    policy_loss        | 0.0155       |\n",
      "|    reward             | -0.014877248 |\n",
      "|    std                | 1.47         |\n",
      "|    value_loss         | 0.000187     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 554            |\n",
      "|    iterations         | 4100           |\n",
      "|    time_elapsed       | 36             |\n",
      "|    total_timesteps    | 20500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.82          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 4099           |\n",
      "|    policy_loss        | -0.00313       |\n",
      "|    reward             | -0.00042272964 |\n",
      "|    std                | 1.49           |\n",
      "|    value_loss         | 7.64e-06       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 555          |\n",
      "|    iterations         | 4200         |\n",
      "|    time_elapsed       | 37           |\n",
      "|    total_timesteps    | 21000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.83        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4199         |\n",
      "|    policy_loss        | 0.0148       |\n",
      "|    reward             | 0.0043926835 |\n",
      "|    std                | 1.51         |\n",
      "|    value_loss         | 7.77e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 554          |\n",
      "|    iterations         | 4300         |\n",
      "|    time_elapsed       | 38           |\n",
      "|    total_timesteps    | 21500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.83        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4299         |\n",
      "|    policy_loss        | 0.0455       |\n",
      "|    reward             | -0.010232092 |\n",
      "|    std                | 1.51         |\n",
      "|    value_loss         | 0.00579      |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | -1.72     |\n",
      "|    reward             | 2.447931  |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 4.45      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 554          |\n",
      "|    iterations         | 4500         |\n",
      "|    time_elapsed       | 40           |\n",
      "|    total_timesteps    | 22500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.81        |\n",
      "|    explained_variance | -2.38e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4499         |\n",
      "|    policy_loss        | -0.0326      |\n",
      "|    reward             | -0.055234447 |\n",
      "|    std                | 1.48         |\n",
      "|    value_loss         | 0.00181      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 555         |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 0.144       |\n",
      "|    reward             | -0.03882865 |\n",
      "|    std                | 1.49        |\n",
      "|    value_loss         | 0.0235      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 4700        |\n",
      "|    time_elapsed       | 42          |\n",
      "|    total_timesteps    | 23500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.83       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4699        |\n",
      "|    policy_loss        | 1.88        |\n",
      "|    reward             | -0.17297146 |\n",
      "|    std                | 1.51        |\n",
      "|    value_loss         | 1.63        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -0.513    |\n",
      "|    reward             | 1.5176181 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 1.85      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 555      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.82    |\n",
      "|    explained_variance | 0.0714   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 8.92     |\n",
      "|    reward             | 2.188979 |\n",
      "|    std                | 1.5      |\n",
      "|    value_loss         | 19.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0.139     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | 7.04      |\n",
      "|    reward             | 0.7889131 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 34.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 5100        |\n",
      "|    time_elapsed       | 45          |\n",
      "|    total_timesteps    | 25500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5099        |\n",
      "|    policy_loss        | -0.742      |\n",
      "|    reward             | -0.32453278 |\n",
      "|    std                | 1.5         |\n",
      "|    value_loss         | 0.165       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -1.11     |\n",
      "|    reward             | -1.803886 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 0.641     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -0.564    |\n",
      "|    reward             | 1.6055942 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 3.43      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0.0725     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | -0.143     |\n",
      "|    reward             | 0.06907002 |\n",
      "|    std                | 1.49       |\n",
      "|    value_loss         | 1.08       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | -0.00582  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | 6.45      |\n",
      "|    reward             | 2.2107882 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 46.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | -0.00061   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | 23.8       |\n",
      "|    reward             | -18.021084 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 470        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | 0.496      |\n",
      "|    reward             | 0.28763375 |\n",
      "|    std                | 1.46       |\n",
      "|    value_loss         | 0.0821     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | -1.56      |\n",
      "|    reward             | -3.1730783 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 2.27       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0.00205    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 1.8        |\n",
      "|    reward             | -1.0557511 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 1.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -3.74      |\n",
      "|    reward             | 0.18022063 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 4.74       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 54       |\n",
      "|    total_timesteps    | 30500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0.00315  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 10.7     |\n",
      "|    reward             | 3.332808 |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 70.4     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | 27.4      |\n",
      "|    reward             | 46.891293 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 599       |\n",
      "-------------------------------------\n",
      "day: 3146, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11007565.83\n",
      "total_reward: 10007565.83\n",
      "total_cost: 7418.38\n",
      "total_trades: 3132\n",
      "Sharpe: 0.879\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 556          |\n",
      "|    iterations         | 6300         |\n",
      "|    time_elapsed       | 56           |\n",
      "|    total_timesteps    | 31500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.8         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6299         |\n",
      "|    policy_loss        | -0.0158      |\n",
      "|    reward             | -0.007157558 |\n",
      "|    std                | 1.47         |\n",
      "|    value_loss         | 0.000541     |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 0.502     |\n",
      "|    reward             | 2.3791475 |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 0.532     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | 0.75654554 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 109        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | 5.75       |\n",
      "|    reward             | -3.5885708 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 21.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | 2.48       |\n",
      "|    reward             | 0.75977796 |\n",
      "|    std                | 1.47       |\n",
      "|    value_loss         | 10.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.81     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | 1.38      |\n",
      "|    reward             | 3.0006268 |\n",
      "|    std                | 1.48      |\n",
      "|    value_loss         | 76.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 557      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 61       |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 8.88e-06 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -65.1    |\n",
      "|    reward             | 7.008564 |\n",
      "|    std                | 1.47     |\n",
      "|    value_loss         | 2.4e+03  |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 556          |\n",
      "|    iterations         | 7000         |\n",
      "|    time_elapsed       | 62           |\n",
      "|    total_timesteps    | 35000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.81        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6999         |\n",
      "|    policy_loss        | 2.96         |\n",
      "|    reward             | -0.024763245 |\n",
      "|    std                | 1.48         |\n",
      "|    value_loss         | 1.66         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 3.18      |\n",
      "|    reward             | 1.6261064 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 4.96      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 7200        |\n",
      "|    time_elapsed       | 64          |\n",
      "|    total_timesteps    | 36000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7199        |\n",
      "|    policy_loss        | -0.729      |\n",
      "|    reward             | -0.28825465 |\n",
      "|    std                | 1.49        |\n",
      "|    value_loss         | 1.11        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 65         |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -0.881     |\n",
      "|    reward             | -3.9149313 |\n",
      "|    std                | 1.49       |\n",
      "|    value_loss         | 9.29       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 557         |\n",
      "|    iterations         | 7400        |\n",
      "|    time_elapsed       | 66          |\n",
      "|    total_timesteps    | 37000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.83       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7399        |\n",
      "|    policy_loss        | -1.17       |\n",
      "|    reward             | -0.38603982 |\n",
      "|    std                | 1.5         |\n",
      "|    value_loss         | 3.01        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | 2.93      |\n",
      "|    reward             | 12.901775 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 95.1      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 557          |\n",
      "|    iterations         | 7600         |\n",
      "|    time_elapsed       | 68           |\n",
      "|    total_timesteps    | 38000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.82        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7599         |\n",
      "|    policy_loss        | 0.186        |\n",
      "|    reward             | -0.077456005 |\n",
      "|    std                | 1.49         |\n",
      "|    value_loss         | 0.0335       |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | 3.72      |\n",
      "|    reward             | -7.041875 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 6.91      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 3.49      |\n",
      "|    reward             | -6.939302 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 6.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | 5.73       |\n",
      "|    reward             | -1.0653471 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 21.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 8000       |\n",
      "|    time_elapsed       | 71         |\n",
      "|    total_timesteps    | 40000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.82      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7999       |\n",
      "|    policy_loss        | -37.8      |\n",
      "|    reward             | -14.779156 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 495        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 75.7      |\n",
      "|    reward             | 0.3939332 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 3.44e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | -0.438     |\n",
      "|    reward             | 0.12485026 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 0.0519     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 74         |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | 4.73       |\n",
      "|    reward             | -2.5422163 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 15.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 558       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | 13.5      |\n",
      "|    reward             | 0.2762805 |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 83.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | -8.08      |\n",
      "|    reward             | -11.921818 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 44.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 558        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 77         |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 7.25       |\n",
      "|    reward             | -0.9093513 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 31.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 48.2       |\n",
      "|    reward             | -7.5777755 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 959        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | -0.000666 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | 33.3      |\n",
      "|    reward             | -34.28333 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 480       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 8900        |\n",
      "|    time_elapsed       | 79          |\n",
      "|    total_timesteps    | 44500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.82       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8899        |\n",
      "|    policy_loss        | -1.39       |\n",
      "|    reward             | -0.22830972 |\n",
      "|    std                | 1.5         |\n",
      "|    value_loss         | 1.17        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.83    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | 7.6      |\n",
      "|    reward             | 2.765801 |\n",
      "|    std                | 1.51     |\n",
      "|    value_loss         | 22.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 81         |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | -7.37      |\n",
      "|    reward             | -0.5209756 |\n",
      "|    std                | 1.51       |\n",
      "|    value_loss         | 17.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | -6.74      |\n",
      "|    reward             | -4.9661436 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 12.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | 23.5      |\n",
      "|    reward             | 1.5396876 |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 133       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 41.4      |\n",
      "|    reward             | -39.90528 |\n",
      "|    std                | 1.5       |\n",
      "|    value_loss         | 773       |\n",
      "-------------------------------------\n",
      "day: 3146, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11122978.78\n",
      "total_reward: 10122978.78\n",
      "total_cost: 1098.58\n",
      "total_trades: 3142\n",
      "Sharpe: 0.883\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 9500        |\n",
      "|    time_elapsed       | 85          |\n",
      "|    total_timesteps    | 47500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.83       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9499        |\n",
      "|    policy_loss        | 1.15        |\n",
      "|    reward             | -0.41016316 |\n",
      "|    std                | 1.5         |\n",
      "|    value_loss         | 0.56        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 86         |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.83      |\n",
      "|    explained_variance | 0.065      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | -4.29      |\n",
      "|    reward             | -2.5266328 |\n",
      "|    std                | 1.5        |\n",
      "|    value_loss         | 12         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 87        |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.82     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 3.49      |\n",
      "|    reward             | 2.9798496 |\n",
      "|    std                | 1.49      |\n",
      "|    value_loss         | 7.94      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.81    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | 5.05     |\n",
      "|    reward             | 1.863354 |\n",
      "|    std                | 1.48     |\n",
      "|    value_loss         | 11.1     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 88          |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.81       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | -1.84       |\n",
      "|    reward             | -0.74508125 |\n",
      "|    std                | 1.48        |\n",
      "|    value_loss         | 30.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 8.28e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -107      |\n",
      "|    reward             | -4.964639 |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 1.07e+03  |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 556          |\n",
      "|    iterations         | 10100        |\n",
      "|    time_elapsed       | 90           |\n",
      "|    total_timesteps    | 50500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.79        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 10099        |\n",
      "|    policy_loss        | -0.196       |\n",
      "|    reward             | -0.055587307 |\n",
      "|    std                | 1.45         |\n",
      "|    value_loss         | 0.0188       |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 10200     |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 51000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10199     |\n",
      "|    policy_loss        | -1.28     |\n",
      "|    reward             | 1.8196046 |\n",
      "|    std                | 1.46      |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 10300     |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 51500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10299     |\n",
      "|    policy_loss        | -4.28     |\n",
      "|    reward             | 2.4287598 |\n",
      "|    std                | 1.46      |\n",
      "|    value_loss         | 10.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 10400      |\n",
      "|    time_elapsed       | 93         |\n",
      "|    total_timesteps    | 52000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10399      |\n",
      "|    policy_loss        | 4.6        |\n",
      "|    reward             | -0.9916465 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 9.44       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | 11.5      |\n",
      "|    reward             | 1.0439154 |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 49.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | 34.9       |\n",
      "|    reward             | -3.5195549 |\n",
      "|    std                | 1.46       |\n",
      "|    value_loss         | 593        |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 556           |\n",
      "|    iterations         | 10700         |\n",
      "|    time_elapsed       | 96            |\n",
      "|    total_timesteps    | 53500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.79         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10699         |\n",
      "|    policy_loss        | 6.26          |\n",
      "|    reward             | 4.7163358e-05 |\n",
      "|    std                | 1.45          |\n",
      "|    value_loss         | 396           |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 10800      |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 54000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.79      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10799      |\n",
      "|    policy_loss        | 3.08       |\n",
      "|    reward             | -0.5682706 |\n",
      "|    std                | 1.45       |\n",
      "|    value_loss         | 3.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 97         |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.8       |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | -3.24      |\n",
      "|    reward             | -0.6172514 |\n",
      "|    std                | 1.46       |\n",
      "|    value_loss         | 5.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 98        |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.79     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -15.1     |\n",
      "|    reward             | -5.469409 |\n",
      "|    std                | 1.45      |\n",
      "|    value_loss         | 80.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.79     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | -5.7      |\n",
      "|    reward             | 1.0976777 |\n",
      "|    std                | 1.45      |\n",
      "|    value_loss         | 14.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | -22.8     |\n",
      "|    reward             | 3.0950537 |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 190       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 11300       |\n",
      "|    time_elapsed       | 101         |\n",
      "|    total_timesteps    | 56500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.8        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11299       |\n",
      "|    policy_loss        | -47.5       |\n",
      "|    reward             | -15.4636965 |\n",
      "|    std                | 1.46        |\n",
      "|    value_loss         | 1.05e+03    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.8      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | -0.46     |\n",
      "|    reward             | 1.0950006 |\n",
      "|    std                | 1.47      |\n",
      "|    value_loss         | 0.167     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 11500      |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 57500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.81      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11499      |\n",
      "|    policy_loss        | 0.341      |\n",
      "|    reward             | -0.9314379 |\n",
      "|    std                | 1.48       |\n",
      "|    value_loss         | 7.01       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 11600     |\n",
      "|    time_elapsed       | 104       |\n",
      "|    total_timesteps    | 58000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.83     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11599     |\n",
      "|    policy_loss        | 7.27      |\n",
      "|    reward             | 2.4360435 |\n",
      "|    std                | 1.51      |\n",
      "|    value_loss         | 19.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 11700      |\n",
      "|    time_elapsed       | 105        |\n",
      "|    total_timesteps    | 58500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.84      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11699      |\n",
      "|    policy_loss        | 15.3       |\n",
      "|    reward             | -0.4380885 |\n",
      "|    std                | 1.52       |\n",
      "|    value_loss         | 103        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 11800      |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 59000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11799      |\n",
      "|    policy_loss        | 5.89       |\n",
      "|    reward             | -22.506363 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 62.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | -6.89      |\n",
      "|    reward             | 13.1025305 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 71.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 12000       |\n",
      "|    time_elapsed       | 107         |\n",
      "|    total_timesteps    | 60000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.86       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11999       |\n",
      "|    policy_loss        | -0.326      |\n",
      "|    reward             | -0.16218302 |\n",
      "|    std                | 1.56        |\n",
      "|    value_loss         | 0.136       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 108       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -0.37     |\n",
      "|    reward             | 5.1851463 |\n",
      "|    std                | 1.55      |\n",
      "|    value_loss         | 1.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 12200     |\n",
      "|    time_elapsed       | 109       |\n",
      "|    total_timesteps    | 61000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12199     |\n",
      "|    policy_loss        | -2.07     |\n",
      "|    reward             | 3.7908852 |\n",
      "|    std                | 1.57      |\n",
      "|    value_loss         | 3.87      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 12300      |\n",
      "|    time_elapsed       | 110        |\n",
      "|    total_timesteps    | 61500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12299      |\n",
      "|    policy_loss        | 3.63       |\n",
      "|    reward             | -0.6782891 |\n",
      "|    std                | 1.56       |\n",
      "|    value_loss         | 1.95       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 12400     |\n",
      "|    time_elapsed       | 111       |\n",
      "|    total_timesteps    | 62000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12399     |\n",
      "|    policy_loss        | 17.8      |\n",
      "|    reward             | 6.0341005 |\n",
      "|    std                | 1.55      |\n",
      "|    value_loss         | 102       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 12500     |\n",
      "|    time_elapsed       | 112       |\n",
      "|    total_timesteps    | 62500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.85     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12499     |\n",
      "|    policy_loss        | 74.9      |\n",
      "|    reward             | 27.788477 |\n",
      "|    std                | 1.54      |\n",
      "|    value_loss         | 1.76e+03  |\n",
      "-------------------------------------\n",
      "day: 3146, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11424444.36\n",
      "total_reward: 10424444.36\n",
      "total_cost: 1069.08\n",
      "total_trades: 3146\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 556          |\n",
      "|    iterations         | 12600        |\n",
      "|    time_elapsed       | 113          |\n",
      "|    total_timesteps    | 63000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.85        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12599        |\n",
      "|    policy_loss        | 0.115        |\n",
      "|    reward             | -0.015491366 |\n",
      "|    std                | 1.54         |\n",
      "|    value_loss         | 0.00303      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 12700      |\n",
      "|    time_elapsed       | 114        |\n",
      "|    total_timesteps    | 63500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.86      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12699      |\n",
      "|    policy_loss        | 4.39       |\n",
      "|    reward             | -0.5548984 |\n",
      "|    std                | 1.55       |\n",
      "|    value_loss         | 15.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 12800     |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 64000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12799     |\n",
      "|    policy_loss        | -5.36     |\n",
      "|    reward             | 1.0708077 |\n",
      "|    std                | 1.57      |\n",
      "|    value_loss         | 8.89      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 12900     |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 64500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.87     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12899     |\n",
      "|    policy_loss        | -3.38     |\n",
      "|    reward             | 2.5930989 |\n",
      "|    std                | 1.56      |\n",
      "|    value_loss         | 5.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 116       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.86     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | -3.86     |\n",
      "|    reward             | 3.367215  |\n",
      "|    std                | 1.56      |\n",
      "|    value_loss         | 13.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 13100      |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 65500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.88      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13099      |\n",
      "|    policy_loss        | -85        |\n",
      "|    reward             | -7.7430806 |\n",
      "|    std                | 1.58       |\n",
      "|    value_loss         | 2.23e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.89      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | -93.4      |\n",
      "|    reward             | -15.495949 |\n",
      "|    std                | 1.6        |\n",
      "|    value_loss         | 2.48e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 13300     |\n",
      "|    time_elapsed       | 119       |\n",
      "|    total_timesteps    | 66500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.89     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13299     |\n",
      "|    policy_loss        | 1.16      |\n",
      "|    reward             | 2.0784357 |\n",
      "|    std                | 1.59      |\n",
      "|    value_loss         | 1.02      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 13400      |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 67000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.89      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13399      |\n",
      "|    policy_loss        | 8.89       |\n",
      "|    reward             | -1.6909608 |\n",
      "|    std                | 1.6        |\n",
      "|    value_loss         | 39         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 13500     |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 67500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | -4.62     |\n",
      "|    reward             | 1.4946914 |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 31.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 13600     |\n",
      "|    time_elapsed       | 122       |\n",
      "|    total_timesteps    | 68000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13599     |\n",
      "|    policy_loss        | 19.9      |\n",
      "|    reward             | 1.8718278 |\n",
      "|    std                | 1.64      |\n",
      "|    value_loss         | 148       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | -0.235    |\n",
      "|    reward             | 7.2066627 |\n",
      "|    std                | 1.64      |\n",
      "|    value_loss         | 3.32      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | 50.2      |\n",
      "|    reward             | 1.7653589 |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 716       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 13900     |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 69500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.91     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13899     |\n",
      "|    policy_loss        | 0.954     |\n",
      "|    reward             | 0.7724713 |\n",
      "|    std                | 1.63      |\n",
      "|    value_loss         | 0.188     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 14000       |\n",
      "|    time_elapsed       | 125         |\n",
      "|    total_timesteps    | 70000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.91       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13999       |\n",
      "|    policy_loss        | -1.19       |\n",
      "|    reward             | -0.56529313 |\n",
      "|    std                | 1.63        |\n",
      "|    value_loss         | 1.09        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 14100      |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 70500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14099      |\n",
      "|    policy_loss        | -4.55      |\n",
      "|    reward             | -0.7557017 |\n",
      "|    std                | 1.61       |\n",
      "|    value_loss         | 31.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 14200     |\n",
      "|    time_elapsed       | 127       |\n",
      "|    total_timesteps    | 71000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.89     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14199     |\n",
      "|    policy_loss        | -4.86     |\n",
      "|    reward             | 2.0209396 |\n",
      "|    std                | 1.6       |\n",
      "|    value_loss         | 6.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 14300      |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 71500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.9       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14299      |\n",
      "|    policy_loss        | 6.72       |\n",
      "|    reward             | 11.7141695 |\n",
      "|    std                | 1.61       |\n",
      "|    value_loss         | 104        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.89      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | 21         |\n",
      "|    reward             | -14.384922 |\n",
      "|    std                | 1.61       |\n",
      "|    value_loss         | 522        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 14500       |\n",
      "|    time_elapsed       | 130         |\n",
      "|    total_timesteps    | 72500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.89       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14499       |\n",
      "|    policy_loss        | 1.01        |\n",
      "|    reward             | -0.06717929 |\n",
      "|    std                | 1.61        |\n",
      "|    value_loss         | 0.222       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 14600    |\n",
      "|    time_elapsed       | 131      |\n",
      "|    total_timesteps    | 73000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.89    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14599    |\n",
      "|    policy_loss        | 0.128    |\n",
      "|    reward             | 2.122012 |\n",
      "|    std                | 1.61     |\n",
      "|    value_loss         | 1.06     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | 4.84      |\n",
      "|    reward             | 1.6526182 |\n",
      "|    std                | 1.61      |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 14800     |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 74000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14799     |\n",
      "|    policy_loss        | -8.31     |\n",
      "|    reward             | 1.3243155 |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 23.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 14900     |\n",
      "|    time_elapsed       | 133       |\n",
      "|    total_timesteps    | 74500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.9      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14899     |\n",
      "|    policy_loss        | -2.46     |\n",
      "|    reward             | 2.8572946 |\n",
      "|    std                | 1.62      |\n",
      "|    value_loss         | 1.55      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 556      |\n",
      "|    iterations         | 15000    |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 75000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.91    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14999    |\n",
      "|    policy_loss        | 20.6     |\n",
      "|    reward             | 7.754621 |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 117      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.91      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 34.4       |\n",
      "|    reward             | -22.088564 |\n",
      "|    std                | 1.63       |\n",
      "|    value_loss         | 1.26e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | 1.53      |\n",
      "|    reward             | 1.3608968 |\n",
      "|    std                | 1.64      |\n",
      "|    value_loss         | 0.757     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | -0.0665   |\n",
      "|    reward             | 0.2454542 |\n",
      "|    std                | 1.65      |\n",
      "|    value_loss         | 0.547     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | 3.61      |\n",
      "|    reward             | 0.2602423 |\n",
      "|    std                | 1.65      |\n",
      "|    value_loss         | 23.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | -3.39     |\n",
      "|    reward             | -4.136722 |\n",
      "|    std                | 1.65      |\n",
      "|    value_loss         | 13.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 15600      |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 78000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.92      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15599      |\n",
      "|    policy_loss        | 15.5       |\n",
      "|    reward             | -11.424812 |\n",
      "|    std                | 1.66       |\n",
      "|    value_loss         | 52.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 15700      |\n",
      "|    time_elapsed       | 141        |\n",
      "|    total_timesteps    | 78500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.92      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15699      |\n",
      "|    policy_loss        | 38.9       |\n",
      "|    reward             | -6.3705626 |\n",
      "|    std                | 1.65       |\n",
      "|    value_loss         | 414        |\n",
      "--------------------------------------\n",
      "day: 3146, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11463635.70\n",
      "total_reward: 10463635.70\n",
      "total_cost: 1011.57\n",
      "total_trades: 3146\n",
      "Sharpe: 0.883\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 556        |\n",
      "|    iterations         | 15800      |\n",
      "|    time_elapsed       | 141        |\n",
      "|    total_timesteps    | 79000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.93      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15799      |\n",
      "|    policy_loss        | -1.37      |\n",
      "|    reward             | 0.43129057 |\n",
      "|    std                | 1.66       |\n",
      "|    value_loss         | 0.671      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.92     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | -3.16     |\n",
      "|    reward             | 1.5982552 |\n",
      "|    std                | 1.65      |\n",
      "|    value_loss         | 2.5       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 556       |\n",
      "|    iterations         | 16000     |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15999     |\n",
      "|    policy_loss        | 0.912     |\n",
      "|    reward             | 0.9877194 |\n",
      "|    std                | 1.67      |\n",
      "|    value_loss         | 3.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 16100     |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 80500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16099     |\n",
      "|    policy_loss        | 10.6      |\n",
      "|    reward             | 2.0355806 |\n",
      "|    std                | 1.69      |\n",
      "|    value_loss         | 24        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -4.21     |\n",
      "|    reward             | 2.5309873 |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 40.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 555      |\n",
      "|    iterations         | 16300    |\n",
      "|    time_elapsed       | 146      |\n",
      "|    total_timesteps    | 81500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.93    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16299    |\n",
      "|    policy_loss        | -38.4    |\n",
      "|    reward             | 4.006106 |\n",
      "|    std                | 1.67     |\n",
      "|    value_loss         | 753      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 555         |\n",
      "|    iterations         | 16400       |\n",
      "|    time_elapsed       | 147         |\n",
      "|    total_timesteps    | 82000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.94       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16399       |\n",
      "|    policy_loss        | 0.391       |\n",
      "|    reward             | -0.06466145 |\n",
      "|    std                | 1.68        |\n",
      "|    value_loss         | 0.0904      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 554        |\n",
      "|    iterations         | 16500      |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 82500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16499      |\n",
      "|    policy_loss        | 0.888      |\n",
      "|    reward             | -0.4428705 |\n",
      "|    std                | 1.68       |\n",
      "|    value_loss         | 1.71       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 554         |\n",
      "|    iterations         | 16600       |\n",
      "|    time_elapsed       | 149         |\n",
      "|    total_timesteps    | 83000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.95       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16599       |\n",
      "|    policy_loss        | 2.57        |\n",
      "|    reward             | -0.67127603 |\n",
      "|    std                | 1.7         |\n",
      "|    value_loss         | 3.25        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 16700     |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 83500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16699     |\n",
      "|    policy_loss        | -8.46     |\n",
      "|    reward             | 1.1872535 |\n",
      "|    std                | 1.69      |\n",
      "|    value_loss         | 11.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | 4.09      |\n",
      "|    reward             | 3.4262087 |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 36.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | 2.38e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | 18.2      |\n",
      "|    reward             | 11.879812 |\n",
      "|    std                | 1.67      |\n",
      "|    value_loss         | 394       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 554          |\n",
      "|    iterations         | 17000        |\n",
      "|    time_elapsed       | 153          |\n",
      "|    total_timesteps    | 85000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.93        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 16999        |\n",
      "|    policy_loss        | -0.0569      |\n",
      "|    reward             | 0.0035004688 |\n",
      "|    std                | 1.67         |\n",
      "|    value_loss         | 0.00193      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 555         |\n",
      "|    iterations         | 17100       |\n",
      "|    time_elapsed       | 154         |\n",
      "|    total_timesteps    | 85500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.93       |\n",
      "|    explained_variance | -2.38e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17099       |\n",
      "|    policy_loss        | 1.45        |\n",
      "|    reward             | 0.039983172 |\n",
      "|    std                | 1.67        |\n",
      "|    value_loss         | 1.01        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 17200     |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 86000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17199     |\n",
      "|    policy_loss        | -10       |\n",
      "|    reward             | 1.8386586 |\n",
      "|    std                | 1.66      |\n",
      "|    value_loss         | 73.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -20.7     |\n",
      "|    reward             | 3.4650488 |\n",
      "|    std                | 1.67      |\n",
      "|    value_loss         | 125       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 17400      |\n",
      "|    time_elapsed       | 156        |\n",
      "|    total_timesteps    | 87000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17399      |\n",
      "|    policy_loss        | -7.28      |\n",
      "|    reward             | -6.8326707 |\n",
      "|    std                | 1.68       |\n",
      "|    value_loss         | 12.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 17500      |\n",
      "|    time_elapsed       | 157        |\n",
      "|    total_timesteps    | 87500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.94      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17499      |\n",
      "|    policy_loss        | 7.86       |\n",
      "|    reward             | -1.8242762 |\n",
      "|    std                | 1.69       |\n",
      "|    value_loss         | 50.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 158        |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | -56.4      |\n",
      "|    reward             | -26.322393 |\n",
      "|    std                | 1.7        |\n",
      "|    value_loss         | 2.14e+03   |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 17700      |\n",
      "|    time_elapsed       | 159        |\n",
      "|    total_timesteps    | 88500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.94      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17699      |\n",
      "|    policy_loss        | 2.18       |\n",
      "|    reward             | 0.49565843 |\n",
      "|    std                | 1.69       |\n",
      "|    value_loss         | 2.21       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 554         |\n",
      "|    iterations         | 17800       |\n",
      "|    time_elapsed       | 160         |\n",
      "|    total_timesteps    | 89000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.94       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17799       |\n",
      "|    policy_loss        | 5.87        |\n",
      "|    reward             | -0.40786088 |\n",
      "|    std                | 1.69        |\n",
      "|    value_loss         | 14.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 554         |\n",
      "|    iterations         | 17900       |\n",
      "|    time_elapsed       | 161         |\n",
      "|    total_timesteps    | 89500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.95       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17899       |\n",
      "|    policy_loss        | -4.19       |\n",
      "|    reward             | -0.80814964 |\n",
      "|    std                | 1.7         |\n",
      "|    value_loss         | 4.75        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | 2.54      |\n",
      "|    reward             | 4.0116415 |\n",
      "|    std                | 1.7       |\n",
      "|    value_loss         | 8.47      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 554        |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 163        |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | 12.2       |\n",
      "|    reward             | -5.9282446 |\n",
      "|    std                | 1.7        |\n",
      "|    value_loss         | 62.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 18200      |\n",
      "|    time_elapsed       | 163        |\n",
      "|    total_timesteps    | 91000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18199      |\n",
      "|    policy_loss        | 14.7       |\n",
      "|    reward             | -10.674367 |\n",
      "|    std                | 1.7        |\n",
      "|    value_loss         | 61.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 555         |\n",
      "|    iterations         | 18300       |\n",
      "|    time_elapsed       | 164         |\n",
      "|    total_timesteps    | 91500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.95       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18299       |\n",
      "|    policy_loss        | 0.121       |\n",
      "|    reward             | 0.057233498 |\n",
      "|    std                | 1.7         |\n",
      "|    value_loss         | 0.0268      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 3.92      |\n",
      "|    reward             | 1.899492  |\n",
      "|    std                | 1.7       |\n",
      "|    value_loss         | 9.63      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 555         |\n",
      "|    iterations         | 18500       |\n",
      "|    time_elapsed       | 166         |\n",
      "|    total_timesteps    | 92500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.94       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18499       |\n",
      "|    policy_loss        | 7.75        |\n",
      "|    reward             | -0.80177474 |\n",
      "|    std                | 1.69        |\n",
      "|    value_loss         | 25.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | 5.16      |\n",
      "|    reward             | 1.0607975 |\n",
      "|    std                | 1.67      |\n",
      "|    value_loss         | 16.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 18700     |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 93500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18699     |\n",
      "|    policy_loss        | -36.4     |\n",
      "|    reward             | -17.34782 |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 658       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 555      |\n",
      "|    iterations         | 18800    |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 94000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.94    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18799    |\n",
      "|    policy_loss        | 47.6     |\n",
      "|    reward             | 8.320384 |\n",
      "|    std                | 1.68     |\n",
      "|    value_loss         | 1.36e+03 |\n",
      "------------------------------------\n",
      "day: 3146, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11426221.52\n",
      "total_reward: 10426221.52\n",
      "total_cost: 998.99\n",
      "total_trades: 3146\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 556         |\n",
      "|    iterations         | 18900       |\n",
      "|    time_elapsed       | 169         |\n",
      "|    total_timesteps    | 94500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.93       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18899       |\n",
      "|    policy_loss        | -0.746      |\n",
      "|    reward             | -0.10158605 |\n",
      "|    std                | 1.67        |\n",
      "|    value_loss         | 0.23        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 19000      |\n",
      "|    time_elapsed       | 170        |\n",
      "|    total_timesteps    | 95000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.94      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18999      |\n",
      "|    policy_loss        | -3.91      |\n",
      "|    reward             | 0.75442296 |\n",
      "|    std                | 1.68       |\n",
      "|    value_loss         | 14.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | 31.3      |\n",
      "|    reward             | 2.1490068 |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 150       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 19200     |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 96000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19199     |\n",
      "|    policy_loss        | -12.3     |\n",
      "|    reward             | -5.570551 |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 52.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 173        |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | 13.8       |\n",
      "|    reward             | -4.4448543 |\n",
      "|    std                | 1.68       |\n",
      "|    value_loss         | 51.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 19400      |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 97000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19399      |\n",
      "|    policy_loss        | 51.6       |\n",
      "|    reward             | -11.667419 |\n",
      "|    std                | 1.68       |\n",
      "|    value_loss         | 1.02e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 19500     |\n",
      "|    time_elapsed       | 175       |\n",
      "|    total_timesteps    | 97500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.95     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | -22.4     |\n",
      "|    reward             | 15.394274 |\n",
      "|    std                | 1.7       |\n",
      "|    value_loss         | 479       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 19600      |\n",
      "|    time_elapsed       | 176        |\n",
      "|    total_timesteps    | 98000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.95      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19599      |\n",
      "|    policy_loss        | -2.05      |\n",
      "|    reward             | 0.77006423 |\n",
      "|    std                | 1.7        |\n",
      "|    value_loss         | 1.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 555        |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 177        |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.95      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | 0.306      |\n",
      "|    reward             | 0.40552083 |\n",
      "|    std                | 1.7        |\n",
      "|    value_loss         | 5.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 178       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | -0.917    |\n",
      "|    reward             | -4.188366 |\n",
      "|    std                | 1.69      |\n",
      "|    value_loss         | 7.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.94     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | -8.62     |\n",
      "|    reward             | -5.139655 |\n",
      "|    std                | 1.69      |\n",
      "|    value_loss         | 28.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 555       |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 179       |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | 44.6      |\n",
      "|    reward             | 4.0924625 |\n",
      "|    std                | 1.68      |\n",
      "|    value_loss         | 379       |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2022-07-06 to  2022-10-04\n",
      "A2C Sharpe Ratio:  -0.23190027513043063\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_504_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1013       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 2          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.09796191 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 952          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054791253 |\n",
      "|    clip_fraction        | 0.0433       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.00964      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00318     |\n",
      "|    reward               | 0.09913477   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0105       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 943        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00452862 |\n",
      "|    clip_fraction        | 0.0132     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 4.96       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00144   |\n",
      "|    reward               | 10.604678  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 13.2       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 927          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009530545 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.00725     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 34.5         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00042     |\n",
      "|    reward               | 0.5174169    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 97.1         |\n",
      "------------------------------------------\n",
      "day: 3146, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4158654.03\n",
      "total_reward: 3158654.03\n",
      "total_cost: 8948.09\n",
      "total_trades: 3123\n",
      "Sharpe: 0.738\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 912          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014116766 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.122       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 42.8         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000896    |\n",
      "|    reward               | 0.015952794  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 44.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 915          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021285128 |\n",
      "|    clip_fraction        | 0.00127      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.00284     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 66.6         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000467    |\n",
      "|    reward               | -0.312549    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 127          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 907         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003965568 |\n",
      "|    clip_fraction        | 0.0201      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.94        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    reward               | -0.24741174 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 9.48        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 905          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012741461 |\n",
      "|    clip_fraction        | 0.00933      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.0277      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.12         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | 0.000157     |\n",
      "|    reward               | 0.14488137   |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 10.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 903          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020360788 |\n",
      "|    clip_fraction        | 0.00205      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.0208       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 46.8         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000561    |\n",
      "|    reward               | 6.334495     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 86.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 905         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005591277 |\n",
      "|    clip_fraction        | 0.0201      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 5.96e-08    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 20.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00112    |\n",
      "|    reward               | 0.4254076   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 25.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 906          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005597444 |\n",
      "|    clip_fraction        | 0.0019       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.0149      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 32.3         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000953    |\n",
      "|    reward               | 0.08993336   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 54.7         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 910           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00052902964 |\n",
      "|    clip_fraction        | 0.00083       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.00778      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 129           |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000316     |\n",
      "|    reward               | -4.5995817    |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 266           |\n",
      "-------------------------------------------\n",
      "day: 3146, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4723420.64\n",
      "total_reward: 3723420.64\n",
      "total_cost: 8373.02\n",
      "total_trades: 3134\n",
      "Sharpe: 0.748\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 908         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003553453 |\n",
      "|    clip_fraction        | 0.0195      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.101       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.2        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00107    |\n",
      "|    reward               | 0.27218717  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 18.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 910          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024057222 |\n",
      "|    clip_fraction        | 0.00366      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.0564       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 97.6         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    reward               | -0.017382419 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 159          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 911          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012703927 |\n",
      "|    clip_fraction        | 0.00132      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.0538       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 120          |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    reward               | 1.9024633    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 309          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 912          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020264538 |\n",
      "|    clip_fraction        | 0.0114       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.376        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13.1         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    reward               | 0.117445976  |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 22.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 914          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016907712 |\n",
      "|    clip_fraction        | 0.000928     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.188        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 142          |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000523    |\n",
      "|    reward               | 0.034046113  |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 254          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 912          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030353875 |\n",
      "|    clip_fraction        | 0.0156       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.111        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 109          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    reward               | -1.8453087   |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 228          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 914          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034579707 |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.539        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.39         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    reward               | 0.94440204   |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 15.8         |\n",
      "------------------------------------------\n",
      "day: 3146, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6726937.12\n",
      "total_reward: 5726937.12\n",
      "total_cost: 8992.13\n",
      "total_trades: 3112\n",
      "Sharpe: 0.813\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 914          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021251575 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.238        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 77.7         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00155     |\n",
      "|    reward               | 0.0028214965 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 275          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 916        |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0022334  |\n",
      "|    clip_fraction        | 0.0112     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.44      |\n",
      "|    explained_variance   | 0.0727     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 147        |\n",
      "|    n_updates            | 200        |\n",
      "|    policy_gradient_loss | -0.00233   |\n",
      "|    reward               | 0.34987444 |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 382        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 914         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003700966 |\n",
      "|    clip_fraction        | 0.0124      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.731       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.53        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.000556    |\n",
      "|    reward               | -0.4547536  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 9.81        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 916          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008807158 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.237        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 119          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000504    |\n",
      "|    reward               | 14.606881    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 353          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 915          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010919571 |\n",
      "|    clip_fraction        | 0.000195     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.242        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 116          |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000456    |\n",
      "|    reward               | 1.2299178    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 269          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017858441 |\n",
      "|    clip_fraction        | 9.77e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.112        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 23.3         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000854    |\n",
      "|    reward               | -0.083783284 |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 90           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018570186 |\n",
      "|    clip_fraction        | 0.00786      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.193        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 189          |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00115     |\n",
      "|    reward               | 11.877061    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 404          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 918          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013353776 |\n",
      "|    clip_fraction        | 0.00283      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.316        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 99.7         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00132     |\n",
      "|    reward               | -0.5898805   |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 219          |\n",
      "------------------------------------------\n",
      "day: 3146, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6950657.79\n",
      "total_reward: 5950657.79\n",
      "total_cost: 9139.01\n",
      "total_trades: 3128\n",
      "Sharpe: 0.823\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 917           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 62            |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038560346 |\n",
      "|    clip_fraction        | 0.00151       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.174         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 47.9          |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.000918     |\n",
      "|    reward               | -0.113224946  |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 159           |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 917        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 64         |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00530017 |\n",
      "|    clip_fraction        | 0.0422     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | 0.244      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 237        |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.00426   |\n",
      "|    reward               | 11.895802  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 395        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018370863 |\n",
      "|    clip_fraction        | 0.00493      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.436        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 70.3         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00106     |\n",
      "|    reward               | 0.08495919   |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 174          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 917           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 69            |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00074243185 |\n",
      "|    clip_fraction        | 0.000439      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.259         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 228           |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -5.61e-05     |\n",
      "|    reward               | 0.012884213   |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 228           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 918          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013050503 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.236        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 240          |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000136    |\n",
      "|    reward               | -5.595782    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 416          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 918         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005603686 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | 0.59        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 37.8        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    reward               | -0.7910546  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 81.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 919          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 75           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028551822 |\n",
      "|    clip_fraction        | 0.00591      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.317        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 165          |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    reward               | -0.36424378  |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 323          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 918          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010100901 |\n",
      "|    clip_fraction        | 0.00937      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.204        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 224          |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.000624    |\n",
      "|    reward               | 1.4291302    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 536          |\n",
      "------------------------------------------\n",
      "day: 3146, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8337485.50\n",
      "total_reward: 7337485.50\n",
      "total_cost: 9190.68\n",
      "total_trades: 3123\n",
      "Sharpe: 0.844\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 918          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030920259 |\n",
      "|    clip_fraction        | 0.00718      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.665        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 26.3         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000451    |\n",
      "|    reward               | -0.5707017   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 64           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 916          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012934508 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.281        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 187          |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.000368    |\n",
      "|    reward               | 0.025380708  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 523          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 917           |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 84            |\n",
      "|    total_timesteps      | 77824         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040774193 |\n",
      "|    clip_fraction        | 0.00112       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.141         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 273           |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | 0.000133      |\n",
      "|    reward               | 0.75696415    |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 627           |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 917            |\n",
      "|    iterations           | 39             |\n",
      "|    time_elapsed         | 87             |\n",
      "|    total_timesteps      | 79872          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0061772587   |\n",
      "|    clip_fraction        | 0.0321         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0.682          |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 23.5           |\n",
      "|    n_updates            | 380            |\n",
      "|    policy_gradient_loss | -0.00493       |\n",
      "|    reward               | -0.00011306167 |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 60.9           |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.002568503  |\n",
      "|    clip_fraction        | 0.00776      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.278        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 289          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00187     |\n",
      "|    reward               | -0.021552159 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 566          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 917         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004283405 |\n",
      "|    clip_fraction        | 0.0212      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.0671      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 268         |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    reward               | 0.43428954  |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 649         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042130076 |\n",
      "|    clip_fraction        | 0.0269       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.833        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00239     |\n",
      "|    reward               | 0.4518235    |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 918         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00136332  |\n",
      "|    clip_fraction        | 0.00264     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.28        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 244         |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00085    |\n",
      "|    reward               | -0.98436195 |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 586         |\n",
      "-----------------------------------------\n",
      "day: 3146, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8212856.85\n",
      "total_reward: 7212856.85\n",
      "total_cost: 9110.79\n",
      "total_trades: 3117\n",
      "Sharpe: 0.839\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 917         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005850497 |\n",
      "|    clip_fraction        | 0.0432      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.079       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 270         |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0039     |\n",
      "|    reward               | -0.875736   |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 472         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011122865 |\n",
      "|    clip_fraction        | 0.00303      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.563        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.42         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | 0.000202     |\n",
      "|    reward               | 0.6406686    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 77.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 102          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020069887 |\n",
      "|    clip_fraction        | 0.0144       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.293        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 348          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    reward               | 4.9578094    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 570          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 917           |\n",
      "|    iterations           | 47            |\n",
      "|    time_elapsed         | 104           |\n",
      "|    total_timesteps      | 96256         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018473007 |\n",
      "|    clip_fraction        | 0.00625       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.306         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 179           |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.000263     |\n",
      "|    reward               | -0.58478016   |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 308           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 917          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 107          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030424106 |\n",
      "|    clip_fraction        | 0.0186       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.215        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 73           |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    reward               | -0.25128686  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 208          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 918          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 109          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020696097 |\n",
      "|    clip_fraction        | 0.00513      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.357        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 178          |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.000866    |\n",
      "|    reward               | -2.896576    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 408          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-07-06 to  2022-10-04\n",
      "PPO Sharpe Ratio:  -0.2598699399513888\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_504_3\n",
      "day: 3146, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10771219.73\n",
      "total_reward: 9771219.73\n",
      "total_cost: 1027.51\n",
      "total_trades: 3137\n",
      "Sharpe: 0.869\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 80        |\n",
      "|    total_timesteps | 12588     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 3.07e+03  |\n",
      "|    critic_loss     | 232       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 12487     |\n",
      "|    reward          | 21.228575 |\n",
      "----------------------------------\n",
      "day: 3146, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11426221.52\n",
      "total_reward: 10426221.52\n",
      "total_cost: 998.99\n",
      "total_trades: 3146\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 161       |\n",
      "|    total_timesteps | 25176     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 824       |\n",
      "|    critic_loss     | 179       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 25075     |\n",
      "|    reward          | 21.228575 |\n",
      "----------------------------------\n",
      "day: 3146, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11426221.52\n",
      "total_reward: 10426221.52\n",
      "total_cost: 998.99\n",
      "total_trades: 3146\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 155       |\n",
      "|    time_elapsed    | 242       |\n",
      "|    total_timesteps | 37764     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 711       |\n",
      "|    critic_loss     | 131       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 37663     |\n",
      "|    reward          | 21.228575 |\n",
      "----------------------------------\n",
      "day: 3146, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11426221.52\n",
      "total_reward: 10426221.52\n",
      "total_cost: 998.99\n",
      "total_trades: 3146\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 155       |\n",
      "|    time_elapsed    | 324       |\n",
      "|    total_timesteps | 50352     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 423       |\n",
      "|    critic_loss     | 102       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 50251     |\n",
      "|    reward          | 21.228575 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 155       |\n",
      "|    time_elapsed    | 405       |\n",
      "|    total_timesteps | 62940     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 127       |\n",
      "|    critic_loss     | 89.5      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 62839     |\n",
      "|    reward          | 21.228575 |\n",
      "----------------------------------\n",
      "day: 3146, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11426221.52\n",
      "total_reward: 10426221.52\n",
      "total_cost: 998.99\n",
      "total_trades: 3146\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 487       |\n",
      "|    total_timesteps | 75528     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 31.9      |\n",
      "|    critic_loss     | 88.7      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 75427     |\n",
      "|    reward          | 21.228575 |\n",
      "----------------------------------\n",
      "day: 3146, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11426221.52\n",
      "total_reward: 10426221.52\n",
      "total_cost: 998.99\n",
      "total_trades: 3146\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 28        |\n",
      "|    fps             | 154       |\n",
      "|    time_elapsed    | 569       |\n",
      "|    total_timesteps | 88116     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 22.2      |\n",
      "|    critic_loss     | 94.2      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 88015     |\n",
      "|    reward          | 21.228575 |\n",
      "----------------------------------\n",
      "day: 3146, episode: 95\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11426221.52\n",
      "total_reward: 10426221.52\n",
      "total_cost: 998.99\n",
      "total_trades: 3146\n",
      "Sharpe: 0.881\n",
      "=================================\n",
      "======DDPG Validation from:  2022-07-06 to  2022-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-04\n",
      "======Trading from:  2022-10-04 to  2023-01-04\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2022-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_567_3\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 589         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 0           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.43       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -0.0222     |\n",
      "|    reward             | 0.053593796 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.00821     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 557        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.44      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.252     |\n",
      "|    reward             | 0.46810454 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.0349     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 521         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.45       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -0.401      |\n",
      "|    reward             | -0.39089835 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.131       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 522        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -0.591     |\n",
      "|    reward             | -0.2855818 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.41       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 527        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.45      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -1.17      |\n",
      "|    reward             | -1.7903537 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 1.51       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 512        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | -0.0446    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 9.07       |\n",
      "|    reward             | -2.5093968 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 105        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 520        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -0.524     |\n",
      "|    reward             | 0.16632988 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.124      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 524        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.47      |\n",
      "|    explained_variance | 0.0283     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -1.76      |\n",
      "|    reward             | -0.8395199 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 1.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 527        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.45      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 6.29       |\n",
      "|    reward             | -0.2929313 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 27.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 532       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 6.67      |\n",
      "|    reward             | 1.6499659 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 59.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 536       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.45     |\n",
      "|    explained_variance | -0.0237   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 14.3      |\n",
      "|    reward             | 0.2922982 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 129       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -7.09e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 18.1      |\n",
      "|    reward             | -6.134978 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 918       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 540         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.46       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -0.0511     |\n",
      "|    reward             | 0.010232616 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.00268     |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 543           |\n",
      "|    iterations         | 1400          |\n",
      "|    time_elapsed       | 12            |\n",
      "|    total_timesteps    | 7000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.47         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1399          |\n",
      "|    policy_loss        | -0.00265      |\n",
      "|    reward             | -0.0009017124 |\n",
      "|    std                | 1.05          |\n",
      "|    value_loss         | 3.67e-06      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 540          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 13           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.48        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | 0.000681     |\n",
      "|    reward             | -3.30268e-05 |\n",
      "|    std                | 1.07         |\n",
      "|    value_loss         | 1.77e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 543            |\n",
      "|    iterations         | 1600           |\n",
      "|    time_elapsed       | 14             |\n",
      "|    total_timesteps    | 8000           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.5           |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1599           |\n",
      "|    policy_loss        | -0.00894       |\n",
      "|    reward             | 0.000109793975 |\n",
      "|    std                | 1.08           |\n",
      "|    value_loss         | 5.86e-05       |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 546       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.51     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.177    |\n",
      "|    reward             | 1.2524871 |\n",
      "|    std                | 1.09      |\n",
      "|    value_loss         | 0.826     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 545      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 4.7      |\n",
      "|    reward             | -1.86472 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 55       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 548      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.5     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 26.6     |\n",
      "|    reward             | 6.697754 |\n",
      "|    std                | 1.09     |\n",
      "|    value_loss         | 269      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 549         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.51       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -0.179      |\n",
      "|    reward             | -0.07697626 |\n",
      "|    std                | 1.09        |\n",
      "|    value_loss         | 0.0157      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 547        |\n",
      "|    iterations         | 2100       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 10500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.52      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2099       |\n",
      "|    policy_loss        | -0.837     |\n",
      "|    reward             | -0.6254196 |\n",
      "|    std                | 1.1        |\n",
      "|    value_loss         | 0.415      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 549         |\n",
      "|    iterations         | 2200        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 11000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.53       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2199        |\n",
      "|    policy_loss        | -1.95       |\n",
      "|    reward             | -0.13595113 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 1.71        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 550         |\n",
      "|    iterations         | 2300        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 11500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.53       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2299        |\n",
      "|    policy_loss        | -0.486      |\n",
      "|    reward             | -0.23487131 |\n",
      "|    std                | 1.11        |\n",
      "|    value_loss         | 0.207       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.53    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -3.08    |\n",
      "|    reward             | 1.946388 |\n",
      "|    std                | 1.12     |\n",
      "|    value_loss         | 12       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 549        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -2.3       |\n",
      "|    reward             | -1.1189761 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 6.27       |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 550           |\n",
      "|    iterations         | 2600          |\n",
      "|    time_elapsed       | 23            |\n",
      "|    total_timesteps    | 13000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.53         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 2599          |\n",
      "|    policy_loss        | -0.136        |\n",
      "|    reward             | -0.0073835356 |\n",
      "|    std                | 1.12          |\n",
      "|    value_loss         | 0.00747       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 549         |\n",
      "|    iterations         | 2700        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 13500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.54       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2699        |\n",
      "|    policy_loss        | 0.0348      |\n",
      "|    reward             | -0.00962786 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.00457     |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | -0.581     |\n",
      "|    reward             | 0.36485174 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.293      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | 2.77       |\n",
      "|    reward             | 0.44190103 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 8.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 551       |\n",
      "|    iterations         | 3000      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 15000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | -0.195    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2999      |\n",
      "|    policy_loss        | 1.22      |\n",
      "|    reward             | 4.0686574 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 1.89      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 15500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.54    |\n",
      "|    explained_variance | 0.00615  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.308   |\n",
      "|    reward             | 5.727479 |\n",
      "|    std                | 1.13     |\n",
      "|    value_loss         | 5.96     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | -0.00271  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -1.19     |\n",
      "|    reward             | -3.716185 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 42        |\n",
      "-------------------------------------\n",
      "day: 3209, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4755185.98\n",
      "total_reward: 3755185.98\n",
      "total_cost: 10385.67\n",
      "total_trades: 3158\n",
      "Sharpe: 0.723\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -0.538     |\n",
      "|    reward             | 0.68084854 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.151      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -0.226     |\n",
      "|    reward             | 0.23992328 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.124      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 551         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | -2.47       |\n",
      "|    reward             | -0.35441217 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 6.7         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -6.2       |\n",
      "|    reward             | -0.2430948 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 23.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -2.72      |\n",
      "|    reward             | -0.7328204 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 5.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 551       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | -8.12     |\n",
      "|    reward             | 6.1407313 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 84.9      |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 550           |\n",
      "|    iterations         | 3900          |\n",
      "|    time_elapsed       | 35            |\n",
      "|    total_timesteps    | 19500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.57         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 3899          |\n",
      "|    policy_loss        | 0.0109        |\n",
      "|    reward             | 5.0703537e-05 |\n",
      "|    std                | 1.16          |\n",
      "|    value_loss         | 3.83e-05      |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 4000        |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 20000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.57       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3999        |\n",
      "|    policy_loss        | -0.0338     |\n",
      "|    reward             | 0.012077829 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 0.000668    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 553           |\n",
      "|    iterations         | 4100          |\n",
      "|    time_elapsed       | 37            |\n",
      "|    total_timesteps    | 20500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.58         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 4099          |\n",
      "|    policy_loss        | 0.00671       |\n",
      "|    reward             | -0.0067286287 |\n",
      "|    std                | 1.18          |\n",
      "|    value_loss         | 0.000137      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 552          |\n",
      "|    iterations         | 4200         |\n",
      "|    time_elapsed       | 37           |\n",
      "|    total_timesteps    | 21000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.58        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4199         |\n",
      "|    policy_loss        | 0.176        |\n",
      "|    reward             | -0.052149378 |\n",
      "|    std                | 1.17         |\n",
      "|    value_loss         | 0.0225       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.59      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | -0.692     |\n",
      "|    reward             | 0.20490855 |\n",
      "|    std                | 1.19       |\n",
      "|    value_loss         | 0.339      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 553          |\n",
      "|    iterations         | 4400         |\n",
      "|    time_elapsed       | 39           |\n",
      "|    total_timesteps    | 22000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.6         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4399         |\n",
      "|    policy_loss        | -0.0143      |\n",
      "|    reward             | 0.0039590765 |\n",
      "|    std                | 1.2          |\n",
      "|    value_loss         | 0.00024      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 552          |\n",
      "|    iterations         | 4500         |\n",
      "|    time_elapsed       | 40           |\n",
      "|    total_timesteps    | 22500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.6         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4499         |\n",
      "|    policy_loss        | -0.0794      |\n",
      "|    reward             | -0.002117583 |\n",
      "|    std                | 1.2          |\n",
      "|    value_loss         | 0.00262      |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 553        |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.63      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | 0.511      |\n",
      "|    reward             | 0.96820533 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 0.112      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 553        |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.62      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -7.83      |\n",
      "|    reward             | 0.41213527 |\n",
      "|    std                | 1.23       |\n",
      "|    value_loss         | 30.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.62      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | 3.5        |\n",
      "|    reward             | -2.6791143 |\n",
      "|    std                | 1.22       |\n",
      "|    value_loss         | 11.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 552       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.63     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 1.35      |\n",
      "|    reward             | 0.5676972 |\n",
      "|    std                | 1.23      |\n",
      "|    value_loss         | 5.79      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.63    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | 0.201    |\n",
      "|    reward             | 2.245621 |\n",
      "|    std                | 1.24     |\n",
      "|    value_loss         | 42.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.64     |\n",
      "|    explained_variance | -0.000132 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | -49.5     |\n",
      "|    reward             | 5.2166553 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 1.34e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 553         |\n",
      "|    iterations         | 5200        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 26000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.63       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5199        |\n",
      "|    policy_loss        | -1.14       |\n",
      "|    reward             | -0.31493744 |\n",
      "|    std                | 1.24        |\n",
      "|    value_loss         | 0.401       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0.0581    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | -5.44     |\n",
      "|    reward             | 0.1574055 |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 24.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | -4.8      |\n",
      "|    reward             | 1.6675944 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 17        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.64     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -1.6      |\n",
      "|    reward             | 4.680184  |\n",
      "|    std                | 1.25      |\n",
      "|    value_loss         | 2.68      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 554        |\n",
      "|    iterations         | 5600       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 28000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.65      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5599       |\n",
      "|    policy_loss        | 18.8       |\n",
      "|    reward             | -3.8216426 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 95.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 553        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.65      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | 17.3       |\n",
      "|    reward             | -27.523003 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 319        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 554        |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.64      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | -0.236     |\n",
      "|    reward             | 0.07081239 |\n",
      "|    std                | 1.25       |\n",
      "|    value_loss         | 0.0235     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 555         |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.65       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 1.42        |\n",
      "|    reward             | -0.21534221 |\n",
      "|    std                | 1.27        |\n",
      "|    value_loss         | 1.57        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 554       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 54        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | 4.18      |\n",
      "|    reward             | 1.7797598 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 10.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 3.17      |\n",
      "|    reward             | 1.1584085 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 9.76      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 553         |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 55          |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | -2.79       |\n",
      "|    reward             | -0.69313097 |\n",
      "|    std                | 1.28        |\n",
      "|    value_loss         | 8.35        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.67     |\n",
      "|    explained_variance | 0.0169    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | 39.2      |\n",
      "|    reward             | 10.904356 |\n",
      "|    std                | 1.29      |\n",
      "|    value_loss         | 518       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 553        |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0.000344   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 119        |\n",
      "|    reward             | -49.327747 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 1.96e+03   |\n",
      "--------------------------------------\n",
      "day: 3209, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8951346.50\n",
      "total_reward: 7951346.50\n",
      "total_cost: 11033.31\n",
      "total_trades: 3193\n",
      "Sharpe: 0.847\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 554        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | -0.783     |\n",
      "|    reward             | -1.4655429 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 0.478      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 554        |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | 1.86       |\n",
      "|    reward             | 0.74131274 |\n",
      "|    std                | 1.29       |\n",
      "|    value_loss         | 3.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 554        |\n",
      "|    iterations         | 6700       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 33500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.68      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6699       |\n",
      "|    policy_loss        | 5.39       |\n",
      "|    reward             | 0.68895197 |\n",
      "|    std                | 1.3        |\n",
      "|    value_loss         | 9.66       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 554         |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 61          |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | 4.55        |\n",
      "|    reward             | -0.07782394 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 12.4        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 554      |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 34500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -1.58    |\n",
      "|    reward             | 2.301035 |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 4.96     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 7000     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 35000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 27.3     |\n",
      "|    reward             | 4.189804 |\n",
      "|    std                | 1.28     |\n",
      "|    value_loss         | 342      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 553        |\n",
      "|    iterations         | 7100       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 35500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.66      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7099       |\n",
      "|    policy_loss        | -0.0219    |\n",
      "|    reward             | 0.00816857 |\n",
      "|    std                | 1.27       |\n",
      "|    value_loss         | 0.000195   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 553          |\n",
      "|    iterations         | 7200         |\n",
      "|    time_elapsed       | 65           |\n",
      "|    total_timesteps    | 36000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.66        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 7199         |\n",
      "|    policy_loss        | -0.00411     |\n",
      "|    reward             | 0.0046990807 |\n",
      "|    std                | 1.27         |\n",
      "|    value_loss         | 1.1e-06      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 7300        |\n",
      "|    time_elapsed       | 66          |\n",
      "|    total_timesteps    | 36500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7299        |\n",
      "|    policy_loss        | -0.031      |\n",
      "|    reward             | -0.14274436 |\n",
      "|    std                | 1.29        |\n",
      "|    value_loss         | 0.000849    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 552           |\n",
      "|    iterations         | 7400          |\n",
      "|    time_elapsed       | 66            |\n",
      "|    total_timesteps    | 37000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.65         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 7399          |\n",
      "|    policy_loss        | 0.444         |\n",
      "|    reward             | -0.0011809279 |\n",
      "|    std                | 1.26          |\n",
      "|    value_loss         | 0.077         |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.65      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | -2.7       |\n",
      "|    reward             | -0.7413283 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 2.52       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 552       |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | 20.1      |\n",
      "|    reward             | -6.903982 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 70.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | -30.7     |\n",
      "|    reward             | -2.388308 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 140       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.65      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | -0.394     |\n",
      "|    reward             | 0.51045394 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 0.119      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 553         |\n",
      "|    iterations         | 7900        |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 39500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.64       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7899        |\n",
      "|    policy_loss        | 0.123       |\n",
      "|    reward             | -0.16088854 |\n",
      "|    std                | 1.25        |\n",
      "|    value_loss         | 0.1         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.65     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -6.12     |\n",
      "|    reward             | 3.3347554 |\n",
      "|    std                | 1.26      |\n",
      "|    value_loss         | 19.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 553        |\n",
      "|    iterations         | 8100       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 40500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.65      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8099       |\n",
      "|    policy_loss        | 8.52       |\n",
      "|    reward             | -0.6802908 |\n",
      "|    std                | 1.26       |\n",
      "|    value_loss         | 28.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | 13.4      |\n",
      "|    reward             | 0.7040467 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 86.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | -0.396    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | -2.89     |\n",
      "|    reward             | 5.7377396 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 5.63      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 553         |\n",
      "|    iterations         | 8400        |\n",
      "|    time_elapsed       | 75          |\n",
      "|    total_timesteps    | 42000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.66       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8399        |\n",
      "|    policy_loss        | -0.986      |\n",
      "|    reward             | -0.17819935 |\n",
      "|    std                | 1.28        |\n",
      "|    value_loss         | 0.18        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 8500        |\n",
      "|    time_elapsed       | 76          |\n",
      "|    total_timesteps    | 42500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.67       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8499        |\n",
      "|    policy_loss        | -0.959      |\n",
      "|    reward             | -0.52321875 |\n",
      "|    std                | 1.28        |\n",
      "|    value_loss         | 0.543       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 553        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 77         |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.66      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 0.103      |\n",
      "|    reward             | 0.29784486 |\n",
      "|    std                | 1.28       |\n",
      "|    value_loss         | 0.59       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 8700        |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 43500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.66       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8699        |\n",
      "|    policy_loss        | 0.717       |\n",
      "|    reward             | -0.07587304 |\n",
      "|    std                | 1.27        |\n",
      "|    value_loss         | 0.243       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 8800      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 44000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8799      |\n",
      "|    policy_loss        | -17.9     |\n",
      "|    reward             | 0.3455887 |\n",
      "|    std                | 1.28      |\n",
      "|    value_loss         | 115       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.66     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 19.6      |\n",
      "|    reward             | 3.7467928 |\n",
      "|    std                | 1.27      |\n",
      "|    value_loss         | 260       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 81       |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.66    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | -0.323   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.27     |\n",
      "|    value_loss         | 0.0289   |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 553           |\n",
      "|    iterations         | 9100          |\n",
      "|    time_elapsed       | 82            |\n",
      "|    total_timesteps    | 45500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.66         |\n",
      "|    explained_variance | 1.79e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 9099          |\n",
      "|    policy_loss        | -0.000207     |\n",
      "|    reward             | -4.522678e-05 |\n",
      "|    std                | 1.28          |\n",
      "|    value_loss         | 2.13e-08      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 9200     |\n",
      "|    time_elapsed       | 83       |\n",
      "|    total_timesteps    | 46000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.68    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9199     |\n",
      "|    policy_loss        | -0.0031  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 2.56e-06 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 553            |\n",
      "|    iterations         | 9300           |\n",
      "|    time_elapsed       | 84             |\n",
      "|    total_timesteps    | 46500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.69          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 9299           |\n",
      "|    policy_loss        | -0.000178      |\n",
      "|    reward             | -7.1702656e-05 |\n",
      "|    std                | 1.32           |\n",
      "|    value_loss         | 1.58e-08       |\n",
      "------------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 553       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 84        |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.72     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | -0.000233 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.36      |\n",
      "|    value_loss         | 4.34e-08  |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 553          |\n",
      "|    iterations         | 9500         |\n",
      "|    time_elapsed       | 85           |\n",
      "|    total_timesteps    | 47500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.76        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9499         |\n",
      "|    policy_loss        | -0.00421     |\n",
      "|    reward             | -0.008975253 |\n",
      "|    std                | 1.41         |\n",
      "|    value_loss         | 4.93e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 9600     |\n",
      "|    time_elapsed       | 86       |\n",
      "|    total_timesteps    | 48000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.79    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9599     |\n",
      "|    policy_loss        | -0.0198  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.45     |\n",
      "|    value_loss         | 0.000139 |\n",
      "------------------------------------\n",
      "day: 3209, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 998430.44\n",
      "total_reward: -1569.56\n",
      "total_cost: 3070.42\n",
      "total_trades: 1201\n",
      "Sharpe: -0.156\n",
      "=================================\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 553      |\n",
      "|    iterations         | 9700     |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 48500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.82    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9699     |\n",
      "|    policy_loss        | -0.00611 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.49     |\n",
      "|    value_loss         | 1.48e-05 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 553         |\n",
      "|    iterations         | 9800        |\n",
      "|    time_elapsed       | 88          |\n",
      "|    total_timesteps    | 49000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.86       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9799        |\n",
      "|    policy_loss        | -9.41e-06   |\n",
      "|    reward             | -0.00114675 |\n",
      "|    std                | 1.55        |\n",
      "|    value_loss         | 7.08e-11    |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 9900     |\n",
      "|    time_elapsed       | 89       |\n",
      "|    total_timesteps    | 49500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.91    |\n",
      "|    explained_variance | 2.38e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9899     |\n",
      "|    policy_loss        | 0.00789  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.63     |\n",
      "|    value_loss         | 2.05e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 10000    |\n",
      "|    time_elapsed       | 90       |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9999     |\n",
      "|    policy_loss        | 0.00209  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.69     |\n",
      "|    value_loss         | 1.85e-06 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 553           |\n",
      "|    iterations         | 10100         |\n",
      "|    time_elapsed       | 91            |\n",
      "|    total_timesteps    | 50500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.98         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10099         |\n",
      "|    policy_loss        | -0.0102       |\n",
      "|    reward             | -0.0016833128 |\n",
      "|    std                | 1.75          |\n",
      "|    value_loss         | 4.14e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 552            |\n",
      "|    iterations         | 10200          |\n",
      "|    time_elapsed       | 92             |\n",
      "|    total_timesteps    | 51000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2             |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 10199          |\n",
      "|    policy_loss        | -0.0557        |\n",
      "|    reward             | -0.00022302664 |\n",
      "|    std                | 1.79           |\n",
      "|    value_loss         | 0.00143        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 552            |\n",
      "|    iterations         | 10300          |\n",
      "|    time_elapsed       | 93             |\n",
      "|    total_timesteps    | 51500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2             |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 10299          |\n",
      "|    policy_loss        | 0.00231        |\n",
      "|    reward             | -0.00033421707 |\n",
      "|    std                | 1.79           |\n",
      "|    value_loss         | 1.86e-06       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 552           |\n",
      "|    iterations         | 10400         |\n",
      "|    time_elapsed       | 94            |\n",
      "|    total_timesteps    | 52000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.02         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10399         |\n",
      "|    policy_loss        | -0.000994     |\n",
      "|    reward             | -0.0003099029 |\n",
      "|    std                | 1.82          |\n",
      "|    value_loss         | 4.13e-07      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 551           |\n",
      "|    iterations         | 10500         |\n",
      "|    time_elapsed       | 95            |\n",
      "|    total_timesteps    | 52500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.04         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 10499         |\n",
      "|    policy_loss        | -0.000436     |\n",
      "|    reward             | -0.0014068065 |\n",
      "|    std                | 1.87          |\n",
      "|    value_loss         | 3.75e-08      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 552          |\n",
      "|    iterations         | 10600        |\n",
      "|    time_elapsed       | 95           |\n",
      "|    total_timesteps    | 53000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.09        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 10599        |\n",
      "|    policy_loss        | 0.0271       |\n",
      "|    reward             | 0.0003915991 |\n",
      "|    std                | 1.96         |\n",
      "|    value_loss         | 0.00011      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 10700       |\n",
      "|    time_elapsed       | 96          |\n",
      "|    total_timesteps    | 53500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.12       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10699       |\n",
      "|    policy_loss        | 0.00353     |\n",
      "|    reward             | 0.014636613 |\n",
      "|    std                | 2.01        |\n",
      "|    value_loss         | 2.65e-05    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 552       |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 97        |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | 5.97      |\n",
      "|    reward             | 0.7865548 |\n",
      "|    std                | 2.06      |\n",
      "|    value_loss         | 9.63      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 10900    |\n",
      "|    time_elapsed       | 98       |\n",
      "|    total_timesteps    | 54500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | -0.0118  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10899    |\n",
      "|    policy_loss        | -15.7    |\n",
      "|    reward             | 4.343135 |\n",
      "|    std                | 2.08     |\n",
      "|    value_loss         | 93.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 99         |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.15      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | -0.392     |\n",
      "|    reward             | 0.44827175 |\n",
      "|    std                | 2.08       |\n",
      "|    value_loss         | 0.0838     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 100        |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | 0.994      |\n",
      "|    reward             | -1.1569026 |\n",
      "|    std                | 2.11       |\n",
      "|    value_loss         | 0.508      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | -0.0116    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | -11        |\n",
      "|    reward             | -3.2144482 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 45.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 102        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | 2.11       |\n",
      "|    reward             | 0.07587894 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 3.81       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 551       |\n",
      "|    iterations         | 11400     |\n",
      "|    time_elapsed       | 103       |\n",
      "|    total_timesteps    | 57000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 0.00345   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11399     |\n",
      "|    policy_loss        | 14.6      |\n",
      "|    reward             | 5.4675417 |\n",
      "|    std                | 2.13      |\n",
      "|    value_loss         | 67.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 11500      |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 57500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | -0.000208  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11499      |\n",
      "|    policy_loss        | 24.7       |\n",
      "|    reward             | -0.5761452 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 328        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 11600       |\n",
      "|    time_elapsed       | 105         |\n",
      "|    total_timesteps    | 58000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11599       |\n",
      "|    policy_loss        | -0.0589     |\n",
      "|    reward             | -0.25063545 |\n",
      "|    std                | 2.15        |\n",
      "|    value_loss         | 0.0181      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 11700       |\n",
      "|    time_elapsed       | 105         |\n",
      "|    total_timesteps    | 58500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11699       |\n",
      "|    policy_loss        | -5.5        |\n",
      "|    reward             | -0.70772433 |\n",
      "|    std                | 2.16        |\n",
      "|    value_loss         | 9.56        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 552       |\n",
      "|    iterations         | 11800     |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 59000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0.191     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11799     |\n",
      "|    policy_loss        | 1.89      |\n",
      "|    reward             | 0.5316127 |\n",
      "|    std                | 2.19      |\n",
      "|    value_loss         | 2.78      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 11900      |\n",
      "|    time_elapsed       | 107        |\n",
      "|    total_timesteps    | 59500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11899      |\n",
      "|    policy_loss        | 2.61       |\n",
      "|    reward             | -3.6429367 |\n",
      "|    std                | 2.17       |\n",
      "|    value_loss         | 2.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 12000      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 60000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | -0.0038    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11999      |\n",
      "|    policy_loss        | -5.62      |\n",
      "|    reward             | -6.4148173 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 25.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 552       |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 109       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | -8.26e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | 11.3      |\n",
      "|    reward             | -8.324349 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 659       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 552         |\n",
      "|    iterations         | 12200       |\n",
      "|    time_elapsed       | 110         |\n",
      "|    total_timesteps    | 61000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12199       |\n",
      "|    policy_loss        | -0.04       |\n",
      "|    reward             | 0.027482186 |\n",
      "|    std                | 2.15        |\n",
      "|    value_loss         | 0.000421    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 551         |\n",
      "|    iterations         | 12300       |\n",
      "|    time_elapsed       | 111         |\n",
      "|    total_timesteps    | 61500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12299       |\n",
      "|    policy_loss        | 1.68        |\n",
      "|    reward             | -0.09779017 |\n",
      "|    std                | 2.14        |\n",
      "|    value_loss         | 0.8         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 12400      |\n",
      "|    time_elapsed       | 112        |\n",
      "|    total_timesteps    | 62000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12399      |\n",
      "|    policy_loss        | -2         |\n",
      "|    reward             | 0.81880164 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 3.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 12500      |\n",
      "|    time_elapsed       | 113        |\n",
      "|    total_timesteps    | 62500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12499      |\n",
      "|    policy_loss        | 0.0475     |\n",
      "|    reward             | 0.15725052 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 3.46       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 114        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | -0.00187   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | 4.5        |\n",
      "|    reward             | 0.05432147 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 11         |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 552      |\n",
      "|    iterations         | 12700    |\n",
      "|    time_elapsed       | 115      |\n",
      "|    total_timesteps    | 63500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | -0.00192 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12699    |\n",
      "|    policy_loss        | 29.2     |\n",
      "|    reward             | 8.565055 |\n",
      "|    std                | 2.14     |\n",
      "|    value_loss         | 299      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 552        |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | -0.000882  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | 104        |\n",
      "|    reward             | -53.853146 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 4.19e+03   |\n",
      "--------------------------------------\n",
      "day: 3209, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10953307.07\n",
      "total_reward: 9953307.07\n",
      "total_cost: 12660.93\n",
      "total_trades: 3199\n",
      "Sharpe: 0.863\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 12900      |\n",
      "|    time_elapsed       | 116        |\n",
      "|    total_timesteps    | 64500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12899      |\n",
      "|    policy_loss        | -0.115     |\n",
      "|    reward             | 0.12666975 |\n",
      "|    std                | 2.11       |\n",
      "|    value_loss         | 0.0928     |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 551      |\n",
      "|    iterations         | 13000    |\n",
      "|    time_elapsed       | 117      |\n",
      "|    total_timesteps    | 65000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12999    |\n",
      "|    policy_loss        | -11.8    |\n",
      "|    reward             | 1.347973 |\n",
      "|    std                | 2.11     |\n",
      "|    value_loss         | 29       |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 13100      |\n",
      "|    time_elapsed       | 118        |\n",
      "|    total_timesteps    | 65500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | -0.00447   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13099      |\n",
      "|    policy_loss        | -3.09      |\n",
      "|    reward             | -3.6748207 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 10.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 551        |\n",
      "|    iterations         | 13200      |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 66000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0.0389     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13199      |\n",
      "|    policy_loss        | 4.78       |\n",
      "|    reward             | 0.10681848 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 7.62       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 550      |\n",
      "|    iterations         | 13300    |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 66500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | 0.103    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13299    |\n",
      "|    policy_loss        | -0.798   |\n",
      "|    reward             | 2.300877 |\n",
      "|    std                | 2.12     |\n",
      "|    value_loss         | 0.884    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 551       |\n",
      "|    iterations         | 13400     |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 67000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | -0.0125   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13399     |\n",
      "|    policy_loss        | -16.4     |\n",
      "|    reward             | -8.337973 |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 119       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 550         |\n",
      "|    iterations         | 13500       |\n",
      "|    time_elapsed       | 122         |\n",
      "|    total_timesteps    | 67500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.17       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 13499       |\n",
      "|    policy_loss        | -0.966      |\n",
      "|    reward             | -0.08672094 |\n",
      "|    std                | 2.11        |\n",
      "|    value_loss         | 0.178       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 13600      |\n",
      "|    time_elapsed       | 123        |\n",
      "|    total_timesteps    | 68000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13599      |\n",
      "|    policy_loss        | -3.93      |\n",
      "|    reward             | 0.66425127 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 11.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 13700     |\n",
      "|    time_elapsed       | 124       |\n",
      "|    total_timesteps    | 68500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0.0161    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13699     |\n",
      "|    policy_loss        | 23.8      |\n",
      "|    reward             | 1.9158592 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 126       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 13800     |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 69000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13799     |\n",
      "|    policy_loss        | -11.7     |\n",
      "|    reward             | -4.850257 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 36        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 13900      |\n",
      "|    time_elapsed       | 126        |\n",
      "|    total_timesteps    | 69500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0.0372     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13899      |\n",
      "|    policy_loss        | 12.5       |\n",
      "|    reward             | -3.9669511 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 43.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 127        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0.000739   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | 74.9       |\n",
      "|    reward             | -10.489187 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 832        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 0.00757   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | -25.9     |\n",
      "|    reward             | 13.841012 |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 372       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 14200     |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 71000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14199     |\n",
      "|    policy_loss        | 1.07      |\n",
      "|    reward             | 0.5667385 |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 0.328     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 14300      |\n",
      "|    time_elapsed       | 129        |\n",
      "|    total_timesteps    | 71500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | -0.215     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14299      |\n",
      "|    policy_loss        | -7.24      |\n",
      "|    reward             | 0.51737314 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 17.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 14400      |\n",
      "|    time_elapsed       | 130        |\n",
      "|    total_timesteps    | 72000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | -0.262     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14399      |\n",
      "|    policy_loss        | 0.22       |\n",
      "|    reward             | -3.6587787 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 0.94       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 14500      |\n",
      "|    time_elapsed       | 131        |\n",
      "|    total_timesteps    | 72500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | -0.00628   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14499      |\n",
      "|    policy_loss        | -14.7      |\n",
      "|    reward             | -0.4203135 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 102        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 14600     |\n",
      "|    time_elapsed       | 132       |\n",
      "|    total_timesteps    | 73000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0.00125   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14599     |\n",
      "|    policy_loss        | 14.6      |\n",
      "|    reward             | -1.050566 |\n",
      "|    std                | 2.13      |\n",
      "|    value_loss         | 134       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 550      |\n",
      "|    iterations         | 14700    |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 73500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0.0104   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14699    |\n",
      "|    policy_loss        | 16.5     |\n",
      "|    reward             | 23.1246  |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 96.6     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 550          |\n",
      "|    iterations         | 14800        |\n",
      "|    time_elapsed       | 134          |\n",
      "|    total_timesteps    | 74000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.17        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14799        |\n",
      "|    policy_loss        | -0.0438      |\n",
      "|    reward             | -0.048742484 |\n",
      "|    std                | 2.12         |\n",
      "|    value_loss         | 0.0129       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | -0.377     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | 8.15       |\n",
      "|    reward             | -0.2636633 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 22.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 136       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | -0.00192  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | 8.07      |\n",
      "|    reward             | 2.1865318 |\n",
      "|    std                | 2.15      |\n",
      "|    value_loss         | 11.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 15100      |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 75500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15099      |\n",
      "|    policy_loss        | 3.46       |\n",
      "|    reward             | -1.5535234 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 3.17       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 550      |\n",
      "|    iterations         | 15200    |\n",
      "|    time_elapsed       | 137      |\n",
      "|    total_timesteps    | 76000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15199    |\n",
      "|    policy_loss        | 2.77     |\n",
      "|    reward             | 5.804388 |\n",
      "|    std                | 2.15     |\n",
      "|    value_loss         | 4        |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 138       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | 95.8      |\n",
      "|    reward             | 28.111305 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 4.97e+03  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 15400     |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 77000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | -0.0244   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15399     |\n",
      "|    policy_loss        | 82.8      |\n",
      "|    reward             | -3.763448 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 2.6e+03   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 15500      |\n",
      "|    time_elapsed       | 140        |\n",
      "|    total_timesteps    | 77500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0.121      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15499      |\n",
      "|    policy_loss        | -4.34      |\n",
      "|    reward             | 0.03488658 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 6.78       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 141       |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | 0.281     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | 2.21      |\n",
      "|    reward             | -1.506735 |\n",
      "|    std                | 2.15      |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 550          |\n",
      "|    iterations         | 15700        |\n",
      "|    time_elapsed       | 142          |\n",
      "|    total_timesteps    | 78500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.19        |\n",
      "|    explained_variance | -0.12        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15699        |\n",
      "|    policy_loss        | -5.71        |\n",
      "|    reward             | -0.018521363 |\n",
      "|    std                | 2.17         |\n",
      "|    value_loss         | 9.39         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 15800     |\n",
      "|    time_elapsed       | 143       |\n",
      "|    total_timesteps    | 79000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0.114     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15799     |\n",
      "|    policy_loss        | 5.35      |\n",
      "|    reward             | 5.5300126 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 5.06      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 17.8      |\n",
      "|    reward             | 12.073821 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 125       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 550      |\n",
      "|    iterations         | 16000    |\n",
      "|    time_elapsed       | 145      |\n",
      "|    total_timesteps    | 80000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.19    |\n",
      "|    explained_variance | 0.00493  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15999    |\n",
      "|    policy_loss        | -67      |\n",
      "|    reward             | 7.137977 |\n",
      "|    std                | 2.17     |\n",
      "|    value_loss         | 654      |\n",
      "------------------------------------\n",
      "day: 3209, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11489301.47\n",
      "total_reward: 10489301.47\n",
      "total_cost: 3149.22\n",
      "total_trades: 3205\n",
      "Sharpe: 0.868\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 550         |\n",
      "|    iterations         | 16100       |\n",
      "|    time_elapsed       | 146         |\n",
      "|    total_timesteps    | 80500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.2        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16099       |\n",
      "|    policy_loss        | 0.293       |\n",
      "|    reward             | -0.12282752 |\n",
      "|    std                | 2.17        |\n",
      "|    value_loss         | 0.0292      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 16200      |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 81000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0.209      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16199      |\n",
      "|    policy_loss        | 3.17       |\n",
      "|    reward             | -1.6277018 |\n",
      "|    std                | 2.19       |\n",
      "|    value_loss         | 8.36       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | -12.7      |\n",
      "|    reward             | -1.5854394 |\n",
      "|    std                | 2.18       |\n",
      "|    value_loss         | 51.7       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 550      |\n",
      "|    iterations         | 16400    |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 82000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16399    |\n",
      "|    policy_loss        | 2.74     |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.2      |\n",
      "|    value_loss         | 3.61     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 16500      |\n",
      "|    time_elapsed       | 149        |\n",
      "|    total_timesteps    | 82500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16499      |\n",
      "|    policy_loss        | 29.7       |\n",
      "|    reward             | -1.8954247 |\n",
      "|    std                | 2.19       |\n",
      "|    value_loss         | 197        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 16600     |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 83000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16599     |\n",
      "|    policy_loss        | 0.0545    |\n",
      "|    reward             | -5.521372 |\n",
      "|    std                | 2.21      |\n",
      "|    value_loss         | 191       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 550         |\n",
      "|    iterations         | 16700       |\n",
      "|    time_elapsed       | 151         |\n",
      "|    total_timesteps    | 83500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.2        |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16699       |\n",
      "|    policy_loss        | -0.0339     |\n",
      "|    reward             | 0.005741868 |\n",
      "|    std                | 2.19        |\n",
      "|    value_loss         | 0.000971    |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -0.0206   |\n",
      "|    reward             | 0.9781973 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 0.774     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 16900      |\n",
      "|    time_elapsed       | 153        |\n",
      "|    total_timesteps    | 84500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0.0539     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16899      |\n",
      "|    policy_loss        | 8.65       |\n",
      "|    reward             | -1.5153395 |\n",
      "|    std                | 2.21       |\n",
      "|    value_loss         | 17.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 17000     |\n",
      "|    time_elapsed       | 154       |\n",
      "|    total_timesteps    | 85000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16999     |\n",
      "|    policy_loss        | -2.06     |\n",
      "|    reward             | 2.7613614 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 4.71      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | -27.5     |\n",
      "|    reward             | 3.6580129 |\n",
      "|    std                | 2.19      |\n",
      "|    value_loss         | 255       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 17200     |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 86000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17199     |\n",
      "|    policy_loss        | 22.5      |\n",
      "|    reward             | 7.466858  |\n",
      "|    std                | 2.19      |\n",
      "|    value_loss         | 221       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 17300     |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 86500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17299     |\n",
      "|    policy_loss        | -110      |\n",
      "|    reward             | 35.871517 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 3e+03     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 549         |\n",
      "|    iterations         | 17400       |\n",
      "|    time_elapsed       | 158         |\n",
      "|    total_timesteps    | 87000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17399       |\n",
      "|    policy_loss        | -0.188      |\n",
      "|    reward             | -0.25619453 |\n",
      "|    std                | 2.2         |\n",
      "|    value_loss         | 0.0893      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | -0.0148   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -2.54     |\n",
      "|    reward             | 1.8648998 |\n",
      "|    std                | 2.21      |\n",
      "|    value_loss         | 4.85      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 549        |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | 2.02       |\n",
      "|    reward             | -2.5960412 |\n",
      "|    std                | 2.2        |\n",
      "|    value_loss         | 2.46       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 549        |\n",
      "|    iterations         | 17700      |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 88500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17699      |\n",
      "|    policy_loss        | 2.02       |\n",
      "|    reward             | -2.9281583 |\n",
      "|    std                | 2.21       |\n",
      "|    value_loss         | 1.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 161       |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | 8.33      |\n",
      "|    reward             | 6.0783014 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 22.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 17900    |\n",
      "|    time_elapsed       | 162      |\n",
      "|    total_timesteps    | 89500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17899    |\n",
      "|    policy_loss        | -18.7    |\n",
      "|    reward             | 6.805968 |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 166      |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 549          |\n",
      "|    iterations         | 18000        |\n",
      "|    time_elapsed       | 163          |\n",
      "|    total_timesteps    | 90000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.22        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 17999        |\n",
      "|    policy_loss        | 0.828        |\n",
      "|    reward             | -0.083339214 |\n",
      "|    std                | 2.22         |\n",
      "|    value_loss         | 0.183        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 549        |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 164        |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | 0.162      |\n",
      "|    reward             | 0.30442426 |\n",
      "|    std                | 2.19       |\n",
      "|    value_loss         | 0.824      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 549         |\n",
      "|    iterations         | 18200       |\n",
      "|    time_elapsed       | 165         |\n",
      "|    total_timesteps    | 91000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18199       |\n",
      "|    policy_loss        | 4.79        |\n",
      "|    reward             | -0.21667694 |\n",
      "|    std                | 2.17        |\n",
      "|    value_loss         | 7.95        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 550        |\n",
      "|    iterations         | 18300      |\n",
      "|    time_elapsed       | 166        |\n",
      "|    total_timesteps    | 91500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18299      |\n",
      "|    policy_loss        | -7.29      |\n",
      "|    reward             | -1.5369452 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 18         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | -3.84     |\n",
      "|    reward             | 1.2996039 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 4.12      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 18500     |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 92500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | -2.53     |\n",
      "|    reward             | 2.9692779 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 19.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 169       |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | -91.3     |\n",
      "|    reward             | 1.2913159 |\n",
      "|    std                | 2.13      |\n",
      "|    value_loss         | 2.19e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 549        |\n",
      "|    iterations         | 18700      |\n",
      "|    time_elapsed       | 170        |\n",
      "|    total_timesteps    | 93500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18699      |\n",
      "|    policy_loss        | 0.965      |\n",
      "|    reward             | -1.7758201 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 549         |\n",
      "|    iterations         | 18800       |\n",
      "|    time_elapsed       | 171         |\n",
      "|    total_timesteps    | 94000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18799       |\n",
      "|    policy_loss        | -3.34       |\n",
      "|    reward             | -0.14944588 |\n",
      "|    std                | 2.15        |\n",
      "|    value_loss         | 1.36        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 18900     |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 94500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18899     |\n",
      "|    policy_loss        | -24.6     |\n",
      "|    reward             | -11.49342 |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 198       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 19000     |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 95000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18999     |\n",
      "|    policy_loss        | -9.46     |\n",
      "|    reward             | 1.5662631 |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 32.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 173       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | 1.41      |\n",
      "|    reward             | -9.077823 |\n",
      "|    std                | 2.11      |\n",
      "|    value_loss         | 18.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 549        |\n",
      "|    iterations         | 19200      |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 96000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19199      |\n",
      "|    policy_loss        | -8.45      |\n",
      "|    reward             | -17.812546 |\n",
      "|    std                | 2.11       |\n",
      "|    value_loss         | 384        |\n",
      "--------------------------------------\n",
      "day: 3209, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11510657.30\n",
      "total_reward: 10510657.30\n",
      "total_cost: 1442.97\n",
      "total_trades: 3206\n",
      "Sharpe: 0.866\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 548        |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 175        |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | 0.7        |\n",
      "|    reward             | -0.5147026 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 0.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 548        |\n",
      "|    iterations         | 19400      |\n",
      "|    time_elapsed       | 176        |\n",
      "|    total_timesteps    | 97000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19399      |\n",
      "|    policy_loss        | -1.85      |\n",
      "|    reward             | -2.7174983 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 4.2        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 19500     |\n",
      "|    time_elapsed       | 177       |\n",
      "|    total_timesteps    | 97500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | -0.894    |\n",
      "|    reward             | 0.3998092 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 3.95      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 549       |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 178       |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | -4.81     |\n",
      "|    reward             | 0.9058822 |\n",
      "|    std                | 2.16      |\n",
      "|    value_loss         | 4.81      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 19700    |\n",
      "|    time_elapsed       | 179      |\n",
      "|    total_timesteps    | 98500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19699    |\n",
      "|    policy_loss        | -11      |\n",
      "|    reward             | 1.541406 |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 43.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 548       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 180       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | -51.6     |\n",
      "|    reward             | 8.8042345 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 895       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 549      |\n",
      "|    iterations         | 19900    |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 99500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19899    |\n",
      "|    policy_loss        | -86.2    |\n",
      "|    reward             | 19.60516 |\n",
      "|    std                | 2.13     |\n",
      "|    value_loss         | 1.45e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 548        |\n",
      "|    iterations         | 20000      |\n",
      "|    time_elapsed       | 182        |\n",
      "|    total_timesteps    | 100000     |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19999      |\n",
      "|    policy_loss        | 2.82       |\n",
      "|    reward             | -0.2633757 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 3.04       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-10-04 to  2023-01-04\n",
      "A2C Sharpe Ratio:  -0.21401757012035091\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_567_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 1025        |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 1           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.032850794 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 937          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048832637 |\n",
      "|    clip_fraction        | 0.0409       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0177      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    reward               | -0.021789568 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 0.00519      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 849          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012972214 |\n",
      "|    clip_fraction        | 0.00459      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.0122      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.28         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00308     |\n",
      "|    reward               | 11.524435    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 11.5         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 854           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 9             |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0011306747  |\n",
      "|    clip_fraction        | 0.000977      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -0.0511       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 34.9          |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00103      |\n",
      "|    reward               | -0.0056167454 |\n",
      "|    std                  | 0.997         |\n",
      "|    value_loss           | 61.2          |\n",
      "-------------------------------------------\n",
      "day: 3209, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4240482.13\n",
      "total_reward: 3240482.13\n",
      "total_cost: 9639.38\n",
      "total_trades: 3182\n",
      "Sharpe: 0.730\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 857          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003746083  |\n",
      "|    clip_fraction        | 0.0165       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.00662      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 49.5         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    reward               | -0.008985347 |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 66.1         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 838           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 14            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043176772 |\n",
      "|    clip_fraction        | 0.00659       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.022         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 78.4          |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | 0.000748      |\n",
      "|    reward               | 2.8092148     |\n",
      "|    std                  | 0.996         |\n",
      "|    value_loss           | 146           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 846          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065984568 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0494       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.27         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    reward               | -1.2002113   |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 6.81         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 850          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023176358 |\n",
      "|    clip_fraction        | 0.00313      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.000902     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 26.4         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00193     |\n",
      "|    reward               | -0.014039829 |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 43.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 855          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003160811 |\n",
      "|    clip_fraction        | 0.00146      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.021        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 64.8         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | 0.000243     |\n",
      "|    reward               | -0.11695902  |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 861         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00510158  |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.327       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00248    |\n",
      "|    reward               | 0.019474426 |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 0.715       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 863          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021732855 |\n",
      "|    clip_fraction        | 0.00645      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0391       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.38         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00105     |\n",
      "|    reward               | 0.0034710895 |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 16.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 865         |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006749223 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.148       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.39        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00681    |\n",
      "|    reward               | 0.90259486  |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "day: 3209, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4390853.61\n",
      "total_reward: 3390853.61\n",
      "total_cost: 9406.73\n",
      "total_trades: 3173\n",
      "Sharpe: 0.717\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 864          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016952392 |\n",
      "|    clip_fraction        | 0.00562      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.417        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.25         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    reward               | -0.34991786  |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 2.79         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 866           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 33            |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068154203 |\n",
      "|    clip_fraction        | 0.0021        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.0646        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 110           |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | 0.000233      |\n",
      "|    reward               | 10.861758     |\n",
      "|    std                  | 0.977         |\n",
      "|    value_loss           | 170           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 867          |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 35           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013627615 |\n",
      "|    clip_fraction        | 0.00444      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.049       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 81.8         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.000248    |\n",
      "|    reward               | -1.0196093   |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 179          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 870          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029433586 |\n",
      "|    clip_fraction        | 0.0022       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.0582      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 54.5         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.0021      |\n",
      "|    reward               | 0.2110424    |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 867          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023055752 |\n",
      "|    clip_fraction        | 0.00815      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.000744    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 101          |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000173    |\n",
      "|    reward               | -4.333359    |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 232          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 870         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001698724 |\n",
      "|    clip_fraction        | 0.00894     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.198       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.000629   |\n",
      "|    reward               | -0.33954895 |\n",
      "|    std                  | 0.974       |\n",
      "|    value_loss           | 44.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 871         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000888708 |\n",
      "|    clip_fraction        | 0.00845     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.137       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48.1        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.000302    |\n",
      "|    reward               | 0.09589934  |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 92.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045933863 |\n",
      "|    clip_fraction        | 0.0194       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0723       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 91.4         |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | 5.48e-05     |\n",
      "|    reward               | -4.209319    |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 212          |\n",
      "------------------------------------------\n",
      "day: 3209, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5935553.79\n",
      "total_reward: 4935553.79\n",
      "total_cost: 9546.61\n",
      "total_trades: 3184\n",
      "Sharpe: 0.777\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033956633 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.416        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13.5         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | 0.000293     |\n",
      "|    reward               | 0.6176756    |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 26.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 874          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.001632256  |\n",
      "|    clip_fraction        | 0.00103      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.111        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 131          |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000158    |\n",
      "|    reward               | -0.031865973 |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 306          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005755209 |\n",
      "|    clip_fraction        | 0.00127      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0756       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 159          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000531    |\n",
      "|    reward               | 0.14262272   |\n",
      "|    std                  | 0.98         |\n",
      "|    value_loss           | 305          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00495978  |\n",
      "|    clip_fraction        | 0.0175      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.72        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.44        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    reward               | -0.13829106 |\n",
      "|    std                  | 0.988       |\n",
      "|    value_loss           | 10.1        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 878           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 58            |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042862608 |\n",
      "|    clip_fraction        | 0.000244      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.168         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 139           |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | 0.000383      |\n",
      "|    reward               | 0.71366537    |\n",
      "|    std                  | 0.99          |\n",
      "|    value_loss           | 347           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012123163 |\n",
      "|    clip_fraction        | 0.000244     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.395        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 33           |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.000724    |\n",
      "|    reward               | 0.25187388   |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 101          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051472974 |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.0664      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 30.4         |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    reward               | -0.14850593  |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 64.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004310587 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.239        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 62.2         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000928    |\n",
      "|    reward               | -11.650257   |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 156          |\n",
      "------------------------------------------\n",
      "day: 3209, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5462350.16\n",
      "total_reward: 4462350.16\n",
      "total_cost: 8859.82\n",
      "total_trades: 3178\n",
      "Sharpe: 0.755\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 878           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046148215 |\n",
      "|    clip_fraction        | 0.00151       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.353         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 64.3          |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.00102      |\n",
      "|    reward               | -0.2513927    |\n",
      "|    std                  | 0.994         |\n",
      "|    value_loss           | 103           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042960495 |\n",
      "|    clip_fraction        | 0.00918      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.191        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 51           |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.0023      |\n",
      "|    reward               | 0.46350193   |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 174          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 878           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 72            |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00095165917 |\n",
      "|    clip_fraction        | 0.00151       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.194         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 107           |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -0.000644     |\n",
      "|    reward               | 4.703132      |\n",
      "|    std                  | 0.991         |\n",
      "|    value_loss           | 268           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026876698 |\n",
      "|    clip_fraction        | 0.00674      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.513        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 14.7         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000785    |\n",
      "|    reward               | 1.3603985    |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 46.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 76           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022259043 |\n",
      "|    clip_fraction        | 0.00347      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.179        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 239          |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000432    |\n",
      "|    reward               | 0.040493067  |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 434          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020713555 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 127          |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.000948    |\n",
      "|    reward               | 1.0499018    |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 380          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 888          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016108288 |\n",
      "|    clip_fraction        | 0.0122       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.728        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.42         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    reward               | 0.2600015    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 11.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 890          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016816587 |\n",
      "|    clip_fraction        | 0.00591      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.288        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 152          |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.000703    |\n",
      "|    reward               | -21.232975   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 351          |\n",
      "------------------------------------------\n",
      "day: 3209, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5804319.35\n",
      "total_reward: 4804319.35\n",
      "total_cost: 8019.71\n",
      "total_trades: 3178\n",
      "Sharpe: 0.766\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 892          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 84           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028742277 |\n",
      "|    clip_fraction        | 0.00791      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.399        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 95.9         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    reward               | 2.2044685    |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 232          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 894          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017058263 |\n",
      "|    clip_fraction        | 0.00229      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.166        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 63.6         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00243     |\n",
      "|    reward               | -0.02612948  |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 75.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 896          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 89           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009178471 |\n",
      "|    clip_fraction        | 0.00142      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.308        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 149          |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.000409    |\n",
      "|    reward               | 1.4555836    |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 361          |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 898        |\n",
      "|    iterations           | 40         |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 81920      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00335027 |\n",
      "|    clip_fraction        | 0.0145     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.445      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 124        |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.00152   |\n",
      "|    reward               | -0.650471  |\n",
      "|    std                  | 0.99       |\n",
      "|    value_loss           | 194        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 899          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018315567 |\n",
      "|    clip_fraction        | 0.00693      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.246        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 198          |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    reward               | -0.017003024 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 257          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 901         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002881262 |\n",
      "|    clip_fraction        | 0.0135      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.294       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 174         |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.000674   |\n",
      "|    reward               | 29.958536   |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 391         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 902          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058183754 |\n",
      "|    clip_fraction        | 0.0236       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.704        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 26.3         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00339     |\n",
      "|    reward               | 2.4422393    |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 58.4         |\n",
      "------------------------------------------\n",
      "day: 3209, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7854053.10\n",
      "total_reward: 6854053.10\n",
      "total_cost: 8352.03\n",
      "total_trades: 3178\n",
      "Sharpe: 0.818\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 904          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010335927 |\n",
      "|    clip_fraction        | 0.00278      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.339        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 239          |\n",
      "|    n_updates            | 430          |\n",
      "|    policy_gradient_loss | -0.000145    |\n",
      "|    reward               | -0.016817097 |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 399          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 904          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 101          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017425243 |\n",
      "|    clip_fraction        | 0.00234      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.165        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 272          |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000538    |\n",
      "|    reward               | 6.8795357    |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 600          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 906          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 103          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029145067 |\n",
      "|    clip_fraction        | 0.0152       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.718        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    reward               | -0.8683093   |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 47.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 908          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 105          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022682077 |\n",
      "|    clip_fraction        | 0.00371      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.309        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 306          |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | -0.00154     |\n",
      "|    reward               | -8.578016    |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 586          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 909          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 108          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021485132 |\n",
      "|    clip_fraction        | 0.0041       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 296          |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000408    |\n",
      "|    reward               | 2.8389204    |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 578          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 911          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 110          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017302462 |\n",
      "|    clip_fraction        | 0.00571      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.125        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 30.1         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    reward               | -0.25461945  |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 74.8         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-10-04 to  2023-01-04\n",
      "PPO Sharpe Ratio:  -0.19276432724144424\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_567_3\n",
      "day: 3209, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1224881.39\n",
      "total_reward: 224881.39\n",
      "total_cost: 951.32\n",
      "total_trades: 797\n",
      "Sharpe: 0.522\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 81       |\n",
      "|    total_timesteps | 12840    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 5.59e+03 |\n",
      "|    critic_loss     | 6.97e+04 |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 12739    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3209, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 25680    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.56e+03 |\n",
      "|    critic_loss     | 67.2     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 25579    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3209, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 251      |\n",
      "|    total_timesteps | 38520    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 2.17e+03 |\n",
      "|    critic_loss     | 2.88     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 38419    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3209, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 51360    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 432      |\n",
      "|    critic_loss     | 0.263    |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 51259    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 422      |\n",
      "|    total_timesteps | 64200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.37     |\n",
      "|    critic_loss     | 0.00488  |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 64099    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3209, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 511      |\n",
      "|    total_timesteps | 77040    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.65     |\n",
      "|    critic_loss     | 0.0112   |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 76939    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3209, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 599      |\n",
      "|    total_timesteps | 89880    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 0.128    |\n",
      "|    critic_loss     | 3.05e-10 |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 89779    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3209, episode: 95\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "======DDPG Validation from:  2022-10-04 to  2023-01-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2023-01-04\n",
      "======Trading from:  2023-01-04 to  2023-04-05\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2023-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_630_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 580        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.47      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -0.169     |\n",
      "|    reward             | 0.29324418 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.266      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 557       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | -0.264    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.84     |\n",
      "|    reward             | 4.5103235 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 2.41      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 543        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.45      |\n",
      "|    explained_variance | 0.0255     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -3.4       |\n",
      "|    reward             | -3.8731952 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 529        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.45      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -3.7       |\n",
      "|    reward             | -1.5963838 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 11         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 537        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.46      |\n",
      "|    explained_variance | 0.0149     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -2.62      |\n",
      "|    reward             | -6.1114764 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 17.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 544       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.46     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 32.5      |\n",
      "|    reward             | -5.896687 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 604       |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 541           |\n",
      "|    iterations         | 700           |\n",
      "|    time_elapsed       | 6             |\n",
      "|    total_timesteps    | 3500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.47         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 699           |\n",
      "|    policy_loss        | -0.012        |\n",
      "|    reward             | -6.249053e-05 |\n",
      "|    std                | 1.06          |\n",
      "|    value_loss         | 4.2e-05       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 535            |\n",
      "|    iterations         | 800            |\n",
      "|    time_elapsed       | 7              |\n",
      "|    total_timesteps    | 4000           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.48          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 799            |\n",
      "|    policy_loss        | -0.000294      |\n",
      "|    reward             | -0.00012148176 |\n",
      "|    std                | 1.07           |\n",
      "|    value_loss         | 6.58e-08       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 536           |\n",
      "|    iterations         | 900           |\n",
      "|    time_elapsed       | 8             |\n",
      "|    total_timesteps    | 4500          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.5          |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 899           |\n",
      "|    policy_loss        | -2.56e-05     |\n",
      "|    reward             | -0.0009023031 |\n",
      "|    std                | 1.08          |\n",
      "|    value_loss         | 4.88e-10      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 542       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.52     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -9.65e-05 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.1       |\n",
      "|    value_loss         | 1.07e-08  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 540         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.54       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 0.000651    |\n",
      "|    reward             | 0.003779808 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 1.76e-07    |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 532           |\n",
      "|    iterations         | 1200          |\n",
      "|    time_elapsed       | 11            |\n",
      "|    total_timesteps    | 6000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.58         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1199          |\n",
      "|    policy_loss        | 0.0104        |\n",
      "|    reward             | -0.0018429544 |\n",
      "|    std                | 1.18          |\n",
      "|    value_loss         | 8.06e-05      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.61    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0261  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.21     |\n",
      "|    value_loss         | 0.000739 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 533            |\n",
      "|    iterations         | 1400           |\n",
      "|    time_elapsed       | 13             |\n",
      "|    total_timesteps    | 7000           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.63          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1399           |\n",
      "|    policy_loss        | -0.000113      |\n",
      "|    reward             | -2.4105111e-06 |\n",
      "|    std                | 1.24           |\n",
      "|    value_loss         | 1.06e-08       |\n",
      "------------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 533      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.00161  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 1.63e-06 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 535          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 14           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.72        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | 0.00229      |\n",
      "|    reward             | 0.0004740284 |\n",
      "|    std                | 1.36         |\n",
      "|    value_loss         | 3.09e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.77    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.0383   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.43     |\n",
      "|    value_loss         | 0.000534 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 528          |\n",
      "|    iterations         | 1800         |\n",
      "|    time_elapsed       | 17           |\n",
      "|    total_timesteps    | 9000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.82        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1799         |\n",
      "|    policy_loss        | 0.00996      |\n",
      "|    reward             | 0.0024674889 |\n",
      "|    std                | 1.5          |\n",
      "|    value_loss         | 4.5e-05      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 532          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 17           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.84        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 0.00183      |\n",
      "|    reward             | -0.001421946 |\n",
      "|    std                | 1.53         |\n",
      "|    value_loss         | 1.28e-05     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 530          |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 18           |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.86        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | -0.000264    |\n",
      "|    reward             | 0.0006482101 |\n",
      "|    std                | 1.56         |\n",
      "|    value_loss         | 4.31e-08     |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 529      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.88    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.000669 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.59     |\n",
      "|    value_loss         | 1.88e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 11000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.92    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.0103  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.64     |\n",
      "|    value_loss         | 3.79e-05 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 532      |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 11500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.96    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.00176 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.72     |\n",
      "|    value_loss         | 1.61e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 531      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.01    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.00306  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.81     |\n",
      "|    value_loss         | 1.91e-06 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 531          |\n",
      "|    iterations         | 2500         |\n",
      "|    time_elapsed       | 23           |\n",
      "|    total_timesteps    | 12500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.04        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2499         |\n",
      "|    policy_loss        | 0.00552      |\n",
      "|    reward             | -0.027254492 |\n",
      "|    std                | 1.86         |\n",
      "|    value_loss         | 0.000128     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 534          |\n",
      "|    iterations         | 2600         |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 13000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.04        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2599         |\n",
      "|    policy_loss        | -0.00408     |\n",
      "|    reward             | -0.008404365 |\n",
      "|    std                | 1.87         |\n",
      "|    value_loss         | 5.06e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 533            |\n",
      "|    iterations         | 2700           |\n",
      "|    time_elapsed       | 25             |\n",
      "|    total_timesteps    | 13500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.06          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 2699           |\n",
      "|    policy_loss        | 0.000373       |\n",
      "|    reward             | -0.00096178707 |\n",
      "|    std                | 1.9            |\n",
      "|    value_loss         | 7.71e-07       |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 532         |\n",
      "|    iterations         | 2800        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 14000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2799        |\n",
      "|    policy_loss        | 0.0108      |\n",
      "|    reward             | 0.021817908 |\n",
      "|    std                | 1.95        |\n",
      "|    value_loss         | 6.67e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.1        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | -0.0427     |\n",
      "|    reward             | 0.004285428 |\n",
      "|    std                | 1.98        |\n",
      "|    value_loss         | 0.000657    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 533         |\n",
      "|    iterations         | 3000        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 15000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.13       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2999        |\n",
      "|    policy_loss        | -0.00686    |\n",
      "|    reward             | 0.015774986 |\n",
      "|    std                | 2.04        |\n",
      "|    value_loss         | 2.07e-05    |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 533         |\n",
      "|    iterations         | 3100        |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 15500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.16       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3099        |\n",
      "|    policy_loss        | 0.182       |\n",
      "|    reward             | -0.01725209 |\n",
      "|    std                | 2.1         |\n",
      "|    value_loss         | 0.00771     |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 3200      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 16000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3199      |\n",
      "|    policy_loss        | -0.11     |\n",
      "|    reward             | 0.6058809 |\n",
      "|    std                | 2.13      |\n",
      "|    value_loss         | 0.439     |\n",
      "-------------------------------------\n",
      "day: 3272, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 927351.55\n",
      "total_reward: -72648.45\n",
      "total_cost: 11833.45\n",
      "total_trades: 2800\n",
      "Sharpe: -0.017\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 533          |\n",
      "|    iterations         | 3300         |\n",
      "|    time_elapsed       | 30           |\n",
      "|    total_timesteps    | 16500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.19        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 3299         |\n",
      "|    policy_loss        | 0.0244       |\n",
      "|    reward             | -0.069239356 |\n",
      "|    std                | 2.15         |\n",
      "|    value_loss         | 0.0066       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 532         |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | 2.53        |\n",
      "|    reward             | -0.20188464 |\n",
      "|    std                | 2.14        |\n",
      "|    value_loss         | 1.34        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 32          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 2.53        |\n",
      "|    reward             | 0.019186521 |\n",
      "|    std                | 2.15        |\n",
      "|    value_loss         | 1.27        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -3.76      |\n",
      "|    reward             | 0.22562863 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 3.1        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | -0.0714    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -9.5       |\n",
      "|    reward             | -0.3062572 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 16.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 19000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.19    |\n",
      "|    explained_variance | -0.016   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 21.1     |\n",
      "|    reward             | 8.780819 |\n",
      "|    std                | 2.15     |\n",
      "|    value_loss         | 148      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0.00174    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | -51        |\n",
      "|    reward             | -24.704767 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 670        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 536         |\n",
      "|    iterations         | 4000        |\n",
      "|    time_elapsed       | 37          |\n",
      "|    total_timesteps    | 20000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3999        |\n",
      "|    policy_loss        | -0.0787     |\n",
      "|    reward             | -0.14849702 |\n",
      "|    std                | 2.17        |\n",
      "|    value_loss         | 0.0141      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 0.542      |\n",
      "|    reward             | 0.22664078 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 0.125      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 2.17       |\n",
      "|    reward             | 0.16517879 |\n",
      "|    std                | 2.17       |\n",
      "|    value_loss         | 2.46       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | -2.52      |\n",
      "|    reward             | -0.3095325 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 10.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | -0.00398   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | -6.47      |\n",
      "|    reward             | -3.9144895 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 56.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | -0.00591  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -23.7     |\n",
      "|    reward             | -7.200616 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 204       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | -0.391     |\n",
      "|    reward             | 0.12464917 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 0.0544     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | 5.51      |\n",
      "|    reward             | -2.014109 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 9.83      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | 17.1       |\n",
      "|    reward             | 0.24775584 |\n",
      "|    std                | 2.17       |\n",
      "|    value_loss         | 69.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 4900       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 24500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4899       |\n",
      "|    policy_loss        | -9.95      |\n",
      "|    reward             | -10.496371 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 32.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 9.55       |\n",
      "|    reward             | -0.8079568 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 26.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 533        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0.000899   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 51.2       |\n",
      "|    reward             | -6.6710243 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 760        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | -0.000564 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | 27.6      |\n",
      "|    reward             | -30.21383 |\n",
      "|    std                | 2.16      |\n",
      "|    value_loss         | 377       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 5300        |\n",
      "|    time_elapsed       | 49          |\n",
      "|    total_timesteps    | 26500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.2        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5299        |\n",
      "|    policy_loss        | 0.63        |\n",
      "|    reward             | -0.17437068 |\n",
      "|    std                | 2.18        |\n",
      "|    value_loss         | 0.22        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 5400       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 27000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5399       |\n",
      "|    policy_loss        | 1.3        |\n",
      "|    reward             | 0.40162882 |\n",
      "|    std                | 2.18       |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -1.53     |\n",
      "|    reward             | -1.564251 |\n",
      "|    std                | 2.16      |\n",
      "|    value_loss         | 8.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 4.74      |\n",
      "|    reward             | 2.5383654 |\n",
      "|    std                | 2.15      |\n",
      "|    value_loss         | 4.02      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 5700       |\n",
      "|    time_elapsed       | 53         |\n",
      "|    total_timesteps    | 28500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5699       |\n",
      "|    policy_loss        | 17.1       |\n",
      "|    reward             | -2.5233245 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 108        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 5800       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 29000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0.0045     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5799       |\n",
      "|    policy_loss        | -24.8      |\n",
      "|    reward             | -6.4257627 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 547        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 535          |\n",
      "|    iterations         | 5900         |\n",
      "|    time_elapsed       | 55           |\n",
      "|    total_timesteps    | 29500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.18        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 5899         |\n",
      "|    policy_loss        | 0.0286       |\n",
      "|    reward             | 0.0013407201 |\n",
      "|    std                | 2.13         |\n",
      "|    value_loss         | 0.000498     |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 56         |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 1.8        |\n",
      "|    reward             | 0.96038187 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 1.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 533        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | -2.35      |\n",
      "|    reward             | -1.0525321 |\n",
      "|    std                | 2.17       |\n",
      "|    value_loss         | 2.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -2.59     |\n",
      "|    reward             | 1.3545052 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 2.04      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 6300      |\n",
      "|    time_elapsed       | 59        |\n",
      "|    total_timesteps    | 31500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6299      |\n",
      "|    policy_loss        | -14.5     |\n",
      "|    reward             | 5.0332036 |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 90.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 533        |\n",
      "|    iterations         | 6400       |\n",
      "|    time_elapsed       | 59         |\n",
      "|    total_timesteps    | 32000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6399       |\n",
      "|    policy_loss        | 5.08       |\n",
      "|    reward             | -3.2623386 |\n",
      "|    std                | 2.2        |\n",
      "|    value_loss         | 140        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 533        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 60         |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | -39.9      |\n",
      "|    reward             | -20.128183 |\n",
      "|    std                | 2.21       |\n",
      "|    value_loss         | 425        |\n",
      "--------------------------------------\n",
      "day: 3272, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8743413.67\n",
      "total_reward: 7743413.67\n",
      "total_cost: 13226.10\n",
      "total_trades: 3260\n",
      "Sharpe: 0.795\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 61          |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.2        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -2.07       |\n",
      "|    reward             | -0.43878442 |\n",
      "|    std                | 2.18        |\n",
      "|    value_loss         | 1.05        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | -0.878    |\n",
      "|    reward             | -1.849902 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 7.22      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | -0.138    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | -1.57     |\n",
      "|    reward             | 1.2044073 |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 11.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 5.59       |\n",
      "|    reward             | -0.3088752 |\n",
      "|    std                | 2.2        |\n",
      "|    value_loss         | 4.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -49.8     |\n",
      "|    reward             | 0.7330871 |\n",
      "|    std                | 2.23      |\n",
      "|    value_loss         | 486       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 66        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | -0.00644  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 80.1      |\n",
      "|    reward             | 10.157126 |\n",
      "|    std                | 2.19      |\n",
      "|    value_loss         | 1.9e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | -5.24     |\n",
      "|    reward             | 22.880028 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 692       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -2.72      |\n",
      "|    reward             | 0.06691705 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 2.54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 69         |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | -2.32      |\n",
      "|    reward             | -0.9370581 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 2.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 70         |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 2.18       |\n",
      "|    reward             | -1.1096777 |\n",
      "|    std                | 2.18       |\n",
      "|    value_loss         | 6.19       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 534      |\n",
      "|    iterations         | 7600     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 38000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7599     |\n",
      "|    policy_loss        | -6.56    |\n",
      "|    reward             | 0.511641 |\n",
      "|    std                | 2.19     |\n",
      "|    value_loss         | 21.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | 0.285     |\n",
      "|    reward             | 3.8586032 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 11.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 72        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 2.98e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 98.2      |\n",
      "|    reward             | 23.434097 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 3.3e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | -1.09     |\n",
      "|    reward             | 0.3817315 |\n",
      "|    std                | 2.19      |\n",
      "|    value_loss         | 0.295     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -8.39     |\n",
      "|    reward             | 0.3857508 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 27.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 2.05      |\n",
      "|    reward             | 2.4331763 |\n",
      "|    std                | 2.21      |\n",
      "|    value_loss         | 1.84      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 8200      |\n",
      "|    time_elapsed       | 76        |\n",
      "|    total_timesteps    | 41000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8199      |\n",
      "|    policy_loss        | -8.46     |\n",
      "|    reward             | 2.6092262 |\n",
      "|    std                | 2.23      |\n",
      "|    value_loss         | 29.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 2.66      |\n",
      "|    reward             | 5.7150717 |\n",
      "|    std                | 2.23      |\n",
      "|    value_loss         | 24.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 533        |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 78         |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | -0.000281  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -42.6      |\n",
      "|    reward             | -37.724907 |\n",
      "|    std                | 2.21       |\n",
      "|    value_loss         | 405        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 79        |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 1.96e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 31.1      |\n",
      "|    reward             | 15.391547 |\n",
      "|    std                | 2.21      |\n",
      "|    value_loss         | 458       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 532         |\n",
      "|    iterations         | 8600        |\n",
      "|    time_elapsed       | 80          |\n",
      "|    total_timesteps    | 43000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8599        |\n",
      "|    policy_loss        | 4.91        |\n",
      "|    reward             | -0.25561893 |\n",
      "|    std                | 2.22        |\n",
      "|    value_loss         | 2.9         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 0.397     |\n",
      "|    reward             | 0.7433806 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | -21        |\n",
      "|    reward             | -2.1247735 |\n",
      "|    std                | 2.18       |\n",
      "|    value_loss         | 42.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 533       |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 83        |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | -5.64     |\n",
      "|    reward             | 1.1711134 |\n",
      "|    std                | 2.16      |\n",
      "|    value_loss         | 35.1      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 533      |\n",
      "|    iterations         | 9000     |\n",
      "|    time_elapsed       | 84       |\n",
      "|    total_timesteps    | 45000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 8999     |\n",
      "|    policy_loss        | -7.45    |\n",
      "|    reward             | 9.303211 |\n",
      "|    std                | 2.19     |\n",
      "|    value_loss         | 22.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | -17.2     |\n",
      "|    reward             | 7.1834145 |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 329       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 534          |\n",
      "|    iterations         | 9200         |\n",
      "|    time_elapsed       | 86           |\n",
      "|    total_timesteps    | 46000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.19        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9199         |\n",
      "|    policy_loss        | 0.434        |\n",
      "|    reward             | -0.062422995 |\n",
      "|    std                | 2.17         |\n",
      "|    value_loss         | 0.0775       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 9300        |\n",
      "|    time_elapsed       | 87          |\n",
      "|    total_timesteps    | 46500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9299        |\n",
      "|    policy_loss        | 0.759       |\n",
      "|    reward             | -0.42349884 |\n",
      "|    std                | 2.17        |\n",
      "|    value_loss         | 1.56        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 87          |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 4.51        |\n",
      "|    reward             | -0.62791747 |\n",
      "|    std                | 2.16        |\n",
      "|    value_loss         | 3.68        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 9500     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 47500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.19    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9499     |\n",
      "|    policy_loss        | -5.46    |\n",
      "|    reward             | 1.115877 |\n",
      "|    std                | 2.17     |\n",
      "|    value_loss         | 8.38     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 9600      |\n",
      "|    time_elapsed       | 89        |\n",
      "|    total_timesteps    | 48000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0.000241  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9599      |\n",
      "|    policy_loss        | 4.66      |\n",
      "|    reward             | 3.1933908 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 33.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 24.1      |\n",
      "|    reward             | 11.112342 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 350       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 91        |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 0.000476  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | 31.5      |\n",
      "|    reward             | -3.019747 |\n",
      "|    std                | 2.22      |\n",
      "|    value_loss         | 471       |\n",
      "-------------------------------------\n",
      "day: 3272, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9526374.96\n",
      "total_reward: 8526374.96\n",
      "total_cost: 13492.72\n",
      "total_trades: 3261\n",
      "Sharpe: 0.800\n",
      "=================================\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 9900      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 49500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9899      |\n",
      "|    policy_loss        | -3.33     |\n",
      "|    reward             | 1.1301738 |\n",
      "|    std                | 2.23      |\n",
      "|    value_loss         | 3.02      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 535         |\n",
      "|    iterations         | 10000       |\n",
      "|    time_elapsed       | 93          |\n",
      "|    total_timesteps    | 50000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.22       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9999        |\n",
      "|    policy_loss        | 3.69        |\n",
      "|    reward             | -0.21537119 |\n",
      "|    std                | 2.24        |\n",
      "|    value_loss         | 3.93        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 10100     |\n",
      "|    time_elapsed       | 94        |\n",
      "|    total_timesteps    | 50500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10099     |\n",
      "|    policy_loss        | -11       |\n",
      "|    reward             | -6.302962 |\n",
      "|    std                | 2.22      |\n",
      "|    value_loss         | 42.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 10200      |\n",
      "|    time_elapsed       | 95         |\n",
      "|    total_timesteps    | 51000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.23      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10199      |\n",
      "|    policy_loss        | -0.263     |\n",
      "|    reward             | -1.8495256 |\n",
      "|    std                | 2.25       |\n",
      "|    value_loss         | 0.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 10300      |\n",
      "|    time_elapsed       | 96         |\n",
      "|    total_timesteps    | 51500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.22      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10299      |\n",
      "|    policy_loss        | 6.54       |\n",
      "|    reward             | -2.9607394 |\n",
      "|    std                | 2.22       |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 534      |\n",
      "|    iterations         | 10400    |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | 25.3     |\n",
      "|    reward             | 27.02158 |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 204      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 10500       |\n",
      "|    time_elapsed       | 98          |\n",
      "|    total_timesteps    | 52500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.22       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10499       |\n",
      "|    policy_loss        | 0.631       |\n",
      "|    reward             | 0.033453636 |\n",
      "|    std                | 2.23        |\n",
      "|    value_loss         | 0.0761      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 10600      |\n",
      "|    time_elapsed       | 98         |\n",
      "|    total_timesteps    | 53000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.23      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10599      |\n",
      "|    policy_loss        | -3.32      |\n",
      "|    reward             | -0.6513674 |\n",
      "|    std                | 2.24       |\n",
      "|    value_loss         | 2.56       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 10700     |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 53500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.24     |\n",
      "|    explained_variance | -0.0219   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10699     |\n",
      "|    policy_loss        | -1.23     |\n",
      "|    reward             | 0.9173524 |\n",
      "|    std                | 2.27      |\n",
      "|    value_loss         | 1.87      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 10800     |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 54000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.24     |\n",
      "|    explained_variance | -0.00419  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10799     |\n",
      "|    policy_loss        | -4.08     |\n",
      "|    reward             | 2.7101421 |\n",
      "|    std                | 2.27      |\n",
      "|    value_loss         | 7.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 10900      |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 54500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.24      |\n",
      "|    explained_variance | 0.00825    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10899      |\n",
      "|    policy_loss        | -15.1      |\n",
      "|    reward             | -0.9777434 |\n",
      "|    std                | 2.28       |\n",
      "|    value_loss         | 27.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 11000     |\n",
      "|    time_elapsed       | 102       |\n",
      "|    total_timesteps    | 55000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.24     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10999     |\n",
      "|    policy_loss        | -11.7     |\n",
      "|    reward             | 0.2583417 |\n",
      "|    std                | 2.27      |\n",
      "|    value_loss         | 376       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 11100      |\n",
      "|    time_elapsed       | 103        |\n",
      "|    total_timesteps    | 55500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.25      |\n",
      "|    explained_variance | -0.0275    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11099      |\n",
      "|    policy_loss        | -43.7      |\n",
      "|    reward             | -39.124016 |\n",
      "|    std                | 2.29       |\n",
      "|    value_loss         | 616        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 11200      |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 56000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.24      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11199      |\n",
      "|    policy_loss        | 2.56       |\n",
      "|    reward             | -0.5654142 |\n",
      "|    std                | 2.28       |\n",
      "|    value_loss         | 1.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 105        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.24      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | -2.04      |\n",
      "|    reward             | -0.2992265 |\n",
      "|    std                | 2.28       |\n",
      "|    value_loss         | 0.733      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 11400      |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 57000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.23      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11399      |\n",
      "|    policy_loss        | 5.08       |\n",
      "|    reward             | -2.5794482 |\n",
      "|    std                | 2.25       |\n",
      "|    value_loss         | 6.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 11500      |\n",
      "|    time_elapsed       | 107        |\n",
      "|    total_timesteps    | 57500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.23      |\n",
      "|    explained_variance | 0.0026     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11499      |\n",
      "|    policy_loss        | 4.21       |\n",
      "|    reward             | -0.3448758 |\n",
      "|    std                | 2.25       |\n",
      "|    value_loss         | 6.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 11600      |\n",
      "|    time_elapsed       | 108        |\n",
      "|    total_timesteps    | 58000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.23      |\n",
      "|    explained_variance | -0.000381  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11599      |\n",
      "|    policy_loss        | -66.2      |\n",
      "|    reward             | -11.052695 |\n",
      "|    std                | 2.25       |\n",
      "|    value_loss         | 963        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 11700     |\n",
      "|    time_elapsed       | 109       |\n",
      "|    total_timesteps    | 58500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 0.00091   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11699     |\n",
      "|    policy_loss        | -15.5     |\n",
      "|    reward             | 16.751268 |\n",
      "|    std                | 2.23      |\n",
      "|    value_loss         | 381       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 11800       |\n",
      "|    time_elapsed       | 110         |\n",
      "|    total_timesteps    | 59000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11799       |\n",
      "|    policy_loss        | -0.347      |\n",
      "|    reward             | -0.22978084 |\n",
      "|    std                | 2.21        |\n",
      "|    value_loss         | 0.0286      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 534         |\n",
      "|    iterations         | 11900       |\n",
      "|    time_elapsed       | 111         |\n",
      "|    total_timesteps    | 59500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11899       |\n",
      "|    policy_loss        | -11.9       |\n",
      "|    reward             | -0.31172842 |\n",
      "|    std                | 2.2         |\n",
      "|    value_loss         | 34.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 535         |\n",
      "|    iterations         | 12000       |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 60000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11999       |\n",
      "|    policy_loss        | 2.56        |\n",
      "|    reward             | -0.42632082 |\n",
      "|    std                | 2.2         |\n",
      "|    value_loss         | 2.72        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -3.94     |\n",
      "|    reward             | -0.511118 |\n",
      "|    std                | 2.19      |\n",
      "|    value_loss         | 8.02      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 12200      |\n",
      "|    time_elapsed       | 114        |\n",
      "|    total_timesteps    | 61000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | -0.0196    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12199      |\n",
      "|    policy_loss        | 2.85       |\n",
      "|    reward             | -0.7259802 |\n",
      "|    std                | 2.21       |\n",
      "|    value_loss         | 13.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 114       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | -0.00884  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | 66.9      |\n",
      "|    reward             | 25.745317 |\n",
      "|    std                | 2.21      |\n",
      "|    value_loss         | 882       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 12400      |\n",
      "|    time_elapsed       | 115        |\n",
      "|    total_timesteps    | 62000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | -5.84e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12399      |\n",
      "|    policy_loss        | 39.1       |\n",
      "|    reward             | -24.354582 |\n",
      "|    std                | 2.2        |\n",
      "|    value_loss         | 918        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 535          |\n",
      "|    iterations         | 12500        |\n",
      "|    time_elapsed       | 116          |\n",
      "|    total_timesteps    | 62500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.2         |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 12499        |\n",
      "|    policy_loss        | 0.835        |\n",
      "|    reward             | -0.011312295 |\n",
      "|    std                | 2.19         |\n",
      "|    value_loss         | 0.304        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 12600      |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 63000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0.0193     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12599      |\n",
      "|    policy_loss        | 1.41       |\n",
      "|    reward             | -2.2202866 |\n",
      "|    std                | 2.17       |\n",
      "|    value_loss         | 3.25       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 535         |\n",
      "|    iterations         | 12700       |\n",
      "|    time_elapsed       | 118         |\n",
      "|    total_timesteps    | 63500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.19       |\n",
      "|    explained_variance | 0.000907    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 12699       |\n",
      "|    policy_loss        | 9.17        |\n",
      "|    reward             | -0.91325057 |\n",
      "|    std                | 2.17        |\n",
      "|    value_loss         | 23.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 12800      |\n",
      "|    time_elapsed       | 119        |\n",
      "|    total_timesteps    | 64000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0.00155    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 12799      |\n",
      "|    policy_loss        | -1.62      |\n",
      "|    reward             | -1.2770653 |\n",
      "|    std                | 2.19       |\n",
      "|    value_loss         | 1.99       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 536      |\n",
      "|    iterations         | 12900    |\n",
      "|    time_elapsed       | 120      |\n",
      "|    total_timesteps    | 64500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0.000714 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12899    |\n",
      "|    policy_loss        | -8.94    |\n",
      "|    reward             | 3.802567 |\n",
      "|    std                | 2.2      |\n",
      "|    value_loss         | 27.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 13000     |\n",
      "|    time_elapsed       | 121       |\n",
      "|    total_timesteps    | 65000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | -0.000763 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12999     |\n",
      "|    policy_loss        | 8.51      |\n",
      "|    reward             | 36.502247 |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 466       |\n",
      "-------------------------------------\n",
      "day: 3272, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9765880.41\n",
      "total_reward: 8765880.41\n",
      "total_cost: 13190.11\n",
      "total_trades: 3264\n",
      "Sharpe: 0.802\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 535          |\n",
      "|    iterations         | 13100        |\n",
      "|    time_elapsed       | 122          |\n",
      "|    total_timesteps    | 65500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.21        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13099        |\n",
      "|    policy_loss        | -0.107       |\n",
      "|    reward             | 0.0056968792 |\n",
      "|    std                | 2.21         |\n",
      "|    value_loss         | 0.00318      |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 13200     |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 66000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.22     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13199     |\n",
      "|    policy_loss        | 0.428     |\n",
      "|    reward             | 0.9747025 |\n",
      "|    std                | 2.22      |\n",
      "|    value_loss         | 1.01      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 13300      |\n",
      "|    time_elapsed       | 124        |\n",
      "|    total_timesteps    | 66500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | -0.000202  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13299      |\n",
      "|    policy_loss        | 11.2       |\n",
      "|    reward             | -1.4990813 |\n",
      "|    std                | 2.21       |\n",
      "|    value_loss         | 17.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 13400     |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 67000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13399     |\n",
      "|    policy_loss        | -2.22     |\n",
      "|    reward             | 2.708782  |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 4.32      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 536       |\n",
      "|    iterations         | 13500     |\n",
      "|    time_elapsed       | 125       |\n",
      "|    total_timesteps    | 67500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | 0.000839  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13499     |\n",
      "|    policy_loss        | -27.6     |\n",
      "|    reward             | 3.5818355 |\n",
      "|    std                | 2.17      |\n",
      "|    value_loss         | 244       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 536      |\n",
      "|    iterations         | 13600    |\n",
      "|    time_elapsed       | 126      |\n",
      "|    total_timesteps    | 68000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0.000746 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13599    |\n",
      "|    policy_loss        | 25.4     |\n",
      "|    reward             | 7.298796 |\n",
      "|    std                | 2.17     |\n",
      "|    value_loss         | 212      |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 536      |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 127      |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0.00034  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | -84.1    |\n",
      "|    reward             | 34.99013 |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 2.86e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 13800      |\n",
      "|    time_elapsed       | 128        |\n",
      "|    total_timesteps    | 69000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13799      |\n",
      "|    policy_loss        | -0.812     |\n",
      "|    reward             | 0.31878006 |\n",
      "|    std                | 2.19       |\n",
      "|    value_loss         | 0.181      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 13900     |\n",
      "|    time_elapsed       | 129       |\n",
      "|    total_timesteps    | 69500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13899     |\n",
      "|    policy_loss        | -10.5     |\n",
      "|    reward             | 1.1137024 |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 28.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 534        |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 130        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0.00237    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | 13.6       |\n",
      "|    reward             | -5.3559384 |\n",
      "|    std                | 2.18       |\n",
      "|    value_loss         | 47.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 534       |\n",
      "|    iterations         | 14100     |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 70500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14099     |\n",
      "|    policy_loss        | -2.96     |\n",
      "|    reward             | 0.2250101 |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 3.84      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 14200      |\n",
      "|    time_elapsed       | 132        |\n",
      "|    total_timesteps    | 71000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14199      |\n",
      "|    policy_loss        | 9.57       |\n",
      "|    reward             | -0.6398093 |\n",
      "|    std                | 2.17       |\n",
      "|    value_loss         | 81.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 14300    |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 71500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.2     |\n",
      "|    explained_variance | 0.00384  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 14299    |\n",
      "|    policy_loss        | 75.2     |\n",
      "|    reward             | -9.10227 |\n",
      "|    std                | 2.18     |\n",
      "|    value_loss         | 958      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 14400     |\n",
      "|    time_elapsed       | 134       |\n",
      "|    total_timesteps    | 72000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.2      |\n",
      "|    explained_variance | -0.000136 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14399     |\n",
      "|    policy_loss        | -133      |\n",
      "|    reward             | -2.970998 |\n",
      "|    std                | 2.18      |\n",
      "|    value_loss         | 4.56e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 14500      |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 72500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14499      |\n",
      "|    policy_loss        | 0.724      |\n",
      "|    reward             | 0.31056228 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 0.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 14600      |\n",
      "|    time_elapsed       | 136        |\n",
      "|    total_timesteps    | 73000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14599      |\n",
      "|    policy_loss        | -2.89      |\n",
      "|    reward             | -1.5443113 |\n",
      "|    std                | 2.16       |\n",
      "|    value_loss         | 3.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 14700     |\n",
      "|    time_elapsed       | 137       |\n",
      "|    total_timesteps    | 73500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0.00123   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14699     |\n",
      "|    policy_loss        | -9.29     |\n",
      "|    reward             | -4.751173 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 17.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 14800      |\n",
      "|    time_elapsed       | 138        |\n",
      "|    total_timesteps    | 74000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0.0439     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14799      |\n",
      "|    policy_loss        | 3.14       |\n",
      "|    reward             | -2.3638856 |\n",
      "|    std                | 2.12       |\n",
      "|    value_loss         | 16.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 14900      |\n",
      "|    time_elapsed       | 139        |\n",
      "|    total_timesteps    | 74500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | -0.00126   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14899      |\n",
      "|    policy_loss        | -16.8      |\n",
      "|    reward             | -1.1480147 |\n",
      "|    std                | 2.1        |\n",
      "|    value_loss         | 53.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0.0143    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | 21.9      |\n",
      "|    reward             | 3.0736752 |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 237       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 535         |\n",
      "|    iterations         | 15100       |\n",
      "|    time_elapsed       | 141         |\n",
      "|    total_timesteps    | 75500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.16       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15099       |\n",
      "|    policy_loss        | -0.094      |\n",
      "|    reward             | -0.07283341 |\n",
      "|    std                | 2.09        |\n",
      "|    value_loss         | 0.0145      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 535         |\n",
      "|    iterations         | 15200       |\n",
      "|    time_elapsed       | 141         |\n",
      "|    total_timesteps    | 76000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.15       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15199       |\n",
      "|    policy_loss        | -5.44       |\n",
      "|    reward             | 0.015214419 |\n",
      "|    std                | 2.08        |\n",
      "|    value_loss         | 11.2        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 535      |\n",
      "|    iterations         | 15300    |\n",
      "|    time_elapsed       | 142      |\n",
      "|    total_timesteps    | 76500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | -0.00624 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15299    |\n",
      "|    policy_loss        | 4.68     |\n",
      "|    reward             | 0.554285 |\n",
      "|    std                | 2.07     |\n",
      "|    value_loss         | 6.73     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 15400      |\n",
      "|    time_elapsed       | 143        |\n",
      "|    total_timesteps    | 77000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.15      |\n",
      "|    explained_variance | -0.0771    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15399      |\n",
      "|    policy_loss        | -7.07      |\n",
      "|    reward             | 0.18485712 |\n",
      "|    std                | 2.07       |\n",
      "|    value_loss         | 15.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 15500     |\n",
      "|    time_elapsed       | 144       |\n",
      "|    total_timesteps    | 77500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0.00134   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15499     |\n",
      "|    policy_loss        | 3.83      |\n",
      "|    reward             | -7.721907 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 23.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 15600     |\n",
      "|    time_elapsed       | 145       |\n",
      "|    total_timesteps    | 78000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | -0.00316  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15599     |\n",
      "|    policy_loss        | -31.3     |\n",
      "|    reward             | -5.579743 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 574       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 536       |\n",
      "|    iterations         | 15700     |\n",
      "|    time_elapsed       | 146       |\n",
      "|    total_timesteps    | 78500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0.00106   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15699     |\n",
      "|    policy_loss        | -56.4     |\n",
      "|    reward             | 0.8737062 |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 1.59e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 15800      |\n",
      "|    time_elapsed       | 147        |\n",
      "|    total_timesteps    | 79000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15799      |\n",
      "|    policy_loss        | -1.49      |\n",
      "|    reward             | 0.25197348 |\n",
      "|    std                | 2.09       |\n",
      "|    value_loss         | 2.24       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 148       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | 1.43      |\n",
      "|    reward             | 1.4278294 |\n",
      "|    std                | 2.11      |\n",
      "|    value_loss         | 1.66      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 16000     |\n",
      "|    time_elapsed       | 149       |\n",
      "|    total_timesteps    | 80000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15999     |\n",
      "|    policy_loss        | -9.79     |\n",
      "|    reward             | 0.7369266 |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 22.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 16100      |\n",
      "|    time_elapsed       | 150        |\n",
      "|    total_timesteps    | 80500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16099      |\n",
      "|    policy_loss        | -23.4      |\n",
      "|    reward             | -1.8200355 |\n",
      "|    std                | 2.11       |\n",
      "|    value_loss         | 109        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 535       |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | 4.7       |\n",
      "|    reward             | 6.6148276 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 20.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 152        |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 3.98e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | -84.5      |\n",
      "|    reward             | -25.046635 |\n",
      "|    std                | 2.09       |\n",
      "|    value_loss         | 1.64e+03   |\n",
      "--------------------------------------\n",
      "day: 3272, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10113568.44\n",
      "total_reward: 9113568.44\n",
      "total_cost: 3634.01\n",
      "total_trades: 3269\n",
      "Sharpe: 0.807\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 536         |\n",
      "|    iterations         | 16400       |\n",
      "|    time_elapsed       | 152         |\n",
      "|    total_timesteps    | 82000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.16       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16399       |\n",
      "|    policy_loss        | 0.963       |\n",
      "|    reward             | 0.053858783 |\n",
      "|    std                | 2.09        |\n",
      "|    value_loss         | 0.218       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 536       |\n",
      "|    iterations         | 16500     |\n",
      "|    time_elapsed       | 153       |\n",
      "|    total_timesteps    | 82500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | -5.64     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | 4.07      |\n",
      "|    reward             | 1.1921078 |\n",
      "|    std                | 2.11      |\n",
      "|    value_loss         | 11.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 16600      |\n",
      "|    time_elapsed       | 154        |\n",
      "|    total_timesteps    | 83000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16599      |\n",
      "|    policy_loss        | 2.45       |\n",
      "|    reward             | -1.4877008 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 1.79       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 536         |\n",
      "|    iterations         | 16700       |\n",
      "|    time_elapsed       | 155         |\n",
      "|    total_timesteps    | 83500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16699       |\n",
      "|    policy_loss        | -4.52       |\n",
      "|    reward             | -0.22618812 |\n",
      "|    std                | 2.14        |\n",
      "|    value_loss         | 5.62        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 536        |\n",
      "|    iterations         | 16800      |\n",
      "|    time_elapsed       | 156        |\n",
      "|    total_timesteps    | 84000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16799      |\n",
      "|    policy_loss        | 18.9       |\n",
      "|    reward             | 0.85609615 |\n",
      "|    std                | 2.13       |\n",
      "|    value_loss         | 60         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 157       |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | 32.6      |\n",
      "|    reward             | 1.1581156 |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 323       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 17000    |\n",
      "|    time_elapsed       | 158      |\n",
      "|    total_timesteps    | 85000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16999    |\n",
      "|    policy_loss        | 15.6     |\n",
      "|    reward             | 8.810627 |\n",
      "|    std                | 2.11     |\n",
      "|    value_loss         | 498      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | 0.24      |\n",
      "|    reward             | -2.249113 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 0.329     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 17200     |\n",
      "|    time_elapsed       | 160       |\n",
      "|    total_timesteps    | 86000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17199     |\n",
      "|    policy_loss        | 5.97      |\n",
      "|    reward             | 1.7274326 |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 11.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 537        |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 161        |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | -14.4      |\n",
      "|    reward             | -1.1306725 |\n",
      "|    std                | 2.1        |\n",
      "|    value_loss         | 85.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 536       |\n",
      "|    iterations         | 17400     |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 87000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17399     |\n",
      "|    policy_loss        | 7.21      |\n",
      "|    reward             | 1.2327319 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 22.8      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 17500    |\n",
      "|    time_elapsed       | 162      |\n",
      "|    total_timesteps    | 87500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.15    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17499    |\n",
      "|    policy_loss        | 11.5     |\n",
      "|    reward             | 9.054368 |\n",
      "|    std                | 2.08     |\n",
      "|    value_loss         | 34.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 17600     |\n",
      "|    time_elapsed       | 163       |\n",
      "|    total_timesteps    | 88000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | -0.000588 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17599     |\n",
      "|    policy_loss        | 70.3      |\n",
      "|    reward             | -5.338297 |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 1.05e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 537         |\n",
      "|    iterations         | 17700       |\n",
      "|    time_elapsed       | 164         |\n",
      "|    total_timesteps    | 88500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.15       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17699       |\n",
      "|    policy_loss        | -1.26       |\n",
      "|    reward             | 0.053863764 |\n",
      "|    std                | 2.08        |\n",
      "|    value_loss         | 0.334       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 17800     |\n",
      "|    time_elapsed       | 165       |\n",
      "|    total_timesteps    | 89000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17799     |\n",
      "|    policy_loss        | -5.3      |\n",
      "|    reward             | 1.3835981 |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 5.62      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | -4.63     |\n",
      "|    reward             | 1.7037486 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 8.58      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 18000     |\n",
      "|    time_elapsed       | 167       |\n",
      "|    total_timesteps    | 90000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17999     |\n",
      "|    policy_loss        | -3.74     |\n",
      "|    reward             | 1.0242585 |\n",
      "|    std                | 2.1       |\n",
      "|    value_loss         | 8.33      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 18100     |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 90500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.15     |\n",
      "|    explained_variance | 0.00111   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18099     |\n",
      "|    policy_loss        | -5.55     |\n",
      "|    reward             | -5.878601 |\n",
      "|    std                | 2.09      |\n",
      "|    value_loss         | 8.91      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 537      |\n",
      "|    iterations         | 18200    |\n",
      "|    time_elapsed       | 169      |\n",
      "|    total_timesteps    | 91000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18199    |\n",
      "|    policy_loss        | 35.6     |\n",
      "|    reward             | 8.29511  |\n",
      "|    std                | 2.1      |\n",
      "|    value_loss         | 296      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 170       |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | 75.2      |\n",
      "|    reward             | -6.014164 |\n",
      "|    std                | 2.12      |\n",
      "|    value_loss         | 1.97e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 18400       |\n",
      "|    time_elapsed       | 170         |\n",
      "|    total_timesteps    | 92000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18399       |\n",
      "|    policy_loss        | 0.0151      |\n",
      "|    reward             | 0.047099713 |\n",
      "|    std                | 2.14        |\n",
      "|    value_loss         | 0.0855      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 18500     |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 92500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.19     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | 1.5       |\n",
      "|    reward             | 1.1819446 |\n",
      "|    std                | 2.16      |\n",
      "|    value_loss         | 1.51      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 537        |\n",
      "|    iterations         | 18600      |\n",
      "|    time_elapsed       | 172        |\n",
      "|    total_timesteps    | 93000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18599      |\n",
      "|    policy_loss        | 9.04       |\n",
      "|    reward             | -5.0455365 |\n",
      "|    std                | 2.14       |\n",
      "|    value_loss         | 21.7       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 18700    |\n",
      "|    time_elapsed       | 173      |\n",
      "|    total_timesteps    | 93500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 18699    |\n",
      "|    policy_loss        | 10.5     |\n",
      "|    reward             | 4.103565 |\n",
      "|    std                | 2.15     |\n",
      "|    value_loss         | 49.8     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 18800     |\n",
      "|    time_elapsed       | 174       |\n",
      "|    total_timesteps    | 94000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18799     |\n",
      "|    policy_loss        | -23.2     |\n",
      "|    reward             | 4.1803308 |\n",
      "|    std                | 2.14      |\n",
      "|    value_loss         | 160       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 537        |\n",
      "|    iterations         | 18900      |\n",
      "|    time_elapsed       | 175        |\n",
      "|    total_timesteps    | 94500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.18      |\n",
      "|    explained_variance | 0.000397   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18899      |\n",
      "|    policy_loss        | -16        |\n",
      "|    reward             | -25.336594 |\n",
      "|    std                | 2.15       |\n",
      "|    value_loss         | 149        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 537          |\n",
      "|    iterations         | 19000        |\n",
      "|    time_elapsed       | 176          |\n",
      "|    total_timesteps    | 95000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.17        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 18999        |\n",
      "|    policy_loss        | -0.169       |\n",
      "|    reward             | -0.110389516 |\n",
      "|    std                | 2.12         |\n",
      "|    value_loss         | 0.0148       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 19100       |\n",
      "|    time_elapsed       | 177         |\n",
      "|    total_timesteps    | 95500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.18       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19099       |\n",
      "|    policy_loss        | -3.44       |\n",
      "|    reward             | -0.20503277 |\n",
      "|    std                | 2.14        |\n",
      "|    value_loss         | 4.24        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 19200    |\n",
      "|    time_elapsed       | 178      |\n",
      "|    total_timesteps    | 96000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.18    |\n",
      "|    explained_variance | -0.583   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 19199    |\n",
      "|    policy_loss        | 3.38     |\n",
      "|    reward             | 7.817431 |\n",
      "|    std                | 2.15     |\n",
      "|    value_loss         | 4.13     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 19300       |\n",
      "|    time_elapsed       | 179         |\n",
      "|    total_timesteps    | 96500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.2        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19299       |\n",
      "|    policy_loss        | 2.57        |\n",
      "|    reward             | -0.89431465 |\n",
      "|    std                | 2.18        |\n",
      "|    value_loss         | 3.44        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 19400      |\n",
      "|    time_elapsed       | 180        |\n",
      "|    total_timesteps    | 97000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19399      |\n",
      "|    policy_loss        | -5.52      |\n",
      "|    reward             | 0.96064943 |\n",
      "|    std                | 2.18       |\n",
      "|    value_loss         | 18.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 19500     |\n",
      "|    time_elapsed       | 181       |\n",
      "|    total_timesteps    | 97500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | -41.2     |\n",
      "|    reward             | 11.112439 |\n",
      "|    std                | 2.2       |\n",
      "|    value_loss         | 438       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 19600      |\n",
      "|    time_elapsed       | 181        |\n",
      "|    total_timesteps    | 98000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.2       |\n",
      "|    explained_variance | -0.000117  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19599      |\n",
      "|    policy_loss        | 89.4       |\n",
      "|    reward             | -2.0363097 |\n",
      "|    std                | 2.18       |\n",
      "|    value_loss         | 2.22e+03   |\n",
      "--------------------------------------\n",
      "day: 3272, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8527832.10\n",
      "total_reward: 7527832.10\n",
      "total_cost: 5391.04\n",
      "total_trades: 3264\n",
      "Sharpe: 0.788\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 19700       |\n",
      "|    time_elapsed       | 182         |\n",
      "|    total_timesteps    | 98500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.21       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19699       |\n",
      "|    policy_loss        | 0.153       |\n",
      "|    reward             | -0.05653579 |\n",
      "|    std                | 2.21        |\n",
      "|    value_loss         | 0.241       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 19800      |\n",
      "|    time_elapsed       | 183        |\n",
      "|    total_timesteps    | 99000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19799      |\n",
      "|    policy_loss        | 6.08       |\n",
      "|    reward             | 0.19445182 |\n",
      "|    std                | 2.21       |\n",
      "|    value_loss         | 5.82       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 19900      |\n",
      "|    time_elapsed       | 184        |\n",
      "|    total_timesteps    | 99500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.21      |\n",
      "|    explained_variance | 0.0664     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19899      |\n",
      "|    policy_loss        | -5.2       |\n",
      "|    reward             | -1.3151703 |\n",
      "|    std                | 2.2        |\n",
      "|    value_loss         | 14.1       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 20000       |\n",
      "|    time_elapsed       | 185         |\n",
      "|    total_timesteps    | 100000      |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.2        |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 19999       |\n",
      "|    policy_loss        | 0.244       |\n",
      "|    reward             | -0.32541677 |\n",
      "|    std                | 2.2         |\n",
      "|    value_loss         | 1.9         |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2023-01-04 to  2023-04-05\n",
      "A2C Sharpe Ratio:  0.4731324444905287\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_630_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 1067       |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 1          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.13184011 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 909          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026560915 |\n",
      "|    clip_fraction        | 0.0196       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0799       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    reward               | -0.004430929 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 0.182        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 896         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002871775 |\n",
      "|    clip_fraction        | 0.0107      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.127       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.000558   |\n",
      "|    reward               | 0.0         |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 0.257       |\n",
      "-----------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 902            |\n",
      "|    iterations           | 4              |\n",
      "|    time_elapsed         | 9              |\n",
      "|    total_timesteps      | 8192           |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.00369974     |\n",
      "|    clip_fraction        | 0.0227         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.41          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.00632       |\n",
      "|    n_updates            | 30             |\n",
      "|    policy_gradient_loss | -0.0011        |\n",
      "|    reward               | -0.00021207474 |\n",
      "|    std                  | 0.994          |\n",
      "|    value_loss           | 0.00658        |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 890          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012937669 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.011        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 0.000146     |\n",
      "|    reward               | -0.004040578 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.0491       |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 889            |\n",
      "|    iterations           | 6              |\n",
      "|    time_elapsed         | 13             |\n",
      "|    total_timesteps      | 12288          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.0032393276   |\n",
      "|    clip_fraction        | 0.0205         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.42          |\n",
      "|    explained_variance   | 0              |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | -0.00387       |\n",
      "|    n_updates            | 50             |\n",
      "|    policy_gradient_loss | -0.00115       |\n",
      "|    reward               | -0.00026962068 |\n",
      "|    std                  | 0.995          |\n",
      "|    value_loss           | 0.0194         |\n",
      "--------------------------------------------\n",
      "day: 3272, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1024297.19\n",
      "total_reward: 24297.19\n",
      "total_cost: 8258.60\n",
      "total_trades: 2877\n",
      "Sharpe: 0.263\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 881           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 16            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.004023847   |\n",
      "|    clip_fraction        | 0.019         |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | -0.0151       |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.00137      |\n",
      "|    reward               | -0.0061615156 |\n",
      "|    std                  | 0.995         |\n",
      "|    value_loss           | 0.00116       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003760538  |\n",
      "|    clip_fraction        | 0.0106       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0112       |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.000405    |\n",
      "|    reward               | 0.0018864054 |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 0.033        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 882          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004681665  |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0106      |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00345     |\n",
      "|    reward               | -0.017883234 |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 0.0123       |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006196455  |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.00444     |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00494     |\n",
      "|    reward               | -0.061087728 |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 0.00192      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 883         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002698977 |\n",
      "|    clip_fraction        | 0.00176     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.462       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000635   |\n",
      "|    reward               | 3.5368183   |\n",
      "|    std                  | 0.977       |\n",
      "|    value_loss           | 0.913       |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033347863 |\n",
      "|    clip_fraction        | 0.0105       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | -0.000139    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 15.9         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00122     |\n",
      "|    reward               | 0.76996154   |\n",
      "|    std                  | 0.979        |\n",
      "|    value_loss           | 33.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003261324 |\n",
      "|    clip_fraction        | 0.000488     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0277       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 31.7         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00041     |\n",
      "|    reward               | -0.19218901  |\n",
      "|    std                  | 0.978        |\n",
      "|    value_loss           | 62.3         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 888           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 32            |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00083965843 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | -0.00143      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 196           |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.000746     |\n",
      "|    reward               | -2.0285072    |\n",
      "|    std                  | 0.978         |\n",
      "|    value_loss           | 325           |\n",
      "-------------------------------------------\n",
      "day: 3272, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3778204.00\n",
      "total_reward: 2778204.00\n",
      "total_cost: 9799.41\n",
      "total_trades: 3244\n",
      "Sharpe: 0.637\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002478784 |\n",
      "|    clip_fraction        | 0.0121      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.000163    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.62        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00048    |\n",
      "|    reward               | 0.022565542 |\n",
      "|    std                  | 0.975       |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 881          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.001907858  |\n",
      "|    clip_fraction        | 0.00186      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.0189       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 92.1         |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00121     |\n",
      "|    reward               | 0.0016822433 |\n",
      "|    std                  | 0.976        |\n",
      "|    value_loss           | 177          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.005077464  |\n",
      "|    clip_fraction        | 0.0148       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.0164       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 38.2         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000366    |\n",
      "|    reward               | -0.011934915 |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 74.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 883          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038223586 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | -0.0233      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    reward               | -0.064856164 |\n",
      "|    std                  | 0.959        |\n",
      "|    value_loss           | 0.0188       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 880         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005507023 |\n",
      "|    clip_fraction        | 0.0323      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00142    |\n",
      "|    reward               | 0.060793556 |\n",
      "|    std                  | 0.953       |\n",
      "|    value_loss           | 2.93        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 872        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 46         |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00509858 |\n",
      "|    clip_fraction        | 0.0337     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 0.0634     |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.00272   |\n",
      "|    reward               | 0.15174678 |\n",
      "|    std                  | 0.945      |\n",
      "|    value_loss           | 0.12       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 872          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023096849 |\n",
      "|    clip_fraction        | 0.0149       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.272        |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.000675    |\n",
      "|    reward               | 0.026365101  |\n",
      "|    std                  | 0.955        |\n",
      "|    value_loss           | 0.646        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016300679 |\n",
      "|    clip_fraction        | 0.00278      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.0387       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 16.5         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000901    |\n",
      "|    reward               | -0.3972021   |\n",
      "|    std                  | 0.957        |\n",
      "|    value_loss           | 33.5         |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3430142.54\n",
      "total_reward: 2430142.54\n",
      "total_cost: 10045.57\n",
      "total_trades: 3241\n",
      "Sharpe: 0.606\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 873          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035642805 |\n",
      "|    clip_fraction        | 0.00874      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.041        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.59         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.000813    |\n",
      "|    reward               | -0.5101829   |\n",
      "|    std                  | 0.947        |\n",
      "|    value_loss           | 8.66         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 873         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002909704 |\n",
      "|    clip_fraction        | 0.0117      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.0197     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 72.7        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00223    |\n",
      "|    reward               | 0.011837551 |\n",
      "|    std                  | 0.947       |\n",
      "|    value_loss           | 144         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016625178 |\n",
      "|    clip_fraction        | 0.000977     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.00437     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 143          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    reward               | -1.5566695   |\n",
      "|    std                  | 0.946        |\n",
      "|    value_loss           | 395          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 60           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024036397 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.244        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.66         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00385     |\n",
      "|    reward               | 0.17528906   |\n",
      "|    std                  | 0.941        |\n",
      "|    value_loss           | 7.56         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033702417 |\n",
      "|    clip_fraction        | 0.00757      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | -0.000384    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 202          |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    reward               | 7.3981357    |\n",
      "|    std                  | 0.941        |\n",
      "|    value_loss           | 393          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009822977 |\n",
      "|    clip_fraction        | 0.000928     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.0621       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 127          |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0015      |\n",
      "|    reward               | 0.19338597   |\n",
      "|    std                  | 0.941        |\n",
      "|    value_loss           | 266          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 877           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013484168 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | -0.0447       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 218           |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.000322     |\n",
      "|    reward               | -0.38907415   |\n",
      "|    std                  | 0.941         |\n",
      "|    value_loss           | 330           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 876           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 70            |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00073861214 |\n",
      "|    clip_fraction        | 0.000342      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | 0.0256        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 448           |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -0.000667     |\n",
      "|    reward               | 4.995312      |\n",
      "|    std                  | 0.942         |\n",
      "|    value_loss           | 666           |\n",
      "-------------------------------------------\n",
      "day: 3272, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6572228.91\n",
      "total_reward: 5572228.91\n",
      "total_cost: 9324.62\n",
      "total_trades: 3249\n",
      "Sharpe: 0.743\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018924109 |\n",
      "|    clip_fraction        | 0.00322      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.175        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 32.5         |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000793    |\n",
      "|    reward               | 0.18513215   |\n",
      "|    std                  | 0.942        |\n",
      "|    value_loss           | 67.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011607124 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.0526       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 313          |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    reward               | 0.03793976   |\n",
      "|    std                  | 0.939        |\n",
      "|    value_loss           | 576          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 875          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030166772 |\n",
      "|    clip_fraction        | 0.00674      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.0212       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 294          |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000926    |\n",
      "|    reward               | 1.4084781    |\n",
      "|    std                  | 0.94         |\n",
      "|    value_loss           | 719          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 876         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003057454 |\n",
      "|    clip_fraction        | 0.0214      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.37        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    reward               | -0.09147331 |\n",
      "|    std                  | 0.936       |\n",
      "|    value_loss           | 19.3        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 876          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010634125 |\n",
      "|    clip_fraction        | 0.00767      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.0716       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 256          |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.000707    |\n",
      "|    reward               | -15.956718   |\n",
      "|    std                  | 0.935        |\n",
      "|    value_loss           | 701          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001256855 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.14         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 198          |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -8.65e-05    |\n",
      "|    reward               | -1.558866    |\n",
      "|    std                  | 0.934        |\n",
      "|    value_loss           | 369          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 877          |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 86           |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030299397 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | -0.027       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 165          |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    reward               | 0.026906747  |\n",
      "|    std                  | 0.932        |\n",
      "|    value_loss           | 450          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 88           |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011937191 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.0551       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 309          |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.000411    |\n",
      "|    reward               | -3.0923696   |\n",
      "|    std                  | 0.931        |\n",
      "|    value_loss           | 742          |\n",
      "------------------------------------------\n",
      "day: 3272, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6772558.76\n",
      "total_reward: 5772558.76\n",
      "total_cost: 9481.72\n",
      "total_trades: 3244\n",
      "Sharpe: 0.753\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 39           |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0078082774 |\n",
      "|    clip_fraction        | 0.0749       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.421        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 36.5         |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | -0.00462     |\n",
      "|    reward               | 1.2292303    |\n",
      "|    std                  | 0.935        |\n",
      "|    value_loss           | 71.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015016438 |\n",
      "|    clip_fraction        | 0.00986      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.103        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 403          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.0029      |\n",
      "|    reward               | -0.10180853  |\n",
      "|    std                  | 0.935        |\n",
      "|    value_loss           | 616          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 83968        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028926646 |\n",
      "|    clip_fraction        | 0.00889      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.0357       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 580          |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    reward               | -0.32137164  |\n",
      "|    std                  | 0.932        |\n",
      "|    value_loss           | 820          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 878         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003952152 |\n",
      "|    clip_fraction        | 0.0282      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.764       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11          |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00427    |\n",
      "|    reward               | -1.3583206  |\n",
      "|    std                  | 0.927       |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 879           |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 100           |\n",
      "|    total_timesteps      | 88064         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090020103 |\n",
      "|    clip_fraction        | 0.000146      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0.136         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 390           |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -0.00038      |\n",
      "|    reward               | 15.687509     |\n",
      "|    std                  | 0.933         |\n",
      "|    value_loss           | 778           |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 102         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000877443 |\n",
      "|    clip_fraction        | 0.00171     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.256       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 213         |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.000647   |\n",
      "|    reward               | -1.3271377  |\n",
      "|    std                  | 0.933       |\n",
      "|    value_loss           | 329         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 878          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004591898 |\n",
      "|    clip_fraction        | 4.88e-05     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | -0.00536     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 250          |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.000624    |\n",
      "|    reward               | -0.3874808   |\n",
      "|    std                  | 0.935        |\n",
      "|    value_loss           | 396          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 879           |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 107           |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032453416 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0.116         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 367           |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -1.91e-05     |\n",
      "|    reward               | -36.56061     |\n",
      "|    std                  | 0.935         |\n",
      "|    value_loss           | 729           |\n",
      "-------------------------------------------\n",
      "day: 3272, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6904174.98\n",
      "total_reward: 5904174.98\n",
      "total_cost: 9170.21\n",
      "total_trades: 3250\n",
      "Sharpe: 0.755\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003584697 |\n",
      "|    clip_fraction        | 0.0246      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.472       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 44.8        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00269    |\n",
      "|    reward               | -2.9263892  |\n",
      "|    std                  | 0.934       |\n",
      "|    value_loss           | 100         |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 878           |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 111           |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029440946 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.35         |\n",
      "|    explained_variance   | 0.172         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 331           |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -1.52e-06     |\n",
      "|    reward               | 0.046143062   |\n",
      "|    std                  | 0.935         |\n",
      "|    value_loss           | 622           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 114          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030216416 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 288          |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00272     |\n",
      "|    reward               | 1.3234168    |\n",
      "|    std                  | 0.935        |\n",
      "|    value_loss           | 631          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2023-01-04 to  2023-04-05\n",
      "PPO Sharpe Ratio:  0.4847411378308728\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_630_3\n",
      "day: 3272, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 82       |\n",
      "|    total_timesteps | 13092    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 1.67e+03 |\n",
      "|    critic_loss     | 7.27     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 12991    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3272, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 166      |\n",
      "|    total_timesteps | 26184    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 898      |\n",
      "|    critic_loss     | 0.0294   |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 26083    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 249      |\n",
      "|    total_timesteps | 39276    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 482      |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 39175    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3272, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 333      |\n",
      "|    total_timesteps | 52368    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 247      |\n",
      "|    critic_loss     | 0.115    |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 52267    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3272, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 418      |\n",
      "|    total_timesteps | 65460    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 169      |\n",
      "|    critic_loss     | 3.92     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 65359    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3272, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 502      |\n",
      "|    total_timesteps | 78552    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.84    |\n",
      "|    critic_loss     | 9.98e-05 |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 78451    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3272, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 586      |\n",
      "|    total_timesteps | 91644    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.98    |\n",
      "|    critic_loss     | 0.0556   |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 91543    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2023-01-04 to  2023-04-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2023-04-05\n",
      "======Trading from:  2023-04-05 to  2023-07-07\n",
      "============================================\n",
      "turbulence_threshold:  14.172279090144063\n",
      "======Model training from:  2010-01-01 to  2023-04-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/a2c/a2c_693_3\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 551          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 0            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.5         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -0.0017      |\n",
      "|    reward             | 0.0014036126 |\n",
      "|    std                | 1.08         |\n",
      "|    value_loss         | 1.88e-06     |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 550       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.542    |\n",
      "|    reward             | 1.5231119 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 0.204     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 515        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | 0.00402    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -1.97      |\n",
      "|    reward             | -2.6749067 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 4.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 508        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0.0114     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.83      |\n",
      "|    reward             | -1.0677338 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 4.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 501       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -3.13     |\n",
      "|    reward             | -4.334444 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 9.04      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 511        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | -2.11e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 21.3       |\n",
      "|    reward             | -4.6956925 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 371        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 512         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.54       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -0.2        |\n",
      "|    reward             | 0.047827043 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.0133      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 509        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 2.87       |\n",
      "|    reward             | -1.3727583 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 1.98       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 510       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 2.74      |\n",
      "|    reward             | 2.1687455 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 2.84      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 511       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 7.95      |\n",
      "|    reward             | 0.4013527 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 17.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 508        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | 0.00038    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 21.4       |\n",
      "|    reward             | 0.23282763 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 241        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 503       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | -1.37e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -12.5     |\n",
      "|    reward             | 5.026329  |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 324       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 505       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 23.9      |\n",
      "|    reward             | 35.730263 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 709       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 505        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -0.481     |\n",
      "|    reward             | 0.52369857 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.0722     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 506        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.54      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -0.89      |\n",
      "|    reward             | -0.2546493 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 0.573      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 504       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -0.644    |\n",
      "|    reward             | 0.7072378 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 2.46      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 506         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.55       |\n",
      "|    explained_variance | 0.00655     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 2.38        |\n",
      "|    reward             | -0.50895023 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 7.08        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 507       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -2.61     |\n",
      "|    reward             | 0.5595041 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 29.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 511       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | -0.000655 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -2.31     |\n",
      "|    reward             | -9.96691  |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 235       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0.0032   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 21.9     |\n",
      "|    reward             | 8.930492 |\n",
      "|    std                | 1.15     |\n",
      "|    value_loss         | 635      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 510       |\n",
      "|    iterations         | 2100      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 10500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2099      |\n",
      "|    policy_loss        | -0.0968   |\n",
      "|    reward             | -0.978763 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 0.149     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 511        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 4.61       |\n",
      "|    reward             | -0.9327748 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 515       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -0.58     |\n",
      "|    reward             | -1.220333 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 1.52      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 516      |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 12000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 8.57     |\n",
      "|    reward             | 3.929602 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 47.7     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 515        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.57      |\n",
      "|    explained_variance | 5.22e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | 5.32       |\n",
      "|    reward             | -3.5854347 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 17.7       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 514      |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 13000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | -0.00165 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | 3.46     |\n",
      "|    reward             | 18.75083 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 114      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 515       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -0.232    |\n",
      "|    reward             | 0.1858355 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 0.0179    |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 517       |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | 2.32      |\n",
      "|    reward             | 0.5647636 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 7.71      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 517          |\n",
      "|    iterations         | 2900         |\n",
      "|    time_elapsed       | 28           |\n",
      "|    total_timesteps    | 14500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.55        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 2899         |\n",
      "|    policy_loss        | -8.81        |\n",
      "|    reward             | -0.016709808 |\n",
      "|    std                | 1.14         |\n",
      "|    value_loss         | 20.8         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 518        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | -8.13      |\n",
      "|    reward             | -2.7706797 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 24.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 517        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | -0.00965   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | -0.14      |\n",
      "|    reward             | -5.8856893 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 0.837      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 519        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0.000424   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | 1.03       |\n",
      "|    reward             | -32.974976 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 39         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 518       |\n",
      "|    iterations         | 3300      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 16500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | 0.000599  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3299      |\n",
      "|    policy_loss        | 30.9      |\n",
      "|    reward             | 29.696762 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 322       |\n",
      "-------------------------------------\n",
      "day: 3335, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 12615858.28\n",
      "total_reward: 11615858.28\n",
      "total_cost: 12441.23\n",
      "total_trades: 3324\n",
      "Sharpe: 0.879\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 517        |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -1.05      |\n",
      "|    reward             | -0.3965349 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 0.596      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 512       |\n",
      "|    iterations         | 3500      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 17500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | -0.00129  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3499      |\n",
      "|    policy_loss        | -6.3      |\n",
      "|    reward             | 0.1824945 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 37.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 511       |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | -0.000866 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -7.22     |\n",
      "|    reward             | 1.8269545 |\n",
      "|    std                | 1.14      |\n",
      "|    value_loss         | 21.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 510      |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 36       |\n",
      "|    total_timesteps    | 18500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -2.26    |\n",
      "|    reward             | 5.21434  |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 3.56     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 508       |\n",
      "|    iterations         | 3800      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 19000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3799      |\n",
      "|    policy_loss        | 11.5      |\n",
      "|    reward             | -4.274205 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 116       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 507        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 8.51e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 19.8       |\n",
      "|    reward             | -31.891485 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 424        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 507      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.55    |\n",
      "|    explained_variance | 0.000283 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 27.6     |\n",
      "|    reward             | 16.17993 |\n",
      "|    std                | 1.14     |\n",
      "|    value_loss         | 1.05e+03 |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 507        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -1.58      |\n",
      "|    reward             | 0.22277968 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 1.83       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 509       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | -0.000534 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | -0.988    |\n",
      "|    reward             | 2.385899  |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 2.38      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 509       |\n",
      "|    iterations         | 4300      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 21500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4299      |\n",
      "|    policy_loss        | -5.69     |\n",
      "|    reward             | 1.9455354 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 17        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 508        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.57      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 0.556      |\n",
      "|    reward             | -1.7092956 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 4.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 508        |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.57      |\n",
      "|    explained_variance | -6.93e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 13.6       |\n",
      "|    reward             | -3.4045787 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 104        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 505       |\n",
      "|    iterations         | 4600      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 23000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 0.00012   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4599      |\n",
      "|    policy_loss        | 3.8       |\n",
      "|    reward             | 2.7879584 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 108       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 504          |\n",
      "|    iterations         | 4700         |\n",
      "|    time_elapsed       | 46           |\n",
      "|    total_timesteps    | 23500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.57        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 4699         |\n",
      "|    policy_loss        | -0.26        |\n",
      "|    reward             | -0.050749436 |\n",
      "|    std                | 1.16         |\n",
      "|    value_loss         | 0.0192       |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 506       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -1.28     |\n",
      "|    reward             | 1.6926631 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 505      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.56    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -2.95    |\n",
      "|    reward             | 2.316494 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 9.14     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 505        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | -0.151     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 4.16       |\n",
      "|    reward             | -0.9333571 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 7.84       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 504        |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 7.35       |\n",
      "|    reward             | 0.98655754 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 44.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 506        |\n",
      "|    iterations         | 5200       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 26000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.53      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5199       |\n",
      "|    policy_loss        | 33.7       |\n",
      "|    reward             | -3.3136134 |\n",
      "|    std                | 1.12       |\n",
      "|    value_loss         | 528        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 506       |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | -1.42e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 9.41      |\n",
      "|    reward             | 26.07377  |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 354       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 505         |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.54       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | 0.137       |\n",
      "|    reward             | -0.09220426 |\n",
      "|    std                | 1.13        |\n",
      "|    value_loss         | 0.0308      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 506         |\n",
      "|    iterations         | 5500        |\n",
      "|    time_elapsed       | 54          |\n",
      "|    total_timesteps    | 27500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.53       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5499        |\n",
      "|    policy_loss        | 1.32        |\n",
      "|    reward             | -0.83376384 |\n",
      "|    std                | 1.12        |\n",
      "|    value_loss         | 1.52        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 507       |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.54     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | -0.784    |\n",
      "|    reward             | 1.2082736 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.43      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 507       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | -3.08     |\n",
      "|    reward             | 0.3015941 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.6       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 507       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 57        |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.53     |\n",
      "|    explained_variance | -0.0014   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | -2.04     |\n",
      "|    reward             | 1.0820223 |\n",
      "|    std                | 1.12      |\n",
      "|    value_loss         | 2.56      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 508        |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 58         |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.55      |\n",
      "|    explained_variance | 0.000192   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 25.2       |\n",
      "|    reward             | -27.758486 |\n",
      "|    std                | 1.13       |\n",
      "|    value_loss         | 574        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 509       |\n",
      "|    iterations         | 6000      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 30000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.55     |\n",
      "|    explained_variance | 0.000172  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5999      |\n",
      "|    policy_loss        | -23.3     |\n",
      "|    reward             | 3.3281775 |\n",
      "|    std                | 1.13      |\n",
      "|    value_loss         | 425       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 508         |\n",
      "|    iterations         | 6100        |\n",
      "|    time_elapsed       | 59          |\n",
      "|    total_timesteps    | 30500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.55       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6099        |\n",
      "|    policy_loss        | -1.44       |\n",
      "|    reward             | -0.38027075 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 1.21        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 509       |\n",
      "|    iterations         | 6200      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 31000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6199      |\n",
      "|    policy_loss        | -0.676    |\n",
      "|    reward             | -1.111267 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 0.733     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 510        |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | -0.952     |\n",
      "|    reward             | -0.5950112 |\n",
      "|    std                | 1.14       |\n",
      "|    value_loss         | 2.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 511       |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 62        |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | 27.9      |\n",
      "|    reward             | 0.8890796 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 210       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 511       |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 16.4      |\n",
      "|    reward             | 2.7371483 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 99.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 511       |\n",
      "|    iterations         | 6600      |\n",
      "|    time_elapsed       | 64        |\n",
      "|    total_timesteps    | 33000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | 0.000153  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6599      |\n",
      "|    policy_loss        | 20.9      |\n",
      "|    reward             | 24.689096 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 507       |\n",
      "-------------------------------------\n",
      "day: 3335, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11117390.43\n",
      "total_reward: 10117390.43\n",
      "total_cost: 12739.24\n",
      "total_trades: 3316\n",
      "Sharpe: 0.862\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 511          |\n",
      "|    iterations         | 6700         |\n",
      "|    time_elapsed       | 65           |\n",
      "|    total_timesteps    | 33500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.56        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 6699         |\n",
      "|    policy_loss        | -0.597       |\n",
      "|    reward             | -0.021546928 |\n",
      "|    std                | 1.15         |\n",
      "|    value_loss         | 0.0836       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 511         |\n",
      "|    iterations         | 6800        |\n",
      "|    time_elapsed       | 66          |\n",
      "|    total_timesteps    | 34000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6799        |\n",
      "|    policy_loss        | -0.745      |\n",
      "|    reward             | -0.10021184 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.855       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 512         |\n",
      "|    iterations         | 6900        |\n",
      "|    time_elapsed       | 67          |\n",
      "|    total_timesteps    | 34500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6899        |\n",
      "|    policy_loss        | 1.76        |\n",
      "|    reward             | -0.88938844 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 2.35        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 513        |\n",
      "|    iterations         | 7000       |\n",
      "|    time_elapsed       | 68         |\n",
      "|    total_timesteps    | 35000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6999       |\n",
      "|    policy_loss        | 3.52       |\n",
      "|    reward             | 0.49589044 |\n",
      "|    std                | 1.15       |\n",
      "|    value_loss         | 9.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 513       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 69        |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -2.26     |\n",
      "|    reward             | 2.4931617 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 10.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 513       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 0.000849  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | 38.4      |\n",
      "|    reward             | 5.3628955 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 560       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 514       |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 70        |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 0.000514  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | -102      |\n",
      "|    reward             | 24.404362 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 2.59e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 515         |\n",
      "|    iterations         | 7400        |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 37000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7399        |\n",
      "|    policy_loss        | -0.232      |\n",
      "|    reward             | -0.30791226 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.168       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 516      |\n",
      "|    iterations         | 7500     |\n",
      "|    time_elapsed       | 72       |\n",
      "|    total_timesteps    | 37500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.57    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7499     |\n",
      "|    policy_loss        | 0.584    |\n",
      "|    reward             | 2.232406 |\n",
      "|    std                | 1.16     |\n",
      "|    value_loss         | 0.772    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 516        |\n",
      "|    iterations         | 7600       |\n",
      "|    time_elapsed       | 73         |\n",
      "|    total_timesteps    | 38000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.56      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7599       |\n",
      "|    policy_loss        | -2.56      |\n",
      "|    reward             | -3.7086985 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 4.99       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 516         |\n",
      "|    iterations         | 7700        |\n",
      "|    time_elapsed       | 74          |\n",
      "|    total_timesteps    | 38500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7699        |\n",
      "|    policy_loss        | -0.0849     |\n",
      "|    reward             | -0.75887865 |\n",
      "|    std                | 1.16        |\n",
      "|    value_loss         | 0.472       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 516       |\n",
      "|    iterations         | 7800      |\n",
      "|    time_elapsed       | 75        |\n",
      "|    total_timesteps    | 39000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7799      |\n",
      "|    policy_loss        | 3         |\n",
      "|    reward             | 3.1894376 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 6.96      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 516        |\n",
      "|    iterations         | 7900       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 39500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.57      |\n",
      "|    explained_variance | 0.00991    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7899       |\n",
      "|    policy_loss        | -39.6      |\n",
      "|    reward             | -3.6632893 |\n",
      "|    std                | 1.16       |\n",
      "|    value_loss         | 973        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 517       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 77        |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | -0.00354  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -22.2     |\n",
      "|    reward             | -8.422494 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 267       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 518         |\n",
      "|    iterations         | 8100        |\n",
      "|    time_elapsed       | 78          |\n",
      "|    total_timesteps    | 40500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.55       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8099        |\n",
      "|    policy_loss        | -0.651      |\n",
      "|    reward             | -0.78893334 |\n",
      "|    std                | 1.14        |\n",
      "|    value_loss         | 0.213       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 518         |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 79          |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.56       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -0.266      |\n",
      "|    reward             | -0.18604325 |\n",
      "|    std                | 1.15        |\n",
      "|    value_loss         | 0.507       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 518       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.56     |\n",
      "|    explained_variance | -0.000814 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 0.602     |\n",
      "|    reward             | 2.3114767 |\n",
      "|    std                | 1.15      |\n",
      "|    value_loss         | 9.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 519       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 80        |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | -8.34e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | -6.78     |\n",
      "|    reward             | 1.4516097 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 25.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 520       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 81        |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | -0.000965 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 3.19      |\n",
      "|    reward             | 4.8308167 |\n",
      "|    std                | 1.16      |\n",
      "|    value_loss         | 29.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 520       |\n",
      "|    iterations         | 8600      |\n",
      "|    time_elapsed       | 82        |\n",
      "|    total_timesteps    | 43000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.58     |\n",
      "|    explained_variance | 0.000988  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8599      |\n",
      "|    policy_loss        | 8.29      |\n",
      "|    reward             | 10.439439 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 110       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 521        |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 83         |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.59      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | 0.282      |\n",
      "|    reward             | 0.03278158 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 0.0691     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 520        |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.59      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | -2.1       |\n",
      "|    reward             | -0.6900709 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.23       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 521        |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 85         |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.6       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | -1.04      |\n",
      "|    reward             | 0.93196833 |\n",
      "|    std                | 1.2        |\n",
      "|    value_loss         | 2.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 521       |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 86        |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.6      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | -3.33     |\n",
      "|    reward             | 2.7327378 |\n",
      "|    std                | 1.2       |\n",
      "|    value_loss         | 8.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 522        |\n",
      "|    iterations         | 9100       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 45500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.59      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9099       |\n",
      "|    policy_loss        | -7.57      |\n",
      "|    reward             | -0.9845416 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 29.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 522        |\n",
      "|    iterations         | 9200       |\n",
      "|    time_elapsed       | 88         |\n",
      "|    total_timesteps    | 46000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | 0.000349   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9199       |\n",
      "|    policy_loss        | -2.11      |\n",
      "|    reward             | 0.26107943 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 386        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 522        |\n",
      "|    iterations         | 9300       |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 46500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | -0.00789   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9299       |\n",
      "|    policy_loss        | -43.2      |\n",
      "|    reward             | -39.759254 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 625        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 522         |\n",
      "|    iterations         | 9400        |\n",
      "|    time_elapsed       | 89          |\n",
      "|    total_timesteps    | 47000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.58       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9399        |\n",
      "|    policy_loss        | 0.451       |\n",
      "|    reward             | 0.031591184 |\n",
      "|    std                | 1.18        |\n",
      "|    value_loss         | 0.202       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 522       |\n",
      "|    iterations         | 9500      |\n",
      "|    time_elapsed       | 90        |\n",
      "|    total_timesteps    | 47500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.59     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9499      |\n",
      "|    policy_loss        | -1.37     |\n",
      "|    reward             | 2.4753482 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 3.19      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 523        |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 91         |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | -0.12      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 3.17       |\n",
      "|    reward             | -3.3606312 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 6.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 524       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 92        |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.58     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 7.14      |\n",
      "|    reward             | 0.7363583 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 20.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 524       |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 93        |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.59     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | -6.41     |\n",
      "|    reward             | 0.9434867 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 31.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 524        |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 94         |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | 0.000308   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | 48.6       |\n",
      "|    reward             | -1.9699379 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 607        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 524       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.57     |\n",
      "|    explained_variance | 0.000454  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 6.58      |\n",
      "|    reward             | 22.308153 |\n",
      "|    std                | 1.17      |\n",
      "|    value_loss         | 141       |\n",
      "-------------------------------------\n",
      "day: 3335, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 12717819.58\n",
      "total_reward: 11717819.58\n",
      "total_cost: 12985.83\n",
      "total_trades: 3322\n",
      "Sharpe: 0.873\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 525         |\n",
      "|    iterations         | 10100       |\n",
      "|    time_elapsed       | 96          |\n",
      "|    total_timesteps    | 50500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.57       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10099       |\n",
      "|    policy_loss        | -2.88       |\n",
      "|    reward             | 0.026514927 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 5.17        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 526        |\n",
      "|    iterations         | 10200      |\n",
      "|    time_elapsed       | 96         |\n",
      "|    total_timesteps    | 51000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10199      |\n",
      "|    policy_loss        | 1.98       |\n",
      "|    reward             | -1.2938874 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 2.22       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 526         |\n",
      "|    iterations         | 10300       |\n",
      "|    time_elapsed       | 97          |\n",
      "|    total_timesteps    | 51500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.58       |\n",
      "|    explained_variance | -0.0149     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10299       |\n",
      "|    policy_loss        | -4.49       |\n",
      "|    reward             | -0.01713736 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 8.03        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 526      |\n",
      "|    iterations         | 10400    |\n",
      "|    time_elapsed       | 98       |\n",
      "|    total_timesteps    | 52000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.58    |\n",
      "|    explained_variance | -0.0059  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 10399    |\n",
      "|    policy_loss        | 2.96     |\n",
      "|    reward             | 5.103008 |\n",
      "|    std                | 1.18     |\n",
      "|    value_loss         | 3.29     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 526       |\n",
      "|    iterations         | 10500     |\n",
      "|    time_elapsed       | 99        |\n",
      "|    total_timesteps    | 52500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.59     |\n",
      "|    explained_variance | -0.00182  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10499     |\n",
      "|    policy_loss        | 21.2      |\n",
      "|    reward             | 11.166293 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 103       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 526       |\n",
      "|    iterations         | 10600     |\n",
      "|    time_elapsed       | 100       |\n",
      "|    total_timesteps    | 53000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.59     |\n",
      "|    explained_variance | 0.000277  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 10599     |\n",
      "|    policy_loss        | -28.4     |\n",
      "|    reward             | 6.5908384 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 572       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 527         |\n",
      "|    iterations         | 10700       |\n",
      "|    time_elapsed       | 101         |\n",
      "|    total_timesteps    | 53500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.58       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10699       |\n",
      "|    policy_loss        | -0.243      |\n",
      "|    reward             | -0.09503727 |\n",
      "|    std                | 1.17        |\n",
      "|    value_loss         | 0.0259      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 527        |\n",
      "|    iterations         | 10800      |\n",
      "|    time_elapsed       | 102        |\n",
      "|    total_timesteps    | 54000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10799      |\n",
      "|    policy_loss        | 1.97       |\n",
      "|    reward             | -1.5459688 |\n",
      "|    std                | 1.17       |\n",
      "|    value_loss         | 2.51       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 528         |\n",
      "|    iterations         | 10900       |\n",
      "|    time_elapsed       | 103         |\n",
      "|    total_timesteps    | 54500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.59       |\n",
      "|    explained_variance | 0.00477     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 10899       |\n",
      "|    policy_loss        | -5.32       |\n",
      "|    reward             | -0.13326053 |\n",
      "|    std                | 1.19        |\n",
      "|    value_loss         | 10.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 527        |\n",
      "|    iterations         | 11000      |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 55000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.59      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 10999      |\n",
      "|    policy_loss        | 1.31       |\n",
      "|    reward             | 0.71879095 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 528       |\n",
      "|    iterations         | 11100     |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 55500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.59     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11099     |\n",
      "|    policy_loss        | 9.45      |\n",
      "|    reward             | 1.9052234 |\n",
      "|    std                | 1.18      |\n",
      "|    value_loss         | 58.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 528       |\n",
      "|    iterations         | 11200     |\n",
      "|    time_elapsed       | 105       |\n",
      "|    total_timesteps    | 56000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.59     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 11199     |\n",
      "|    policy_loss        | 9.97      |\n",
      "|    reward             | 3.6535237 |\n",
      "|    std                | 1.19      |\n",
      "|    value_loss         | 39.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 528        |\n",
      "|    iterations         | 11300      |\n",
      "|    time_elapsed       | 106        |\n",
      "|    total_timesteps    | 56500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -1.58      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 11299      |\n",
      "|    policy_loss        | -19.9      |\n",
      "|    reward             | -0.9811912 |\n",
      "|    std                | 1.18       |\n",
      "|    value_loss         | 1.22e+03   |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 529          |\n",
      "|    iterations         | 11400        |\n",
      "|    time_elapsed       | 107          |\n",
      "|    total_timesteps    | 57000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -1.58        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 11399        |\n",
      "|    policy_loss        | 0.0518       |\n",
      "|    reward             | 0.0040289364 |\n",
      "|    std                | 1.18         |\n",
      "|    value_loss         | 0.00169      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 530           |\n",
      "|    iterations         | 11500         |\n",
      "|    time_elapsed       | 108           |\n",
      "|    total_timesteps    | 57500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.59         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 11499         |\n",
      "|    policy_loss        | -0.0267       |\n",
      "|    reward             | -0.0043078694 |\n",
      "|    std                | 1.18          |\n",
      "|    value_loss         | 0.000407      |\n",
      "-----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 529      |\n",
      "|    iterations         | 11600    |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 58000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.6     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11599    |\n",
      "|    policy_loss        | -0.00107 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.2      |\n",
      "|    value_loss         | 7.09e-06 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 529      |\n",
      "|    iterations         | 11700    |\n",
      "|    time_elapsed       | 110      |\n",
      "|    total_timesteps    | 58500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.62    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11699    |\n",
      "|    policy_loss        | -0.00128 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.22     |\n",
      "|    value_loss         | 9.71e-07 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 529            |\n",
      "|    iterations         | 11800          |\n",
      "|    time_elapsed       | 111            |\n",
      "|    total_timesteps    | 59000          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.65          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 11799          |\n",
      "|    policy_loss        | 0.00368        |\n",
      "|    reward             | -2.7352653e-05 |\n",
      "|    std                | 1.26           |\n",
      "|    value_loss         | 1.57e-05       |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 529         |\n",
      "|    iterations         | 11900       |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 59500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -1.66       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 11899       |\n",
      "|    policy_loss        | 0.207       |\n",
      "|    reward             | 0.014062049 |\n",
      "|    std                | 1.27        |\n",
      "|    value_loss         | 0.0273      |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 530      |\n",
      "|    iterations         | 12000    |\n",
      "|    time_elapsed       | 113      |\n",
      "|    total_timesteps    | 60000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.67    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 11999    |\n",
      "|    policy_loss        | 0.00458  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.29     |\n",
      "|    value_loss         | 1.32e-05 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 530       |\n",
      "|    iterations         | 12100     |\n",
      "|    time_elapsed       | 113       |\n",
      "|    total_timesteps    | 60500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.7      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12099     |\n",
      "|    policy_loss        | -0.00351  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.32      |\n",
      "|    value_loss         | 3.03e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 531      |\n",
      "|    iterations         | 12200    |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 61000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.73    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12199    |\n",
      "|    policy_loss        | 0.000451 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.36     |\n",
      "|    value_loss         | 9.76e-08 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 531       |\n",
      "|    iterations         | 12300     |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 61500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.77     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12299     |\n",
      "|    policy_loss        | -0.000675 |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.43      |\n",
      "|    value_loss         | 1.23e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 531      |\n",
      "|    iterations         | 12400    |\n",
      "|    time_elapsed       | 116      |\n",
      "|    total_timesteps    | 62000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.83    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12399    |\n",
      "|    policy_loss        | 0.0017   |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.51     |\n",
      "|    value_loss         | 4.94e-07 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 531      |\n",
      "|    iterations         | 12500    |\n",
      "|    time_elapsed       | 117      |\n",
      "|    total_timesteps    | 62500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -1.89    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 12499    |\n",
      "|    policy_loss        | -0.00631 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.6      |\n",
      "|    value_loss         | 9.67e-06 |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 531       |\n",
      "|    iterations         | 12600     |\n",
      "|    time_elapsed       | 118       |\n",
      "|    total_timesteps    | 63000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -1.93     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 12599     |\n",
      "|    policy_loss        | -1.41     |\n",
      "|    reward             | 0.024093  |\n",
      "|    std                | 1.66      |\n",
      "|    value_loss         | 0.627     |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 531           |\n",
      "|    iterations         | 12700         |\n",
      "|    time_elapsed       | 119           |\n",
      "|    total_timesteps    | 63500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.92         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 12699         |\n",
      "|    policy_loss        | -0.0135       |\n",
      "|    reward             | -0.0008139527 |\n",
      "|    std                | 1.65          |\n",
      "|    value_loss         | 5.52e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 532           |\n",
      "|    iterations         | 12800         |\n",
      "|    time_elapsed       | 120           |\n",
      "|    total_timesteps    | 64000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.93         |\n",
      "|    explained_variance | 1.19e-07      |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 12799         |\n",
      "|    policy_loss        | -0.00292      |\n",
      "|    reward             | -0.0011370004 |\n",
      "|    std                | 1.67          |\n",
      "|    value_loss         | 4.81e-06      |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 532            |\n",
      "|    iterations         | 12900          |\n",
      "|    time_elapsed       | 121            |\n",
      "|    total_timesteps    | 64500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -1.95          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 12899          |\n",
      "|    policy_loss        | -0.00253       |\n",
      "|    reward             | -0.00015235197 |\n",
      "|    std                | 1.7            |\n",
      "|    value_loss         | 4.08e-06       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 532           |\n",
      "|    iterations         | 13000         |\n",
      "|    time_elapsed       | 122           |\n",
      "|    total_timesteps    | 65000         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -1.98         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 12999         |\n",
      "|    policy_loss        | 0.0156        |\n",
      "|    reward             | 0.00063894916 |\n",
      "|    std                | 1.75          |\n",
      "|    value_loss         | 6.57e-05      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 532           |\n",
      "|    iterations         | 13100         |\n",
      "|    time_elapsed       | 122           |\n",
      "|    total_timesteps    | 65500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.02         |\n",
      "|    explained_variance | -1.19e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 13099         |\n",
      "|    policy_loss        | -3.71e-05     |\n",
      "|    reward             | -0.0004015896 |\n",
      "|    std                | 1.82          |\n",
      "|    value_loss         | 9.14e-08      |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 532       |\n",
      "|    iterations         | 13200     |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 66000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.05     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 13199     |\n",
      "|    policy_loss        | -0.00548  |\n",
      "|    reward             | 0.0       |\n",
      "|    std                | 1.88      |\n",
      "|    value_loss         | 9.85e-06  |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 533      |\n",
      "|    iterations         | 13300    |\n",
      "|    time_elapsed       | 124      |\n",
      "|    total_timesteps    | 66500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13299    |\n",
      "|    policy_loss        | 0.00511  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 1.9      |\n",
      "|    value_loss         | 1.17e-05 |\n",
      "------------------------------------\n",
      "day: 3335, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1003534.29\n",
      "total_reward: 3534.29\n",
      "total_cost: 7694.46\n",
      "total_trades: 2184\n",
      "Sharpe: 0.128\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 533          |\n",
      "|    iterations         | 13400        |\n",
      "|    time_elapsed       | 125          |\n",
      "|    total_timesteps    | 67000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.08        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13399        |\n",
      "|    policy_loss        | 0.000275     |\n",
      "|    reward             | 0.0011011078 |\n",
      "|    std                | 1.94         |\n",
      "|    value_loss         | 1.5e-07      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 534          |\n",
      "|    iterations         | 13500        |\n",
      "|    time_elapsed       | 126          |\n",
      "|    total_timesteps    | 67500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.11        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13499        |\n",
      "|    policy_loss        | 0.0183       |\n",
      "|    reward             | 0.0026539767 |\n",
      "|    std                | 1.99         |\n",
      "|    value_loss         | 0.000104     |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 533          |\n",
      "|    iterations         | 13600        |\n",
      "|    time_elapsed       | 127          |\n",
      "|    total_timesteps    | 68000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.13        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 13599        |\n",
      "|    policy_loss        | 0.00972      |\n",
      "|    reward             | 0.0017686038 |\n",
      "|    std                | 2.04         |\n",
      "|    value_loss         | 4.2e-05      |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 534      |\n",
      "|    iterations         | 13700    |\n",
      "|    time_elapsed       | 128      |\n",
      "|    total_timesteps    | 68500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.17    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13699    |\n",
      "|    policy_loss        | -0.00161 |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.12     |\n",
      "|    value_loss         | 9.4e-07  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 534      |\n",
      "|    iterations         | 13800    |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 69000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.21    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 13799    |\n",
      "|    policy_loss        | -0.0132  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.21     |\n",
      "|    value_loss         | 3.36e-05 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 534            |\n",
      "|    iterations         | 13900          |\n",
      "|    time_elapsed       | 130            |\n",
      "|    total_timesteps    | 69500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.25          |\n",
      "|    explained_variance | -1.19e-07      |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 13899          |\n",
      "|    policy_loss        | -0.0794        |\n",
      "|    reward             | -0.00092834106 |\n",
      "|    std                | 2.29           |\n",
      "|    value_loss         | 0.00772        |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 14000      |\n",
      "|    time_elapsed       | 130        |\n",
      "|    total_timesteps    | 70000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.25      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 13999      |\n",
      "|    policy_loss        | 0.563      |\n",
      "|    reward             | 0.08998175 |\n",
      "|    std                | 2.31       |\n",
      "|    value_loss         | 0.0448     |\n",
      "--------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 535           |\n",
      "|    iterations         | 14100         |\n",
      "|    time_elapsed       | 131           |\n",
      "|    total_timesteps    | 70500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -2.27         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 14099         |\n",
      "|    policy_loss        | -0.196        |\n",
      "|    reward             | -0.0050717015 |\n",
      "|    std                | 2.34          |\n",
      "|    value_loss         | 0.00514       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 535         |\n",
      "|    iterations         | 14200       |\n",
      "|    time_elapsed       | 132         |\n",
      "|    total_timesteps    | 71000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.27       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14199       |\n",
      "|    policy_loss        | 0.145       |\n",
      "|    reward             | -0.07791262 |\n",
      "|    std                | 2.34        |\n",
      "|    value_loss         | 0.0181      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 536         |\n",
      "|    iterations         | 14300       |\n",
      "|    time_elapsed       | 133         |\n",
      "|    total_timesteps    | 71500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.27       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14299       |\n",
      "|    policy_loss        | -0.622      |\n",
      "|    reward             | -0.58838105 |\n",
      "|    std                | 2.35        |\n",
      "|    value_loss         | 0.146       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 535         |\n",
      "|    iterations         | 14400       |\n",
      "|    time_elapsed       | 134         |\n",
      "|    total_timesteps    | 72000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.27       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14399       |\n",
      "|    policy_loss        | -0.0372     |\n",
      "|    reward             | -0.36274752 |\n",
      "|    std                | 2.35        |\n",
      "|    value_loss         | 0.0998      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 535        |\n",
      "|    iterations         | 14500      |\n",
      "|    time_elapsed       | 135        |\n",
      "|    total_timesteps    | 72500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.28      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 14499      |\n",
      "|    policy_loss        | 2.99       |\n",
      "|    reward             | -0.3763348 |\n",
      "|    std                | 2.37       |\n",
      "|    value_loss         | 2.32       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 536         |\n",
      "|    iterations         | 14600       |\n",
      "|    time_elapsed       | 136         |\n",
      "|    total_timesteps    | 73000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.29       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14599       |\n",
      "|    policy_loss        | -0.282      |\n",
      "|    reward             | -0.03579568 |\n",
      "|    std                | 2.39        |\n",
      "|    value_loss         | 0.0157      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 536          |\n",
      "|    iterations         | 14700        |\n",
      "|    time_elapsed       | 137          |\n",
      "|    total_timesteps    | 73500        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.29        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 14699        |\n",
      "|    policy_loss        | 0.168        |\n",
      "|    reward             | -0.048328713 |\n",
      "|    std                | 2.39         |\n",
      "|    value_loss         | 0.00603      |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 536         |\n",
      "|    iterations         | 14800       |\n",
      "|    time_elapsed       | 137         |\n",
      "|    total_timesteps    | 74000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.3        |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14799       |\n",
      "|    policy_loss        | 0.637       |\n",
      "|    reward             | -0.35730958 |\n",
      "|    std                | 2.42        |\n",
      "|    value_loss         | 0.22        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 537         |\n",
      "|    iterations         | 14900       |\n",
      "|    time_elapsed       | 138         |\n",
      "|    total_timesteps    | 74500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.3        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 14899       |\n",
      "|    policy_loss        | 0.357       |\n",
      "|    reward             | -0.07535523 |\n",
      "|    std                | 2.42        |\n",
      "|    value_loss         | 0.0724      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 15000     |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 75000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.29     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 14999     |\n",
      "|    policy_loss        | 0.873     |\n",
      "|    reward             | 0.5264886 |\n",
      "|    std                | 2.39      |\n",
      "|    value_loss         | 0.475     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 537         |\n",
      "|    iterations         | 15100       |\n",
      "|    time_elapsed       | 140         |\n",
      "|    total_timesteps    | 75500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.29       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15099       |\n",
      "|    policy_loss        | 2.71        |\n",
      "|    reward             | -0.33496606 |\n",
      "|    std                | 2.39        |\n",
      "|    value_loss         | 2.4         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 15200     |\n",
      "|    time_elapsed       | 141       |\n",
      "|    total_timesteps    | 76000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.29     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15199     |\n",
      "|    policy_loss        | 12.4      |\n",
      "|    reward             | 1.1276422 |\n",
      "|    std                | 2.4       |\n",
      "|    value_loss         | 27.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 537       |\n",
      "|    iterations         | 15300     |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 76500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.3      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15299     |\n",
      "|    policy_loss        | -10.5     |\n",
      "|    reward             | -12.41449 |\n",
      "|    std                | 2.42      |\n",
      "|    value_loss         | 71        |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 15400    |\n",
      "|    time_elapsed       | 143      |\n",
      "|    total_timesteps    | 77000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.29    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 15399    |\n",
      "|    policy_loss        | -0.0536  |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 2.4      |\n",
      "|    value_loss         | 0.000464 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 15500       |\n",
      "|    time_elapsed       | 143         |\n",
      "|    total_timesteps    | 77500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.31       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 15499       |\n",
      "|    policy_loss        | 0.0186      |\n",
      "|    reward             | 0.008539044 |\n",
      "|    std                | 2.43        |\n",
      "|    value_loss         | 9.39e-05    |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 539          |\n",
      "|    iterations         | 15600        |\n",
      "|    time_elapsed       | 144          |\n",
      "|    total_timesteps    | 78000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.32        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15599        |\n",
      "|    policy_loss        | -0.00582     |\n",
      "|    reward             | 0.0033784017 |\n",
      "|    std                | 2.47         |\n",
      "|    value_loss         | 8.34e-06     |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 539            |\n",
      "|    iterations         | 15700          |\n",
      "|    time_elapsed       | 145            |\n",
      "|    total_timesteps    | 78500          |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -2.34          |\n",
      "|    explained_variance | 0              |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 15699          |\n",
      "|    policy_loss        | 0.0124         |\n",
      "|    reward             | -0.00030208932 |\n",
      "|    std                | 2.52           |\n",
      "|    value_loss         | 3.98e-05       |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 539          |\n",
      "|    iterations         | 15800        |\n",
      "|    time_elapsed       | 146          |\n",
      "|    total_timesteps    | 79000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -2.37        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 15799        |\n",
      "|    policy_loss        | -0.124       |\n",
      "|    reward             | -0.043390393 |\n",
      "|    std                | 2.6          |\n",
      "|    value_loss         | 0.0144       |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 15900     |\n",
      "|    time_elapsed       | 147       |\n",
      "|    total_timesteps    | 79500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.38     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 15899     |\n",
      "|    policy_loss        | -0.0878   |\n",
      "|    reward             | 0.1823168 |\n",
      "|    std                | 2.6       |\n",
      "|    value_loss         | 0.00436   |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 16000      |\n",
      "|    time_elapsed       | 148        |\n",
      "|    total_timesteps    | 80000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.39      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 15999      |\n",
      "|    policy_loss        | -1.37      |\n",
      "|    reward             | -2.0314574 |\n",
      "|    std                | 2.63       |\n",
      "|    value_loss         | 0.729      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 539         |\n",
      "|    iterations         | 16100       |\n",
      "|    time_elapsed       | 149         |\n",
      "|    total_timesteps    | 80500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.39       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 16099       |\n",
      "|    policy_loss        | 1.16        |\n",
      "|    reward             | -0.03234973 |\n",
      "|    std                | 2.64        |\n",
      "|    value_loss         | 0.346       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 16200     |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 81000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.39     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16199     |\n",
      "|    policy_loss        | -9.38     |\n",
      "|    reward             | 2.2749274 |\n",
      "|    std                | 2.64      |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 539        |\n",
      "|    iterations         | 16300      |\n",
      "|    time_elapsed       | 150        |\n",
      "|    total_timesteps    | 81500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.38      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16299      |\n",
      "|    policy_loss        | 3.07       |\n",
      "|    reward             | -2.9120831 |\n",
      "|    std                | 2.63       |\n",
      "|    value_loss         | 3.9        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 16400     |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 82000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.37     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16399     |\n",
      "|    policy_loss        | -5.02     |\n",
      "|    reward             | -0.854889 |\n",
      "|    std                | 2.59      |\n",
      "|    value_loss         | 6.26      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 16500     |\n",
      "|    time_elapsed       | 152       |\n",
      "|    total_timesteps    | 82500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.38     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16499     |\n",
      "|    policy_loss        | 10.3      |\n",
      "|    reward             | 1.2524257 |\n",
      "|    std                | 2.62      |\n",
      "|    value_loss         | 18        |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 539      |\n",
      "|    iterations         | 16600    |\n",
      "|    time_elapsed       | 153      |\n",
      "|    total_timesteps    | 83000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.38    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 16599    |\n",
      "|    policy_loss        | 0.571    |\n",
      "|    reward             | 3.802287 |\n",
      "|    std                | 2.62     |\n",
      "|    value_loss         | 55.7     |\n",
      "------------------------------------\n",
      "day: 3335, episode: 25\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9345625.02\n",
      "total_reward: 8345625.02\n",
      "total_cost: 5940.70\n",
      "total_trades: 3317\n",
      "Sharpe: 0.826\n",
      "=================================\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 540        |\n",
      "|    iterations         | 16700      |\n",
      "|    time_elapsed       | 154        |\n",
      "|    total_timesteps    | 83500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.38      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16699      |\n",
      "|    policy_loss        | -0.316     |\n",
      "|    reward             | 0.08927217 |\n",
      "|    std                | 2.62       |\n",
      "|    value_loss         | 0.022      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 16800     |\n",
      "|    time_elapsed       | 155       |\n",
      "|    total_timesteps    | 84000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.39     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16799     |\n",
      "|    policy_loss        | -2.02     |\n",
      "|    reward             | 2.4553304 |\n",
      "|    std                | 2.64      |\n",
      "|    value_loss         | 0.721     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 16900     |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 84500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 16899     |\n",
      "|    policy_loss        | 0.337     |\n",
      "|    reward             | 1.0786895 |\n",
      "|    std                | 2.66      |\n",
      "|    value_loss         | 0.564     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 539        |\n",
      "|    iterations         | 17000      |\n",
      "|    time_elapsed       | 157        |\n",
      "|    total_timesteps    | 85000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.4       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 16999      |\n",
      "|    policy_loss        | -4.55      |\n",
      "|    reward             | -3.1129906 |\n",
      "|    std                | 2.68       |\n",
      "|    value_loss         | 3.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 17100     |\n",
      "|    time_elapsed       | 158       |\n",
      "|    total_timesteps    | 85500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17099     |\n",
      "|    policy_loss        | 14.1      |\n",
      "|    reward             | 1.3162246 |\n",
      "|    std                | 2.68      |\n",
      "|    value_loss         | 49.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 17200     |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 86000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17199     |\n",
      "|    policy_loss        | 19.2      |\n",
      "|    reward             | 6.2216644 |\n",
      "|    std                | 2.67      |\n",
      "|    value_loss         | 145       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 17300      |\n",
      "|    time_elapsed       | 160        |\n",
      "|    total_timesteps    | 86500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.41      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17299      |\n",
      "|    policy_loss        | -1.63      |\n",
      "|    reward             | -1.4183686 |\n",
      "|    std                | 2.68       |\n",
      "|    value_loss         | 149        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 17400       |\n",
      "|    time_elapsed       | 161         |\n",
      "|    total_timesteps    | 87000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.41       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17399       |\n",
      "|    policy_loss        | 0.723       |\n",
      "|    reward             | -0.25662816 |\n",
      "|    std                | 2.69        |\n",
      "|    value_loss         | 0.0743      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 17500     |\n",
      "|    time_elapsed       | 162       |\n",
      "|    total_timesteps    | 87500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.41     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17499     |\n",
      "|    policy_loss        | -6.03     |\n",
      "|    reward             | 2.8375766 |\n",
      "|    std                | 2.68      |\n",
      "|    value_loss         | 7.89      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 539        |\n",
      "|    iterations         | 17600      |\n",
      "|    time_elapsed       | 163        |\n",
      "|    total_timesteps    | 88000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.41      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17599      |\n",
      "|    policy_loss        | -22.9      |\n",
      "|    reward             | -4.5001607 |\n",
      "|    std                | 2.69       |\n",
      "|    value_loss         | 117        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 538      |\n",
      "|    iterations         | 17700    |\n",
      "|    time_elapsed       | 164      |\n",
      "|    total_timesteps    | 88500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.41    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 17699    |\n",
      "|    policy_loss        | -2.54    |\n",
      "|    reward             | 0.999767 |\n",
      "|    std                | 2.69     |\n",
      "|    value_loss         | 1.74     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 17800      |\n",
      "|    time_elapsed       | 165        |\n",
      "|    total_timesteps    | 89000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.41      |\n",
      "|    explained_variance | -6.57e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 17799      |\n",
      "|    policy_loss        | -68.7      |\n",
      "|    reward             | 0.31458226 |\n",
      "|    std                | 2.7        |\n",
      "|    value_loss         | 628        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 17900     |\n",
      "|    time_elapsed       | 166       |\n",
      "|    total_timesteps    | 89500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.41     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 17899     |\n",
      "|    policy_loss        | 80.9      |\n",
      "|    reward             | 7.2269635 |\n",
      "|    std                | 2.71      |\n",
      "|    value_loss         | 1.11e+03  |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 538         |\n",
      "|    iterations         | 18000       |\n",
      "|    time_elapsed       | 167         |\n",
      "|    total_timesteps    | 90000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.4        |\n",
      "|    explained_variance | -0.000485   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 17999       |\n",
      "|    policy_loss        | 48.4        |\n",
      "|    reward             | -0.50692594 |\n",
      "|    std                | 2.67        |\n",
      "|    value_loss         | 432         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 538        |\n",
      "|    iterations         | 18100      |\n",
      "|    time_elapsed       | 168        |\n",
      "|    total_timesteps    | 90500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.4       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18099      |\n",
      "|    policy_loss        | 0.36       |\n",
      "|    reward             | 0.36676118 |\n",
      "|    std                | 2.66       |\n",
      "|    value_loss         | 0.193      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 18200     |\n",
      "|    time_elapsed       | 168       |\n",
      "|    total_timesteps    | 91000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18199     |\n",
      "|    policy_loss        | 3.61      |\n",
      "|    reward             | -5.055982 |\n",
      "|    std                | 2.67      |\n",
      "|    value_loss         | 2.14      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 538       |\n",
      "|    iterations         | 18300     |\n",
      "|    time_elapsed       | 169       |\n",
      "|    total_timesteps    | 91500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18299     |\n",
      "|    policy_loss        | 24.3      |\n",
      "|    reward             | -1.852152 |\n",
      "|    std                | 2.67      |\n",
      "|    value_loss         | 122       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 18400     |\n",
      "|    time_elapsed       | 170       |\n",
      "|    total_timesteps    | 92000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.41     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18399     |\n",
      "|    policy_loss        | 2.91      |\n",
      "|    reward             | 0.7699159 |\n",
      "|    std                | 2.7       |\n",
      "|    value_loss         | 9.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 18500     |\n",
      "|    time_elapsed       | 171       |\n",
      "|    total_timesteps    | 92500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18499     |\n",
      "|    policy_loss        | 18.9      |\n",
      "|    reward             | -4.202377 |\n",
      "|    std                | 2.68      |\n",
      "|    value_loss         | 79.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 18600     |\n",
      "|    time_elapsed       | 172       |\n",
      "|    total_timesteps    | 93000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 18599     |\n",
      "|    policy_loss        | 34.4      |\n",
      "|    reward             | 10.924949 |\n",
      "|    std                | 2.66      |\n",
      "|    value_loss         | 381       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 539         |\n",
      "|    iterations         | 18700       |\n",
      "|    time_elapsed       | 173         |\n",
      "|    total_timesteps    | 93500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.39       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18699       |\n",
      "|    policy_loss        | -0.0101     |\n",
      "|    reward             | 0.007236566 |\n",
      "|    std                | 2.65        |\n",
      "|    value_loss         | 0.0407      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 539        |\n",
      "|    iterations         | 18800      |\n",
      "|    time_elapsed       | 174        |\n",
      "|    total_timesteps    | 94000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.4       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 18799      |\n",
      "|    policy_loss        | -3.7       |\n",
      "|    reward             | 0.11909144 |\n",
      "|    std                | 2.65       |\n",
      "|    value_loss         | 4.08       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 539         |\n",
      "|    iterations         | 18900       |\n",
      "|    time_elapsed       | 175         |\n",
      "|    total_timesteps    | 94500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.4        |\n",
      "|    explained_variance | -0.00308    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18899       |\n",
      "|    policy_loss        | 8.32        |\n",
      "|    reward             | -0.49460718 |\n",
      "|    std                | 2.66        |\n",
      "|    value_loss         | 43.1        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 539         |\n",
      "|    iterations         | 19000       |\n",
      "|    time_elapsed       | 176         |\n",
      "|    total_timesteps    | 95000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -2.41       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 18999       |\n",
      "|    policy_loss        | -7.56       |\n",
      "|    reward             | -0.17273626 |\n",
      "|    std                | 2.69        |\n",
      "|    value_loss         | 15.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 19100     |\n",
      "|    time_elapsed       | 177       |\n",
      "|    total_timesteps    | 95500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.41     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19099     |\n",
      "|    policy_loss        | -14.5     |\n",
      "|    reward             | 1.0214363 |\n",
      "|    std                | 2.7       |\n",
      "|    value_loss         | 53.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 539       |\n",
      "|    iterations         | 19200     |\n",
      "|    time_elapsed       | 177       |\n",
      "|    total_timesteps    | 96000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.41     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19199     |\n",
      "|    policy_loss        | 61.7      |\n",
      "|    reward             | 14.242273 |\n",
      "|    std                | 2.71      |\n",
      "|    value_loss         | 840       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 539        |\n",
      "|    iterations         | 19300      |\n",
      "|    time_elapsed       | 178        |\n",
      "|    total_timesteps    | 96500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.41      |\n",
      "|    explained_variance | 5.07e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19299      |\n",
      "|    policy_loss        | -56.3      |\n",
      "|    reward             | -38.176907 |\n",
      "|    std                | 2.69       |\n",
      "|    value_loss         | 694        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 539        |\n",
      "|    iterations         | 19400      |\n",
      "|    time_elapsed       | 179        |\n",
      "|    total_timesteps    | 97000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.41      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19399      |\n",
      "|    policy_loss        | -0.326     |\n",
      "|    reward             | 0.13464735 |\n",
      "|    std                | 2.69       |\n",
      "|    value_loss         | 0.025      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 19500     |\n",
      "|    time_elapsed       | 180       |\n",
      "|    total_timesteps    | 97500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19499     |\n",
      "|    policy_loss        | -2.3      |\n",
      "|    reward             | -2.624856 |\n",
      "|    std                | 2.68      |\n",
      "|    value_loss         | 1.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 19600     |\n",
      "|    time_elapsed       | 181       |\n",
      "|    total_timesteps    | 98000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.41     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19599     |\n",
      "|    policy_loss        | 9.26      |\n",
      "|    reward             | -2.412004 |\n",
      "|    std                | 2.68      |\n",
      "|    value_loss         | 30.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 540        |\n",
      "|    iterations         | 19700      |\n",
      "|    time_elapsed       | 182        |\n",
      "|    total_timesteps    | 98500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -2.39      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 19699      |\n",
      "|    policy_loss        | 5.58       |\n",
      "|    reward             | -1.3875014 |\n",
      "|    std                | 2.64       |\n",
      "|    value_loss         | 8.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 19800     |\n",
      "|    time_elapsed       | 183       |\n",
      "|    total_timesteps    | 99000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19799     |\n",
      "|    policy_loss        | -13.8     |\n",
      "|    reward             | -9.635458 |\n",
      "|    std                | 2.66      |\n",
      "|    value_loss         | 77.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 19900     |\n",
      "|    time_elapsed       | 184       |\n",
      "|    total_timesteps    | 99500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19899     |\n",
      "|    policy_loss        | 27.6      |\n",
      "|    reward             | -4.783712 |\n",
      "|    std                | 2.66      |\n",
      "|    value_loss         | 268       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 540       |\n",
      "|    iterations         | 20000     |\n",
      "|    time_elapsed       | 185       |\n",
      "|    total_timesteps    | 100000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.4      |\n",
      "|    explained_variance | 0.000194  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 19999     |\n",
      "|    policy_loss        | 30.8      |\n",
      "|    reward             | 13.169493 |\n",
      "|    std                | 2.67      |\n",
      "|    value_loss         | 876       |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2023-04-05 to  2023-07-07\n",
      "A2C Sharpe Ratio:  0.5109892924158569\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ppo/ppo_693_3\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    fps             | 1053     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 2048     |\n",
      "| train/             |          |\n",
      "|    reward          | 0.184816 |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 956          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031822715 |\n",
      "|    clip_fraction        | 0.0404       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0452       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00175     |\n",
      "|    reward               | 0.073695436  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.146        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 935           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 6             |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00038998085 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00885       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 12.7          |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.000228     |\n",
      "|    reward               | -4.14676      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 24            |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 916          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.007420856  |\n",
      "|    clip_fraction        | 0.0526       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.71         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00199     |\n",
      "|    reward               | -0.023102615 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 18.5         |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 899           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 11            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014583612 |\n",
      "|    clip_fraction        | 4.88e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.000699     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 15            |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.000276     |\n",
      "|    reward               | -0.0019359346 |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 39.8          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 901          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018600868 |\n",
      "|    clip_fraction        | 0.00181      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0372       |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000208    |\n",
      "|    reward               | 0.0077538746 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 0.115        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 903         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005334846 |\n",
      "|    clip_fraction        | 0.0511      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | -0.0283     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00315    |\n",
      "|    reward               | -0.08294256 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.00661     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 905          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048242696 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.207        |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    reward               | 0.03903005   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.423        |\n",
      "------------------------------------------\n",
      "day: 3335, episode: 35\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1273934.07\n",
      "total_reward: 273934.07\n",
      "total_cost: 10473.40\n",
      "total_trades: 3276\n",
      "Sharpe: 0.769\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 892          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035887323 |\n",
      "|    clip_fraction        | 0.0219       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.0593       |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00149     |\n",
      "|    reward               | 0.01604352   |\n",
      "|    std                  | 0.988        |\n",
      "|    value_loss           | 0.155        |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006406778 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.086       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    reward               | 0.08668164  |\n",
      "|    std                  | 0.974       |\n",
      "|    value_loss           | 0.28        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 888          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019653614 |\n",
      "|    clip_fraction        | 0.00308      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.0159       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 27           |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00169     |\n",
      "|    reward               | 4.007202     |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 54.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 890          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028842264 |\n",
      "|    clip_fraction        | 0.00859      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | -0.0134      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 11.3         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    reward               | 0.7756939    |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 15           |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 887           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 29            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048370595 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.044         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 112           |\n",
      "|    n_updates            | 120           |\n",
      "|    policy_gradient_loss | -0.000949     |\n",
      "|    reward               | -20.936697    |\n",
      "|    std                  | 0.97          |\n",
      "|    value_loss           | 205           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 887          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024496417 |\n",
      "|    clip_fraction        | 0.00493      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.0312       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 238          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    reward               | -0.74182934  |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 411          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 886           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026318102 |\n",
      "|    clip_fraction        | 0.000293      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.276         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 36.2          |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000745     |\n",
      "|    reward               | -0.6407606    |\n",
      "|    std                  | 0.97          |\n",
      "|    value_loss           | 35.2          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 889          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018946724 |\n",
      "|    clip_fraction        | 0.00342      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.117        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 161          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00118     |\n",
      "|    reward               | 3.7840831    |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 352          |\n",
      "------------------------------------------\n",
      "day: 3335, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7436327.74\n",
      "total_reward: 6436327.74\n",
      "total_cost: 9970.02\n",
      "total_trades: 3301\n",
      "Sharpe: 0.796\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 888           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 39            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024001818 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.281         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 91.7          |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -8.06e-05     |\n",
      "|    reward               | 1.1382947     |\n",
      "|    std                  | 0.972         |\n",
      "|    value_loss           | 146           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 887          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012297044 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.17         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 177          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00161     |\n",
      "|    reward               | 0.02087052   |\n",
      "|    std                  | 0.97         |\n",
      "|    value_loss           | 303          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007993808 |\n",
      "|    clip_fraction        | 0.000635     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.102        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 197          |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.000302    |\n",
      "|    reward               | 1.1679975    |\n",
      "|    std                  | 0.971        |\n",
      "|    value_loss           | 437          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 887          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 46           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020066383 |\n",
      "|    clip_fraction        | 0.00942      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.751        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.3          |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000438    |\n",
      "|    reward               | -1.4351925   |\n",
      "|    std                  | 0.973        |\n",
      "|    value_loss           | 8.42         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 889          |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 48           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007371616 |\n",
      "|    clip_fraction        | 0.000684     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.295        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 139          |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00109     |\n",
      "|    reward               | 16.638699    |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 326          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014644142 |\n",
      "|    clip_fraction        | 0.00332      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.38         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 97.8         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000655    |\n",
      "|    reward               | -0.617411    |\n",
      "|    std                  | 0.975        |\n",
      "|    value_loss           | 213          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012598551 |\n",
      "|    clip_fraction        | 0.000781     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.39        |\n",
      "|    explained_variance   | 0.191        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 130          |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00097     |\n",
      "|    reward               | -0.31326082  |\n",
      "|    std                  | 0.974        |\n",
      "|    value_loss           | 245          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 886           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 55            |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037946203 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.39         |\n",
      "|    explained_variance   | 0.179         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 290           |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -1.27e-05     |\n",
      "|    reward               | 3.7840774     |\n",
      "|    std                  | 0.976         |\n",
      "|    value_loss           | 523           |\n",
      "-------------------------------------------\n",
      "day: 3335, episode: 45\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7652115.29\n",
      "total_reward: 6652115.29\n",
      "total_cost: 10523.70\n",
      "total_trades: 3307\n",
      "Sharpe: 0.803\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 884         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002122618 |\n",
      "|    clip_fraction        | 0.00811     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.666       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.9        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00103    |\n",
      "|    reward               | 0.7767097   |\n",
      "|    std                  | 0.978       |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 885           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 60            |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00092628587 |\n",
      "|    clip_fraction        | 0.00137       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.273         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 235           |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.00062      |\n",
      "|    reward               | -20.483486    |\n",
      "|    std                  | 0.98          |\n",
      "|    value_loss           | 443           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023810337 |\n",
      "|    clip_fraction        | 0.00527      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.179        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 198          |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    reward               | -0.39358646  |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 472          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 885           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 64            |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032015613 |\n",
      "|    clip_fraction        | 0.000781      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.551         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 18.2          |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.000565     |\n",
      "|    reward               | -0.19178683   |\n",
      "|    std                  | 0.976         |\n",
      "|    value_loss           | 70.2          |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 884           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021660796 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.4          |\n",
      "|    explained_variance   | 0.271         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 235           |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | 0.000607      |\n",
      "|    reward               | 8.012362      |\n",
      "|    std                  | 0.976         |\n",
      "|    value_loss           | 510           |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001654644 |\n",
      "|    clip_fraction        | 0.00264     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.547       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 83.5        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00238    |\n",
      "|    reward               | 1.7727317   |\n",
      "|    std                  | 0.979       |\n",
      "|    value_loss           | 164         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034518251 |\n",
      "|    clip_fraction        | 0.00615      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.326        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 192          |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -0.000621    |\n",
      "|    reward               | -0.008490789 |\n",
      "|    std                  | 0.981        |\n",
      "|    value_loss           | 376          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 74           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019522421 |\n",
      "|    clip_fraction        | 0.00356      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.0863       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 355          |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    reward               | 0.23057798   |\n",
      "|    std                  | 0.984        |\n",
      "|    value_loss           | 657          |\n",
      "------------------------------------------\n",
      "day: 3335, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9407193.16\n",
      "total_reward: 8407193.16\n",
      "total_cost: 10727.90\n",
      "total_trades: 3306\n",
      "Sharpe: 0.835\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 885         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003917764 |\n",
      "|    clip_fraction        | 0.0313      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | 0.865       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0016     |\n",
      "|    reward               | -0.13842261 |\n",
      "|    std                  | 0.989       |\n",
      "|    value_loss           | 20.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030070702 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.285        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 325          |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.000726    |\n",
      "|    reward               | 3.7553005    |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 695          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 886          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026231378 |\n",
      "|    clip_fraction        | 0.062        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.147        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 210          |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | 0.00338      |\n",
      "|    reward               | 0.55940574   |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 349          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 886           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 83            |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00089761836 |\n",
      "|    clip_fraction        | 0.0062        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.41         |\n",
      "|    explained_variance   | 0.221         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 243           |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | 0.000802      |\n",
      "|    reward               | 0.065873615   |\n",
      "|    std                  | 0.992         |\n",
      "|    value_loss           | 431           |\n",
      "-------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 887        |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 85         |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00201069 |\n",
      "|    clip_fraction        | 0.0143     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | 0.153      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 287        |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | 0.00109    |\n",
      "|    reward               | -4.482999  |\n",
      "|    std                  | 0.992      |\n",
      "|    value_loss           | 768        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 887         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003472263 |\n",
      "|    clip_fraction        | 0.0186      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.627       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 46.7        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00181    |\n",
      "|    reward               | 0.16483457  |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 71.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 886         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002185483 |\n",
      "|    clip_fraction        | 0.0105      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 322         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.000342   |\n",
      "|    reward               | -15.960724  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 783         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 885          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043325974 |\n",
      "|    clip_fraction        | 0.0202       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.000955    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 404          |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.000912    |\n",
      "|    reward               | 3.3102782    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 706          |\n",
      "------------------------------------------\n",
      "day: 3335, episode: 55\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10215679.27\n",
      "total_reward: 9215679.27\n",
      "total_cost: 10718.58\n",
      "total_trades: 3315\n",
      "Sharpe: 0.847\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 885           |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 94            |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00075381267 |\n",
      "|    clip_fraction        | 0.00513       |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.394         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 157           |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -0.00106      |\n",
      "|    reward               | -0.4057746    |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 252           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 884          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009771399 |\n",
      "|    clip_fraction        | 0.00703      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.19         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 543          |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.00107     |\n",
      "|    reward               | 2.1160629    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 837          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 100          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048649777 |\n",
      "|    clip_fraction        | 0.0282       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.418        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 82.3         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    reward               | 0.5836297    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 155          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 879           |\n",
      "|    iterations           | 44            |\n",
      "|    time_elapsed         | 102           |\n",
      "|    total_timesteps      | 90112         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00078931736 |\n",
      "|    clip_fraction        | 9.77e-05      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.268         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 434           |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.000434     |\n",
      "|    reward               | 0.0031267088  |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 717           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 879          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027468516 |\n",
      "|    clip_fraction        | 0.0299       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.0281       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 368          |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | 0.000261     |\n",
      "|    reward               | -8.877951    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 743          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005066676 |\n",
      "|    clip_fraction        | 0.0364      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00356    |\n",
      "|    reward               | 0.4028772   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 16.4        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 879           |\n",
      "|    iterations           | 47            |\n",
      "|    time_elapsed         | 109           |\n",
      "|    total_timesteps      | 96256         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090007694 |\n",
      "|    clip_fraction        | 0.000879      |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.283         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 243           |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.000492     |\n",
      "|    reward               | -3.9486833    |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 614           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 880          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024276967 |\n",
      "|    clip_fraction        | 0.0109       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.427        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 94.4         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00173     |\n",
      "|    reward               | 1.3576827    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 214          |\n",
      "------------------------------------------\n",
      "day: 3335, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8145998.97\n",
      "total_reward: 7145998.97\n",
      "total_cost: 10085.35\n",
      "total_trades: 3312\n",
      "Sharpe: 0.818\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 879         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 114         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004396335 |\n",
      "|    clip_fraction        | 0.00596     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 168         |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00128    |\n",
      "|    reward               | 0.13725136  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 332         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2023-04-05 to  2023-07-07\n",
      "PPO Sharpe Ratio:  0.4782229865522718\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cpu device\n",
      "Logging to tensorboard_log/ddpg/ddpg_693_3\n",
      "day: 3335, episode: 65\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 85        |\n",
      "|    total_timesteps | 13344     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.33e+03 |\n",
      "|    critic_loss     | 1.1e+03   |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 13243     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 170       |\n",
      "|    total_timesteps | 26688     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.13e+03 |\n",
      "|    critic_loss     | 524       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 26587     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 3335, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 256       |\n",
      "|    total_timesteps | 40032     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 0.77      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 39931     |\n",
      "|    reward          | 0.0       |\n",
      "----------------------------------\n",
      "day: 3335, episode: 75\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 342      |\n",
      "|    total_timesteps | 53376    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -539     |\n",
      "|    critic_loss     | 0.0381   |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 53275    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3335, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 429      |\n",
      "|    total_timesteps | 66720    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -270     |\n",
      "|    critic_loss     | 0.126    |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 66619    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3335, episode: 85\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 514      |\n",
      "|    total_timesteps | 80064    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.48    |\n",
      "|    critic_loss     | 1.44e-05 |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 79963    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 599      |\n",
      "|    total_timesteps | 93408    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.41    |\n",
      "|    critic_loss     | 0.0184   |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 93307    |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "day: 3335, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "======DDPG Validation from:  2023-04-05 to  2023-07-07\n",
      "======Best Model Retraining from:  2010-01-01 to  2023-07-07\n",
      "======Trading from:  2023-07-07 to  2023-10-05\n",
      "Ensemble Strategy took:  156.2749002814293  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "9f0cbf89-5f4b-4691-9e43-daa093ebceae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.066155</td>\n",
       "      <td>-0.064834</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.369875</td>\n",
       "      <td>0.356605</td>\n",
       "      <td>0.369875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.132435</td>\n",
       "      <td>-0.13904</td>\n",
       "      <td>-0.13678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.386822</td>\n",
       "      <td>0.379058</td>\n",
       "      <td>0.386822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>378</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>PPO</td>\n",
       "      <td>-0.390841</td>\n",
       "      <td>0.097004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>441</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>PPO</td>\n",
       "      <td>-0.175468</td>\n",
       "      <td>-0.106816</td>\n",
       "      <td>-0.126724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>504</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.2319</td>\n",
       "      <td>-0.25987</td>\n",
       "      <td>-0.2319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>567</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.214018</td>\n",
       "      <td>-0.192764</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>630</td>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>2023-04-05</td>\n",
       "      <td>PPO</td>\n",
       "      <td>0.473132</td>\n",
       "      <td>0.484741</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>693</td>\n",
       "      <td>2023-04-05</td>\n",
       "      <td>2023-07-07</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.510989</td>\n",
       "      <td>0.478223</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n",
       "0  126  2021-01-04  2021-04-06       DDPG  -0.066155  -0.064834         0.0\n",
       "1  189  2021-04-06  2021-07-06       DDPG   0.369875   0.356605    0.369875\n",
       "2  252  2021-07-06  2021-10-04        A2C  -0.132435   -0.13904    -0.13678\n",
       "3  315  2021-10-04  2022-01-03       DDPG   0.386822   0.379058    0.386822\n",
       "4  378  2022-01-03  2022-04-04        PPO  -0.390841   0.097004         0.0\n",
       "5  441  2022-04-04  2022-07-06        PPO  -0.175468  -0.106816   -0.126724\n",
       "6  504  2022-07-06  2022-10-04       DDPG    -0.2319   -0.25987     -0.2319\n",
       "7  567  2022-10-04  2023-01-04       DDPG  -0.214018  -0.192764         0.0\n",
       "8  630  2023-01-04  2023-04-05        PPO   0.473132   0.484741         0.0\n",
       "9  693  2023-04-05  2023-07-07        A2C   0.510989   0.478223         0.0"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model as ONNX file ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.onnx\n",
    "from stable_baselines3 import DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "train_ddpg = ensemble_agent.get_model('ddpg',ensemble_agent.train_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ddpg.save(\"test_one_ticket_ddpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OnnxablePolicy(th.nn.Module):\n",
    "#     def __init__(self, actor: th.nn.Module):\n",
    "#         super().__init__()\n",
    "#         self.actor = actor\n",
    "\n",
    "#     def forward(self, observation: th.Tensor) -> th.Tensor:\n",
    "#         # NOTE: You may have to postprocess (unnormalize) actions\n",
    "#         # to the correct bounds (see commented code below)\n",
    "#         return self.actor(observation, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Actor.forward() got an unexpected keyword argument 'deterministic'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m observation_size \u001b[38;5;241m=\u001b[39m train_ddpg\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      4\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39mobservation_size)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43monnxable_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdummy_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest_one_ticket_ddpg.onnx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/onnx/utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[1;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/onnx/utils.py:1613\u001b[0m, in \u001b[0;36m_export\u001b[0;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m   1610\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1611\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[0;32m-> 1613\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[1;32m   1627\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1628\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[1;32m   1629\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/onnx/utils.py:1135\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[0;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m   1134\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[0;32m-> 1135\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/onnx/utils.py:1011\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m   1006\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[1;32m   1007\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     )\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1012\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[1;32m   1013\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/onnx/utils.py:915\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[0;34m(model, args)\u001b[0m\n\u001b[1;32m    913\u001b[0m prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[1;32m    914\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 915\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[1;32m    924\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/jit/_trace.py:1296\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[0;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m   1295\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m-> 1296\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mONNXTracedModule\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/jit/_trace.py:138\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[0;32m--> 138\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/jit/_trace.py:129\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    128\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[0;32m--> 129\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[1;32m    131\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1501\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[0;32mIn[156], line 9\u001b[0m, in \u001b[0;36mOnnxablePolicy.forward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# NOTE: You may have to postprocess (unnormalize) actions\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# to the correct bounds (see commented code below)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1501\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1503\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[0;31mTypeError\u001b[0m: Actor.forward() got an unexpected keyword argument 'deterministic'"
     ]
    }
   ],
   "source": [
    "# onnxable_model = OnnxablePolicy(train_ddpg.policy.actor)\n",
    "\n",
    "# observation_size = train_ddpg.observation_space.shape\n",
    "# dummy_input = th.randn(1, *observation_size)\n",
    "# th.onnx.export(\n",
    "#     onnxable_model,\n",
    "#     dummy_input,\n",
    "#     \"test_one_ticket_ddpg.onnx\",\n",
    "#     opset_version=17,\n",
    "#     input_names=[\"input\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "X4JKB--8tj1g"
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.14331618503123894\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value,pd.DataFrame(temp)],ignore_index=True)\n",
    "    # df_account_value = df_account_value.append(temp,ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-04-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000000.0</td>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_value        date  daily_return    datadate\n",
       "0      1000000.0  2021-04-06           NaN  2021-04-06\n",
       "1      1000000.0  2021-04-07           0.0  2021-04-07\n",
       "2      1000000.0  2021-04-08           0.0  2021-04-08\n",
       "3      1000000.0  2021-04-09           0.0  2021-04-09\n",
       "4      1000000.0  2021-04-12           0.0  2021-04-12"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "0e2b0bc2-840c-47fd-87d4-01201d8e4e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "ab0971b8-10b0-4fb1-a151-71a1de89cdf2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.008191\n",
      "Cumulative returns     0.020604\n",
      "Annual volatility      0.210057\n",
      "Sharpe ratio           0.143316\n",
      "Calmar ratio           0.035828\n",
      "Stability              0.048689\n",
      "Max drawdown          -0.228629\n",
      "Omega ratio            1.031278\n",
      "Sortino ratio          0.208825\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.978980\n",
      "Daily value at risk   -0.026345\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "c233f613-67a3-4882-8710-c1839247590e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n",
      "Shape of DataFrame:  (629, 8)\n",
      "Annual return         -0.005147\n",
      "Cumulative returns    -0.012799\n",
      "Annual volatility      0.155208\n",
      "Sharpe ratio           0.044257\n",
      "Calmar ratio          -0.023460\n",
      "Stability              0.075724\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            1.007529\n",
      "Sortino ratio          0.061979\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.987304\n",
      "Daily value at risk   -0.019527\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "df_dji_ = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(df_dji_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhJ9whD75WTs",
    "outputId": "8ae25787-8400-4357-ecc0-af7538689cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dji:             date           dji\n",
      "0    2021-04-06  1.000000e+06\n",
      "1    2021-04-07  1.000479e+06\n",
      "2    2021-04-08  1.002194e+06\n",
      "3    2021-04-09  1.011079e+06\n",
      "4    2021-04-12  1.009427e+06\n",
      "..          ...           ...\n",
      "625  2023-09-28  1.007063e+06\n",
      "626  2023-09-29  1.002311e+06\n",
      "627  2023-10-02  1.000093e+06\n",
      "628  2023-10-03  9.872014e+05\n",
      "629  2023-10-04           NaN\n",
      "\n",
      "[630 rows x 2 columns]\n",
      "df_dji:                       dji\n",
      "date                    \n",
      "2021-04-06  1.000000e+06\n",
      "2021-04-07  1.000479e+06\n",
      "2021-04-08  1.002194e+06\n",
      "2021-04-09  1.011079e+06\n",
      "2021-04-12  1.009427e+06\n",
      "...                  ...\n",
      "2023-09-28  1.007063e+06\n",
      "2023-09-29  1.002311e+06\n",
      "2023-10-02  1.000093e+06\n",
      "2023-10-03  9.872014e+05\n",
      "2023-10-04           NaN\n",
      "\n",
      "[630 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dji = pd.DataFrame()\n",
    "df_dji['date'] = df_account_value['date']\n",
    "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji.csv\")\n",
    "df_dji = df_dji.set_index(df_dji.columns[0])\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with buy-and-hold strategy\n",
    "compare with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Buy&Hold Stats===========\n",
      "Shape of DataFrame:  (629, 8)\n",
      "Annual return          0.139779\n",
      "Cumulative returns     0.386201\n",
      "Annual volatility      0.282198\n",
      "Sharpe ratio           0.605186\n",
      "Calmar ratio           0.452172\n",
      "Stability              0.325460\n",
      "Max drawdown          -0.309128\n",
      "Omega ratio            1.107078\n",
      "Sortino ratio          0.880937\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.953207\n",
      "Daily value at risk   -0.034876\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Buy&Hold Stats===========\")\n",
    "df_hold_ = get_baseline(\n",
    "        ticker=\"AAPL\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(df_hold_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    hold\n",
      "date                    \n",
      "2021-04-06  1.000000e+06\n",
      "2021-04-07  1.013390e+06\n",
      "2021-04-08  1.032882e+06\n",
      "2021-04-09  1.053799e+06\n",
      "2021-04-12  1.039854e+06\n"
     ]
    }
   ],
   "source": [
    "print(df_hold.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_hold:             date          hold\n",
      "0    2021-04-06  1.000000e+06\n",
      "1    2021-04-07  1.013390e+06\n",
      "2    2021-04-08  1.032882e+06\n",
      "3    2021-04-09  1.053799e+06\n",
      "4    2021-04-12  1.039854e+06\n",
      "..          ...           ...\n",
      "625  2023-09-28  1.372452e+06\n",
      "626  2023-09-29  1.376633e+06\n",
      "627  2023-10-02  1.397056e+06\n",
      "628  2023-10-03  1.386201e+06\n",
      "629  2023-10-04           NaN\n",
      "\n",
      "[630 rows x 2 columns]\n",
      "df_hold:                      hold\n",
      "date                    \n",
      "2021-04-06  1.000000e+06\n",
      "2021-04-07  1.013390e+06\n",
      "2021-04-08  1.032882e+06\n",
      "2021-04-09  1.053799e+06\n",
      "2021-04-12  1.039854e+06\n",
      "...                  ...\n",
      "2023-09-28  1.372452e+06\n",
      "2023-09-29  1.376633e+06\n",
      "2023-10-02  1.397056e+06\n",
      "2023-10-03  1.386201e+06\n",
      "2023-10-04           NaN\n",
      "\n",
      "[630 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_hold = pd.DataFrame()\n",
    "df_hold['date'] = df_account_value['date']\n",
    "df_hold['hold'] = df_hold_['close'] / df_hold_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_hold: \", df_hold)\n",
    "df_hold.to_csv(\"df_hold.csv\")\n",
    "df_hold = df_hold.set_index(df_hold.columns[0])\n",
    "print(\"df_hold: \", df_hold)\n",
    "df_hold.to_csv(\"df_hold+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "615e8d79-f3d7-47e9-c886-3cd18e4535f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_result_ensemble.columns:  Index(['ensemble'], dtype='object')\n",
      "df_trade_date:         datadate\n",
      "0    2021-01-04\n",
      "1    2021-01-05\n",
      "2    2021-01-06\n",
      "3    2021-01-07\n",
      "4    2021-01-08\n",
      "..          ...\n",
      "748  2023-12-22\n",
      "749  2023-12-26\n",
      "750  2023-12-27\n",
      "751  2023-12-28\n",
      "752  2023-12-29\n",
      "\n",
      "[753 rows x 1 columns]\n",
      "df_result_ensemble:                  ensemble\n",
      "date                    \n",
      "2021-04-06  1.000000e+06\n",
      "2021-04-07  1.000000e+06\n",
      "2021-04-08  1.000000e+06\n",
      "2021-04-09  1.000000e+06\n",
      "2021-04-12  1.000000e+06\n",
      "...                  ...\n",
      "2023-09-28  1.003095e+06\n",
      "2023-09-29  1.006150e+06\n",
      "2023-10-02  1.021075e+06\n",
      "2023-10-03  1.013209e+06\n",
      "2023-10-04  1.020604e+06\n",
      "\n",
      "[630 rows x 1 columns]\n",
      "==============Compare to DJIA===========\n",
      "result:                  ensemble           dji\n",
      "date                                  \n",
      "2021-04-06  1.000000e+06  1.000000e+06\n",
      "2021-04-07  1.000000e+06  1.000479e+06\n",
      "2021-04-08  1.000000e+06  1.002194e+06\n",
      "2021-04-09  1.000000e+06  1.011079e+06\n",
      "2021-04-12  1.000000e+06  1.009427e+06\n",
      "...                  ...           ...\n",
      "2023-09-28  1.003095e+06  1.007063e+06\n",
      "2023-09-29  1.006150e+06  1.002311e+06\n",
      "2023-10-02  1.021075e+06  1.000093e+06\n",
      "2023-10-03  1.013209e+06  9.872014e+05\n",
      "2023-10-04  1.020604e+06           NaN\n",
      "\n",
      "[630 rows x 2 columns]\n",
      "==============Compare to Buy&Hold===========\n",
      "result_hold:                  ensemble           dji          hold\n",
      "date                                                \n",
      "2021-04-06  1.000000e+06  1.000000e+06  1.000000e+06\n",
      "2021-04-07  1.000000e+06  1.000479e+06  1.013390e+06\n",
      "2021-04-08  1.000000e+06  1.002194e+06  1.032882e+06\n",
      "2021-04-09  1.000000e+06  1.011079e+06  1.053799e+06\n",
      "2021-04-12  1.000000e+06  1.009427e+06  1.039854e+06\n",
      "...                  ...           ...           ...\n",
      "2023-09-28  1.003095e+06  1.007063e+06  1.372452e+06\n",
      "2023-09-29  1.006150e+06  1.002311e+06  1.376633e+06\n",
      "2023-10-02  1.021075e+06  1.000093e+06  1.397056e+06\n",
      "2023-10-03  1.013209e+06  9.872014e+05  1.386201e+06\n",
      "2023-10-04  1.020604e+06           NaN           NaN\n",
      "\n",
      "[630 rows x 3 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"==============Compare to DJIA===========\")\n",
    "# %matplotlib inline\n",
    "# # S&P 500: ^GSPC\n",
    "# # Dow Jones Index: ^DJI\n",
    "# # NASDAQ 100: ^NDX\n",
    "# backtest_plot(df_account_value, \n",
    "#               baseline_ticker = '^DJI', \n",
    "#               baseline_start = df_account_value.loc[0,'date'],\n",
    "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "df.to_csv(\"df.csv\")\n",
    "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
    "df_result_ensemble = df_result_ensemble.set_index('date')\n",
    "\n",
    "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
    "\n",
    "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
    "print(\"df_trade_date: \", df_trade_date)\n",
    "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
    "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
    "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
    "print(\"df_result_ensemble: \", df_result_ensemble)\n",
    "\n",
    "print(\"==============Compare to DJIA===========\")\n",
    "result = pd.DataFrame()\n",
    "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
    "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
    "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "\n",
    "print(\"==============Compare to Buy&Hold===========\")\n",
    "result = pd.merge(result, df_hold, left_index=True, right_index=True)\n",
    "print(\"result_hold: \", result)\n",
    "\n",
    "result.to_csv(\"result.csv\")\n",
    "result.columns = ['ensemble', 'dji', 'hold']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
