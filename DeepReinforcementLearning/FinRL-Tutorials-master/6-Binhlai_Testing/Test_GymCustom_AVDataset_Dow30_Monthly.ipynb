{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch\n",
    "## BinhLai_Dataset_Monthly\n",
    "Network with 128 neutrons and 2 layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Check Additional Packages](#1.1)\n",
    "    * [2.2. Import Packages](#1.2)\n",
    "    * [2.3. Create Folders & Relevant Configurations¶](#1.3)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess fundamental data](#3)        \n",
    "    * [4.1. Import the financial data](#3.1)\n",
    "    * [4.2. Specify items needed to calculate financial ratios](#3.2)\n",
    "    * [4.3. Turn the final_ratios to daily basis](#3.3)\n",
    "    * [4.4. Merge stock price data and ratios into one dataframe](#3.4)\n",
    "    * [4.5. Finish data preparation](#3.5)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. Set up the training environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Train DRL Agent](#5)  \n",
    "* [7.Backtest Our Strategy](#6)  \n",
    "    * [7.1. BackTest with DJIA](#6.1)\n",
    "    * [7.2. BackTest with Buy&Hold Strategy](#6.2)\n",
    "* [8.Save & load model](#7)\n",
    "    * [8.1. Save model](#7.1)\n",
    "    * [8.2. Load model](#7.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If continuing the previous training, jump to [**this point**](#8.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "<a id='1.1'></a>\n",
    "## 2.1. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.2'></a>\n",
    "## 2.2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "import math\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "# from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Create Folders & Relevant Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TEST_END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "TRAIN_START_DATE = '2000-01-01'\n",
    "TRAIN_END_DATE = '2021-01-01'\n",
    "TEST_START_DATE = '2021-01-01'\n",
    "TEST_END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Load price data from csv file\n",
    "tic_dir = './' + DATA_SAVE_DIR + '/sp500_price_monthly.csv'\n",
    "df = pd.read_csv(tic_dir,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is available locally, we can skip downloading steps and jump directly to part [**4.5.Finish data preparation**](#3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Price Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download data from YFinance\n",
    "# tic_dir = './' + DATA_SAVE_DIR + '/sp500_ticker.csv'\n",
    "# tic_list = pd.read_csv(tic_dir,index_col=0)\n",
    "# SP_500_TICKER = np.array(tic_list.tic).tolist()\n",
    "# df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                      end_date = TEST_END_DATE,\n",
    "#                      ticker_list = SP_500_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GiRuFOTOtj1Y",
    "outputId": "402874c0-b13f-437b-a67f-a83f88de66eb"
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DSw4ZEzVtj1Z",
    "outputId": "94617d16-432c-40eb-f758-16d2fdab09e0"
   },
   "outputs": [],
   "source": [
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV3HrZHLh1hy",
    "outputId": "73944c23-5a4e-49f8-b9e5-da382b4fc7f5"
   },
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4hYkeaPiICHS",
    "outputId": "87cca0b1-8d3c-4a61-e061-ea0d9989daa1"
   },
   "outputs": [],
   "source": [
    "# df.sort_values(['date','tic']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2vryMsdNL9H",
    "outputId": "6691ba9b-e613-412b-dba5-dee592bb0ff2"
   },
   "outputs": [],
   "source": [
    "# len(df.tic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcNyXa7RNPrF",
    "outputId": "edb04575-9b82-4d5e-f13a-55c884214725"
   },
   "outputs": [],
   "source": [
    "# df.tic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "<a id='3'></a>\n",
    "# Part 4: Preprocess fundamental data\n",
    "- Import finanical data downloaded from Alpha Vantage\n",
    "- Preprocess the dataset and calculate financial ratios\n",
    "- Turn yearly ratio into daily basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "## 4.1 Import the financial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define configurations of the collecting data & download data via Alpha Vantage API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# api_key = 'GZWRUSYXT18ZTR6C'\n",
    "# features_cols = ['fiscalDateEnding','totalRevenue','costOfRevenue','sellingGeneralAndAdministrative','researchAndDevelopment','depreciation','interestExpense','totalCurrentLiabilities','incomeTaxExpense','netIncome','commonStockSharesOutstanding','cashAndCashEquivalentsAtCarryingValue','cashAndShortTermInvestments','operatingCashflow','totalLiabilities','inventory','currentNetReceivables','propertyPlantEquipment','capitalExpenditures','longTermInvestments','totalShareholderEquity','longTermDebt','retainedEarnings','dividendPayoutCommonStock','paymentsForRepurchaseOfCommonStock','treasuryStock','currentLongTermDebt']\n",
    "# price_cols = ['open','high','low','close','volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to download fundamental data from financial reports by ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_fundamental(ticket):\n",
    "#     # Download income statement\n",
    "#     url = f'https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol={ticket}&apikey={api_key}'\n",
    "#     r = requests.get(url)\n",
    "#     data_is = r.json()\n",
    "\n",
    "#     # Download balance sheet\n",
    "#     url = f'https://www.alphavantage.co/query?function=BALANCE_SHEET&symbol={ticket}&apikey={api_key}'\n",
    "#     r = requests.get(url)\n",
    "#     data_bs = r.json()\n",
    "\n",
    "#     # Download cash flow\n",
    "#     url = f'https://www.alphavantage.co/query?function=CASH_FLOW&symbol={ticket}&apikey={api_key}'\n",
    "#     r = requests.get(url)\n",
    "#     data_cf = r.json()\n",
    "\n",
    "#     df_is = pd.json_normalize(data_is['annualReports'])\n",
    "#     df_bs = pd.json_normalize(data_bs['annualReports'])\n",
    "#     df_cf = pd.json_normalize(data_cf['annualReports'])\n",
    "\n",
    "#     merged_df = df_is.merge(df_bs).merge(df_cf)\n",
    "#     merged_df['tic'] = ticket\n",
    "#     merged_df = merged_df[['tic']+features_cols]\n",
    "#     merged_df['fiscalDateEnding'] = pd.to_datetime(merged_df.fiscalDateEnding,format='mixed')\n",
    "\n",
    "#     return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to download stock price by ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_price(ticket):\n",
    "#     url = f'https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY&symbol={ticket}&apikey={api_key}'\n",
    "#     r = requests.get(url)\n",
    "#     data_monthly_price = r.json()\n",
    "\n",
    "#     price_monthly = pd.DataFrame.from_dict(data_monthly_price['Monthly Time Series'], orient='index')\n",
    "#     price_monthly.columns = price_cols\n",
    "#     price_monthly['fiscalDateEnding'] = pd.to_datetime(price_monthly.index,format='mixed')\n",
    "#     price_monthly.reset_index(inplace=True,drop=True)\n",
    "#     price_monthly = price_monthly[['fiscalDateEnding','open','high','low','close','volume']]\n",
    "#     return price_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to merge monthly stock price into yearly fundamental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_price_to_df(merged_df,price_monthly):\n",
    "#     merged_df['year'] = merged_df.fiscalDateEnding.dt.year\n",
    "#     merged_df['month'] = merged_df.fiscalDateEnding.dt.month\n",
    "#     price_monthly['year'] = price_monthly.fiscalDateEnding.dt.year\n",
    "#     price_monthly['month'] = price_monthly.fiscalDateEnding.dt.month\n",
    "#     merged_final = pd.merge(merged_df,price_monthly,how=\"left\",on=['year','month'])\n",
    "#     merged_final.drop(columns=['year','month','fiscalDateEnding_y'],inplace=True)\n",
    "#     merged_final = df_final.rename(columns={'fiscalDateEnding_x': 'date'})\n",
    "#     merged_final['tic'] = ticket\n",
    "\n",
    "#     merged_columns = [merged_final.columns[-1]]\n",
    "#     for i in range(0,len(merged_final.columns)-1):\n",
    "#         merged_columns.append(merged_final.columns[i])\n",
    "#     merged_final = merged_final[merged_columns]\n",
    "    \n",
    "#     return merged_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_tics = SP_500_TICKER[453:]\n",
    "# print(download_tics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_data = pd.DataFrame()\n",
    "\n",
    "# for ticket in download_tics:\n",
    "#     df_fund = collect_fundamental(ticket)\n",
    "#     fund_data = pd.concat([fund_data, df_fund], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_data.to_csv('sp500_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dropped tickers which are not available data to download from Alpha Vantage\n",
    "\n",
    "# tics_1 = pd.DataFrame()\n",
    "# tics_2 = pd.DataFrame()\n",
    "# tics_1['tic'] = df.tic.unique()\n",
    "# tics_2['tic'] = fund_data.tic.unique()\n",
    "\n",
    "# merged_df = tics_1.merge(tics_2, how='outer', indicator=True)\n",
    "# unique_in_df = merged_df[merged_df['_merge'] == 'left_only']\n",
    "# unique_in_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check reach download limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url = f'https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol=IBM&apikey=GZWRUSYXT18ZTR6C'\n",
    "# r = requests.get(url)\n",
    "# data_is = r.json()\n",
    "# data_is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_data = pd.read_csv('./' + DATA_SAVE_DIR + '/sp500_fundamental.csv',index_col=0)\n",
    "# # fund_data = fund_data.rename(columns={'fiscalDateEnding_x':'date'})\n",
    "# fund_data['date'] = pd.to_datetime(fund_data.date,format='mixed')\n",
    "# fund_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL TOOL: merge yearly fundamental data to daily stock price\n",
    "\n",
    "# start_date = df.iloc[0].date\n",
    "# end_date = df.iloc[-1].date\n",
    "# date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "# date_range = pd.DataFrame(date_range)\n",
    "# date_range.columns = ['date']\n",
    "\n",
    "# fund_data = fund_data.rename(columns={'fiscalDateEnding':'date'})\n",
    "# fund_data['date'] = pd.to_datetime(fund_data.date,format='mixed')\n",
    "\n",
    "# fund_data_with_price = pd.DataFrame()\n",
    "# for ticket in fund_data.tic.unique():\n",
    "#     price_by_ticket = df[df.tic == ticket]\n",
    "#     price_by_ticket['date'] = pd.to_datetime(price_by_ticket['date'],format='mixed')\n",
    "#     price_by_ticket = pd.merge(date_range,price_by_ticket,how='left')\n",
    "#     price_by_ticket.bfill(inplace=True)\n",
    "#     price_by_ticket = pd.merge(fund_data.loc[fund_data.tic==ticket],price_by_ticket,how='left',on=['date'])\n",
    "#     price_by_ticket.drop(columns=['tic_y','day'],inplace=True,axis=0)\n",
    "#     price_by_ticket = price_by_ticket.rename(columns={'tic_x':'tic'})\n",
    "#     fund_data_with_price = pd.concat([fund_data_with_price, price_by_ticket], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning ####\n",
    "Refine the data before computing fundamental ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(fund_data.info(),'\\n')\n",
    "# print(fund_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The company with zero totalRevenue might cause problems for computing ratios while it is the denominator in some formulars.<br>\n",
    "Deleting all tickers containing this issue is a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_revenue_tics = fund_data[fund_data.totalRevenue == 0].tic.unique()\n",
    "# fund_data = fund_data[~fund_data.tic.isin(zero_revenue_tics)]\n",
    "# fund_data[fund_data.totalRevenue == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While removing zero revenue data was neccessary, other columns with zero values might indicate some potential issues that require further investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_nan = fund_data.eq(0).any()\n",
    "# column_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_data = fund_data.fillna(0)\n",
    "# for i in fund_data.columns:\n",
    "#     print(i,'\\n',fund_data[i].unique(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Compute fundamental ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define support functions before computing fundamental ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to count positive values within a window\n",
    "# def count_positives(window):\n",
    "#   return window[window > 0].count()\n",
    "\n",
    "# def count_positives_window(data,k):\n",
    "#     df = pd.DataFrame(data, columns=['values'])\n",
    "\n",
    "#     # Create a new column with the adjusted positive count (using a shifted window)\n",
    "#     df['positive_count'] = df['values'].rolling(window=k, min_periods=1).apply(count_positives)\n",
    "\n",
    "#     # Set positive_count of the first k rows to positive_count of row k\n",
    "#     df.loc[:k-1,'positive_count'] = df.loc[k-1].positive_count\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to min value within a window\n",
    "# def get_min(window):\n",
    "#   return window.min()\n",
    "\n",
    "# def min_in_window(data,k):\n",
    "#     df = pd.DataFrame(data, columns=['values'])\n",
    "\n",
    "#     # Create a new column with the min in the window (using a shifted window)\n",
    "#     df['min'] = df['values'].rolling(window=k, min_periods=1).apply(get_min)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, data and supporting functions are ready for computing fundamental ratios required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_fund_ratios(df_final):\n",
    "#     pos_count_window = 10  # number of years to look back for count_positive_profit\n",
    "#     min_ebit_window = 4  # number of years to look back for min_ebit\n",
    "    \n",
    "#     gross_income = df_final.totalRevenue - df_final.costOfRevenue\n",
    "#     ebit = gross_income - df_final.sellingGeneralAndAdministrative\n",
    "#     profit = ebit - df_final.interestExpense - df_final.incomeTaxExpense\n",
    "#     market_equity = df_final.commonStockSharesOutstanding * df_final.close\n",
    "#     market_asset = df_final.totalLiabilities + market_equity\n",
    "    \n",
    "#     # Gross profit margin\n",
    "#     gross_profit_margin = (gross_income/df_final.totalRevenue).to_frame('gross_profit_margin')\n",
    "    \n",
    "#     # SGA Expense / Gross Profit\n",
    "#     sga_ratio = (df_final.sellingGeneralAndAdministrative/gross_income).to_frame('sga_ratio')\n",
    "    \n",
    "#     # Depreciation / Gross Profit\n",
    "#     dep_ratio = (df_final.depreciation/gross_income).to_frame('dep_ratio')\n",
    "    \n",
    "#     # EBIT / Bond interest\n",
    "#     ebit_on_int = (ebit/df_final.interestExpense).to_frame('ebit_on_int')\n",
    "    \n",
    "#     # Profit margin\n",
    "#     profit_margin = (profit/df_final.totalRevenue).to_frame('profit_margin')\n",
    "    \n",
    "#     # Amount of positive profit within a window\n",
    "#     count_positive_profit = count_positives_window(profit,pos_count_window)\n",
    "#     count_positive_profit = count_positive_profit['positive_count'].to_frame('count_positive_profit')\n",
    "    \n",
    "#     # Cash And Short Term Investments / Total Liabilities\n",
    "#     csti_on_liabilities = (df_final.cashAndShortTermInvestments/df_final.totalLiabilities).to_frame('csti_on_liabilities')\n",
    "    \n",
    "#     # Inventory / EBIT\n",
    "#     inventory_on_ebit = (df_final.inventory / ebit).to_frame('inventory_on_ebit')\n",
    "    \n",
    "#     # Receivable / Revenue\n",
    "#     receivable_on_rev = (df_final.currentNetReceivables / df_final.totalRevenue).to_frame('receivable_on_rev')\n",
    "    \n",
    "#     # ROA\n",
    "#     roa = (profit / market_asset).to_frame('roa')\n",
    "    \n",
    "#     # ROE\n",
    "#     roe = (profit / market_equity).to_frame('roe')\n",
    "    \n",
    "#     # Long-term debt / minEBIT\n",
    "#     min_ebit = min_in_window(ebit,min_ebit_window)['min']\n",
    "#     debt_on_min_ebit = (df_final.longTermDebt / min_ebit).to_frame('debt_on_min_ebit')\n",
    "    \n",
    "#     # Total Liabilities / Total Equity\n",
    "#     liabilities_on_equity = (df_final.totalLiabilities / market_equity).to_frame('liabilities_on_equity')\n",
    "    \n",
    "#     # Capital Expenditures / EBIT\n",
    "#     capital_cost_on_ebit = (df_final.capitalExpenditures / ebit).to_frame('capital_cost_on_ebit')\n",
    "    \n",
    "#     # EPS / MP\n",
    "#     eps = profit / df_final.commonStockSharesOutstanding\n",
    "#     eps_on_mp = (eps / df_final.close).to_frame('eps_on_mp')\n",
    "    \n",
    "#     # Cash and Stock Dividend & Repurchase Common / MP\n",
    "#     dividend_on_mp = ((df_final.dividendPayoutCommonStock + df_final.paymentsForRepurchaseOfCommonStock) / df_final.close).to_frame('dividend_on_mp')\n",
    "    \n",
    "#     # MP / BV\n",
    "#     mp_on_bv = (df_final.close / (df_final.cashAndShortTermInvestments + df_final.currentNetReceivables*0.8 +df_final.inventory*0.5 + df_final.propertyPlantEquipment*0.2 + df_final.longTermInvestments - df_final.totalLiabilities)).to_frame('mp_on_bv')\n",
    "\n",
    "#     # Create a dataframe that merges all the ratios\n",
    "#     ratios = pd.concat([df_final.date,df_final.tic,gross_profit_margin,sga_ratio,dep_ratio,\n",
    "#                     ebit_on_int,profit_margin,count_positive_profit,csti_on_liabilities,\n",
    "#                     inventory_on_ebit,receivable_on_rev,roa,roe,debt_on_min_ebit,\n",
    "#                    liabilities_on_equity,capital_cost_on_ebit,eps_on_mp,dividend_on_mp,mp_on_bv], axis=1)\n",
    "\n",
    "#     return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios = compute_fund_ratios(fund_data)\n",
    "# ratios.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check columns with inf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_list = ratios.columns.drop(['date','tic'])\n",
    "# check_ratios = ratios[ratio_list]\n",
    "# inf_cols = check_ratios.columns[~np.isfinite(check_ratios).all()]\n",
    "# inf_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check rows with inf values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_inf = ratios[((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]\n",
    "# print(len(ratios[((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]))\n",
    "# ratio_inf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ebit_on_int** has infinity values for companies without banking support. We need to address these cases separately: <br>\n",
    " * Replace positive inf with maximum value in ebit_on_int\n",
    " * Replace negative inf with minimum value in ebit_on_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_finite = ratios[~((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]\n",
    "# ratios['ebit_on_int'] = ratios.ebit_on_int.replace(np.inf,ratio_finite.ebit_on_int.max())\n",
    "# ratios['ebit_on_int'] = ratios.ebit_on_int.replace(-np.inf,ratio_finite.ebit_on_int.min())\n",
    "# print(len(ratios[((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While data processing greatly reduces the number of inf values, some still remain.<br>\n",
    "Since the remain inf values represent a small portion of the data, we can safely remove the corresponding rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios = ratios[~((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]\n",
    "# ratios.reset_index(inplace=True,drop=True)\n",
    "# check_ratios = ratios[ratio_list]\n",
    "# inf_cols = check_ratios.columns[~np.isfinite(check_ratios).all()]\n",
    "# inf_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.3'></a>\n",
    "## 4.3 Turn the final_ratios to monthly basis¶\n",
    "After our initial inspection, we’ll want to dig deeper to investigate the following:\n",
    "- The data type of each variable.\n",
    "- How discrete/categorical data is coded (and whether we need to make any changes).\n",
    "- How the data are scaled.\n",
    "- Whether there is missing data and how it is coded.\n",
    "- Whether there are outliers.\n",
    "- The distributions of continuous features.\n",
    "- The relationships between pairs of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types\n",
    "It is important to check the data type for each feature. <br>\n",
    "The **date** should be in datetime type <br>\n",
    "The **ratios** should be read in as float64 — and categorical variables should be stored as object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ratios['date'] = pd.to_datetime(ratios['date'],format='mixed')\n",
    "# ratios.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is no missing values from all ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn the final_ratios to monthly basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # UTIL TOOL: Get monthly price for each company\n",
    "\n",
    "# def get_end_month_price (df):\n",
    "#     # Set date as index\n",
    "#     df.set_index('date', inplace=True)\n",
    "#     # Resample by month and get the last day of each month\n",
    "#     last_day_of_month = df.resample('M').last()\n",
    "#     df = df.reset_index()\n",
    "#     return last_day_of_month\n",
    "\n",
    "# df['date'] = pd.to_datetime(df.date,format='mixed')\n",
    "# monthly_price = pd.DataFrame()\n",
    "# for ticket in fund_data.tic.unique():\n",
    "#     price_by_tic = get_end_month_price(df[df.tic == ticket])\n",
    "#     monthly_price = pd.concat([monthly_price, price_by_tic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_date = df.iloc[0].date\n",
    "# end_date = df.iloc[-1].date\n",
    "# date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "# date_range = pd.DataFrame(date_range)\n",
    "# date_range.columns = ['date']\n",
    "# date_range.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate missing data in the middle of years for each ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# daily_ratios = pd.DataFrame()\n",
    "# for ticket in ratios.tic.unique():\n",
    "#     # Interpolate data for each ticket\n",
    "#     one_tic_ratios = pd.merge(date_range,ratios[ratios.tic == ticket],how=\"left\",on=['date'])\n",
    "#     one_tic_ratios['tic'] = ticket\n",
    "#     one_tic_ratios = one_tic_ratios.interpolate('linear')\n",
    "#     daily_ratios = pd.concat([daily_ratios, one_tic_ratios], ignore_index=True)\n",
    "\n",
    "#     # Check data by drawing it out\n",
    "#     # %matplotlib inline\n",
    "#     # plt.figure(figsize=(16, 4)) \n",
    "#     # plt.plot(one_tic_ratios.date, one_tic_ratios.gross_profit_margin, color = 'red')\n",
    "#     # plt.title(f'Gross profit margin of {ticket}')\n",
    "#     # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily_ratios.dropna(subset='gross_profit_margin',inplace=True)\n",
    "# daily_ratios.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.4'></a>\n",
    "## 4.4 Merge stock price data and ratios into one dataframe\n",
    "- Merge the price dataframe preprocessed in Part 3 and the ratio dataframe created in this part\n",
    "- Since the prices are daily and ratios are quartely, we have NAs in the ratio columns after merging the two dataframes. We deal with this by backfilling the ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_date = pd.DataFrame(df)[['date','tic','close']]\n",
    "# df_date.columns = ['date','tic','close']\n",
    "# df_date['date'] = pd.to_datetime(df_date.date)\n",
    "# final_ratios = pd.merge(df_date,daily_ratios,how=\"left\",on=['date','tic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_ratios.dropna(subset=['gross_profit_margin'],inplace=True,how='any')\n",
    "# final_ratios.reset_index(drop=True,inplace=True)\n",
    "# final_ratios.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_ratios.to_csv('./' + DATA_SAVE_DIR + '/sp500_ready_data_monthly.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.5'></a>\n",
    "## 4.5 Finish data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>gross_profit_margin</th>\n",
       "      <th>sga_ratio</th>\n",
       "      <th>dep_ratio</th>\n",
       "      <th>ebit_on_int</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>count_positive_profit</th>\n",
       "      <th>csti_on_liabilities</th>\n",
       "      <th>inventory_on_ebit</th>\n",
       "      <th>receivable_on_rev</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>debt_on_min_ebit</th>\n",
       "      <th>liabilities_on_equity</th>\n",
       "      <th>capital_cost_on_ebit</th>\n",
       "      <th>eps_on_mp</th>\n",
       "      <th>dividend_on_mp</th>\n",
       "      <th>mp_on_bv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-12-31</td>\n",
       "      <td>WYNN</td>\n",
       "      <td>73.214867</td>\n",
       "      <td>0.211086</td>\n",
       "      <td>0.442929</td>\n",
       "      <td>0.447057</td>\n",
       "      <td>2.207989</td>\n",
       "      <td>0.059447</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.293089</td>\n",
       "      <td>0.176490</td>\n",
       "      <td>0.044797</td>\n",
       "      <td>0.018542</td>\n",
       "      <td>0.027270</td>\n",
       "      <td>6.640253</td>\n",
       "      <td>0.470693</td>\n",
       "      <td>0.576794</td>\n",
       "      <td>0.027270</td>\n",
       "      <td>1.628273e+07</td>\n",
       "      <td>-3.950014e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-31</td>\n",
       "      <td>WYNN</td>\n",
       "      <td>82.021240</td>\n",
       "      <td>0.213774</td>\n",
       "      <td>0.431861</td>\n",
       "      <td>0.435689</td>\n",
       "      <td>2.336475</td>\n",
       "      <td>0.065084</td>\n",
       "      <td>7.915068</td>\n",
       "      <td>0.293346</td>\n",
       "      <td>0.168655</td>\n",
       "      <td>0.044943</td>\n",
       "      <td>0.020557</td>\n",
       "      <td>0.030128</td>\n",
       "      <td>6.528927</td>\n",
       "      <td>0.468225</td>\n",
       "      <td>0.546087</td>\n",
       "      <td>0.030128</td>\n",
       "      <td>1.594436e+07</td>\n",
       "      <td>-3.974793e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-02-28</td>\n",
       "      <td>WYNN</td>\n",
       "      <td>86.674721</td>\n",
       "      <td>0.216202</td>\n",
       "      <td>0.421864</td>\n",
       "      <td>0.425421</td>\n",
       "      <td>2.452528</td>\n",
       "      <td>0.070176</td>\n",
       "      <td>7.838356</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>0.161577</td>\n",
       "      <td>0.045075</td>\n",
       "      <td>0.022376</td>\n",
       "      <td>0.032710</td>\n",
       "      <td>6.428374</td>\n",
       "      <td>0.465995</td>\n",
       "      <td>0.518353</td>\n",
       "      <td>0.032710</td>\n",
       "      <td>1.563873e+07</td>\n",
       "      <td>-3.997174e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>WYNN</td>\n",
       "      <td>89.657188</td>\n",
       "      <td>0.218889</td>\n",
       "      <td>0.410796</td>\n",
       "      <td>0.414053</td>\n",
       "      <td>2.581014</td>\n",
       "      <td>0.075813</td>\n",
       "      <td>7.753425</td>\n",
       "      <td>0.293836</td>\n",
       "      <td>0.153742</td>\n",
       "      <td>0.045221</td>\n",
       "      <td>0.024391</td>\n",
       "      <td>0.035569</td>\n",
       "      <td>6.317048</td>\n",
       "      <td>0.463527</td>\n",
       "      <td>0.487647</td>\n",
       "      <td>0.035569</td>\n",
       "      <td>1.530035e+07</td>\n",
       "      <td>-4.021954e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-04-30</td>\n",
       "      <td>WYNN</td>\n",
       "      <td>104.108864</td>\n",
       "      <td>0.221490</td>\n",
       "      <td>0.400084</td>\n",
       "      <td>0.403052</td>\n",
       "      <td>2.705356</td>\n",
       "      <td>0.081269</td>\n",
       "      <td>7.671233</td>\n",
       "      <td>0.294084</td>\n",
       "      <td>0.146159</td>\n",
       "      <td>0.045362</td>\n",
       "      <td>0.026341</td>\n",
       "      <td>0.038335</td>\n",
       "      <td>6.209312</td>\n",
       "      <td>0.461139</td>\n",
       "      <td>0.457931</td>\n",
       "      <td>0.038335</td>\n",
       "      <td>1.497289e+07</td>\n",
       "      <td>-4.045934e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77024</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>DLR</td>\n",
       "      <td>136.375320</td>\n",
       "      <td>0.585775</td>\n",
       "      <td>0.141156</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.425107</td>\n",
       "      <td>0.411360</td>\n",
       "      <td>1.084932</td>\n",
       "      <td>0.064895</td>\n",
       "      <td>0.064176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035026</td>\n",
       "      <td>0.055106</td>\n",
       "      <td>-3.871747</td>\n",
       "      <td>0.571512</td>\n",
       "      <td>1.265520</td>\n",
       "      <td>0.055106</td>\n",
       "      <td>1.163776e+07</td>\n",
       "      <td>-6.910170e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77025</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>DLR</td>\n",
       "      <td>133.439255</td>\n",
       "      <td>0.586954</td>\n",
       "      <td>0.139685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.318181</td>\n",
       "      <td>0.411244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>-3.660527</td>\n",
       "      <td>0.553281</td>\n",
       "      <td>1.274745</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>1.134100e+07</td>\n",
       "      <td>-7.065183e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77026</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>DLR</td>\n",
       "      <td>139.269424</td>\n",
       "      <td>0.586954</td>\n",
       "      <td>0.139685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.318181</td>\n",
       "      <td>0.411244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>-3.660527</td>\n",
       "      <td>0.553281</td>\n",
       "      <td>1.274745</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>1.134100e+07</td>\n",
       "      <td>-7.065183e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77027</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>DLR</td>\n",
       "      <td>145.565582</td>\n",
       "      <td>0.586954</td>\n",
       "      <td>0.139685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.318181</td>\n",
       "      <td>0.411244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>-3.660527</td>\n",
       "      <td>0.553281</td>\n",
       "      <td>1.274745</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>1.134100e+07</td>\n",
       "      <td>-7.065183e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77028</th>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>DLR</td>\n",
       "      <td>142.210007</td>\n",
       "      <td>0.586954</td>\n",
       "      <td>0.139685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.318181</td>\n",
       "      <td>0.411244</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.070316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034707</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>-3.660527</td>\n",
       "      <td>0.553281</td>\n",
       "      <td>1.274745</td>\n",
       "      <td>0.053909</td>\n",
       "      <td>1.134100e+07</td>\n",
       "      <td>-7.065183e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>77029 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date   tic       close  gross_profit_margin  sga_ratio  dep_ratio  \\\n",
       "0     2010-12-31  WYNN   73.214867             0.211086   0.442929   0.447057   \n",
       "1     2011-01-31  WYNN   82.021240             0.213774   0.431861   0.435689   \n",
       "2     2011-02-28  WYNN   86.674721             0.216202   0.421864   0.425421   \n",
       "3     2011-03-31  WYNN   89.657188             0.218889   0.410796   0.414053   \n",
       "4     2011-04-30  WYNN  104.108864             0.221490   0.400084   0.403052   \n",
       "...          ...   ...         ...                  ...        ...        ...   \n",
       "77024 2023-11-30   DLR  136.375320             0.585775   0.141156   0.000000   \n",
       "77025 2023-12-31   DLR  133.439255             0.586954   0.139685   0.000000   \n",
       "77026 2024-01-31   DLR  139.269424             0.586954   0.139685   0.000000   \n",
       "77027 2024-02-29   DLR  145.565582             0.586954   0.139685   0.000000   \n",
       "77028 2024-03-31   DLR  142.210007             0.586954   0.139685   0.000000   \n",
       "\n",
       "       ebit_on_int  profit_margin  count_positive_profit  csti_on_liabilities  \\\n",
       "0         2.207989       0.059447               8.000000             0.293089   \n",
       "1         2.336475       0.065084               7.915068             0.293346   \n",
       "2         2.452528       0.070176               7.838356             0.293578   \n",
       "3         2.581014       0.075813               7.753425             0.293836   \n",
       "4         2.705356       0.081269               7.671233             0.294084   \n",
       "...            ...            ...                    ...                  ...   \n",
       "77024     6.425107       0.411360               1.084932             0.064895   \n",
       "77025     6.318181       0.411244               1.000000             0.070316   \n",
       "77026     6.318181       0.411244               1.000000             0.070316   \n",
       "77027     6.318181       0.411244               1.000000             0.070316   \n",
       "77028     6.318181       0.411244               1.000000             0.070316   \n",
       "\n",
       "       inventory_on_ebit  receivable_on_rev       roa       roe  \\\n",
       "0               0.176490           0.044797  0.018542  0.027270   \n",
       "1               0.168655           0.044943  0.020557  0.030128   \n",
       "2               0.161577           0.045075  0.022376  0.032710   \n",
       "3               0.153742           0.045221  0.024391  0.035569   \n",
       "4               0.146159           0.045362  0.026341  0.038335   \n",
       "...                  ...                ...       ...       ...   \n",
       "77024           0.064176           0.000000  0.035026  0.055106   \n",
       "77025           0.000000           0.000000  0.034707  0.053909   \n",
       "77026           0.000000           0.000000  0.034707  0.053909   \n",
       "77027           0.000000           0.000000  0.034707  0.053909   \n",
       "77028           0.000000           0.000000  0.034707  0.053909   \n",
       "\n",
       "       debt_on_min_ebit  liabilities_on_equity  capital_cost_on_ebit  \\\n",
       "0              6.640253               0.470693              0.576794   \n",
       "1              6.528927               0.468225              0.546087   \n",
       "2              6.428374               0.465995              0.518353   \n",
       "3              6.317048               0.463527              0.487647   \n",
       "4              6.209312               0.461139              0.457931   \n",
       "...                 ...                    ...                   ...   \n",
       "77024         -3.871747               0.571512              1.265520   \n",
       "77025         -3.660527               0.553281              1.274745   \n",
       "77026         -3.660527               0.553281              1.274745   \n",
       "77027         -3.660527               0.553281              1.274745   \n",
       "77028         -3.660527               0.553281              1.274745   \n",
       "\n",
       "       eps_on_mp  dividend_on_mp      mp_on_bv  \n",
       "0       0.027270    1.628273e+07 -3.950014e-08  \n",
       "1       0.030128    1.594436e+07 -3.974793e-08  \n",
       "2       0.032710    1.563873e+07 -3.997174e-08  \n",
       "3       0.035569    1.530035e+07 -4.021954e-08  \n",
       "4       0.038335    1.497289e+07 -4.045934e-08  \n",
       "...          ...             ...           ...  \n",
       "77024   0.055106    1.163776e+07 -6.910170e-09  \n",
       "77025   0.053909    1.134100e+07 -7.065183e-09  \n",
       "77026   0.053909    1.134100e+07 -7.065183e-09  \n",
       "77027   0.053909    1.134100e+07 -7.065183e-09  \n",
       "77028   0.053909    1.134100e+07 -7.065183e-09  \n",
       "\n",
       "[77029 rows x 20 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the data is available in the data storage, load processed_full from readied data\n",
    "processed_full = pd.read_csv('./' + DATA_SAVE_DIR + '/sp500_ready_data_monthly.csv',index_col=0)\n",
    "processed_full['date'] = pd.to_datetime(processed_full.date,format='mixed')\n",
    "processed_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 77029 entries, 0 to 77028\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   date                   77029 non-null  datetime64[ns]\n",
      " 1   tic                    77029 non-null  object        \n",
      " 2   close                  77029 non-null  float64       \n",
      " 3   gross_profit_margin    77029 non-null  float64       \n",
      " 4   sga_ratio              77029 non-null  float64       \n",
      " 5   dep_ratio              77029 non-null  float64       \n",
      " 6   ebit_on_int            77029 non-null  float64       \n",
      " 7   profit_margin          77029 non-null  float64       \n",
      " 8   count_positive_profit  77029 non-null  float64       \n",
      " 9   csti_on_liabilities    77029 non-null  float64       \n",
      " 10  inventory_on_ebit      77029 non-null  float64       \n",
      " 11  receivable_on_rev      77029 non-null  float64       \n",
      " 12  roa                    77029 non-null  float64       \n",
      " 13  roe                    77029 non-null  float64       \n",
      " 14  debt_on_min_ebit       77029 non-null  float64       \n",
      " 15  liabilities_on_equity  77029 non-null  float64       \n",
      " 16  capital_cost_on_ebit   77029 non-null  float64       \n",
      " 17  eps_on_mp              77029 non-null  float64       \n",
      " 18  dividend_on_mp         77029 non-null  float64       \n",
      " 19  mp_on_bv               77029 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(18), object(1)\n",
      "memory usage: 12.3+ MB\n"
     ]
    }
   ],
   "source": [
    "processed_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['WBA', 'AAPL', 'MSFT', 'V', 'UNH', 'JNJ', 'JPM', 'WMT', 'PG', 'HD',\n",
       "       'CVX', 'MRK', 'KO', 'CSCO', 'MCD', 'CRM', 'NKE', 'DIS', 'AMGN',\n",
       "       'CAT', 'VZ', 'INTC', 'BA', 'IBM', 'HON', 'AXP', 'GS', 'MMM', 'DOW',\n",
       "       'TRV'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_full = processed_full[processed_full.tic.isin(DOW_30_TICKER)]\n",
    "processed_full.tic.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset **TRAIN_START_DATE** and **TEST_END_DATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_START_DATE:  2009-09-30 \n",
      "\n",
      "TRAIN_END_DATE:  2021-01-01 \n",
      "\n",
      "TEST_START_DATE:  2021-01-01 \n",
      "\n",
      "TEST_END_DATE:  2024-03-31 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START_DATE = processed_full.date.min().strftime(\"%Y-%m-%d\")\n",
    "TEST_END_DATE = processed_full.date.max().strftime(\"%Y-%m-%d\")\n",
    "print('TRAIN_START_DATE: ',TRAIN_START_DATE,'\\n')\n",
    "print('TRAIN_END_DATE: ',TRAIN_END_DATE,'\\n')\n",
    "print('TEST_START_DATE: ',TEST_START_DATE,'\\n')\n",
    "print('TEST_END_DATE: ',TEST_END_DATE,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "## 5.1 Training & Trade Data Split\n",
    "- Training data period: 2009-01-01 to 2020-01-01\n",
    "- Trade data period: 2020-01-01 to 2023-12-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3655\n",
      "1140\n"
     ]
    }
   ],
   "source": [
    "train_data = data_split(processed_full, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "trade_data = data_split(processed_full, TEST_START_DATE, TEST_END_DATE)\n",
    "# Check the length of the two datasets\n",
    "print(len(train_data))\n",
    "print(len(trade_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>gross_profit_margin</th>\n",
       "      <th>sga_ratio</th>\n",
       "      <th>dep_ratio</th>\n",
       "      <th>ebit_on_int</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>count_positive_profit</th>\n",
       "      <th>csti_on_liabilities</th>\n",
       "      <th>inventory_on_ebit</th>\n",
       "      <th>receivable_on_rev</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>debt_on_min_ebit</th>\n",
       "      <th>liabilities_on_equity</th>\n",
       "      <th>capital_cost_on_ebit</th>\n",
       "      <th>eps_on_mp</th>\n",
       "      <th>dividend_on_mp</th>\n",
       "      <th>mp_on_bv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>5.604187</td>\n",
       "      <td>0.304696</td>\n",
       "      <td>0.317372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.374233</td>\n",
       "      <td>0.111106</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.479352</td>\n",
       "      <td>0.050986</td>\n",
       "      <td>0.078336</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.945331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.145352</td>\n",
       "      <td>0.128194</td>\n",
       "      <td>0.945331</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.618375e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>V</td>\n",
       "      <td>15.578794</td>\n",
       "      <td>0.732890</td>\n",
       "      <td>0.247976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>33.121739</td>\n",
       "      <td>0.296050</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.670885</td>\n",
       "      <td>0.639013</td>\n",
       "      <td>0.064245</td>\n",
       "      <td>0.091882</td>\n",
       "      <td>0.155239</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.689547</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.155239</td>\n",
       "      <td>1.902586e+08</td>\n",
       "      <td>-1.542455e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-10-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>5.699428</td>\n",
       "      <td>0.304017</td>\n",
       "      <td>0.314630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32.630120</td>\n",
       "      <td>0.113588</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.433146</td>\n",
       "      <td>0.053108</td>\n",
       "      <td>0.078857</td>\n",
       "      <td>0.230731</td>\n",
       "      <td>0.963965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.174256</td>\n",
       "      <td>0.129614</td>\n",
       "      <td>0.963965</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.647365e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-10-31</td>\n",
       "      <td>V</td>\n",
       "      <td>17.077837</td>\n",
       "      <td>0.735809</td>\n",
       "      <td>0.245074</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.047435</td>\n",
       "      <td>0.303752</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.673774</td>\n",
       "      <td>0.632417</td>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.095074</td>\n",
       "      <td>0.158929</td>\n",
       "      <td>0.014221</td>\n",
       "      <td>0.676396</td>\n",
       "      <td>0.077720</td>\n",
       "      <td>0.158929</td>\n",
       "      <td>1.809975e+08</td>\n",
       "      <td>-1.856696e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-11-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>6.044419</td>\n",
       "      <td>0.303359</td>\n",
       "      <td>0.311976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>37.716462</td>\n",
       "      <td>0.115990</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.388430</td>\n",
       "      <td>0.055161</td>\n",
       "      <td>0.079362</td>\n",
       "      <td>0.233329</td>\n",
       "      <td>0.981998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.202228</td>\n",
       "      <td>0.130989</td>\n",
       "      <td>0.981998</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.675419e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3650</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>UNH</td>\n",
       "      <td>335.059784</td>\n",
       "      <td>0.707009</td>\n",
       "      <td>0.229394</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>84.243536</td>\n",
       "      <td>0.519019</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.156063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.098794</td>\n",
       "      <td>0.300780</td>\n",
       "      <td>0.421057</td>\n",
       "      <td>0.400580</td>\n",
       "      <td>0.399884</td>\n",
       "      <td>0.014640</td>\n",
       "      <td>0.421057</td>\n",
       "      <td>2.636544e+07</td>\n",
       "      <td>-7.671028e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3651</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>V</td>\n",
       "      <td>213.664169</td>\n",
       "      <td>0.697420</td>\n",
       "      <td>0.133718</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.193962</td>\n",
       "      <td>0.441171</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.441947</td>\n",
       "      <td>0.269140</td>\n",
       "      <td>0.080325</td>\n",
       "      <td>0.022702</td>\n",
       "      <td>0.025326</td>\n",
       "      <td>3.067416</td>\n",
       "      <td>0.115467</td>\n",
       "      <td>0.054096</td>\n",
       "      <td>0.025326</td>\n",
       "      <td>5.456423e+07</td>\n",
       "      <td>-9.743528e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3652</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>VZ</td>\n",
       "      <td>48.816624</td>\n",
       "      <td>0.618534</td>\n",
       "      <td>0.397880</td>\n",
       "      <td>0.179892</td>\n",
       "      <td>11.250294</td>\n",
       "      <td>0.295529</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.078657</td>\n",
       "      <td>0.037589</td>\n",
       "      <td>0.186426</td>\n",
       "      <td>0.078316</td>\n",
       "      <td>0.187464</td>\n",
       "      <td>2.972570</td>\n",
       "      <td>1.393680</td>\n",
       "      <td>0.425241</td>\n",
       "      <td>0.187464</td>\n",
       "      <td>2.096007e+08</td>\n",
       "      <td>-2.217831e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3653</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>WBA</td>\n",
       "      <td>33.866371</td>\n",
       "      <td>-0.009526</td>\n",
       "      <td>-1.021455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-39.212650</td>\n",
       "      <td>-0.202215</td>\n",
       "      <td>6.334247</td>\n",
       "      <td>0.012180</td>\n",
       "      <td>-0.305243</td>\n",
       "      <td>0.048514</td>\n",
       "      <td>-0.293656</td>\n",
       "      <td>-0.918168</td>\n",
       "      <td>-0.310980</td>\n",
       "      <td>2.088399</td>\n",
       "      <td>-0.052435</td>\n",
       "      <td>-0.918168</td>\n",
       "      <td>8.263157e+07</td>\n",
       "      <td>-8.529535e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3654</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>WMT</td>\n",
       "      <td>45.710587</td>\n",
       "      <td>0.248178</td>\n",
       "      <td>0.837881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.754111</td>\n",
       "      <td>0.024076</td>\n",
       "      <td>4.084699</td>\n",
       "      <td>0.176538</td>\n",
       "      <td>2.007618</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>0.045633</td>\n",
       "      <td>0.109269</td>\n",
       "      <td>2.188798</td>\n",
       "      <td>1.392779</td>\n",
       "      <td>0.460734</td>\n",
       "      <td>0.109269</td>\n",
       "      <td>2.090669e+08</td>\n",
       "      <td>-4.641799e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3655 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   tic       close  gross_profit_margin  sga_ratio  dep_ratio  \\\n",
       "0    2009-09-30  AAPL    5.604187             0.304696   0.317372   0.000000   \n",
       "1    2009-09-30     V   15.578794             0.732890   0.247976   0.000000   \n",
       "2    2009-10-31  AAPL    5.699428             0.304017   0.314630   0.000000   \n",
       "3    2009-10-31     V   17.077837             0.735809   0.245074   0.000000   \n",
       "4    2009-11-30  AAPL    6.044419             0.303359   0.311976   0.000000   \n",
       "...         ...   ...         ...                  ...        ...        ...   \n",
       "3650 2020-12-31   UNH  335.059784             0.707009   0.229394   0.005501   \n",
       "3651 2020-12-31     V  213.664169             0.697420   0.133718   0.000000   \n",
       "3652 2020-12-31    VZ   48.816624             0.618534   0.397880   0.179892   \n",
       "3653 2020-12-31   WBA   33.866371            -0.009526  -1.021455   0.000000   \n",
       "3654 2020-12-31   WMT   45.710587             0.248178   0.837881   0.000000   \n",
       "\n",
       "      ebit_on_int  profit_margin  count_positive_profit  csti_on_liabilities  \\\n",
       "0       27.374233       0.111106              10.000000             1.479352   \n",
       "1       33.121739       0.296050              10.000000             0.670885   \n",
       "2       32.630120       0.113588              10.000000             1.433146   \n",
       "3       36.047435       0.303752              10.000000             0.673774   \n",
       "4       37.716462       0.115990              10.000000             1.388430   \n",
       "...           ...            ...                    ...                  ...   \n",
       "3650    84.243536       0.519019              10.000000             0.156063   \n",
       "3651    26.193962       0.441171               9.000000             0.441947   \n",
       "3652    11.250294       0.295529              10.000000             0.078657   \n",
       "3653   -39.212650      -0.202215               6.334247             0.012180   \n",
       "3654     9.754111       0.024076               4.084699             0.176538   \n",
       "\n",
       "      inventory_on_ebit  receivable_on_rev       roa       roe  \\\n",
       "0              0.050986           0.078336  0.228046  0.945331   \n",
       "1              0.639013           0.064245  0.091882  0.155239   \n",
       "2              0.053108           0.078857  0.230731  0.963965   \n",
       "3              0.632417           0.063802  0.095074  0.158929   \n",
       "4              0.055161           0.079362  0.233329  0.981998   \n",
       "...                 ...                ...       ...       ...   \n",
       "3650           0.000000           0.098794  0.300780  0.421057   \n",
       "3651           0.269140           0.080325  0.022702  0.025326   \n",
       "3652           0.037589           0.186426  0.078316  0.187464   \n",
       "3653          -0.305243           0.048514 -0.293656 -0.918168   \n",
       "3654           2.007618           0.011682  0.045633  0.109269   \n",
       "\n",
       "      debt_on_min_ebit  liabilities_on_equity  capital_cost_on_ebit  \\\n",
       "0             0.000000               3.145352              0.128194   \n",
       "1             0.014702               0.689547              0.080336   \n",
       "2             0.000000               3.174256              0.129614   \n",
       "3             0.014221               0.676396              0.077720   \n",
       "4             0.000000               3.202228              0.130989   \n",
       "...                ...                    ...                   ...   \n",
       "3650          0.400580               0.399884              0.014640   \n",
       "3651          3.067416               0.115467              0.054096   \n",
       "3652          2.972570               1.393680              0.425241   \n",
       "3653         -0.310980               2.088399             -0.052435   \n",
       "3654          2.188798               1.392779              0.460734   \n",
       "\n",
       "      eps_on_mp  dividend_on_mp      mp_on_bv  \n",
       "0      0.945331    0.000000e+00  2.618375e-10  \n",
       "1      0.155239    1.902586e+08 -1.542455e-08  \n",
       "2      0.963965    0.000000e+00  2.647365e-10  \n",
       "3      0.158929    1.809975e+08 -1.856696e-08  \n",
       "4      0.981998    0.000000e+00  2.675419e-10  \n",
       "...         ...             ...           ...  \n",
       "3650   0.421057    2.636544e+07 -7.671028e-09  \n",
       "3651   0.025326    5.456423e+07 -9.743528e-09  \n",
       "3652   0.187464    2.096007e+08 -2.217831e-10  \n",
       "3653  -0.918168    8.263157e+07 -8.529535e-10  \n",
       "3654   0.109269    2.090669e+08 -4.641799e-10  \n",
       "\n",
       "[3655 rows x 20 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.reset_index(drop=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>gross_profit_margin</th>\n",
       "      <th>sga_ratio</th>\n",
       "      <th>dep_ratio</th>\n",
       "      <th>ebit_on_int</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>count_positive_profit</th>\n",
       "      <th>csti_on_liabilities</th>\n",
       "      <th>inventory_on_ebit</th>\n",
       "      <th>receivable_on_rev</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>debt_on_min_ebit</th>\n",
       "      <th>liabilities_on_equity</th>\n",
       "      <th>capital_cost_on_ebit</th>\n",
       "      <th>eps_on_mp</th>\n",
       "      <th>dividend_on_mp</th>\n",
       "      <th>mp_on_bv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>129.504669</td>\n",
       "      <td>0.319523</td>\n",
       "      <td>0.218454</td>\n",
       "      <td>0.103238</td>\n",
       "      <td>27.902059</td>\n",
       "      <td>0.203952</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.306527</td>\n",
       "      <td>0.064123</td>\n",
       "      <td>0.139187</td>\n",
       "      <td>0.026718</td>\n",
       "      <td>0.030204</td>\n",
       "      <td>14.161230</td>\n",
       "      <td>0.131329</td>\n",
       "      <td>0.113005</td>\n",
       "      <td>0.030204</td>\n",
       "      <td>7.477079e+08</td>\n",
       "      <td>-3.792017e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>217.708099</td>\n",
       "      <td>0.711596</td>\n",
       "      <td>0.302525</td>\n",
       "      <td>0.033894</td>\n",
       "      <td>11.155836</td>\n",
       "      <td>0.416334</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.203956</td>\n",
       "      <td>0.299618</td>\n",
       "      <td>0.201385</td>\n",
       "      <td>0.068308</td>\n",
       "      <td>0.099096</td>\n",
       "      <td>25.551601</td>\n",
       "      <td>0.448228</td>\n",
       "      <td>0.047456</td>\n",
       "      <td>0.099096</td>\n",
       "      <td>3.560891e+07</td>\n",
       "      <td>-5.854302e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>AXP</td>\n",
       "      <td>111.826149</td>\n",
       "      <td>0.176514</td>\n",
       "      <td>1.697894</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.105818</td>\n",
       "      <td>-0.202210</td>\n",
       "      <td>5.084932</td>\n",
       "      <td>0.187780</td>\n",
       "      <td>-0.719761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.029549</td>\n",
       "      <td>-0.082446</td>\n",
       "      <td>-12.795540</td>\n",
       "      <td>1.768630</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>-0.082446</td>\n",
       "      <td>2.463522e+07</td>\n",
       "      <td>-1.551313e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>BA</td>\n",
       "      <td>194.190002</td>\n",
       "      <td>-0.085335</td>\n",
       "      <td>-0.658330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.249637</td>\n",
       "      <td>-0.165574</td>\n",
       "      <td>6.084932</td>\n",
       "      <td>0.146597</td>\n",
       "      <td>-1.805860</td>\n",
       "      <td>0.034362</td>\n",
       "      <td>-0.024994</td>\n",
       "      <td>-0.044602</td>\n",
       "      <td>-9.837667</td>\n",
       "      <td>0.782739</td>\n",
       "      <td>-0.186545</td>\n",
       "      <td>-0.044602</td>\n",
       "      <td>4.960369e+06</td>\n",
       "      <td>-1.598125e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>CAT</td>\n",
       "      <td>171.592255</td>\n",
       "      <td>0.303381</td>\n",
       "      <td>0.364843</td>\n",
       "      <td>0.164570</td>\n",
       "      <td>16.041624</td>\n",
       "      <td>0.155662</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.147812</td>\n",
       "      <td>1.418429</td>\n",
       "      <td>0.396750</td>\n",
       "      <td>0.042134</td>\n",
       "      <td>0.070558</td>\n",
       "      <td>3.278032</td>\n",
       "      <td>0.674961</td>\n",
       "      <td>0.261999</td>\n",
       "      <td>0.070558</td>\n",
       "      <td>2.032006e+07</td>\n",
       "      <td>-9.165359e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>UNH</td>\n",
       "      <td>491.661804</td>\n",
       "      <td>0.737986</td>\n",
       "      <td>0.199189</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>67.659889</td>\n",
       "      <td>0.566194</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.169496</td>\n",
       "      <td>0.012749</td>\n",
       "      <td>0.104865</td>\n",
       "      <td>0.313486</td>\n",
       "      <td>0.423878</td>\n",
       "      <td>16.703334</td>\n",
       "      <td>0.352142</td>\n",
       "      <td>0.015417</td>\n",
       "      <td>0.423878</td>\n",
       "      <td>2.747652e+07</td>\n",
       "      <td>-8.568411e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>V</td>\n",
       "      <td>282.640015</td>\n",
       "      <td>0.704715</td>\n",
       "      <td>0.118495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-30.854037</td>\n",
       "      <td>0.523667</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.314608</td>\n",
       "      <td>0.240010</td>\n",
       "      <td>0.078065</td>\n",
       "      <td>0.035065</td>\n",
       "      <td>0.039327</td>\n",
       "      <td>13.643292</td>\n",
       "      <td>0.121541</td>\n",
       "      <td>0.053296</td>\n",
       "      <td>0.039327</td>\n",
       "      <td>6.881727e+07</td>\n",
       "      <td>-8.084724e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>VZ</td>\n",
       "      <td>40.020000</td>\n",
       "      <td>0.567685</td>\n",
       "      <td>0.430544</td>\n",
       "      <td>0.196397</td>\n",
       "      <td>7.840333</td>\n",
       "      <td>0.245525</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.006308</td>\n",
       "      <td>0.047495</td>\n",
       "      <td>0.194038</td>\n",
       "      <td>0.067325</td>\n",
       "      <td>0.204009</td>\n",
       "      <td>74.999502</td>\n",
       "      <td>2.030192</td>\n",
       "      <td>0.567144</td>\n",
       "      <td>0.204009</td>\n",
       "      <td>2.883466e+08</td>\n",
       "      <td>-1.361466e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>WBA</td>\n",
       "      <td>21.260000</td>\n",
       "      <td>-0.066540</td>\n",
       "      <td>-3.659463</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-75.089655</td>\n",
       "      <td>-0.300945</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.010845</td>\n",
       "      <td>-0.189589</td>\n",
       "      <td>0.038307</td>\n",
       "      <td>-0.473656</td>\n",
       "      <td>-2.002519</td>\n",
       "      <td>-0.208762</td>\n",
       "      <td>3.227792</td>\n",
       "      <td>-0.048609</td>\n",
       "      <td>-2.002519</td>\n",
       "      <td>7.399533e+07</td>\n",
       "      <td>-4.587726e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>WMT</td>\n",
       "      <td>58.411064</td>\n",
       "      <td>0.243754</td>\n",
       "      <td>0.829020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.449516</td>\n",
       "      <td>0.029082</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>2.032134</td>\n",
       "      <td>0.013571</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.014171</td>\n",
       "      <td>-6.715134</td>\n",
       "      <td>0.126712</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0.014171</td>\n",
       "      <td>1.624698e+08</td>\n",
       "      <td>-5.866425e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   tic       close  gross_profit_margin  sga_ratio  dep_ratio  \\\n",
       "0    2021-01-31  AAPL  129.504669             0.319523   0.218454   0.103238   \n",
       "1    2021-01-31  AMGN  217.708099             0.711596   0.302525   0.033894   \n",
       "2    2021-01-31   AXP  111.826149             0.176514   1.697894   0.000000   \n",
       "3    2021-01-31    BA  194.190002            -0.085335  -0.658330   0.000000   \n",
       "4    2021-01-31   CAT  171.592255             0.303381   0.364843   0.164570   \n",
       "...         ...   ...         ...                  ...        ...        ...   \n",
       "1135 2024-02-29   UNH  491.661804             0.737986   0.199189   0.004011   \n",
       "1136 2024-02-29     V  282.640015             0.704715   0.118495   0.000000   \n",
       "1137 2024-02-29    VZ   40.020000             0.567685   0.430544   0.196397   \n",
       "1138 2024-02-29   WBA   21.260000            -0.066540  -3.659463  -0.000000   \n",
       "1139 2024-02-29   WMT   58.411064             0.243754   0.829020   0.000000   \n",
       "\n",
       "      ebit_on_int  profit_margin  count_positive_profit  csti_on_liabilities  \\\n",
       "0       27.902059       0.203952              10.000000             0.306527   \n",
       "1       11.155836       0.416334               7.000000             0.203956   \n",
       "2       -2.105818      -0.202210               5.084932             0.187780   \n",
       "3       -4.249637      -0.165574               6.084932             0.146597   \n",
       "4       16.041624       0.155662              10.000000             0.147812   \n",
       "...           ...            ...                    ...                  ...   \n",
       "1135    67.659889       0.566194              10.000000             0.169496   \n",
       "1136   -30.854037       0.523667               8.000000             0.314608   \n",
       "1137     7.840333       0.245525              10.000000             0.006308   \n",
       "1138   -75.089655      -0.300945               9.000000             0.010845   \n",
       "1139    10.449516       0.029082               1.000000             0.108676   \n",
       "\n",
       "      inventory_on_ebit  receivable_on_rev       roa       roe  \\\n",
       "0              0.064123           0.139187  0.026718  0.030204   \n",
       "1              0.299618           0.201385  0.068308  0.099096   \n",
       "2             -0.719761           0.000000 -0.029549 -0.082446   \n",
       "3             -1.805860           0.034362 -0.024994 -0.044602   \n",
       "4              1.418429           0.396750  0.042134  0.070558   \n",
       "...                 ...                ...       ...       ...   \n",
       "1135           0.012749           0.104865  0.313486  0.423878   \n",
       "1136           0.240010           0.078065  0.035065  0.039327   \n",
       "1137           0.047495           0.194038  0.067325  0.204009   \n",
       "1138          -0.189589           0.038307 -0.473656 -2.002519   \n",
       "1139           2.032134           0.013571  0.012578  0.014171   \n",
       "\n",
       "      debt_on_min_ebit  liabilities_on_equity  capital_cost_on_ebit  \\\n",
       "0            14.161230               0.131329              0.113005   \n",
       "1            25.551601               0.448228              0.047456   \n",
       "2           -12.795540               1.768630              0.001715   \n",
       "3            -9.837667               0.782739             -0.186545   \n",
       "4             3.278032               0.674961              0.261999   \n",
       "...                ...                    ...                   ...   \n",
       "1135         16.703334               0.352142              0.015417   \n",
       "1136         13.643292               0.121541              0.053296   \n",
       "1137         74.999502               2.030192              0.567144   \n",
       "1138         -0.208762               3.227792             -0.048609   \n",
       "1139         -6.715134               0.126712              0.762846   \n",
       "\n",
       "      eps_on_mp  dividend_on_mp      mp_on_bv  \n",
       "0      0.030204    7.477079e+08 -3.792017e-09  \n",
       "1      0.099096    3.560891e+07 -5.854302e-09  \n",
       "2     -0.082446    2.463522e+07 -1.551313e-09  \n",
       "3     -0.044602    4.960369e+06 -1.598125e-09  \n",
       "4      0.070558    2.032006e+07 -9.165359e-09  \n",
       "...         ...             ...           ...  \n",
       "1135   0.423878    2.747652e+07 -8.568411e-09  \n",
       "1136   0.039327    6.881727e+07 -8.084724e-09  \n",
       "1137   0.204009    2.883466e+08 -1.361466e-10  \n",
       "1138  -2.002519    7.399533e+07 -4.587726e-10  \n",
       "1139   0.014171    1.624698e+08 -5.866425e-10  \n",
       "\n",
       "[1140 rows x 20 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_data = trade_data.reset_index(drop=True)\n",
    "trade_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "## 5.2 Set up the training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# from gym.spaces import Box\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# from stable_baselines3.common import logger\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        # stock_dim,\n",
    "        hmax,\n",
    "        initial_amount,\n",
    "        buy_cost_pct,\n",
    "        sell_cost_pct,\n",
    "        reward_scaling,\n",
    "        state_space,\n",
    "        action_space,\n",
    "        tech_indicator_list,\n",
    "        stop_loss,\n",
    "        punishment_rate,\n",
    "        make_plots=False,\n",
    "        print_verbosity=10,\n",
    "        row=0,\n",
    "        initial=True,\n",
    "        previous_state=[],\n",
    "        model_name=\"\",\n",
    "        mode=\"\",\n",
    "        iteration=\"\",\n",
    "    ):\n",
    "        # self.row = row\n",
    "        self.df = df\n",
    "        # self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.buy_cost_pct = buy_cost_pct\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.punishment_rate = punishment_rate\n",
    "        self.stop_loss = stop_loss # the game stops when the asset loses more than stop_loss percent\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.state_space,)\n",
    "        )\n",
    "        # self.data = self.df.loc[self.row, :]\n",
    "        self.terminal = False\n",
    "        self.make_plots = make_plots\n",
    "        self.print_verbosity = print_verbosity\n",
    "        # self.turbulence_threshold = turbulence_threshold\n",
    "        # self.risk_indicator_col = risk_indicator_col\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "        self.iteration = iteration\n",
    "        self.portfolio_columns = ['tic','price','buy_price','amount','weight']\n",
    "        self.row = 0\n",
    "        \n",
    "        # initalize state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "        # self.reset()\n",
    "        self._seed()\n",
    "\n",
    "    def _buy_stock(self, action):\n",
    "        def _do_buy():\n",
    "            if self.data.close > 0: # Buy only if the price is > 0 (no missing data in this particular date)\n",
    "                buy_amount = (self.portfolio.loc[0].price * action)\n",
    "                buy_num_shares = math.floor(buy_amount/self.data.close)\n",
    "                if buy_num_shares > 0:\n",
    "                    if self.portfolio[self.portfolio.tic == self.data.tic].empty:\n",
    "                        selected_index = len(self.portfolio)\n",
    "                        selected_row = [self.data.tic,0,self.data.close,0,0]\n",
    "                    else:\n",
    "                        selected_index = self.portfolio[self.portfolio.tic == self.data.tic].index[0]\n",
    "                        selected_row = self.portfolio.loc[selected_index]\n",
    "                        selected_row[2] = (buy_num_shares*self.data.close + selected_row[2]*selected_row[3]) \\\n",
    "                                    /(buy_num_shares + selected_row[3])\n",
    "                    selected_row[1] = self.data.close\n",
    "                    selected_row[3] += buy_num_shares\n",
    "                    self.portfolio.loc[selected_index] = selected_row\n",
    "    \n",
    "                    # Update remain capital\n",
    "                    capital_row = self.portfolio.loc[0]\n",
    "                    capital_change = buy_num_shares * self.data.close * (1 + self.buy_cost_pct)\n",
    "                    capital_row[1] -= capital_change\n",
    "                    capital_row[2] -= capital_change\n",
    "                    self.portfolio.loc[0] = capital_row\n",
    "                    \n",
    "                    self._compute_weight()\n",
    "                    self.cost += selected_row[1] * buy_num_shares * self.buy_cost_pct\n",
    "                    self.trades += 1\n",
    "                else:\n",
    "                    # Punish a certain amount of money if buying without avaiable capital\n",
    "                    self.reward += -10 * self.initial_amount * self.punishment_rate * self.reward_scaling\n",
    "                    # print(f'Set punishment for unavailable buying: {self.reward}')\n",
    "                    \n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "\n",
    "            return buy_num_shares\n",
    "\n",
    "        buy_num_shares = _do_buy()\n",
    "        return buy_num_shares\n",
    "    \n",
    "    def _sell_stock(self, action):\n",
    "        def _do_sell_normal():\n",
    "            # TODO: Punish a certain amount of money if selling without avaiable stock in the inventory\n",
    "            if self.data.close > 0: # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                # perform sell action based on the sign of the action\n",
    "                selected_row = self.portfolio[self.portfolio.tic == self.data.tic]\n",
    "                if (selected_row.empty) | (selected_row.amount.any() == False):             \n",
    "                    sell_num_shares = 0\n",
    "                    \n",
    "                    # Punish a certain amount of money if selling without avaiable stock in the inventory\n",
    "                    self.reward += -10 * self.initial_amount * self.punishment_rate * self.reward_scaling\n",
    "                \n",
    "                else:\n",
    "                    sell_num_shares = math.floor(abs(action) * selected_row.amount) \n",
    "                    sell_amount = self.data.close * sell_num_shares * (1 - self.sell_cost_pct)\n",
    "\n",
    "                    # Update reward only when closing a deal\n",
    "                    buy_amount = selected_row.buy_price * sell_num_shares\n",
    "                    self.reward += (sell_amount - buy_amount).values[0].item() * self.reward_scaling\n",
    "                    \n",
    "                    # update stock row in the portfolio\n",
    "                    selected_index = selected_row.index[0]\n",
    "                    selected_row = self.portfolio.loc[selected_index]\n",
    "                    selected_row[1] = self.data.close\n",
    "                    selected_row[3] -= sell_num_shares\n",
    "\n",
    "                    # Update remain capital\n",
    "                    capital_row = self.portfolio.loc[0]\n",
    "                    capital_row[1] += sell_amount\n",
    "                    capital_row[2] += sell_amount\n",
    "\n",
    "                    # Update changes to portfolio\n",
    "                    self.portfolio.loc[selected_index] = selected_row\n",
    "                    self.portfolio.loc[0] = capital_row\n",
    "                    self._compute_weight()\n",
    "                    self.cost += selected_row[1] * sell_num_shares * self.sell_cost_pct\n",
    "                    self.trades += 1\n",
    "            else:\n",
    "                sell_num_shares = 0\n",
    "\n",
    "            return sell_num_shares\n",
    "\n",
    "        sell_num_shares = _do_sell_normal()\n",
    "        return sell_num_shares\n",
    "\n",
    "    def _compute_weight(self):\n",
    "        nav = sum(self.portfolio.price*self.portfolio.amount)\n",
    "        self.portfolio['weight'] = self.portfolio.apply(lambda x: x.price * x.amount / nav, axis=1)\n",
    "    \n",
    "    # def _make_plot(self):\n",
    "    #     plt.plot(self.asset_memory, \"r\")\n",
    "    #     plt.savefig(\"results/account_value_trade_{}.png\".format(self.episode))\n",
    "    #     plt.close()\n",
    "\n",
    "    def step(self, actions):\n",
    "\n",
    "        current_total_asset = sum(self.portfolio.price * self.portfolio.amount)\n",
    "        self.terminal = (self.row >= len(self.df.index.unique()) - 1) | (current_total_asset < self.initial_amount*(1-self.stop_loss))\n",
    "        # print(f'Action of step {self.row} is {actions[0]}')\n",
    "        \n",
    "        # --> IN CASE THE STEP IS THE TERMINATED STEP\n",
    "        if self.terminal: \n",
    "            print(f\"Episode: {self.episode}\")\n",
    "            # if self.make_plots:\n",
    "            #     self._make_plot()\n",
    "                \n",
    "            # Summary the training performance after an episode\n",
    "            end_total_asset = sum(self.portfolio.price*self.portfolio.amount)\n",
    "            tot_reward = end_total_asset - self.initial_amount\n",
    "\n",
    "            # Summary total_value\n",
    "            # df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            # df_total_value.columns = [\"account_value\"]\n",
    "            # df_total_value[\"date\"] = self.date_memory\n",
    "            # df_total_value[\"daily_return\"] = df_total_value[\"account_value\"].pct_change(1)\n",
    "            # if df_total_value[\"daily_return\"].std() != 0:\n",
    "            #     sharpe = ((252 ** 0.5)* df_total_value[\"daily_return\"].mean()/ df_total_value[\"daily_return\"].std())\n",
    "\n",
    "            # Take tot_reward into account\n",
    "            self.reward = tot_reward\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            self.reward = self.reward * self.reward_scaling\n",
    "            \n",
    "            # Summary rewards\n",
    "            # df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            # df_rewards.columns = [\"account_rewards\"]\n",
    "            # df_rewards[\"date\"] = self.date_memory[:-1]\n",
    "\n",
    "            # Print out training results after a certain amount of episodes\n",
    "            if self.episode % self.print_verbosity == 0:\n",
    "                print(f\"row: {self.row}, episode: {self.episode}\")\n",
    "                print(f\"begin_total_asset: {self.asset_memory[0]:0.2f}\")\n",
    "                print(f\"end_total_asset: {end_total_asset:0.2f}\")\n",
    "                print(f\"total_reward: {tot_reward:0.2f}\")\n",
    "                print(f\"total_cost: {self.cost:0.2f}\")\n",
    "                print(f\"total_trades: {self.trades}\")\n",
    "                # if df_total_value[\"daily_return\"].std() != 0:\n",
    "                #     print(f\"Sharpe: {sharpe:0.3f}\")\n",
    "                print(\"=================================\")\n",
    "\n",
    "            # if (self.model_name != \"\") and (self.mode != \"\"):\n",
    "                # df_actions = self.save_action_memory()\n",
    "                # df_actions.to_csv(\n",
    "                #     \"results/actions_{}_{}_{}.csv\".format(\n",
    "                #         self.mode, self.model_name, self.iteration\n",
    "                #     )\n",
    "                # )\n",
    "                # df_total_value.to_csv(\n",
    "                #     \"results/account_value_{}_{}_{}.csv\".format(\n",
    "                #         self.mode, self.model_name, self.iteration\n",
    "                #     ),\n",
    "                #     index=False,\n",
    "                # )\n",
    "                # df_rewards.to_csv(\n",
    "                #     \"results/account_rewards_{}_{}_{}.csv\".format(\n",
    "                #         self.mode, self.model_name, self.iteration\n",
    "                #     ),\n",
    "                #     index=False,\n",
    "                # )\n",
    "                # plt.plot(self.asset_memory, \"r\")\n",
    "                # plt.savefig(\n",
    "                #     \"results/account_value_{}_{}_{}.png\".format(\n",
    "                #         self.mode, self.model_name, self.iteration\n",
    "                #     ),\n",
    "                #     index=False,\n",
    "                # )\n",
    "                # plt.close()\n",
    "            \n",
    "            truncated = False  # we do not limit the number of steps here\n",
    "            # Optionally we can pass additional info, we are not using that for now\n",
    "            info = {}\n",
    "\n",
    "\n",
    "            return (\n",
    "                np.array(self.state).astype(np.float32),\n",
    "                self.reward,\n",
    "                self.terminal,\n",
    "                truncated,\n",
    "                info,\n",
    "            )\n",
    "\n",
    "        # --> IN A NORMAL STEP\n",
    "        else: \n",
    "\n",
    "            # begin_total_asset = sum(self.portfolio.price * self.portfolio.amount)\n",
    "\n",
    "            # Set a punishment at each step to push the agent decide an action\n",
    "            self.reward -= self.initial_amount * self.punishment_rate * self.reward_scaling\n",
    "            \n",
    "            # update previous_portfolio\n",
    "            self.previous_port = self.portfolio.copy()\n",
    "        \n",
    "            # sell_number_share = 0 # Default value at each transaction to detect reward value\n",
    "            self.reward = 0\n",
    "            if actions[0] > 0:\n",
    "                self._buy_stock(actions[0])\n",
    "            else:\n",
    "                sell_number_share = self._sell_stock(actions[0])\n",
    "\n",
    "            # actions = actions * self.hmax  # actions initially is scaled between 0 to 1\n",
    "            # actions = actions.astype(int)  # convert into integer because we can't by fraction of shares\n",
    "            # print(type(actions))\n",
    "            self.actions_memory.append(actions)\n",
    "\n",
    "            # Update selected row in the dataset based on state: s -> s+1\n",
    "            self.row += 1\n",
    "            self.data = self.df.loc[self.row]\n",
    "            self.state = self._update_state()\n",
    "    \n",
    "            end_total_asset = sum(self.portfolio.price * self.portfolio.amount)\n",
    "            # print(f'Begin asset: {begin_total_asset}, End asset: {end_total_asset}')\n",
    "\n",
    "            # Update asset memory\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            self.date_memory.append(self._get_date())\n",
    "\n",
    "            # Update reward\n",
    "            # if self.reward == 0:\n",
    "            #     self.reward = end_total_asset - self.initial_amount\n",
    "\n",
    "            if (self.row >= len(self.df.index.unique()) - 1):\n",
    "                self.reward += (end_total_asset - self.initial_amount)  * self.reward_scaling\n",
    "            \n",
    "            self.rewards_memory.append(self.reward)\n",
    "            # self.reward = self.reward * self.reward_scaling\n",
    "\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "        \n",
    "        # return self.state, self.reward, self.terminal, {}\n",
    "    \n",
    "        return (\n",
    "            np.array(self.state).astype(np.float32),\n",
    "            self.reward,\n",
    "            self.terminal,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # initiate state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # Reset asset_memory\n",
    "        if self.initial:\n",
    "            self.asset_memory = [self.initial_amount]\n",
    "        else:\n",
    "            previous_total_asset = sum(self.previous_port.price * self.previous_port.amount)\n",
    "            self.asset_memory = [previous_total_asset]\n",
    "\n",
    "        # Reset support variables\n",
    "        # self.row = 0\n",
    "        # self.data = self.df.loc[self.row, :]\n",
    "        # self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "        self.episode += 1\n",
    "\n",
    "        return np.array(self.state).astype(np.float32), {}\n",
    "        # return self.state\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        return self.state\n",
    "\n",
    "    def _initiate_state(self):\n",
    "        \n",
    "        # Reset portfolio & previous_portfolio\n",
    "        if self.initial:\n",
    "            self.portfolio = pd.DataFrame([['cap',self.initial_amount,self.initial_amount,1,1]])\n",
    "            self.portfolio.columns = self.portfolio_columns\n",
    "            self.previous_port = self.portfolio.copy()\n",
    "        else:\n",
    "            self.portfolio = self.previous_port.copy()\n",
    "\n",
    "        # Reset data\n",
    "        self.row = 0\n",
    "        self.data = self.df.loc[self.row]\n",
    "        \n",
    "         # Reset state\n",
    "        state = self._update_state()\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def _update_state(self):\n",
    "\n",
    "        # if the stock appear in the portfolio already\n",
    "        if self.portfolio[self.portfolio.tic == self.data.tic].empty:\n",
    "            state = ([self.portfolio.loc[0].price] + [self.data['close']] + [0] + [0] + [0] + sum([[self.data[tech]] for tech in self.tech_indicator_list], []))\n",
    "            \n",
    "        else:\n",
    "            # Update stock's prices in portfolio before updating state\n",
    "            selected_index = self.portfolio[self.portfolio.tic == self.data.tic].index[0]\n",
    "            selected_row = self.portfolio.loc[selected_index]\n",
    "            selected_row['price'] = self.data.close\n",
    "            self.portfolio.loc[selected_index] = selected_row\n",
    "            self._compute_weight()\n",
    "            selected_row = self.portfolio.loc[selected_index]\n",
    "            # print(\"Update portfolio at \",self.data.tic,\" price: \", self.data.close,\"; with weight: \", selected_row.weight)\n",
    "            \n",
    "            state = (\n",
    "                    [self.portfolio.iloc[0].price]\n",
    "                    + [(selected_row.buy_price/selected_row.price-1)]\n",
    "                    + [self.data.close]\n",
    "                    + [selected_row.amount]\n",
    "                    + [selected_row.weight]\n",
    "                    + sum([[self.data[tech]] for tech in self.tech_indicator_list], [])\n",
    "                )\n",
    "    \n",
    "        return state\n",
    "\n",
    "    def _get_date(self):\n",
    "        # return self.data.date\n",
    "        return self.row\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        asset_list = self.asset_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame(\n",
    "            {\"date\": date_list, \"account_value\": asset_list}\n",
    "        )\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        \n",
    "        date_list = self.date_memory[:-1]\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame({\"date\": date_list, \"actions\": action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>\n",
    "## 5.3 Initialize Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State's space include current capital amount, the return of the current stock, the weight of this stock in the portfolio, and the indicators decided in ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Dimension: 1, State Space: 22\n"
     ]
    }
   ],
   "source": [
    "ratio_list = train_data.columns.drop(['date','tic','close'])\n",
    "\n",
    "action_dimension = 1 # k float in range (-1,1) to decide sell (k<0) or buy (k>0) decisions\n",
    "state_space = 1 + 4*action_dimension + len(ratio_list)\n",
    "print(f\"Action Dimension: {action_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "# Parameters for the environment\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.01,\n",
    "    \"state_space\": state_space, \n",
    "    \"tech_indicator_list\": ratio_list, \n",
    "    \"action_space\": action_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"stop_loss\": 0.3,\n",
    "    \"print_verbosity\":4,\n",
    "    \"punishment_rate\": 0.001\n",
    "}\n",
    "\n",
    "#Establish the training environment using StockTradingEnv() class\n",
    "e_train_gym = StockTradingEnv(df = train_data, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10853693], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = e_train_gym.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(e_train_gym, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN, DDPG\n",
    "\n",
    "# Instantiate the env\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1.0000000e+06,  1.2950467e+02,  0.0000000e+00,  0.0000000e+00,\n",
      "        0.0000000e+00,  3.1952330e-01,  2.1845427e-01,  1.0323844e-01,\n",
      "        2.7902060e+01,  2.0395163e-01,  1.0000000e+01,  3.0652669e-01,\n",
      "        6.4122640e-02,  1.3918664e-01,  2.6717506e-02,  3.0203652e-02,\n",
      "        1.4161230e+01,  1.3132939e-01,  1.1300485e-01,  3.0203652e-02,\n",
      "        7.4770790e+08, -3.7920169e-09], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "# Get samples from train_data\n",
    "test_env_data = trade_data\n",
    "# test_env_data = trade_data.iloc[:20]\n",
    "# test_env_data.close = [10,100,15,90,20,80,25,85,18,92,10,100,15,90,20,80,25,85,18,92]\n",
    "\n",
    "# Parameters for the environment\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.001,\n",
    "    \"state_space\": state_space, \n",
    "    \"tech_indicator_list\": ratio_list, \n",
    "    \"action_space\": action_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"stop_loss\": 0.3,\n",
    "    \"print_verbosity\":4,\n",
    "    \"punishment_rate\": 0.001\n",
    "}\n",
    "\n",
    "#Establish the training environment using StockTradingEnv() class\n",
    "test_train_gym = StockTradingEnv(df = test_env_data, **env_kwargs)\n",
    "\n",
    "# test reset state\n",
    "print(test_train_gym.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, reward: -0.1\n",
      "Step 2, reward: -0.1\n",
      "Step 3, reward: -0.1\n",
      "Step 4, reward: -0.1\n",
      "Step 5, reward: 0.0\n",
      "Step 6, reward: 0.0\n",
      "Step 7, reward: -0.1\n",
      "Step 8, reward: -0.1\n",
      "Step 9, reward: -0.1\n",
      "Step 10, reward: -0.1\n",
      "Step 11, reward: -0.1\n",
      "Step 12, reward: -0.1\n",
      "Step 13, reward: 0.0\n",
      "Step 14, reward: 0.0\n",
      "Step 15, reward: -0.1\n",
      "Step 16, reward: -0.1\n",
      "Step 17, reward: -0.1\n",
      "Step 18, reward: 0.0\n",
      "Step 19, reward: 0.0\n",
      "Step 20, reward: -0.1\n",
      "Step 21, reward: 0.0\n",
      "Step 22, reward: -0.1\n",
      "Step 23, reward: 0.0\n",
      "Step 24, reward: -0.1\n",
      "Step 25, reward: -0.1\n",
      "Step 26, reward: -0.1\n",
      "Step 27, reward: -0.1\n",
      "Step 28, reward: -0.1\n",
      "Step 29, reward: -0.1\n",
      "Step 30, reward: -0.1\n",
      "Step 31, reward: -0.1\n",
      "Step 32, reward: -0.1\n",
      "Step 33, reward: -0.1\n",
      "Step 34, reward: -0.1\n",
      "Step 35, reward: -0.1\n",
      "Step 36, reward: -0.1\n",
      "Step 37, reward: 0.0\n",
      "Step 38, reward: -0.1\n",
      "Step 39, reward: -0.1\n",
      "Step 40, reward: -0.1\n",
      "Step 41, reward: -0.1\n",
      "Step 42, reward: -0.1\n",
      "Step 43, reward: 0.0\n",
      "Step 44, reward: 0.0\n",
      "Step 45, reward: -0.1\n",
      "Step 46, reward: 0.0\n",
      "Step 47, reward: -0.1\n",
      "Step 48, reward: 0.0\n",
      "Step 49, reward: 0.0\n",
      "Step 50, reward: -0.1\n",
      "Step 51, reward: 0.0\n",
      "Step 52, reward: -0.1\n",
      "Step 53, reward: -0.1\n",
      "Step 54, reward: -0.1\n",
      "Step 55, reward: -0.1\n",
      "Step 56, reward: -0.1\n",
      "Step 57, reward: -0.1\n",
      "Step 58, reward: 0.0\n",
      "Step 59, reward: -0.1\n",
      "Step 60, reward: -0.1\n",
      "Step 61, reward: -0.1\n",
      "Step 62, reward: -0.1\n",
      "Step 63, reward: -0.1\n",
      "Step 64, reward: -0.1\n",
      "Step 65, reward: -0.1\n",
      "Step 66, reward: -0.1\n",
      "Step 67, reward: -0.1\n",
      "Step 68, reward: -0.1\n",
      "Step 69, reward: -0.1\n",
      "Step 70, reward: -0.1\n",
      "Step 71, reward: -0.1\n",
      "Step 72, reward: -0.1\n",
      "Step 73, reward: 0.0\n",
      "Step 74, reward: 0.0\n",
      "Step 75, reward: -0.1\n",
      "Step 76, reward: 0.0\n",
      "Step 77, reward: 0.0\n",
      "Step 78, reward: 0.0\n",
      "Step 79, reward: 0.0\n",
      "Step 80, reward: -0.1\n",
      "Step 81, reward: 0.0\n",
      "Step 82, reward: -0.1\n",
      "Step 83, reward: 0.0\n",
      "Step 84, reward: 0.0\n",
      "Step 85, reward: -0.1\n",
      "Step 86, reward: -0.1\n",
      "Step 87, reward: -0.1\n",
      "Step 88, reward: 0.0\n",
      "Step 89, reward: -0.1\n",
      "Step 90, reward: 0.0\n",
      "Step 91, reward: 0.0\n",
      "Step 92, reward: -0.1\n",
      "Step 93, reward: 0.0\n",
      "Step 94, reward: -0.1\n",
      "Step 95, reward: -0.1\n",
      "Step 96, reward: -0.1\n",
      "Step 97, reward: -0.1\n",
      "Step 98, reward: -0.1\n",
      "Step 99, reward: -0.1\n",
      "Step 100, reward: -0.1\n",
      "Step 101, reward: -0.1\n",
      "Step 102, reward: -0.1\n",
      "Step 103, reward: 0.0\n",
      "Step 104, reward: 0.0\n",
      "Step 105, reward: 0.0\n",
      "Step 106, reward: 0.0\n",
      "Step 107, reward: 0.0\n",
      "Step 108, reward: 0.0\n",
      "Step 109, reward: 0.0\n",
      "Step 110, reward: -0.1\n",
      "Step 111, reward: 0.0\n",
      "Step 112, reward: -0.1\n",
      "Step 113, reward: 0.0\n",
      "Step 114, reward: 0.0\n",
      "Step 115, reward: -0.1\n",
      "Step 116, reward: 0.0\n",
      "Step 117, reward: 0.0\n",
      "Step 118, reward: 0.0\n",
      "Step 119, reward: 0.0\n",
      "Step 120, reward: 0.0\n",
      "Step 121, reward: 0.0\n",
      "Step 122, reward: 0.0\n",
      "Step 123, reward: 0.0\n",
      "Step 124, reward: 0.0\n",
      "Step 125, reward: 0.0\n",
      "Step 126, reward: 0.0\n",
      "Step 127, reward: 0.0\n",
      "Step 128, reward: -0.1\n",
      "Step 129, reward: -0.1\n",
      "Step 130, reward: 0.0\n",
      "Step 131, reward: -0.1\n",
      "Step 132, reward: 0.0\n",
      "Step 133, reward: 0.0\n",
      "Step 134, reward: 0.0\n",
      "Step 135, reward: 0.0\n",
      "Step 136, reward: 0.0\n",
      "Step 137, reward: 0.0\n",
      "Step 138, reward: 0.0\n",
      "Step 139, reward: 0.0\n",
      "Step 140, reward: -0.1\n",
      "Step 141, reward: 0.0\n",
      "Step 142, reward: 0.0\n",
      "Step 143, reward: 0.0\n",
      "Step 144, reward: -0.1\n",
      "Step 145, reward: -0.1\n",
      "Step 146, reward: -0.1\n",
      "Step 147, reward: 0.0\n",
      "Step 148, reward: 0.0\n",
      "Step 149, reward: 0.0\n",
      "Step 150, reward: 0.0\n",
      "Step 151, reward: 0.0\n",
      "Step 152, reward: 0.0\n",
      "Step 153, reward: 0.0\n",
      "Step 154, reward: -0.1\n",
      "Step 155, reward: -0.1\n",
      "Step 156, reward: 0.0\n",
      "Step 157, reward: 0.0\n",
      "Step 158, reward: 0.0\n",
      "Step 159, reward: -0.1\n",
      "Step 160, reward: 0.0\n",
      "Step 161, reward: -0.1\n",
      "Step 162, reward: 0.0\n",
      "Step 163, reward: 0.0\n",
      "Step 164, reward: 0.0\n",
      "Step 165, reward: 0.0\n",
      "Step 166, reward: 0.0\n",
      "Step 167, reward: 0.0\n",
      "Step 168, reward: 0.0\n",
      "Step 169, reward: 0.0\n",
      "Step 170, reward: -0.1\n",
      "Step 171, reward: -0.1\n",
      "Step 172, reward: 0.0\n",
      "Step 173, reward: 0.0\n",
      "Step 174, reward: 0.0\n",
      "Step 175, reward: -0.1\n",
      "Step 176, reward: 0.0\n",
      "Step 177, reward: -0.1\n",
      "Step 178, reward: 0.0\n",
      "Step 179, reward: 0.0\n",
      "Step 180, reward: 0.0\n",
      "Step 181, reward: -0.1\n",
      "Step 182, reward: 0.0\n",
      "Step 183, reward: 0.0\n",
      "Step 184, reward: -0.1\n",
      "Step 185, reward: 0.0\n",
      "Step 186, reward: 0.0\n",
      "Step 187, reward: 0.0\n",
      "Step 188, reward: 0.0\n",
      "Step 189, reward: 0.0\n",
      "Step 190, reward: 0.0\n",
      "Step 191, reward: 0.0\n",
      "Step 192, reward: 0.0\n",
      "Step 193, reward: 0.0\n",
      "Step 194, reward: 0.0\n",
      "Step 195, reward: 0.0\n",
      "Step 196, reward: 0.0\n",
      "Step 197, reward: 0.0\n",
      "Step 198, reward: 0.0\n",
      "Step 199, reward: 0.0\n",
      "Step 200, reward: 0.0\n",
      "Step 201, reward: 0.0\n",
      "Step 202, reward: -0.1\n",
      "Step 203, reward: 0.0\n",
      "Step 204, reward: 0.0\n",
      "Step 205, reward: -0.1\n",
      "Step 206, reward: 0.0\n",
      "Step 207, reward: 0.0\n",
      "Step 208, reward: 0.0\n",
      "Step 209, reward: 0.0\n",
      "Step 210, reward: 0.0\n",
      "Step 211, reward: 0.0\n",
      "Step 212, reward: 0.0\n",
      "Step 213, reward: 0.0\n",
      "Step 214, reward: 0.0\n",
      "Step 215, reward: -0.1\n",
      "Step 216, reward: -0.1\n",
      "Step 217, reward: 0.0\n",
      "Step 218, reward: 0.0\n",
      "Step 219, reward: -0.1\n",
      "Step 220, reward: 0.0\n",
      "Step 221, reward: 0.0\n",
      "Step 222, reward: 0.0\n",
      "Step 223, reward: 0.0\n",
      "Step 224, reward: 0.0\n",
      "Step 225, reward: 0.0\n",
      "Step 226, reward: 0.0\n",
      "Step 227, reward: 0.0\n",
      "Step 228, reward: 0.0\n",
      "Step 229, reward: 0.0\n",
      "Step 230, reward: 0.0\n",
      "Step 231, reward: 0.0\n",
      "Step 232, reward: 0.0\n",
      "Step 233, reward: 0.0\n",
      "Step 234, reward: 0.0\n",
      "Step 235, reward: -0.1\n",
      "Step 236, reward: -0.1\n",
      "Step 237, reward: 0.0\n",
      "Step 238, reward: 0.0\n",
      "Step 239, reward: 0.0\n",
      "Step 240, reward: 0.0\n",
      "Step 241, reward: 0.0\n",
      "Step 242, reward: 0.0\n",
      "Step 243, reward: -0.1\n",
      "Step 244, reward: 0.0\n",
      "Step 245, reward: 0.0\n",
      "Step 246, reward: 0.0\n",
      "Step 247, reward: 0.0\n",
      "Step 248, reward: 0.0\n",
      "Step 249, reward: 0.0\n",
      "Step 250, reward: 0.0\n",
      "Step 251, reward: 0.0\n",
      "Step 252, reward: 0.0\n",
      "Step 253, reward: 0.0\n",
      "Step 254, reward: 0.0\n",
      "Step 255, reward: 0.0\n",
      "Step 256, reward: 0.0\n",
      "Step 257, reward: 0.0\n",
      "Step 258, reward: 0.0\n",
      "Step 259, reward: 0.0\n",
      "Step 260, reward: 0.0\n",
      "Step 261, reward: 0.0\n",
      "Step 262, reward: 0.0\n",
      "Step 263, reward: 0.0\n",
      "Step 264, reward: 0.0\n",
      "Step 265, reward: 0.0\n",
      "Step 266, reward: 0.0\n",
      "Step 267, reward: 0.0\n",
      "Step 268, reward: 0.0\n",
      "Step 269, reward: 0.0\n",
      "Step 270, reward: 0.0\n",
      "Step 271, reward: 0.0\n",
      "Step 272, reward: 0.0\n",
      "Step 273, reward: 0.0\n",
      "Step 274, reward: 0.0\n",
      "Step 275, reward: 0.0\n",
      "Step 276, reward: 0.0\n",
      "Step 277, reward: 0.0\n",
      "Step 278, reward: 0.0\n",
      "Step 279, reward: 0.0\n",
      "Step 280, reward: 0.0\n",
      "Step 281, reward: 0.0\n",
      "Step 282, reward: 0.0\n",
      "Step 283, reward: 0.0\n",
      "Step 284, reward: 0.0\n",
      "Step 285, reward: 0.0\n",
      "Step 286, reward: 0.0\n",
      "Step 287, reward: 0.0\n",
      "Step 288, reward: 0.0\n",
      "Step 289, reward: 0.0\n",
      "Step 290, reward: 0.0\n",
      "Step 291, reward: 0.0\n",
      "Step 292, reward: 0.0\n",
      "Step 293, reward: 0.0\n",
      "Step 294, reward: 0.0\n",
      "Step 295, reward: 0.0\n",
      "Step 296, reward: 0.0\n",
      "Step 297, reward: 0.0\n",
      "Step 298, reward: 0.0\n",
      "Step 299, reward: 0.0\n",
      "Step 300, reward: -0.1\n",
      "Step 301, reward: 0.0\n",
      "Step 302, reward: 0.0\n",
      "Step 303, reward: 0.0\n",
      "Step 304, reward: 0.0\n",
      "Step 305, reward: 0.0\n",
      "Step 306, reward: 0.0\n",
      "Step 307, reward: 0.0\n",
      "Step 308, reward: 0.0\n",
      "Step 309, reward: 0.0\n",
      "Step 310, reward: 0.0\n",
      "Step 311, reward: 0.0\n",
      "Step 312, reward: 0.0\n",
      "Step 313, reward: 0.0\n",
      "Step 314, reward: 0.0\n",
      "Step 315, reward: 0.0\n",
      "Step 316, reward: 0.0\n",
      "Step 317, reward: 0.0\n",
      "Step 318, reward: 0.0\n",
      "Step 319, reward: 0.0\n",
      "Step 320, reward: 0.0\n",
      "Step 321, reward: 0.0\n",
      "Step 322, reward: 0.0\n",
      "Step 323, reward: 0.0\n",
      "Step 324, reward: 0.0\n",
      "Step 325, reward: 0.0\n",
      "Step 326, reward: 0.0\n",
      "Step 327, reward: 0.0\n",
      "Step 328, reward: 0.0\n",
      "Step 329, reward: 0.0\n",
      "Step 330, reward: 0.0\n",
      "Step 331, reward: 0.0\n",
      "Step 332, reward: 0.0\n",
      "Step 333, reward: 0.0\n",
      "Step 334, reward: 0.0\n",
      "Step 335, reward: 0.0\n",
      "Step 336, reward: 0.0\n",
      "Step 337, reward: 0.0\n",
      "Step 338, reward: 0.0\n",
      "Step 339, reward: 0.0\n",
      "Step 340, reward: 0.0\n",
      "Step 341, reward: 0.0\n",
      "Step 342, reward: -0.1\n",
      "Step 343, reward: 0.0\n",
      "Step 344, reward: 0.0\n",
      "Step 345, reward: 0.0\n",
      "Step 346, reward: 0.0\n",
      "Step 347, reward: 0.0\n",
      "Step 348, reward: 0.0\n",
      "Step 349, reward: 0.0\n",
      "Step 350, reward: 0.0\n",
      "Step 351, reward: 0.0\n",
      "Step 352, reward: 0.0\n",
      "Step 353, reward: 0.0\n",
      "Step 354, reward: 0.0\n",
      "Step 355, reward: 0.0\n",
      "Step 356, reward: 0.0\n",
      "Step 357, reward: 0.0\n",
      "Step 358, reward: 0.0\n",
      "Step 359, reward: 0.0\n",
      "Step 360, reward: 0.0\n",
      "Step 361, reward: 0.0\n",
      "Step 362, reward: 0.0\n",
      "Step 363, reward: 0.0\n",
      "Step 364, reward: 0.0\n",
      "Step 365, reward: 0.0\n",
      "Step 366, reward: 0.0\n",
      "Step 367, reward: 0.0\n",
      "Step 368, reward: 0.0\n",
      "Step 369, reward: 0.0\n",
      "Step 370, reward: 0.0\n",
      "Step 371, reward: 0.0\n",
      "Step 372, reward: 0.0\n",
      "Step 373, reward: 0.0\n",
      "Step 374, reward: 0.0\n",
      "Step 375, reward: 0.0\n",
      "Step 376, reward: 0.0\n",
      "Step 377, reward: 0.0\n",
      "Step 378, reward: 0.0\n",
      "Step 379, reward: 0.0\n",
      "Step 380, reward: 0.0\n",
      "Step 381, reward: 0.0\n",
      "Step 382, reward: 0.0\n",
      "Step 383, reward: 0.0\n",
      "Step 384, reward: 0.0\n",
      "Step 385, reward: 0.0\n",
      "Step 386, reward: 0.0\n",
      "Step 387, reward: 0.0\n",
      "Step 388, reward: 0.0\n",
      "Step 389, reward: 0.0\n",
      "Step 390, reward: 0.0\n",
      "Step 391, reward: 0.0\n",
      "Step 392, reward: 0.0\n",
      "Step 393, reward: 0.0\n",
      "Step 394, reward: -0.1\n",
      "Step 395, reward: -0.1\n",
      "Step 396, reward: 0.0\n",
      "Step 397, reward: 0.0\n",
      "Step 398, reward: 0.0\n",
      "Step 399, reward: 0.0\n",
      "Step 400, reward: 0.0\n",
      "Step 401, reward: 0.0\n",
      "Step 402, reward: -0.1\n",
      "Step 403, reward: 0.0\n",
      "Step 404, reward: 0.0\n",
      "Step 405, reward: 0.0\n",
      "Step 406, reward: 0.0\n",
      "Step 407, reward: 0.0\n",
      "Step 408, reward: 0.0\n",
      "Step 409, reward: 0.0\n",
      "Step 410, reward: 0.0\n",
      "Step 411, reward: 0.0\n",
      "Step 412, reward: 0.0\n",
      "Step 413, reward: 0.0\n",
      "Step 414, reward: 0.0\n",
      "Step 415, reward: 0.0\n",
      "Step 416, reward: 0.0\n",
      "Step 417, reward: 0.0\n",
      "Step 418, reward: 0.0\n",
      "Step 419, reward: 0.0\n",
      "Step 420, reward: 0.0\n",
      "Step 421, reward: 0.0\n",
      "Step 422, reward: -0.1\n",
      "Step 423, reward: 0.0\n",
      "Step 424, reward: 0.0\n",
      "Step 425, reward: 0.0\n",
      "Step 426, reward: 0.0\n",
      "Step 427, reward: 0.0\n",
      "Step 428, reward: 0.0\n",
      "Step 429, reward: 0.0\n",
      "Step 430, reward: 0.0\n",
      "Step 431, reward: 0.0\n",
      "Step 432, reward: 0.0\n",
      "Step 433, reward: 0.0\n",
      "Step 434, reward: 0.0\n",
      "Step 435, reward: 0.0\n",
      "Step 436, reward: 0.0\n",
      "Step 437, reward: 0.0\n",
      "Step 438, reward: 0.0\n",
      "Step 439, reward: 0.0\n",
      "Step 440, reward: 0.0\n",
      "Step 441, reward: 0.0\n",
      "Step 442, reward: 0.0\n",
      "Step 443, reward: 0.0\n",
      "Step 444, reward: 0.0\n",
      "Step 445, reward: 0.0\n",
      "Step 446, reward: -0.1\n",
      "Step 447, reward: 0.0\n",
      "Step 448, reward: 0.0\n",
      "Step 449, reward: 0.0\n",
      "Step 450, reward: 0.0\n",
      "Step 451, reward: 0.0\n",
      "Step 452, reward: 0.0\n",
      "Step 453, reward: 0.0\n",
      "Step 454, reward: 0.0\n",
      "Step 455, reward: 0.0\n",
      "Step 456, reward: -0.1\n",
      "Step 457, reward: 0.0\n",
      "Step 458, reward: 0.0\n",
      "Step 459, reward: 0.0\n",
      "Step 460, reward: 0.0\n",
      "Step 461, reward: 0.0\n",
      "Step 462, reward: 0.0\n",
      "Step 463, reward: 0.0\n",
      "Step 464, reward: 0.0\n",
      "Step 465, reward: 0.0\n",
      "Step 466, reward: 0.0\n",
      "Step 467, reward: 0.0\n",
      "Step 468, reward: 0.0\n",
      "Step 469, reward: 0.0\n",
      "Step 470, reward: 0.0\n",
      "Step 471, reward: 0.0\n",
      "Step 472, reward: 0.0\n",
      "Step 473, reward: 0.0\n",
      "Step 474, reward: 0.0\n",
      "Step 475, reward: 0.0\n",
      "Step 476, reward: 0.0\n",
      "Step 477, reward: 0.0\n",
      "Step 478, reward: 0.0\n",
      "Step 479, reward: 0.0\n",
      "Step 480, reward: 0.0\n",
      "Step 481, reward: 0.0\n",
      "Step 482, reward: 0.0\n",
      "Step 483, reward: 0.0\n",
      "Step 484, reward: 0.0\n",
      "Step 485, reward: 0.0\n",
      "Step 486, reward: 0.0\n",
      "Step 487, reward: 0.0\n",
      "Step 488, reward: 0.0\n",
      "Step 489, reward: 0.0\n",
      "Step 490, reward: 0.0\n",
      "Step 491, reward: 0.0\n",
      "Step 492, reward: 0.0\n",
      "Step 493, reward: 0.0\n",
      "Step 494, reward: 0.0\n",
      "Step 495, reward: 0.0\n",
      "Step 496, reward: 0.0\n",
      "Step 497, reward: 0.0\n",
      "Step 498, reward: 0.0\n",
      "Step 499, reward: 0.0\n",
      "Step 500, reward: 0.0\n",
      "Step 501, reward: 0.0\n",
      "Step 502, reward: 0.0\n",
      "Step 503, reward: 0.0\n",
      "Step 504, reward: 0.0\n",
      "Step 505, reward: 0.0\n",
      "Step 506, reward: 0.0\n",
      "Step 507, reward: 0.0\n",
      "Step 508, reward: 0.0\n",
      "Step 509, reward: 0.0\n",
      "Step 510, reward: 0.0\n",
      "Step 511, reward: 0.0\n",
      "Step 512, reward: 0.0\n",
      "Step 513, reward: 0.0\n",
      "Step 514, reward: 0.0\n",
      "Step 515, reward: 0.0\n",
      "Step 516, reward: 0.0\n",
      "Step 517, reward: 0.0\n",
      "Step 518, reward: 0.0\n",
      "Step 519, reward: 0.0\n",
      "Step 520, reward: 0.0\n",
      "Step 521, reward: 0.0\n",
      "Step 522, reward: 0.0\n",
      "Step 523, reward: 0.0\n",
      "Step 524, reward: 0.0\n",
      "Step 525, reward: 0.0\n",
      "Step 526, reward: 0.0\n",
      "Step 527, reward: 0.0\n",
      "Step 528, reward: 0.0\n",
      "Step 529, reward: 0.0\n",
      "Step 530, reward: 0.0\n",
      "Step 531, reward: 0.0\n",
      "Step 532, reward: 0.0\n",
      "Step 533, reward: 0.0\n",
      "Step 534, reward: 0.0\n",
      "Step 535, reward: 0.0\n",
      "Step 536, reward: 0.0\n",
      "Step 537, reward: 0.0\n",
      "Step 538, reward: 0.0\n",
      "Step 539, reward: 0.0\n",
      "Step 540, reward: 0.0\n",
      "Step 541, reward: 0.0\n",
      "Step 542, reward: 0.0\n",
      "Step 543, reward: 0.0\n",
      "Step 544, reward: 0.0\n",
      "Step 545, reward: 0.0\n",
      "Step 546, reward: 0.0\n",
      "Step 547, reward: 0.0\n",
      "Step 548, reward: 0.0\n",
      "Step 549, reward: 0.0\n",
      "Step 550, reward: 0.0\n",
      "Step 551, reward: 0.0\n",
      "Step 552, reward: 0.0\n",
      "Step 553, reward: 0.0\n",
      "Step 554, reward: 0.0\n",
      "Step 555, reward: 0.0\n",
      "Step 556, reward: 0.0\n",
      "Step 557, reward: 0.0\n",
      "Step 558, reward: 0.0\n",
      "Step 559, reward: 0.0\n",
      "Step 560, reward: 0.0\n",
      "Step 561, reward: 0.0\n",
      "Step 562, reward: -0.1\n",
      "Step 563, reward: 0.0\n",
      "Step 564, reward: 0.0\n",
      "Step 565, reward: 0.0\n",
      "Step 566, reward: 0.0\n",
      "Step 567, reward: 0.0\n",
      "Step 568, reward: 0.0\n",
      "Step 569, reward: 0.0\n",
      "Step 570, reward: 0.0\n",
      "Step 571, reward: 0.0\n",
      "Step 572, reward: 0.0\n",
      "Step 573, reward: 0.0\n",
      "Step 574, reward: 0.0\n",
      "Step 575, reward: 0.0\n",
      "Step 576, reward: 0.0\n",
      "Step 577, reward: 0.0\n",
      "Step 578, reward: 0.0\n",
      "Step 579, reward: 0.0\n",
      "Step 580, reward: 0.0\n",
      "Step 581, reward: 0.0\n",
      "Step 582, reward: 0.0\n",
      "Step 583, reward: 0.0\n",
      "Step 584, reward: 0.0\n",
      "Step 585, reward: 0.0\n",
      "Step 586, reward: 0.0\n",
      "Step 587, reward: 0.0\n",
      "Step 588, reward: 0.0\n",
      "Step 589, reward: 0.0\n",
      "Step 590, reward: 0.0\n",
      "Step 591, reward: 0.0\n",
      "Step 592, reward: 0.0\n",
      "Step 593, reward: 0.0\n",
      "Step 594, reward: 0.0\n",
      "Step 595, reward: 0.0\n",
      "Step 596, reward: 0.0\n",
      "Step 597, reward: 0.0\n",
      "Step 598, reward: 0.0\n",
      "Step 599, reward: 0.0\n",
      "Step 600, reward: 0.0\n",
      "Step 601, reward: 0.0\n",
      "Step 602, reward: 0.0\n",
      "Step 603, reward: 0.0\n",
      "Step 604, reward: 0.0\n",
      "Step 605, reward: 0.0\n",
      "Step 606, reward: 0.0\n",
      "Step 607, reward: 0.0\n",
      "Step 608, reward: 0.0\n",
      "Step 609, reward: 0.0\n",
      "Step 610, reward: 0.0\n",
      "Step 611, reward: -0.1\n",
      "Step 612, reward: -0.1\n",
      "Step 613, reward: -0.1\n",
      "Step 614, reward: 0.0\n",
      "Step 615, reward: 0.0\n",
      "Step 616, reward: 0.0\n",
      "Step 617, reward: 0.0\n",
      "Step 618, reward: 0.0\n",
      "Step 619, reward: 0.0\n",
      "Step 620, reward: 0.0\n",
      "Step 621, reward: 0.0\n",
      "Step 622, reward: 0.0\n",
      "Step 623, reward: 0.0\n",
      "Step 624, reward: 0.0\n",
      "Step 625, reward: 0.0\n",
      "Step 626, reward: 0.0\n",
      "Step 627, reward: 0.0\n",
      "Step 628, reward: 0.0\n",
      "Step 629, reward: 0.0\n",
      "Step 630, reward: 0.0\n",
      "Step 631, reward: 0.0\n",
      "Step 632, reward: 0.0\n",
      "Step 633, reward: 0.0\n",
      "Step 634, reward: 0.0\n",
      "Step 635, reward: 0.0\n",
      "Step 636, reward: 0.0\n",
      "Step 637, reward: 0.0\n",
      "Step 638, reward: 0.0\n",
      "Step 639, reward: 0.0\n",
      "Step 640, reward: 0.0\n",
      "Step 641, reward: 0.0\n",
      "Step 642, reward: 0.0\n",
      "Step 643, reward: 0.0\n",
      "Step 644, reward: 0.0\n",
      "Step 645, reward: 0.0\n",
      "Step 646, reward: 0.0\n",
      "Step 647, reward: 0.0\n",
      "Step 648, reward: 0.0\n",
      "Step 649, reward: 0.0\n",
      "Step 650, reward: 0.0\n",
      "Step 651, reward: 0.0\n",
      "Step 652, reward: 0.0\n",
      "Step 653, reward: 0.0\n",
      "Step 654, reward: 0.0\n",
      "Step 655, reward: 0.0\n",
      "Step 656, reward: 0.0\n",
      "Step 657, reward: 0.0\n",
      "Step 658, reward: -0.1\n",
      "Step 659, reward: -0.1\n",
      "Step 660, reward: 0.0\n",
      "Step 661, reward: 0.0\n",
      "Step 662, reward: 0.0\n",
      "Step 663, reward: 0.0\n",
      "Step 664, reward: 0.0\n",
      "Step 665, reward: 0.0\n",
      "Step 666, reward: 0.0\n",
      "Step 667, reward: 0.0\n",
      "Step 668, reward: 0.0\n",
      "Step 669, reward: 0.0\n",
      "Step 670, reward: 0.0\n",
      "Step 671, reward: 0.0\n",
      "Step 672, reward: 0.0\n",
      "Step 673, reward: 0.0\n",
      "Step 674, reward: 0.0\n",
      "Step 675, reward: 0.0\n",
      "Step 676, reward: -0.1\n",
      "Step 677, reward: -0.1\n",
      "Step 678, reward: 0.0\n",
      "Step 679, reward: -0.1\n",
      "Step 680, reward: 0.0\n",
      "Step 681, reward: 0.0\n",
      "Step 682, reward: 0.0\n",
      "Step 683, reward: 0.0\n",
      "Step 684, reward: 0.0\n",
      "Step 685, reward: 0.0\n",
      "Step 686, reward: 0.0\n",
      "Step 687, reward: 0.0\n",
      "Step 688, reward: 0.0\n",
      "Step 689, reward: 0.0\n",
      "Step 690, reward: 0.0\n",
      "Step 691, reward: 0.0\n",
      "Step 692, reward: 0.0\n",
      "Step 693, reward: 0.0\n",
      "Step 694, reward: 0.0\n",
      "Step 695, reward: 0.0\n",
      "Step 696, reward: 0.0\n",
      "Step 697, reward: 0.0\n",
      "Step 698, reward: 0.0\n",
      "Step 699, reward: 0.0\n",
      "Step 700, reward: 0.0\n",
      "Step 701, reward: 0.0\n",
      "Step 702, reward: 0.0\n",
      "Step 703, reward: 0.0\n",
      "Step 704, reward: 0.0\n",
      "Step 705, reward: 0.0\n",
      "Step 706, reward: 0.0\n",
      "Step 707, reward: 0.0\n",
      "Step 708, reward: 0.0\n",
      "Step 709, reward: 0.0\n",
      "Step 710, reward: 0.0\n",
      "Step 711, reward: 0.0\n",
      "Step 712, reward: 0.0\n",
      "Step 713, reward: 0.0\n",
      "Step 714, reward: 0.0\n",
      "Step 715, reward: 0.0\n",
      "Step 716, reward: 0.0\n",
      "Step 717, reward: 0.0\n",
      "Step 718, reward: -0.1\n",
      "Step 719, reward: 0.0\n",
      "Step 720, reward: 0.0\n",
      "Step 721, reward: 0.0\n",
      "Step 722, reward: 0.0\n",
      "Step 723, reward: 0.0\n",
      "Step 724, reward: 0.0\n",
      "Step 725, reward: 0.0\n",
      "Step 726, reward: 0.0\n",
      "Step 727, reward: 0.0\n",
      "Step 728, reward: 0.0\n",
      "Step 729, reward: 0.0\n",
      "Step 730, reward: 0.0\n",
      "Step 731, reward: 0.0\n",
      "Step 732, reward: 0.0\n",
      "Step 733, reward: -0.1\n",
      "Step 734, reward: 0.0\n",
      "Step 735, reward: 0.0\n",
      "Step 736, reward: 0.0\n",
      "Step 737, reward: 0.0\n",
      "Step 738, reward: 0.0\n",
      "Step 739, reward: 0.0\n",
      "Step 740, reward: 0.0\n",
      "Step 741, reward: 0.0\n",
      "Step 742, reward: 0.0\n",
      "Step 743, reward: 0.0\n",
      "Step 744, reward: 0.0\n",
      "Step 745, reward: 0.0\n",
      "Step 746, reward: 0.0\n",
      "Step 747, reward: 0.0\n",
      "Step 748, reward: 0.0\n",
      "Step 749, reward: 0.0\n",
      "Step 750, reward: 0.0\n",
      "Step 751, reward: 0.0\n",
      "Step 752, reward: 0.0\n",
      "Step 753, reward: 0.0\n",
      "Step 754, reward: -0.1\n",
      "Step 755, reward: 0.0\n",
      "Step 756, reward: 0.0\n",
      "Step 757, reward: 0.0\n",
      "Step 758, reward: 0.0\n",
      "Step 759, reward: 0.0\n",
      "Step 760, reward: 0.0\n",
      "Step 761, reward: 0.0\n",
      "Step 762, reward: 0.0\n",
      "Step 763, reward: 0.0\n",
      "Step 764, reward: 0.0\n",
      "Step 765, reward: 0.0\n",
      "Step 766, reward: -0.1\n",
      "Step 767, reward: 0.0\n",
      "Step 768, reward: 0.0\n",
      "Step 769, reward: 0.0\n",
      "Step 770, reward: 0.0\n",
      "Step 771, reward: 0.0\n",
      "Step 772, reward: 0.0\n",
      "Step 773, reward: 0.0\n",
      "Step 774, reward: 0.0\n",
      "Step 775, reward: 0.0\n",
      "Step 776, reward: 0.0\n",
      "Step 777, reward: 0.0\n",
      "Step 778, reward: 0.0\n",
      "Step 779, reward: 0.0\n",
      "Step 780, reward: 0.0\n",
      "Step 781, reward: 0.0\n",
      "Step 782, reward: -0.1\n",
      "Step 783, reward: -0.1\n",
      "Step 784, reward: 0.0\n",
      "Step 785, reward: 0.0\n",
      "Step 786, reward: 0.0\n",
      "Step 787, reward: 0.0\n",
      "Step 788, reward: 0.0\n",
      "Step 789, reward: 0.0\n",
      "Step 790, reward: 0.0\n",
      "Step 791, reward: 0.0\n",
      "Step 792, reward: -0.1\n",
      "Step 793, reward: 0.0\n",
      "Step 794, reward: 0.0\n",
      "Step 795, reward: 0.0\n",
      "Step 796, reward: -0.1\n",
      "Step 797, reward: -0.1\n",
      "Step 798, reward: -0.1\n",
      "Step 799, reward: 0.0\n",
      "Step 800, reward: 0.0\n",
      "Step 801, reward: 0.0\n",
      "Step 802, reward: 0.0\n",
      "Step 803, reward: 0.0\n",
      "Step 804, reward: 0.0\n",
      "Step 805, reward: 0.0\n",
      "Step 806, reward: 0.0\n",
      "Step 807, reward: 0.0\n",
      "Step 808, reward: 0.0\n",
      "Step 809, reward: 0.0\n",
      "Step 810, reward: 0.0\n",
      "Step 811, reward: 0.0\n",
      "Step 812, reward: 0.0\n",
      "Step 813, reward: 0.0\n",
      "Step 814, reward: 0.0\n",
      "Step 815, reward: 0.0\n",
      "Step 816, reward: 0.0\n",
      "Step 817, reward: 0.0\n",
      "Step 818, reward: 0.0\n",
      "Step 819, reward: 0.0\n",
      "Step 820, reward: 0.0\n",
      "Step 821, reward: 0.0\n",
      "Step 822, reward: 0.0\n",
      "Step 823, reward: 0.0\n",
      "Step 824, reward: 0.0\n",
      "Step 825, reward: 0.0\n",
      "Step 826, reward: -0.1\n",
      "Step 827, reward: 0.0\n",
      "Step 828, reward: -0.1\n",
      "Step 829, reward: 0.0\n",
      "Step 830, reward: -0.1\n",
      "Step 831, reward: 0.0\n",
      "Step 832, reward: -0.1\n",
      "Step 833, reward: 0.0\n",
      "Step 834, reward: 0.0\n",
      "Step 835, reward: 0.0\n",
      "Step 836, reward: 0.0\n",
      "Step 837, reward: 0.0\n",
      "Step 838, reward: 0.0\n",
      "Step 839, reward: 0.0\n",
      "Step 840, reward: 0.0\n",
      "Step 841, reward: 0.0\n",
      "Step 842, reward: 0.0\n",
      "Step 843, reward: 0.0\n",
      "Step 844, reward: 0.0\n",
      "Step 845, reward: 0.0\n",
      "Step 846, reward: 0.0\n",
      "Step 847, reward: 0.0\n",
      "Step 848, reward: 0.0\n",
      "Step 849, reward: 0.0\n",
      "Step 850, reward: 0.0\n",
      "Step 851, reward: 0.0\n",
      "Step 852, reward: 0.0\n",
      "Step 853, reward: 0.0\n",
      "Step 854, reward: 0.0\n",
      "Step 855, reward: 0.0\n",
      "Step 856, reward: 0.0\n",
      "Step 857, reward: 0.0\n",
      "Step 858, reward: 0.0\n",
      "Step 859, reward: 0.0\n",
      "Step 860, reward: 0.0\n",
      "Step 861, reward: 0.0\n",
      "Step 862, reward: 0.0\n",
      "Step 863, reward: 0.0\n",
      "Step 864, reward: 0.0\n",
      "Step 865, reward: 0.0\n",
      "Step 866, reward: 0.0\n",
      "Step 867, reward: 0.0\n",
      "Step 868, reward: 0.0\n",
      "Step 869, reward: 0.0\n",
      "Step 870, reward: 0.0\n",
      "Step 871, reward: 0.0\n",
      "Step 872, reward: 0.0\n",
      "Step 873, reward: 0.0\n",
      "Step 874, reward: 0.0\n",
      "Step 875, reward: 0.0\n",
      "Step 876, reward: -0.1\n",
      "Step 877, reward: 0.0\n",
      "Step 878, reward: 0.0\n",
      "Step 879, reward: 0.0\n",
      "Step 880, reward: 0.0\n",
      "Step 881, reward: 0.0\n",
      "Step 882, reward: 0.0\n",
      "Step 883, reward: 0.0\n",
      "Step 884, reward: 0.0\n",
      "Step 885, reward: 0.0\n",
      "Step 886, reward: 0.0\n",
      "Step 887, reward: 0.0\n",
      "Step 888, reward: 0.0\n",
      "Step 889, reward: 0.0\n",
      "Step 890, reward: 0.0\n",
      "Step 891, reward: 0.0\n",
      "Step 892, reward: 0.0\n",
      "Step 893, reward: 0.0\n",
      "Step 894, reward: -0.1\n",
      "Step 895, reward: 0.0\n",
      "Step 896, reward: 0.0\n",
      "Step 897, reward: 0.0\n",
      "Step 898, reward: 0.0\n",
      "Step 899, reward: 0.0\n",
      "Step 900, reward: 0.0\n",
      "Step 901, reward: 0.0\n",
      "Step 902, reward: 0.0\n",
      "Step 903, reward: 0.0\n",
      "Step 904, reward: 0.0\n",
      "Step 905, reward: 0.0\n",
      "Step 906, reward: 0.0\n",
      "Step 907, reward: 0.0\n",
      "Step 908, reward: 0.0\n",
      "Step 909, reward: 0.0\n",
      "Step 910, reward: 0.0\n",
      "Step 911, reward: 0.0\n",
      "Step 912, reward: 0.0\n",
      "Step 913, reward: 0.0\n",
      "Step 914, reward: 0.0\n",
      "Step 915, reward: 0.0\n",
      "Step 916, reward: 0.0\n",
      "Step 917, reward: 0.0\n",
      "Step 918, reward: 0.0\n",
      "Step 919, reward: 0.0\n",
      "Step 920, reward: 0.0\n",
      "Step 921, reward: 0.0\n",
      "Step 922, reward: 0.0\n",
      "Step 923, reward: 0.0\n",
      "Step 924, reward: 0.0\n",
      "Step 925, reward: 0.0\n",
      "Step 926, reward: 0.0\n",
      "Step 927, reward: 0.0\n",
      "Step 928, reward: 0.0\n",
      "Step 929, reward: 0.0\n",
      "Step 930, reward: 0.0\n",
      "Step 931, reward: 0.0\n",
      "Step 932, reward: 0.0\n",
      "Step 933, reward: 0.0\n",
      "Step 934, reward: 0.0\n",
      "Step 935, reward: 0.0\n",
      "Step 936, reward: 0.0\n",
      "Step 937, reward: 0.0\n",
      "Step 938, reward: 0.0\n",
      "Step 939, reward: 0.0\n",
      "Step 940, reward: 0.0\n",
      "Step 941, reward: 0.0\n",
      "Step 942, reward: 0.0\n",
      "Step 943, reward: 0.0\n",
      "Step 944, reward: 0.0\n",
      "Step 945, reward: 0.0\n",
      "Step 946, reward: 0.0\n",
      "Step 947, reward: 0.0\n",
      "Step 948, reward: 0.0\n",
      "Step 949, reward: 0.0\n",
      "Step 950, reward: 0.0\n",
      "Step 951, reward: 0.0\n",
      "Step 952, reward: 0.0\n",
      "Step 953, reward: 0.0\n",
      "Step 954, reward: 0.0\n",
      "Step 955, reward: 0.0\n",
      "Step 956, reward: 0.0\n",
      "Step 957, reward: 0.0\n",
      "Step 958, reward: 0.0\n",
      "Step 959, reward: 0.0\n",
      "Step 960, reward: 0.0\n",
      "Step 961, reward: 0.0\n",
      "Step 962, reward: 0.0\n",
      "Step 963, reward: 0.0\n",
      "Step 964, reward: 0.0\n",
      "Step 965, reward: 0.0\n",
      "Step 966, reward: 0.0\n",
      "Step 967, reward: 0.0\n",
      "Step 968, reward: 0.0\n",
      "Step 969, reward: 0.0\n",
      "Step 970, reward: 0.0\n",
      "Step 971, reward: 0.0\n",
      "Step 972, reward: 0.0\n",
      "Step 973, reward: 0.0\n",
      "Step 974, reward: 0.0\n",
      "Step 975, reward: 0.0\n",
      "Step 976, reward: 0.0\n",
      "Step 977, reward: 0.0\n",
      "Step 978, reward: 0.0\n",
      "Step 979, reward: 0.0\n",
      "Step 980, reward: 0.0\n",
      "Step 981, reward: 0.0\n",
      "Step 982, reward: 0.0\n",
      "Step 983, reward: 0.0\n",
      "Step 984, reward: 0.0\n",
      "Step 985, reward: 0.0\n",
      "Step 986, reward: 0.0\n",
      "Step 987, reward: 0.0\n",
      "Step 988, reward: 0.0\n",
      "Step 989, reward: 0.0\n",
      "Step 990, reward: 0.0\n",
      "Step 991, reward: 0.0\n",
      "Step 992, reward: 0.0\n",
      "Step 993, reward: 0.0\n",
      "Step 994, reward: 0.0\n",
      "Step 995, reward: 0.0\n",
      "Step 996, reward: 0.0\n",
      "Step 997, reward: 0.0\n",
      "Step 998, reward: 0.0\n",
      "Step 999, reward: 0.0\n",
      "Step 1000, reward: 0.0\n",
      "Step 1001, reward: 0.0\n",
      "Step 1002, reward: 0.0\n",
      "Step 1003, reward: 0.0\n",
      "Step 1004, reward: 0.0\n",
      "Step 1005, reward: 0.0\n",
      "Step 1006, reward: 0.0\n",
      "Step 1007, reward: 0.0\n",
      "Step 1008, reward: 0.0\n",
      "Step 1009, reward: 0.0\n",
      "Step 1010, reward: 0.0\n",
      "Step 1011, reward: 0.0\n",
      "Step 1012, reward: 0.0\n",
      "Step 1013, reward: 0.0\n",
      "Step 1014, reward: 0.0\n",
      "Step 1015, reward: 0.0\n",
      "Step 1016, reward: 0.0\n",
      "Step 1017, reward: 0.0\n",
      "Step 1018, reward: 0.0\n",
      "Step 1019, reward: 0.0\n",
      "Step 1020, reward: 0.0\n",
      "Step 1021, reward: 0.0\n",
      "Step 1022, reward: 0.0\n",
      "Step 1023, reward: 0.0\n",
      "Step 1024, reward: 0.0\n",
      "Step 1025, reward: 0.0\n",
      "Step 1026, reward: 0.0\n",
      "Step 1027, reward: 0.0\n",
      "Step 1028, reward: -0.1\n",
      "Step 1029, reward: -0.1\n",
      "Step 1030, reward: 0.0\n",
      "Step 1031, reward: -0.1\n",
      "Step 1032, reward: -0.1\n",
      "Step 1033, reward: 0.0\n",
      "Step 1034, reward: 0.0\n",
      "Step 1035, reward: 0.0\n",
      "Step 1036, reward: 0.0\n",
      "Step 1037, reward: 0.0\n",
      "Step 1038, reward: 0.0\n",
      "Step 1039, reward: 0.0\n",
      "Step 1040, reward: 0.0\n",
      "Step 1041, reward: 0.0\n",
      "Step 1042, reward: 0.0\n",
      "Step 1043, reward: 0.0\n",
      "Step 1044, reward: 0.0\n",
      "Step 1045, reward: 0.0\n",
      "Step 1046, reward: 0.0\n",
      "Step 1047, reward: 0.0\n",
      "Step 1048, reward: 0.0\n",
      "Step 1049, reward: 0.0\n",
      "Step 1050, reward: 0.0\n",
      "Step 1051, reward: 0.0\n",
      "Step 1052, reward: 0.0\n",
      "Step 1053, reward: 0.0\n",
      "Step 1054, reward: 0.0\n",
      "Step 1055, reward: 0.0\n",
      "Step 1056, reward: 0.0\n",
      "Step 1057, reward: 0.0\n",
      "Step 1058, reward: 0.0\n",
      "Step 1059, reward: 0.0\n",
      "Step 1060, reward: 0.0\n",
      "Step 1061, reward: 0.0\n",
      "Step 1062, reward: 0.0\n",
      "Step 1063, reward: 0.0\n",
      "Step 1064, reward: 0.0\n",
      "Step 1065, reward: 0.0\n",
      "Step 1066, reward: 0.0\n",
      "Step 1067, reward: 0.0\n",
      "Step 1068, reward: 0.0\n",
      "Step 1069, reward: 0.0\n",
      "Step 1070, reward: 0.0\n",
      "Step 1071, reward: 0.0\n",
      "Step 1072, reward: 0.0\n",
      "Step 1073, reward: 0.0\n",
      "Step 1074, reward: 0.0\n",
      "Step 1075, reward: 0.0\n",
      "Step 1076, reward: 0.0\n",
      "Step 1077, reward: 0.0\n",
      "Step 1078, reward: 0.0\n",
      "Step 1079, reward: 0.0\n",
      "Step 1080, reward: 0.0\n",
      "Step 1081, reward: 0.0\n",
      "Step 1082, reward: 0.0\n",
      "Step 1083, reward: 0.0\n",
      "Step 1084, reward: 0.0\n",
      "Step 1085, reward: 0.0\n",
      "Step 1086, reward: 0.0\n",
      "Step 1087, reward: 0.0\n",
      "Step 1088, reward: 0.0\n",
      "Step 1089, reward: 0.0\n",
      "Step 1090, reward: 0.0\n",
      "Step 1091, reward: 0.0\n",
      "Step 1092, reward: 0.0\n",
      "Step 1093, reward: 0.0\n",
      "Step 1094, reward: 0.0\n",
      "Step 1095, reward: 0.0\n",
      "Step 1096, reward: 0.0\n",
      "Step 1097, reward: 0.0\n",
      "Step 1098, reward: 0.0\n",
      "Step 1099, reward: 0.0\n",
      "Step 1100, reward: 0.0\n",
      "Step 1101, reward: 0.0\n",
      "Step 1102, reward: 0.0\n",
      "Step 1103, reward: 0.0\n",
      "Step 1104, reward: 0.0\n",
      "Step 1105, reward: 0.0\n",
      "Step 1106, reward: 0.0\n",
      "Step 1107, reward: 0.0\n",
      "Step 1108, reward: 0.0\n",
      "Step 1109, reward: 0.0\n",
      "Step 1110, reward: 0.0\n",
      "Step 1111, reward: 0.0\n",
      "Step 1112, reward: 0.0\n",
      "Step 1113, reward: 0.0\n",
      "Step 1114, reward: 0.0\n",
      "Step 1115, reward: -0.1\n",
      "Step 1116, reward: 0.0\n",
      "Step 1117, reward: 0.0\n",
      "Step 1118, reward: 0.0\n",
      "Step 1119, reward: 0.0\n",
      "Step 1120, reward: 0.0\n",
      "Step 1121, reward: 0.0\n",
      "Step 1122, reward: 0.0\n",
      "Step 1123, reward: 0.0\n",
      "Step 1124, reward: 0.0\n",
      "Step 1125, reward: 0.0\n",
      "Step 1126, reward: 0.0\n",
      "Step 1127, reward: 0.0\n",
      "Step 1128, reward: 0.0\n",
      "Step 1129, reward: 0.0\n",
      "Step 1130, reward: 0.0\n",
      "Step 1131, reward: 0.0\n",
      "Step 1132, reward: 0.0\n",
      "Step 1133, reward: 0.0\n",
      "Step 1134, reward: 0.0\n",
      "Step 1135, reward: 0.0\n",
      "Step 1136, reward: 0.0\n",
      "Step 1137, reward: 0.0\n",
      "Step 1138, reward: 0.0\n",
      "Step 1139, reward: 41.19597404548084\n",
      "Episode: 1\n",
      "Step 1139, reward: 41.19597404548084\n"
     ]
    }
   ],
   "source": [
    "# state,reward,terminal,truncated,info = test_train_gym.step(test_train_gym.action_space.sample())\n",
    "# print(reward,' type ',type(reward),'\\n')\n",
    "# print(terminal,'\\n')\n",
    "# print(state,'\\n')\n",
    "# print(test_train_gym.portfolio,'\\n')\n",
    "# print(test_train_gym.previous_port)\n",
    "\n",
    "for i in range(0,len(test_env_data)):\n",
    "    state,reward,terminal,truncated,info = test_train_gym.step(test_train_gym.action_space.sample())\n",
    "    print(f'Step {test_train_gym.row}, reward: {test_train_gym.reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Train DRL Agents\n",
    "* The DRL algorithms are from **Stable Baselines 3**.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "935680"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_length = len(e_train_gym.df)\n",
    "episode_amount = 256\n",
    "total_training_step = episode_length*episode_amount\n",
    "total_training_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='8.1'></a>\n",
    "If the training starts from the previous model, the section should **run all cells above** this line and then jump to [**Load trained model**](#7.2.1) to keep the traning going."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 3655, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 3655, 'clip_range': 0.1}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent_ppo = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": episode_length,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": episode_length,\n",
    "    \"clip_range\":0.1\n",
    "}\n",
    "\n",
    "policy_kwargs = dict(net_arch=dict(pi=[128, 128], vf=[128, 128]))\n",
    "\n",
    "model_ppo = agent_ppo.get_model(\"ppo\",model_kwargs = PPO_PARAMS, policy_kwargs=policy_kwargs,tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_ppo/\",verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to tensorboard_log/test_ppo/ppo_33\n",
      "Episode: 5\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 131       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 27        |\n",
      "|    total_timesteps | 3655      |\n",
      "| train/             |           |\n",
      "|    reward          | 279.70554 |\n",
      "----------------------------------\n",
      "Episode: 6\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 132           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 54            |\n",
      "|    total_timesteps      | 7310          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028600814 |\n",
      "|    clip_fraction        | 0.00287       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -8.09e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 358           |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -9.05e-05     |\n",
      "|    reward               | 275.64587     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 718           |\n",
      "-------------------------------------------\n",
      "Episode: 7\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 83           |\n",
      "|    total_timesteps      | 10965        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023513716 |\n",
      "|    clip_fraction        | 0.0724       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 5.96e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 346          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.000552    |\n",
      "|    reward               | 248.74911    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 694          |\n",
      "------------------------------------------\n",
      "Episode: 8\n",
      "row: 3654, episode: 8\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5334522.71\n",
      "total_reward: 4334522.71\n",
      "total_cost: 1398142.53\n",
      "total_trades: 2858\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 109          |\n",
      "|    total_timesteps      | 14620        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017497231 |\n",
      "|    clip_fraction        | 0.0384       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.000121    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 281          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.000126    |\n",
      "|    reward               | 433.45227    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 562          |\n",
      "------------------------------------------\n",
      "Episode: 9\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 135           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 135           |\n",
      "|    total_timesteps      | 18275         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013682201 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 9.99e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 852           |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | 0.000194      |\n",
      "|    reward               | 335.72165     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 1.71e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 10\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 136          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 160          |\n",
      "|    total_timesteps      | 21930        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012639116 |\n",
      "|    clip_fraction        | 0.00826      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000252    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 508          |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.000131    |\n",
      "|    reward               | 206.1209     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.02e+03     |\n",
      "------------------------------------------\n",
      "Episode: 11\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 138          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 185          |\n",
      "|    total_timesteps      | 25585        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031832207 |\n",
      "|    clip_fraction        | 0.0836       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.0023      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 189          |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.000477    |\n",
      "|    reward               | 296.75137    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 378          |\n",
      "------------------------------------------\n",
      "Episode: 12\n",
      "row: 3654, episode: 12\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2362999.37\n",
      "total_reward: 1362999.37\n",
      "total_cost: 734073.55\n",
      "total_trades: 2863\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 139          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 210          |\n",
      "|    total_timesteps      | 29240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003207437 |\n",
      "|    clip_fraction        | 0.000109     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000881    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 395          |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | 9.28e-05     |\n",
      "|    reward               | 136.29994    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 790          |\n",
      "------------------------------------------\n",
      "Episode: 13\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 139          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 236          |\n",
      "|    total_timesteps      | 32895        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037795035 |\n",
      "|    clip_fraction        | 0.0754       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00672     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 81.8         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000921    |\n",
      "|    reward               | 501.96295    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 164          |\n",
      "------------------------------------------\n",
      "Episode: 14\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 139           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 261           |\n",
      "|    total_timesteps      | 36550         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011426197 |\n",
      "|    clip_fraction        | 8.21e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.000171     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.14e+03      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -8.94e-05     |\n",
      "|    reward               | 264.48004     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 2.28e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 15\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 40205       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001864977 |\n",
      "|    clip_fraction        | 0.0962      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.00109    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 312         |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.000989   |\n",
      "|    reward               | 166.37045   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 624         |\n",
      "-----------------------------------------\n",
      "Episode: 16\n",
      "row: 3654, episode: 16\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4352803.80\n",
      "total_reward: 3352803.80\n",
      "total_cost: 1354825.06\n",
      "total_trades: 3023\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 312          |\n",
      "|    total_timesteps      | 43860        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031271474 |\n",
      "|    clip_fraction        | 0.0731       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00373     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 122          |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00046     |\n",
      "|    reward               | 335.2804     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 243          |\n",
      "------------------------------------------\n",
      "Episode: 17\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 338         |\n",
      "|    total_timesteps      | 47515       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001990322 |\n",
      "|    clip_fraction        | 0.0189      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.000375   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 503         |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00095    |\n",
      "|    reward               | 172.17842   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 1.01e+03    |\n",
      "-----------------------------------------\n",
      "Episode: 18\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 364          |\n",
      "|    total_timesteps      | 51170        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037439137 |\n",
      "|    clip_fraction        | 0.13         |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00283     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 130          |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000514    |\n",
      "|    reward               | 137.16347    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 260          |\n",
      "------------------------------------------\n",
      "Episode: 19\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 140         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 390         |\n",
      "|    total_timesteps      | 54825       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002181124 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.00444    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 81.9        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.000205   |\n",
      "|    reward               | 268.05038   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 164         |\n",
      "-----------------------------------------\n",
      "Episode: 20\n",
      "row: 3654, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3962748.84\n",
      "total_reward: 2962748.84\n",
      "total_cost: 1157462.59\n",
      "total_trades: 3035\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 417          |\n",
      "|    total_timesteps      | 58480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011448356 |\n",
      "|    clip_fraction        | 0.00536      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000406    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 320          |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.000405    |\n",
      "|    reward               | 296.27487    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 640          |\n",
      "------------------------------------------\n",
      "Episode: 21\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 140           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 443           |\n",
      "|    total_timesteps      | 62135         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064926356 |\n",
      "|    clip_fraction        | 0.00153       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.000184     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 391           |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | 7.36e-05      |\n",
      "|    reward               | 238.76363     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 783           |\n",
      "-------------------------------------------\n",
      "Episode: 22\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 469          |\n",
      "|    total_timesteps      | 65790        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012223761 |\n",
      "|    clip_fraction        | 0.00665      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000525    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 253          |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.000197    |\n",
      "|    reward               | 150.45012    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 505          |\n",
      "------------------------------------------\n",
      "Episode: 23\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 139          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 497          |\n",
      "|    total_timesteps      | 69445        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008259461 |\n",
      "|    clip_fraction        | 0.00263      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00241     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 98.7         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -8.62e-05    |\n",
      "|    reward               | 466.49966    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 198          |\n",
      "------------------------------------------\n",
      "Episode: 24\n",
      "row: 3654, episode: 24\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4626696.39\n",
      "total_reward: 3626696.39\n",
      "total_cost: 1385234.65\n",
      "total_trades: 3162\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 138          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 528          |\n",
      "|    total_timesteps      | 73100        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010193859 |\n",
      "|    clip_fraction        | 0.0044       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 9.38e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 980          |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000682    |\n",
      "|    reward               | 362.66965    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.96e+03     |\n",
      "------------------------------------------\n",
      "Episode: 25\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 137           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 557           |\n",
      "|    total_timesteps      | 76755         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014318715 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.000224     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 588           |\n",
      "|    n_updates            | 200           |\n",
      "|    policy_gradient_loss | 3.83e-05      |\n",
      "|    reward               | 244.4012      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 1.18e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 26\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 137          |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 585          |\n",
      "|    total_timesteps      | 80410        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013424378 |\n",
      "|    clip_fraction        | 0.00763      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000736    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 264          |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000164    |\n",
      "|    reward               | 135.44957    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 529          |\n",
      "------------------------------------------\n",
      "Episode: 27\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 136           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 614           |\n",
      "|    total_timesteps      | 84065         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027592518 |\n",
      "|    clip_fraction        | 0.000492      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.00331      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 79.2          |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | 3.01e-05      |\n",
      "|    reward               | 239.99413     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 159           |\n",
      "-------------------------------------------\n",
      "Episode: 28\n",
      "row: 3654, episode: 28\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3298150.54\n",
      "total_reward: 2298150.54\n",
      "total_cost: 1235634.44\n",
      "total_trades: 3194\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 136           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 642           |\n",
      "|    total_timesteps      | 87720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00075505173 |\n",
      "|    clip_fraction        | 0.0014        |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.000547     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 254           |\n",
      "|    n_updates            | 230           |\n",
      "|    policy_gradient_loss | -9.9e-05      |\n",
      "|    reward               | 229.81505     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 508           |\n",
      "-------------------------------------------\n",
      "Episode: 29\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 136          |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 671          |\n",
      "|    total_timesteps      | 91375        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009359023 |\n",
      "|    clip_fraction        | 0.0704       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000575    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 233          |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000868    |\n",
      "|    reward               | 250.55128    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 465          |\n",
      "------------------------------------------\n",
      "Episode: 30\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 699         |\n",
      "|    total_timesteps      | 95030       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007408187 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | -0.000512   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 277         |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    reward               | 152.98283   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 554         |\n",
      "-----------------------------------------\n",
      "Episode: 31\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 728          |\n",
      "|    total_timesteps      | 98685        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017901333 |\n",
      "|    clip_fraction        | 0.0193       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00149     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 101          |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.000536    |\n",
      "|    reward               | 233.91478    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 202          |\n",
      "------------------------------------------\n",
      "Episode: 32\n",
      "row: 3654, episode: 32\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2375714.58\n",
      "total_reward: 1375714.58\n",
      "total_cost: 1151910.13\n",
      "total_trades: 3293\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 757          |\n",
      "|    total_timesteps      | 102340       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037715205 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000835    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 241          |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.000942    |\n",
      "|    reward               | 137.57146    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 481          |\n",
      "------------------------------------------\n",
      "Episode: 33\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 785          |\n",
      "|    total_timesteps      | 105995       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003812379 |\n",
      "|    clip_fraction        | 2.74e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00135     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 81.4         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | 4.57e-05     |\n",
      "|    reward               | 118.72202    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 163          |\n",
      "------------------------------------------\n",
      "Episode: 34\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 134           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 813           |\n",
      "|    total_timesteps      | 109650        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5552165e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.00259      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 60.2          |\n",
      "|    n_updates            | 290           |\n",
      "|    policy_gradient_loss | -8.96e-06     |\n",
      "|    reward               | 253.08182     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 121           |\n",
      "-------------------------------------------\n",
      "Episode: 35\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 134           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 842           |\n",
      "|    total_timesteps      | 113305        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014523974 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.000224      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 283           |\n",
      "|    n_updates            | 300           |\n",
      "|    policy_gradient_loss | -7.03e-05     |\n",
      "|    reward               | 143.34558     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 566           |\n",
      "-------------------------------------------\n",
      "Episode: 36\n",
      "row: 3654, episode: 36\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3007670.84\n",
      "total_reward: 2007670.84\n",
      "total_cost: 1139734.72\n",
      "total_trades: 3244\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 872          |\n",
      "|    total_timesteps      | 116960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.356153e-05 |\n",
      "|    clip_fraction        | 0.000109     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.00127     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 88.9         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -1.89e-05    |\n",
      "|    reward               | 200.76709    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 178          |\n",
      "------------------------------------------\n",
      "Episode: 37\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 133          |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 900          |\n",
      "|    total_timesteps      | 120615       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069612386 |\n",
      "|    clip_fraction        | 0.193        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.000645     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 177          |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.000745    |\n",
      "|    reward               | 108.31232    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 354          |\n",
      "------------------------------------------\n",
      "Episode: 38\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 133           |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 931           |\n",
      "|    total_timesteps      | 124270        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4967288e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.000379     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 50            |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -2.07e-05     |\n",
      "|    reward               | 257.2956      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 100           |\n",
      "-------------------------------------------\n",
      "Episode: 39\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 963          |\n",
      "|    total_timesteps      | 127925       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.518283e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 6.38e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 294          |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -9.52e-06    |\n",
      "|    reward               | 93.08383     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 588          |\n",
      "------------------------------------------\n",
      "Episode: 40\n",
      "row: 3654, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4199196.46\n",
      "total_reward: 3199196.46\n",
      "total_cost: 1665027.14\n",
      "total_trades: 3325\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 132           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 996           |\n",
      "|    total_timesteps      | 131580        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014369242 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.00053      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 36.9          |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -1.74e-05     |\n",
      "|    reward               | 319.91965     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 74            |\n",
      "-------------------------------------------\n",
      "Episode: 41\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 1027          |\n",
      "|    total_timesteps      | 135235        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8372717e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.000182      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 457           |\n",
      "|    n_updates            | 360           |\n",
      "|    policy_gradient_loss | -1.14e-05     |\n",
      "|    reward               | 133.38986     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 915           |\n",
      "-------------------------------------------\n",
      "Episode: 42\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 1060          |\n",
      "|    total_timesteps      | 138890        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011068333 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.000136     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 77            |\n",
      "|    n_updates            | 370           |\n",
      "|    policy_gradient_loss | 3.67e-05      |\n",
      "|    reward               | 73.836105     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 154           |\n",
      "-------------------------------------------\n",
      "Episode: 43\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 1093          |\n",
      "|    total_timesteps      | 142545        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7583157e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.000206     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 22.6          |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -1.53e-05     |\n",
      "|    reward               | 311.55142     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 45.6          |\n",
      "-------------------------------------------\n",
      "Episode: 44\n",
      "row: 3654, episode: 44\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2330914.40\n",
      "total_reward: 1330914.40\n",
      "total_cost: 915887.86\n",
      "total_trades: 3322\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 129           |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 1126          |\n",
      "|    total_timesteps      | 146200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015632664 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.00128       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 434           |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -3.64e-06     |\n",
      "|    reward               | 133.09145     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 869           |\n",
      "-------------------------------------------\n",
      "Episode: 45\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 1157         |\n",
      "|    total_timesteps      | 149855       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005359844 |\n",
      "|    clip_fraction        | 0.000301     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000463    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 77           |\n",
      "|    n_updates            | 400          |\n",
      "|    policy_gradient_loss | -0.00012     |\n",
      "|    reward               | 193.31914    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 154          |\n",
      "------------------------------------------\n",
      "Episode: 46\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 129           |\n",
      "|    iterations           | 42            |\n",
      "|    time_elapsed         | 1187          |\n",
      "|    total_timesteps      | 153510        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029844017 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.000329      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 165           |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | 2.65e-05      |\n",
      "|    reward               | 161.8275      |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 330           |\n",
      "-------------------------------------------\n",
      "Episode: 47\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 1216         |\n",
      "|    total_timesteps      | 157165       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017046597 |\n",
      "|    clip_fraction        | 0.0637       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.000376     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 115          |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000393    |\n",
      "|    reward               | 141.95114    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 230          |\n",
      "------------------------------------------\n",
      "Episode: 48\n",
      "row: 3654, episode: 48\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3366740.41\n",
      "total_reward: 2366740.41\n",
      "total_cost: 1326518.86\n",
      "total_trades: 3273\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 129           |\n",
      "|    iterations           | 44            |\n",
      "|    time_elapsed         | 1244          |\n",
      "|    total_timesteps      | 160820        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087790313 |\n",
      "|    clip_fraction        | 0.112         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | 0.00102       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 87.8          |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.000699     |\n",
      "|    reward               | 236.67404     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 176           |\n",
      "-------------------------------------------\n",
      "Episode: 49\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 1273        |\n",
      "|    total_timesteps      | 164475      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000940628 |\n",
      "|    clip_fraction        | 0.00695     |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | 7.41e-05    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 249         |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.000114    |\n",
      "|    reward               | 202.27411   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 498         |\n",
      "-----------------------------------------\n",
      "Episode: 50\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 46           |\n",
      "|    time_elapsed         | 1305         |\n",
      "|    total_timesteps      | 168130       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034271409 |\n",
      "|    clip_fraction        | 0.0621       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 2.21e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 180          |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | -0.000276    |\n",
      "|    reward               | 257.9531     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 361          |\n",
      "------------------------------------------\n",
      "Episode: 51\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 47            |\n",
      "|    time_elapsed         | 1335          |\n",
      "|    total_timesteps      | 171785        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029800436 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.000129      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 295           |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -6.67e-05     |\n",
      "|    reward               | 137.45065     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 591           |\n",
      "-------------------------------------------\n",
      "Episode: 52\n",
      "row: 3654, episode: 52\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3319766.28\n",
      "total_reward: 2319766.28\n",
      "total_cost: 1205652.89\n",
      "total_trades: 3191\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 1367         |\n",
      "|    total_timesteps      | 175440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013488644 |\n",
      "|    clip_fraction        | 0.00703      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000534    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 81.9         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000602    |\n",
      "|    reward               | 231.97662    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 164          |\n",
      "------------------------------------------\n",
      "Episode: 53\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 49            |\n",
      "|    time_elapsed         | 1397          |\n",
      "|    total_timesteps      | 179095        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.6517604e-05 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 3.01e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 238           |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | 0.000103      |\n",
      "|    reward               | 240.25755     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 476           |\n",
      "-------------------------------------------\n",
      "Episode: 54\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 50            |\n",
      "|    time_elapsed         | 1426          |\n",
      "|    total_timesteps      | 182750        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060411845 |\n",
      "|    clip_fraction        | 0.00317       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 0.000475      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 255           |\n",
      "|    n_updates            | 490           |\n",
      "|    policy_gradient_loss | 8.69e-05      |\n",
      "|    reward               | 192.96564     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 510           |\n",
      "-------------------------------------------\n",
      "Episode: 55\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 51            |\n",
      "|    time_elapsed         | 1457          |\n",
      "|    total_timesteps      | 186405        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00097194745 |\n",
      "|    clip_fraction        | 0.00271       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | -0.00016      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 163           |\n",
      "|    n_updates            | 500           |\n",
      "|    policy_gradient_loss | -0.000148     |\n",
      "|    reward               | 336.76572     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 326           |\n",
      "-------------------------------------------\n",
      "Episode: 56\n",
      "row: 3654, episode: 56\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3750357.48\n",
      "total_reward: 2750357.48\n",
      "total_cost: 1077622.86\n",
      "total_trades: 3036\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 1489         |\n",
      "|    total_timesteps      | 190060       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031704898 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000429    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 506          |\n",
      "|    n_updates            | 510          |\n",
      "|    policy_gradient_loss | -0.000637    |\n",
      "|    reward               | 275.03574    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 1.01e+03     |\n",
      "------------------------------------------\n",
      "Episode: 57\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 53            |\n",
      "|    time_elapsed         | 1521          |\n",
      "|    total_timesteps      | 193715        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020158937 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.43         |\n",
      "|    explained_variance   | 6.7e-05       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 335           |\n",
      "|    n_updates            | 520           |\n",
      "|    policy_gradient_loss | -2.34e-05     |\n",
      "|    reward               | 226.07191     |\n",
      "|    std                  | 1.01          |\n",
      "|    value_loss           | 670           |\n",
      "-------------------------------------------\n",
      "Episode: 58\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 54           |\n",
      "|    time_elapsed         | 1550         |\n",
      "|    total_timesteps      | 197370       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017613746 |\n",
      "|    clip_fraction        | 0.0566       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.000107    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 225          |\n",
      "|    n_updates            | 530          |\n",
      "|    policy_gradient_loss | -0.000296    |\n",
      "|    reward               | 174.55922    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 449          |\n",
      "------------------------------------------\n",
      "Episode: 59\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 1581        |\n",
      "|    total_timesteps      | 201025      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002180103 |\n",
      "|    clip_fraction        | 0.0334      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | -0.000326   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 132         |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.000245   |\n",
      "|    reward               | 228.60484   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 265         |\n",
      "-----------------------------------------\n",
      "Episode: 60\n",
      "row: 3654, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3075130.24\n",
      "total_reward: 2075130.24\n",
      "total_cost: 1158504.67\n",
      "total_trades: 3186\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 56            |\n",
      "|    time_elapsed         | 1607          |\n",
      "|    total_timesteps      | 204680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054084766 |\n",
      "|    clip_fraction        | 0.000356      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 3.35e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 230           |\n",
      "|    n_updates            | 550           |\n",
      "|    policy_gradient_loss | -0.000146     |\n",
      "|    reward               | 207.51303     |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 460           |\n",
      "-------------------------------------------\n",
      "Episode: 61\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 1633         |\n",
      "|    total_timesteps      | 208335       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.523183e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 4.21e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 189          |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | 1.43e-05     |\n",
      "|    reward               | 270.35065    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 378          |\n",
      "------------------------------------------\n",
      "Episode: 62\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 58            |\n",
      "|    time_elapsed         | 1663          |\n",
      "|    total_timesteps      | 211990        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035552104 |\n",
      "|    clip_fraction        | 5.47e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 6.6e-05       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 324           |\n",
      "|    n_updates            | 570           |\n",
      "|    policy_gradient_loss | -2.66e-05     |\n",
      "|    reward               | 215.3766      |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 648           |\n",
      "-------------------------------------------\n",
      "Episode: 63\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 1697         |\n",
      "|    total_timesteps      | 215645       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013248213 |\n",
      "|    clip_fraction        | 0.00892      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -4.12e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 204          |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | -7.66e-05    |\n",
      "|    reward               | 320.12964    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 407          |\n",
      "------------------------------------------\n",
      "Episode: 64\n",
      "row: 3654, episode: 64\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4321852.16\n",
      "total_reward: 3321852.16\n",
      "total_cost: 1190041.91\n",
      "total_trades: 3047\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 1726          |\n",
      "|    total_timesteps      | 219300        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00063072355 |\n",
      "|    clip_fraction        | 0.00137       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.000386     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 456           |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -0.000371     |\n",
      "|    reward               | 332.1852      |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 912           |\n",
      "-------------------------------------------\n",
      "Episode: 65\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 61            |\n",
      "|    time_elapsed         | 1755          |\n",
      "|    total_timesteps      | 222955        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1033509e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000112      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 491           |\n",
      "|    n_updates            | 600           |\n",
      "|    policy_gradient_loss | 0.00016       |\n",
      "|    reward               | 112.43689     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 982           |\n",
      "-------------------------------------------\n",
      "Episode: 66\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 62            |\n",
      "|    time_elapsed         | 1783          |\n",
      "|    total_timesteps      | 226610        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00035900378 |\n",
      "|    clip_fraction        | 5.47e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.00186      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 54.1          |\n",
      "|    n_updates            | 610           |\n",
      "|    policy_gradient_loss | 4.8e-06       |\n",
      "|    reward               | 192.16985     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 108           |\n",
      "-------------------------------------------\n",
      "Episode: 67\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 63           |\n",
      "|    time_elapsed         | 1812         |\n",
      "|    total_timesteps      | 230265       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025827184 |\n",
      "|    clip_fraction        | 0.033        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.0003      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 160          |\n",
      "|    n_updates            | 620          |\n",
      "|    policy_gradient_loss | -0.00037     |\n",
      "|    reward               | 351.26816    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 321          |\n",
      "------------------------------------------\n",
      "Episode: 68\n",
      "row: 3654, episode: 68\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3726362.84\n",
      "total_reward: 2726362.84\n",
      "total_cost: 1138123.99\n",
      "total_trades: 2942\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 64            |\n",
      "|    time_elapsed         | 1839          |\n",
      "|    total_timesteps      | 233920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0463489e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 5.05e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 550           |\n",
      "|    n_updates            | 630           |\n",
      "|    policy_gradient_loss | -3.84e-06     |\n",
      "|    reward               | 272.6363      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.1e+03       |\n",
      "-------------------------------------------\n",
      "Episode: 69\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 1866         |\n",
      "|    total_timesteps      | 237575       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022260644 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 1.23e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 329          |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.000187    |\n",
      "|    reward               | 279.30905    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 658          |\n",
      "------------------------------------------\n",
      "Episode: 70\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 66           |\n",
      "|    time_elapsed         | 1896         |\n",
      "|    total_timesteps      | 241230       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029144732 |\n",
      "|    clip_fraction        | 0.0423       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 5.93e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 345          |\n",
      "|    n_updates            | 650          |\n",
      "|    policy_gradient_loss | -0.000379    |\n",
      "|    reward               | 268.82098    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 689          |\n",
      "------------------------------------------\n",
      "Episode: 71\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 1923         |\n",
      "|    total_timesteps      | 244885       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018237433 |\n",
      "|    clip_fraction        | 0.028        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000112     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 318          |\n",
      "|    n_updates            | 660          |\n",
      "|    policy_gradient_loss | -0.000164    |\n",
      "|    reward               | 328.49927    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 636          |\n",
      "------------------------------------------\n",
      "Episode: 72\n",
      "row: 3654, episode: 72\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3697849.06\n",
      "total_reward: 2697849.06\n",
      "total_cost: 973209.20\n",
      "total_trades: 2913\n",
      "=================================\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 127            |\n",
      "|    iterations           | 68             |\n",
      "|    time_elapsed         | 1950           |\n",
      "|    total_timesteps      | 248540         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000107107626 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.46          |\n",
      "|    explained_variance   | 0.000225       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 478            |\n",
      "|    n_updates            | 670            |\n",
      "|    policy_gradient_loss | -6.68e-05      |\n",
      "|    reward               | 269.7849       |\n",
      "|    std                  | 1.04           |\n",
      "|    value_loss           | 957            |\n",
      "--------------------------------------------\n",
      "Episode: 73\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 69            |\n",
      "|    time_elapsed         | 1977          |\n",
      "|    total_timesteps      | 252195        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072357204 |\n",
      "|    clip_fraction        | 0.0507        |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 3.67e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 320           |\n",
      "|    n_updates            | 680           |\n",
      "|    policy_gradient_loss | -0.000324     |\n",
      "|    reward               | 179.55688     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 640           |\n",
      "-------------------------------------------\n",
      "Episode: 74\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 2004         |\n",
      "|    total_timesteps      | 255850       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013801832 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -0.000174    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 139          |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -0.000442    |\n",
      "|    reward               | 284.86795    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 279          |\n",
      "------------------------------------------\n",
      "Episode: 75\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 71            |\n",
      "|    time_elapsed         | 2030          |\n",
      "|    total_timesteps      | 259505        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9599126e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 1.19e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 358           |\n",
      "|    n_updates            | 700           |\n",
      "|    policy_gradient_loss | 1.95e-05      |\n",
      "|    reward               | 297.91708     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 716           |\n",
      "-------------------------------------------\n",
      "Episode: 76\n",
      "row: 3654, episode: 76\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4409980.74\n",
      "total_reward: 3409980.74\n",
      "total_cost: 993918.53\n",
      "total_trades: 2754\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 2056         |\n",
      "|    total_timesteps      | 263160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001430094 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -0.000385    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 393          |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -5.22e-05    |\n",
      "|    reward               | 340.99808    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 785          |\n",
      "------------------------------------------\n",
      "Episode: 77\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 2083         |\n",
      "|    total_timesteps      | 266815       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017833085 |\n",
      "|    clip_fraction        | 0.0167       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000199     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 516          |\n",
      "|    n_updates            | 720          |\n",
      "|    policy_gradient_loss | -0.000826    |\n",
      "|    reward               | 359.66418    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.03e+03     |\n",
      "------------------------------------------\n",
      "Episode: 78\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 2110         |\n",
      "|    total_timesteps      | 270470       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009101931 |\n",
      "|    clip_fraction        | 0.00454      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000121     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 574          |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -3.66e-05    |\n",
      "|    reward               | 114.002655   |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.15e+03     |\n",
      "------------------------------------------\n",
      "Episode: 79\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 75            |\n",
      "|    time_elapsed         | 2138          |\n",
      "|    total_timesteps      | 274125        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081569207 |\n",
      "|    clip_fraction        | 0.00197       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.00268      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 55.5          |\n",
      "|    n_updates            | 740           |\n",
      "|    policy_gradient_loss | -0.000909     |\n",
      "|    reward               | 358.99405     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 111           |\n",
      "-------------------------------------------\n",
      "Episode: 80\n",
      "row: 3654, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3041823.25\n",
      "total_reward: 2041823.25\n",
      "total_cost: 1202625.61\n",
      "total_trades: 3075\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 2166         |\n",
      "|    total_timesteps      | 277780       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003225282 |\n",
      "|    clip_fraction        | 5.47e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 7.75e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 571          |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | 0.000127     |\n",
      "|    reward               | 204.18233    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.14e+03     |\n",
      "------------------------------------------\n",
      "Episode: 81\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 2193         |\n",
      "|    total_timesteps      | 281435       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003635456 |\n",
      "|    clip_fraction        | 2.74e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.000252    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 181          |\n",
      "|    n_updates            | 760          |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    reward               | 117.83861    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 362          |\n",
      "------------------------------------------\n",
      "Episode: 82\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 78            |\n",
      "|    time_elapsed         | 2221          |\n",
      "|    total_timesteps      | 285090        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029304123 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.00128      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 58.4          |\n",
      "|    n_updates            | 770           |\n",
      "|    policy_gradient_loss | 8.3e-05       |\n",
      "|    reward               | 299.9529      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 117           |\n",
      "-------------------------------------------\n",
      "Episode: 83\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 79            |\n",
      "|    time_elapsed         | 2251          |\n",
      "|    total_timesteps      | 288745        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00042987405 |\n",
      "|    clip_fraction        | 5.47e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000157      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 397           |\n",
      "|    n_updates            | 780           |\n",
      "|    policy_gradient_loss | -0.000197     |\n",
      "|    reward               | 314.22992     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 794           |\n",
      "-------------------------------------------\n",
      "Episode: 84\n",
      "row: 3654, episode: 84\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3209813.17\n",
      "total_reward: 2209813.17\n",
      "total_cost: 1059122.88\n",
      "total_trades: 3065\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 2278          |\n",
      "|    total_timesteps      | 292400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3763365e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 5.51e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 436           |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | 0.000111      |\n",
      "|    reward               | 220.98132     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 872           |\n",
      "-------------------------------------------\n",
      "Episode: 85\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 2309         |\n",
      "|    total_timesteps      | 296055       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.575126e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.000257    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 213          |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -3.81e-05    |\n",
      "|    reward               | 172.94911    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 425          |\n",
      "------------------------------------------\n",
      "Episode: 86\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 82            |\n",
      "|    time_elapsed         | 2336          |\n",
      "|    total_timesteps      | 299710        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047384817 |\n",
      "|    clip_fraction        | 0.000137      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000771     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 129           |\n",
      "|    n_updates            | 810           |\n",
      "|    policy_gradient_loss | -9.83e-05     |\n",
      "|    reward               | 277.26324     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 258           |\n",
      "-------------------------------------------\n",
      "Episode: 87\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 83           |\n",
      "|    time_elapsed         | 2365         |\n",
      "|    total_timesteps      | 303365       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007932114 |\n",
      "|    clip_fraction        | 0.00167      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000108     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 338          |\n",
      "|    n_updates            | 820          |\n",
      "|    policy_gradient_loss | -0.000229    |\n",
      "|    reward               | 159.77768    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 676          |\n",
      "------------------------------------------\n",
      "Episode: 88\n",
      "row: 3654, episode: 88\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5815903.86\n",
      "total_reward: 4815903.86\n",
      "total_cost: 1624642.47\n",
      "total_trades: 3090\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 84           |\n",
      "|    time_elapsed         | 2393         |\n",
      "|    total_timesteps      | 307020       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015154822 |\n",
      "|    clip_fraction        | 0.0166       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.000326    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 110          |\n",
      "|    n_updates            | 830          |\n",
      "|    policy_gradient_loss | -0.00033     |\n",
      "|    reward               | 481.5904     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 219          |\n",
      "------------------------------------------\n",
      "Episode: 89\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 2421          |\n",
      "|    total_timesteps      | 310675        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00083330274 |\n",
      "|    clip_fraction        | 0.00202       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000189      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.04e+03      |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | -0.000751     |\n",
      "|    reward               | 204.02766     |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 2.08e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 90\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 2448        |\n",
      "|    total_timesteps      | 314330      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000528202 |\n",
      "|    clip_fraction        | 0.000219    |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.000307    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 181         |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -9.1e-05    |\n",
      "|    reward               | 116.34913   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 361         |\n",
      "-----------------------------------------\n",
      "Episode: 91\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 2477          |\n",
      "|    total_timesteps      | 317985        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00087988423 |\n",
      "|    clip_fraction        | 0.00295       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.00106      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 57.2          |\n",
      "|    n_updates            | 860           |\n",
      "|    policy_gradient_loss | -0.000636     |\n",
      "|    reward               | 323.1204      |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 115           |\n",
      "-------------------------------------------\n",
      "Episode: 92\n",
      "row: 3654, episode: 92\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4738270.13\n",
      "total_reward: 3738270.13\n",
      "total_cost: 1472523.55\n",
      "total_trades: 3085\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 2504         |\n",
      "|    total_timesteps      | 321640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012023072 |\n",
      "|    clip_fraction        | 0.00919      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 9.84e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 462          |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    reward               | 373.82703    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 924          |\n",
      "------------------------------------------\n",
      "Episode: 93\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 2533         |\n",
      "|    total_timesteps      | 325295       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015158256 |\n",
      "|    clip_fraction        | 0.0117       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -5.73e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 621          |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | -0.000704    |\n",
      "|    reward               | 147.59874    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 1.24e+03     |\n",
      "------------------------------------------\n",
      "Episode: 94\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 90            |\n",
      "|    time_elapsed         | 2565          |\n",
      "|    total_timesteps      | 328950        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017116201 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000447     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 93.2          |\n",
      "|    n_updates            | 890           |\n",
      "|    policy_gradient_loss | 0.000223      |\n",
      "|    reward               | 105.69373     |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 186           |\n",
      "-------------------------------------------\n",
      "Episode: 95\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 91            |\n",
      "|    time_elapsed         | 2595          |\n",
      "|    total_timesteps      | 332605        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5039822e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.48         |\n",
      "|    explained_variance   | -0.00139      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 46.9          |\n",
      "|    n_updates            | 900           |\n",
      "|    policy_gradient_loss | 1.18e-05      |\n",
      "|    reward               | 194.73708     |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 94.1          |\n",
      "-------------------------------------------\n",
      "Episode: 96\n",
      "row: 3654, episode: 96\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1807067.77\n",
      "total_reward: 807067.77\n",
      "total_cost: 1091874.85\n",
      "total_trades: 3315\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 2624         |\n",
      "|    total_timesteps      | 336260       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035562313 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | -5.38e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 164          |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000365    |\n",
      "|    reward               | 80.70678     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 329          |\n",
      "------------------------------------------\n",
      "Episode: 97\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 93            |\n",
      "|    time_elapsed         | 2653          |\n",
      "|    total_timesteps      | 339915        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034188456 |\n",
      "|    clip_fraction        | 0.00112       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.48         |\n",
      "|    explained_variance   | -0.000983     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 26.8          |\n",
      "|    n_updates            | 920           |\n",
      "|    policy_gradient_loss | -0.000112     |\n",
      "|    reward               | 193.0998      |\n",
      "|    std                  | 1.07          |\n",
      "|    value_loss           | 54.2          |\n",
      "-------------------------------------------\n",
      "Episode: 98\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 2681         |\n",
      "|    total_timesteps      | 343570       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036297303 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 6.5e-05      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 162          |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.000125    |\n",
      "|    reward               | 78.469795    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 325          |\n",
      "------------------------------------------\n",
      "Episode: 99\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 2710         |\n",
      "|    total_timesteps      | 347225       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005125571 |\n",
      "|    clip_fraction        | 0.000301     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.000352     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 25.6         |\n",
      "|    n_updates            | 940          |\n",
      "|    policy_gradient_loss | 0.000333     |\n",
      "|    reward               | 267.04288    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 51.6         |\n",
      "------------------------------------------\n",
      "Episode: 100\n",
      "row: 3654, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2234871.70\n",
      "total_reward: 1234871.70\n",
      "total_cost: 1344868.78\n",
      "total_trades: 3387\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 96            |\n",
      "|    time_elapsed         | 2741          |\n",
      "|    total_timesteps      | 350880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7553609e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.48         |\n",
      "|    explained_variance   | 0.000107      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 315           |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | -4.06e-07     |\n",
      "|    reward               | 123.48717     |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 631           |\n",
      "-------------------------------------------\n",
      "Episode: 101\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 2772        |\n",
      "|    total_timesteps      | 354535      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001496531 |\n",
      "|    clip_fraction        | 0.0108      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | -0.000297   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 65.3        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.000687   |\n",
      "|    reward               | 176.56097   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 131         |\n",
      "-----------------------------------------\n",
      "Episode: 102\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 127         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 2801        |\n",
      "|    total_timesteps      | 358190      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000737841 |\n",
      "|    clip_fraction        | 0.00512     |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.48       |\n",
      "|    explained_variance   | 0.000106    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 136         |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | 3.13e-05    |\n",
      "|    reward               | 219.05022   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 271         |\n",
      "-----------------------------------------\n",
      "Episode: 103\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 2830         |\n",
      "|    total_timesteps      | 361845       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021254781 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 5.73e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 211          |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -0.000268    |\n",
      "|    reward               | 430.90167    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 422          |\n",
      "------------------------------------------\n",
      "Episode: 104\n",
      "row: 3654, episode: 104\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2941628.91\n",
      "total_reward: 1941628.91\n",
      "total_cost: 1078028.42\n",
      "total_trades: 3222\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 2858         |\n",
      "|    total_timesteps      | 365500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019604722 |\n",
      "|    clip_fraction        | 0.0358       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.000208     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 832          |\n",
      "|    n_updates            | 990          |\n",
      "|    policy_gradient_loss | 0.000152     |\n",
      "|    reward               | 194.16289    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 1.66e+03     |\n",
      "------------------------------------------\n",
      "Episode: 105\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 101           |\n",
      "|    time_elapsed         | 2887          |\n",
      "|    total_timesteps      | 369155        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011758522 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.48         |\n",
      "|    explained_variance   | 4.68e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 164           |\n",
      "|    n_updates            | 1000          |\n",
      "|    policy_gradient_loss | -1.38e-05     |\n",
      "|    reward               | 181.6762      |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 329           |\n",
      "-------------------------------------------\n",
      "Episode: 106\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 102          |\n",
      "|    time_elapsed         | 2916         |\n",
      "|    total_timesteps      | 372810       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029688294 |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | -0.000108    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 144          |\n",
      "|    n_updates            | 1010         |\n",
      "|    policy_gradient_loss | -0.000375    |\n",
      "|    reward               | 88.916695    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 288          |\n",
      "------------------------------------------\n",
      "Episode: 107\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 103           |\n",
      "|    time_elapsed         | 2944          |\n",
      "|    total_timesteps      | 376465        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016043529 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.48         |\n",
      "|    explained_variance   | -0.000267     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 33            |\n",
      "|    n_updates            | 1020          |\n",
      "|    policy_gradient_loss | -0.000122     |\n",
      "|    reward               | 184.52136     |\n",
      "|    std                  | 1.06          |\n",
      "|    value_loss           | 66.5          |\n",
      "-------------------------------------------\n",
      "Episode: 108\n",
      "row: 3654, episode: 108\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2798246.44\n",
      "total_reward: 1798246.44\n",
      "total_cost: 1082636.88\n",
      "total_trades: 3255\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 2979         |\n",
      "|    total_timesteps      | 380120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021239244 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.000289     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 149          |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -0.000234    |\n",
      "|    reward               | 179.82465    |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 298          |\n",
      "------------------------------------------\n",
      "Episode: 109\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 105          |\n",
      "|    time_elapsed         | 3011         |\n",
      "|    total_timesteps      | 383775       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018835881 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | 0.000132     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 142          |\n",
      "|    n_updates            | 1040         |\n",
      "|    policy_gradient_loss | -0.000318    |\n",
      "|    reward               | 199.0989     |\n",
      "|    std                  | 1.06         |\n",
      "|    value_loss           | 283          |\n",
      "------------------------------------------\n",
      "Episode: 110\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 106           |\n",
      "|    time_elapsed         | 3044          |\n",
      "|    total_timesteps      | 387430        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017153432 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.48         |\n",
      "|    explained_variance   | -4.7e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 174           |\n",
      "|    n_updates            | 1050          |\n",
      "|    policy_gradient_loss | 4.76e-06      |\n",
      "|    reward               | 125.886314    |\n",
      "|    std                  | 1.07          |\n",
      "|    value_loss           | 348           |\n",
      "-------------------------------------------\n",
      "Episode: 111\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 3078         |\n",
      "|    total_timesteps      | 391085       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.054752e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.48        |\n",
      "|    explained_variance   | -7.98e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 68.2         |\n",
      "|    n_updates            | 1060         |\n",
      "|    policy_gradient_loss | 0.000174     |\n",
      "|    reward               | 174.16766    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 137          |\n",
      "------------------------------------------\n",
      "Episode: 112\n",
      "row: 3654, episode: 112\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2699113.90\n",
      "total_reward: 1699113.90\n",
      "total_cost: 1135300.06\n",
      "total_trades: 3327\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 108           |\n",
      "|    time_elapsed         | 3111          |\n",
      "|    total_timesteps      | 394740        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00065710786 |\n",
      "|    clip_fraction        | 0.00156       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.49         |\n",
      "|    explained_variance   | -5.54e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 133           |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | -3.14e-06     |\n",
      "|    reward               | 169.91139     |\n",
      "|    std                  | 1.07          |\n",
      "|    value_loss           | 266           |\n",
      "-------------------------------------------\n",
      "Episode: 113\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 3144         |\n",
      "|    total_timesteps      | 398395       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026572722 |\n",
      "|    clip_fraction        | 0.0689       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.000135    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 126          |\n",
      "|    n_updates            | 1080         |\n",
      "|    policy_gradient_loss | -0.000555    |\n",
      "|    reward               | 58.105816    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 253          |\n",
      "------------------------------------------\n",
      "Episode: 114\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 110          |\n",
      "|    time_elapsed         | 3176         |\n",
      "|    total_timesteps      | 402050       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035642206 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.00293     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13.8         |\n",
      "|    n_updates            | 1090         |\n",
      "|    policy_gradient_loss | 0.00116      |\n",
      "|    reward               | 301.84628    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 28.1         |\n",
      "------------------------------------------\n",
      "Episode: 115\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 111           |\n",
      "|    time_elapsed         | 3209          |\n",
      "|    total_timesteps      | 405705        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015608378 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.49         |\n",
      "|    explained_variance   | -5.16e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 408           |\n",
      "|    n_updates            | 1100          |\n",
      "|    policy_gradient_loss | 0.000405      |\n",
      "|    reward               | 130.14226     |\n",
      "|    std                  | 1.07          |\n",
      "|    value_loss           | 817           |\n",
      "-------------------------------------------\n",
      "Episode: 116\n",
      "row: 3654, episode: 116\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2922576.28\n",
      "total_reward: 1922576.28\n",
      "total_cost: 1263210.36\n",
      "total_trades: 3317\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 112          |\n",
      "|    time_elapsed         | 3241         |\n",
      "|    total_timesteps      | 409360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018145766 |\n",
      "|    clip_fraction        | 0.0158       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.000421     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 73.8         |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -0.000443    |\n",
      "|    reward               | 192.25763    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 148          |\n",
      "------------------------------------------\n",
      "Episode: 117\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 113          |\n",
      "|    time_elapsed         | 3272         |\n",
      "|    total_timesteps      | 413015       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0018458618 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -8.11e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 164          |\n",
      "|    n_updates            | 1120         |\n",
      "|    policy_gradient_loss | -0.000476    |\n",
      "|    reward               | 114.36722    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 327          |\n",
      "------------------------------------------\n",
      "Episode: 118\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 114          |\n",
      "|    time_elapsed         | 3300         |\n",
      "|    total_timesteps      | 416670       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.843215e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.000308    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 56.7         |\n",
      "|    n_updates            | 1130         |\n",
      "|    policy_gradient_loss | 3.74e-05     |\n",
      "|    reward               | 226.0594     |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "Episode: 119\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 126         |\n",
      "|    iterations           | 115         |\n",
      "|    time_elapsed         | 3328        |\n",
      "|    total_timesteps      | 420325      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000725174 |\n",
      "|    clip_fraction        | 0.00175     |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | -5.41e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 227         |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -9.41e-05   |\n",
      "|    reward               | 167.69919   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 455         |\n",
      "-----------------------------------------\n",
      "Episode: 120\n",
      "row: 3654, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2594868.25\n",
      "total_reward: 1594868.25\n",
      "total_cost: 1338521.51\n",
      "total_trades: 3363\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 3353         |\n",
      "|    total_timesteps      | 423980       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007006684 |\n",
      "|    clip_fraction        | 0.00129      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 8.52e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 124          |\n",
      "|    n_updates            | 1150         |\n",
      "|    policy_gradient_loss | -5.74e-05    |\n",
      "|    reward               | 159.48683    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 248          |\n",
      "------------------------------------------\n",
      "Episode: 121\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 117          |\n",
      "|    time_elapsed         | 3378         |\n",
      "|    total_timesteps      | 427635       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.803831e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 0.000238     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 112          |\n",
      "|    n_updates            | 1160         |\n",
      "|    policy_gradient_loss | 2.55e-05     |\n",
      "|    reward               | 97.282326    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 223          |\n",
      "------------------------------------------\n",
      "Episode: 122\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 118          |\n",
      "|    time_elapsed         | 3406         |\n",
      "|    total_timesteps      | 431290       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006878771 |\n",
      "|    clip_fraction        | 0.002        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.000276    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 40.6         |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | 5.36e-05     |\n",
      "|    reward               | 63.402977    |\n",
      "|    std                  | 1.07         |\n",
      "|    value_loss           | 81.4         |\n",
      "------------------------------------------\n",
      "Episode: 123\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 126            |\n",
      "|    iterations           | 119            |\n",
      "|    time_elapsed         | 3435           |\n",
      "|    total_timesteps      | 434945         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.23594755e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.49          |\n",
      "|    explained_variance   | -0.000672      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 16.7           |\n",
      "|    n_updates            | 1180           |\n",
      "|    policy_gradient_loss | -3.86e-05      |\n",
      "|    reward               | 70.69824       |\n",
      "|    std                  | 1.08           |\n",
      "|    value_loss           | 33.6           |\n",
      "--------------------------------------------\n",
      "Episode: 124\n",
      "row: 3654, episode: 124\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2672547.52\n",
      "total_reward: 1672547.52\n",
      "total_cost: 1022708.23\n",
      "total_trades: 3298\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 3463         |\n",
      "|    total_timesteps      | 438600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017401119 |\n",
      "|    clip_fraction        | 0.0112       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.0012      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 21.3         |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.000159    |\n",
      "|    reward               | 167.25475    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 42.7         |\n",
      "------------------------------------------\n",
      "Episode: 125\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 121           |\n",
      "|    time_elapsed         | 3493          |\n",
      "|    total_timesteps      | 442255        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023245325 |\n",
      "|    clip_fraction        | 0.000246      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | 0.000233      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 124           |\n",
      "|    n_updates            | 1200          |\n",
      "|    policy_gradient_loss | 0.000378      |\n",
      "|    reward               | 107.827934    |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 249           |\n",
      "-------------------------------------------\n",
      "Episode: 126\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 3526         |\n",
      "|    total_timesteps      | 445910       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028378183 |\n",
      "|    clip_fraction        | 0.0997       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | -0.000685    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 50.9         |\n",
      "|    n_updates            | 1210         |\n",
      "|    policy_gradient_loss | -0.000328    |\n",
      "|    reward               | 103.24014    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 102          |\n",
      "------------------------------------------\n",
      "Episode: 127\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 3556         |\n",
      "|    total_timesteps      | 449565       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017427872 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | -0.000161    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 46.3         |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -0.000653    |\n",
      "|    reward               | 179.8939     |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 92.7         |\n",
      "------------------------------------------\n",
      "Episode: 128\n",
      "row: 3654, episode: 128\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2425606.49\n",
      "total_reward: 1425606.49\n",
      "total_cost: 1209409.60\n",
      "total_trades: 3300\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 126       |\n",
      "|    iterations           | 124       |\n",
      "|    time_elapsed         | 3586      |\n",
      "|    total_timesteps      | 453220    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0037327 |\n",
      "|    clip_fraction        | 0.122     |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -1.5      |\n",
      "|    explained_variance   | 4.37e-05  |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 144       |\n",
      "|    n_updates            | 1230      |\n",
      "|    policy_gradient_loss | -0.000248 |\n",
      "|    reward               | 142.56065 |\n",
      "|    std                  | 1.08      |\n",
      "|    value_loss           | 288       |\n",
      "---------------------------------------\n",
      "Episode: 129\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 125          |\n",
      "|    time_elapsed         | 3616         |\n",
      "|    total_timesteps      | 456875       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.316599e-05 |\n",
      "|    clip_fraction        | 8.21e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | -0.00015     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 89.7         |\n",
      "|    n_updates            | 1240         |\n",
      "|    policy_gradient_loss | -9.01e-05    |\n",
      "|    reward               | 77.080055    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 179          |\n",
      "------------------------------------------\n",
      "Episode: 130\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 126           |\n",
      "|    time_elapsed         | 3646          |\n",
      "|    total_timesteps      | 460530        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8429491e-05 |\n",
      "|    clip_fraction        | 0.000109      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | -0.000979     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 25.4          |\n",
      "|    n_updates            | 1250          |\n",
      "|    policy_gradient_loss | -7.17e-05     |\n",
      "|    reward               | 141.96292     |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 51            |\n",
      "-------------------------------------------\n",
      "Episode: 131\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 127          |\n",
      "|    time_elapsed         | 3678         |\n",
      "|    total_timesteps      | 464185       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040185126 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | -8.34e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 89           |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -0.000848    |\n",
      "|    reward               | 169.32957    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 178          |\n",
      "------------------------------------------\n",
      "Episode: 132\n",
      "row: 3654, episode: 132\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4761528.68\n",
      "total_reward: 3761528.68\n",
      "total_cost: 1870787.08\n",
      "total_trades: 3243\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 3708          |\n",
      "|    total_timesteps      | 467840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021206753 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | -6.03e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 127           |\n",
      "|    n_updates            | 1270          |\n",
      "|    policy_gradient_loss | 2.18e-05      |\n",
      "|    reward               | 376.15286     |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 254           |\n",
      "-------------------------------------------\n",
      "Episode: 133\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 129           |\n",
      "|    time_elapsed         | 3739          |\n",
      "|    total_timesteps      | 471495        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8897832e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | 0.000494      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 638           |\n",
      "|    n_updates            | 1280          |\n",
      "|    policy_gradient_loss | 3.75e-05      |\n",
      "|    reward               | 86.25863      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 1.28e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 134\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 3768          |\n",
      "|    total_timesteps      | 475150        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3811416e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | -0.00114      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 32            |\n",
      "|    n_updates            | 1290          |\n",
      "|    policy_gradient_loss | -2.24e-05     |\n",
      "|    reward               | 114.846756    |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 64.1          |\n",
      "-------------------------------------------\n",
      "Episode: 135\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 125           |\n",
      "|    iterations           | 131           |\n",
      "|    time_elapsed         | 3800          |\n",
      "|    total_timesteps      | 478805        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018651494 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | 0.000397      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 57.3          |\n",
      "|    n_updates            | 1300          |\n",
      "|    policy_gradient_loss | -5.42e-05     |\n",
      "|    reward               | 94.62454      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 115           |\n",
      "-------------------------------------------\n",
      "Episode: 136\n",
      "row: 3654, episode: 136\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3824238.02\n",
      "total_reward: 2824238.02\n",
      "total_cost: 1487233.00\n",
      "total_trades: 3210\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 132          |\n",
      "|    time_elapsed         | 3828         |\n",
      "|    total_timesteps      | 482460       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020530343 |\n",
      "|    clip_fraction        | 0.0474       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | 0.000145     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 38.6         |\n",
      "|    n_updates            | 1310         |\n",
      "|    policy_gradient_loss | -0.000444    |\n",
      "|    reward               | 282.4238     |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 77.3         |\n",
      "------------------------------------------\n",
      "Episode: 137\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 133          |\n",
      "|    time_elapsed         | 3859         |\n",
      "|    total_timesteps      | 486115       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045439973 |\n",
      "|    clip_fraction        | 0.122        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | 0.000172     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 358          |\n",
      "|    n_updates            | 1320         |\n",
      "|    policy_gradient_loss | -0.000687    |\n",
      "|    reward               | 178.19275    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 717          |\n",
      "------------------------------------------\n",
      "Episode: 138\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 125           |\n",
      "|    iterations           | 134           |\n",
      "|    time_elapsed         | 3892          |\n",
      "|    total_timesteps      | 489770        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2997384e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | 0.000212      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 141           |\n",
      "|    n_updates            | 1330          |\n",
      "|    policy_gradient_loss | 2.25e-06      |\n",
      "|    reward               | 246.14156     |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 282           |\n",
      "-------------------------------------------\n",
      "Episode: 139\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 125           |\n",
      "|    iterations           | 135           |\n",
      "|    time_elapsed         | 3924          |\n",
      "|    total_timesteps      | 493425        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00030798698 |\n",
      "|    clip_fraction        | 0.000137      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.49         |\n",
      "|    explained_variance   | 5.92e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 270           |\n",
      "|    n_updates            | 1340          |\n",
      "|    policy_gradient_loss | -6.48e-05     |\n",
      "|    reward               | 331.9172      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 541           |\n",
      "-------------------------------------------\n",
      "Episode: 140\n",
      "row: 3654, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2899516.00\n",
      "total_reward: 1899516.00\n",
      "total_cost: 1332741.06\n",
      "total_trades: 3255\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 3956         |\n",
      "|    total_timesteps      | 497080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011442603 |\n",
      "|    clip_fraction        | 0.00501      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | 4.15e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 494          |\n",
      "|    n_updates            | 1350         |\n",
      "|    policy_gradient_loss | -0.000857    |\n",
      "|    reward               | 189.9516     |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 989          |\n",
      "------------------------------------------\n",
      "Episode: 141\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 137          |\n",
      "|    time_elapsed         | 3988         |\n",
      "|    total_timesteps      | 500735       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058097467 |\n",
      "|    clip_fraction        | 0.199        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -1.92e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 159          |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -0.00033     |\n",
      "|    reward               | 114.55022    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 318          |\n",
      "------------------------------------------\n",
      "Episode: 142\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 4022         |\n",
      "|    total_timesteps      | 504390       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015439859 |\n",
      "|    clip_fraction        | 0.0308       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.000638    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 56.4         |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | 0.000112     |\n",
      "|    reward               | 332.22784    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 113          |\n",
      "------------------------------------------\n",
      "Episode: 143\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 125           |\n",
      "|    iterations           | 139           |\n",
      "|    time_elapsed         | 4055          |\n",
      "|    total_timesteps      | 508045        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045180588 |\n",
      "|    clip_fraction        | 0.000192      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.49         |\n",
      "|    explained_variance   | 8.64e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 495           |\n",
      "|    n_updates            | 1380          |\n",
      "|    policy_gradient_loss | -0.000142     |\n",
      "|    reward               | 291.1795      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 990           |\n",
      "-------------------------------------------\n",
      "Episode: 144\n",
      "row: 3654, episode: 144\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3467691.75\n",
      "total_reward: 2467691.75\n",
      "total_cost: 1159637.44\n",
      "total_trades: 3186\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 125           |\n",
      "|    iterations           | 140           |\n",
      "|    time_elapsed         | 4087          |\n",
      "|    total_timesteps      | 511700        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012621166 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.49         |\n",
      "|    explained_variance   | 0.000112      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 378           |\n",
      "|    n_updates            | 1390          |\n",
      "|    policy_gradient_loss | -2.45e-05     |\n",
      "|    reward               | 246.76918     |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 757           |\n",
      "-------------------------------------------\n",
      "Episode: 145\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 125          |\n",
      "|    iterations           | 141          |\n",
      "|    time_elapsed         | 4119         |\n",
      "|    total_timesteps      | 515355       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021994384 |\n",
      "|    clip_fraction        | 0.0201       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.49        |\n",
      "|    explained_variance   | -0.000113    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 270          |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -0.000524    |\n",
      "|    reward               | 264.07605    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 540          |\n",
      "------------------------------------------\n",
      "Episode: 146\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 125           |\n",
      "|    iterations           | 142           |\n",
      "|    time_elapsed         | 4151          |\n",
      "|    total_timesteps      | 519010        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011272703 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | 7.19e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 309           |\n",
      "|    n_updates            | 1410          |\n",
      "|    policy_gradient_loss | -1.92e-05     |\n",
      "|    reward               | 336.7294      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 618           |\n",
      "-------------------------------------------\n",
      "Episode: 147\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 124          |\n",
      "|    iterations           | 143          |\n",
      "|    time_elapsed         | 4182         |\n",
      "|    total_timesteps      | 522665       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.620427e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | 7.02e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 505          |\n",
      "|    n_updates            | 1420         |\n",
      "|    policy_gradient_loss | 1.21e-05     |\n",
      "|    reward               | 136.50046    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 1.01e+03     |\n",
      "------------------------------------------\n",
      "Episode: 148\n",
      "row: 3654, episode: 148\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3543070.96\n",
      "total_reward: 2543070.96\n",
      "total_cost: 1218563.19\n",
      "total_trades: 3238\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 124           |\n",
      "|    iterations           | 144           |\n",
      "|    time_elapsed         | 4215          |\n",
      "|    total_timesteps      | 526320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00075925916 |\n",
      "|    clip_fraction        | 0.00175       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | -0.00137      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 80.4          |\n",
      "|    n_updates            | 1430          |\n",
      "|    policy_gradient_loss | -0.000556     |\n",
      "|    reward               | 254.3071      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 161           |\n",
      "-------------------------------------------\n",
      "Episode: 149\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 124          |\n",
      "|    iterations           | 145          |\n",
      "|    time_elapsed         | 4246         |\n",
      "|    total_timesteps      | 529975       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026282896 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | -5.72e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 285          |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -0.00049     |\n",
      "|    reward               | 154.02383    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 570          |\n",
      "------------------------------------------\n",
      "Episode: 150\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 146         |\n",
      "|    time_elapsed         | 4275        |\n",
      "|    total_timesteps      | 533630      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001193543 |\n",
      "|    clip_fraction        | 0.00807     |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | -0.000636   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 1450        |\n",
      "|    policy_gradient_loss | -0.000391   |\n",
      "|    reward               | 126.728226  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 205         |\n",
      "-----------------------------------------\n",
      "Episode: 151\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 124           |\n",
      "|    iterations           | 147           |\n",
      "|    time_elapsed         | 4306          |\n",
      "|    total_timesteps      | 537285        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018398807 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | -0.0016       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 68.4          |\n",
      "|    n_updates            | 1460          |\n",
      "|    policy_gradient_loss | 7.8e-05       |\n",
      "|    reward               | 205.39001     |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 137           |\n",
      "-------------------------------------------\n",
      "Episode: 152\n",
      "row: 3654, episode: 152\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3019516.64\n",
      "total_reward: 2019516.64\n",
      "total_cost: 1274719.82\n",
      "total_trades: 3161\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 124          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 4337         |\n",
      "|    total_timesteps      | 540940       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047364165 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | 0.00039      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 185          |\n",
      "|    n_updates            | 1470         |\n",
      "|    policy_gradient_loss | -0.000438    |\n",
      "|    reward               | 201.95166    |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 370          |\n",
      "------------------------------------------\n",
      "Episode: 153\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 124           |\n",
      "|    iterations           | 149           |\n",
      "|    time_elapsed         | 4370          |\n",
      "|    total_timesteps      | 544595        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00068420405 |\n",
      "|    clip_fraction        | 0.00189       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | -2.74e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 179           |\n",
      "|    n_updates            | 1480          |\n",
      "|    policy_gradient_loss | -4.38e-05     |\n",
      "|    reward               | 224.9202      |\n",
      "|    std                  | 1.08          |\n",
      "|    value_loss           | 358           |\n",
      "-------------------------------------------\n",
      "Episode: 154\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 124          |\n",
      "|    iterations           | 150          |\n",
      "|    time_elapsed         | 4404         |\n",
      "|    total_timesteps      | 548250       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.873351e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.5         |\n",
      "|    explained_variance   | 0.000155     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 223          |\n",
      "|    n_updates            | 1490         |\n",
      "|    policy_gradient_loss | 1.53e-05     |\n",
      "|    reward               | 211.8268     |\n",
      "|    std                  | 1.08         |\n",
      "|    value_loss           | 445          |\n",
      "------------------------------------------\n",
      "Episode: 155\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 124           |\n",
      "|    iterations           | 151           |\n",
      "|    time_elapsed         | 4432          |\n",
      "|    total_timesteps      | 551905        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00081351545 |\n",
      "|    clip_fraction        | 0.0354        |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.5          |\n",
      "|    explained_variance   | 0.000113      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 197           |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | -0.000324     |\n",
      "|    reward               | 187.62355     |\n",
      "|    std                  | 1.09          |\n",
      "|    value_loss           | 393           |\n",
      "-------------------------------------------\n",
      "Episode: 156\n",
      "row: 3654, episode: 156\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3365658.97\n",
      "total_reward: 2365658.97\n",
      "total_cost: 1309989.98\n",
      "total_trades: 3241\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 124           |\n",
      "|    iterations           | 152           |\n",
      "|    time_elapsed         | 4464          |\n",
      "|    total_timesteps      | 555560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013632173 |\n",
      "|    clip_fraction        | 0.002         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.51         |\n",
      "|    explained_variance   | 0.000139      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 154           |\n",
      "|    n_updates            | 1510          |\n",
      "|    policy_gradient_loss | -0.000138     |\n",
      "|    reward               | 236.5659      |\n",
      "|    std                  | 1.1           |\n",
      "|    value_loss           | 307           |\n",
      "-------------------------------------------\n",
      "Episode: 157\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 124           |\n",
      "|    iterations           | 153           |\n",
      "|    time_elapsed         | 4494          |\n",
      "|    total_timesteps      | 559215        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018144216 |\n",
      "|    clip_fraction        | 0.000356      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.52         |\n",
      "|    explained_variance   | -9.27e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 247           |\n",
      "|    n_updates            | 1520          |\n",
      "|    policy_gradient_loss | 0.000105      |\n",
      "|    reward               | 103.70698     |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 494           |\n",
      "-------------------------------------------\n",
      "Episode: 158\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 124          |\n",
      "|    iterations           | 154          |\n",
      "|    time_elapsed         | 4523         |\n",
      "|    total_timesteps      | 562870       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027016033 |\n",
      "|    clip_fraction        | 0.0631       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | 1.6e-05      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 45.8         |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | 0.000512     |\n",
      "|    reward               | 55.876995    |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 91.7         |\n",
      "------------------------------------------\n",
      "Episode: 159\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 124           |\n",
      "|    iterations           | 155           |\n",
      "|    time_elapsed         | 4551          |\n",
      "|    total_timesteps      | 566525        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3877348e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.52         |\n",
      "|    explained_variance   | -0.00311      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 12.6          |\n",
      "|    n_updates            | 1540          |\n",
      "|    policy_gradient_loss | 1.94e-05      |\n",
      "|    reward               | 110.852684    |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 25.8          |\n",
      "-------------------------------------------\n",
      "Episode: 160\n",
      "row: 3654, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2022434.25\n",
      "total_reward: 1022434.25\n",
      "total_cost: 1078651.36\n",
      "total_trades: 3261\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 124          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 4588         |\n",
      "|    total_timesteps      | 570180       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011337486 |\n",
      "|    clip_fraction        | 0.00391      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | 3.99e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 52.3         |\n",
      "|    n_updates            | 1550         |\n",
      "|    policy_gradient_loss | -0.000331    |\n",
      "|    reward               | 102.24342    |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 105          |\n",
      "------------------------------------------\n",
      "Episode: 161\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 124           |\n",
      "|    iterations           | 157           |\n",
      "|    time_elapsed         | 4623          |\n",
      "|    total_timesteps      | 573835        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00093714433 |\n",
      "|    clip_fraction        | 0.0211        |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.52         |\n",
      "|    explained_variance   | -7.26e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 44.6          |\n",
      "|    n_updates            | 1560          |\n",
      "|    policy_gradient_loss | 0.000495      |\n",
      "|    reward               | 109.32513     |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 89.3          |\n",
      "-------------------------------------------\n",
      "Episode: 162\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 123           |\n",
      "|    iterations           | 158           |\n",
      "|    time_elapsed         | 4660          |\n",
      "|    total_timesteps      | 577490        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024477797 |\n",
      "|    clip_fraction        | 0.0118        |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.52         |\n",
      "|    explained_variance   | -0.000218     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 51.5          |\n",
      "|    n_updates            | 1570          |\n",
      "|    policy_gradient_loss | 0.00013       |\n",
      "|    reward               | 171.15123     |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 103           |\n",
      "-------------------------------------------\n",
      "Episode: 163\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 159          |\n",
      "|    time_elapsed         | 4694         |\n",
      "|    total_timesteps      | 581145       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026537657 |\n",
      "|    clip_fraction        | 0.0491       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | 0.000142     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 129          |\n",
      "|    n_updates            | 1580         |\n",
      "|    policy_gradient_loss | -0.000239    |\n",
      "|    reward               | 54.01561     |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 258          |\n",
      "------------------------------------------\n",
      "Episode: 164\n",
      "row: 3654, episode: 164\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3149259.17\n",
      "total_reward: 2149259.17\n",
      "total_cost: 1307590.81\n",
      "total_trades: 3373\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 123           |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 4729          |\n",
      "|    total_timesteps      | 584800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016788111 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.52         |\n",
      "|    explained_variance   | -0.00151      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 12.3          |\n",
      "|    n_updates            | 1590          |\n",
      "|    policy_gradient_loss | -5.45e-05     |\n",
      "|    reward               | 214.92592     |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 24.8          |\n",
      "-------------------------------------------\n",
      "Episode: 165\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 123           |\n",
      "|    iterations           | 161           |\n",
      "|    time_elapsed         | 4764          |\n",
      "|    total_timesteps      | 588455        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046978277 |\n",
      "|    clip_fraction        | 5.47e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.52         |\n",
      "|    explained_variance   | 0.000444      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 205           |\n",
      "|    n_updates            | 1600          |\n",
      "|    policy_gradient_loss | -0.000288     |\n",
      "|    reward               | 120.30741     |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 410           |\n",
      "-------------------------------------------\n",
      "Episode: 166\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 4798         |\n",
      "|    total_timesteps      | 592110       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059567415 |\n",
      "|    clip_fraction        | 0.131        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.52        |\n",
      "|    explained_variance   | -0.000127    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 62.9         |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    reward               | 191.8814     |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 126          |\n",
      "------------------------------------------\n",
      "Episode: 167\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 123           |\n",
      "|    iterations           | 163           |\n",
      "|    time_elapsed         | 4829          |\n",
      "|    total_timesteps      | 595765        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017895507 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.53         |\n",
      "|    explained_variance   | 0.000216      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 163           |\n",
      "|    n_updates            | 1620          |\n",
      "|    policy_gradient_loss | -7.95e-05     |\n",
      "|    reward               | 168.34544     |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 326           |\n",
      "-------------------------------------------\n",
      "Episode: 168\n",
      "row: 3654, episode: 168\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2634104.49\n",
      "total_reward: 1634104.49\n",
      "total_cost: 1133522.99\n",
      "total_trades: 3277\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 4861         |\n",
      "|    total_timesteps      | 599420       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008975799 |\n",
      "|    clip_fraction        | 0.0218       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | 0.000135     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 125          |\n",
      "|    n_updates            | 1630         |\n",
      "|    policy_gradient_loss | -0.000301    |\n",
      "|    reward               | 163.41045    |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 249          |\n",
      "------------------------------------------\n",
      "Episode: 169\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 123          |\n",
      "|    iterations           | 165          |\n",
      "|    time_elapsed         | 4895         |\n",
      "|    total_timesteps      | 603075       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.379968e-05 |\n",
      "|    clip_fraction        | 0.056        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | 5.97e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 117          |\n",
      "|    n_updates            | 1640         |\n",
      "|    policy_gradient_loss | -0.00032     |\n",
      "|    reward               | 81.29368     |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 235          |\n",
      "------------------------------------------\n",
      "Episode: 170\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 123           |\n",
      "|    iterations           | 166           |\n",
      "|    time_elapsed         | 4926          |\n",
      "|    total_timesteps      | 606730        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00090784294 |\n",
      "|    clip_fraction        | 0.00257       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.53         |\n",
      "|    explained_variance   | 0.00016       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 28            |\n",
      "|    n_updates            | 1650          |\n",
      "|    policy_gradient_loss | -0.000408     |\n",
      "|    reward               | 158.32405     |\n",
      "|    std                  | 1.11          |\n",
      "|    value_loss           | 56.3          |\n",
      "-------------------------------------------\n",
      "Episode: 171\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 123           |\n",
      "|    iterations           | 167           |\n",
      "|    time_elapsed         | 4957          |\n",
      "|    total_timesteps      | 610385        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024239549 |\n",
      "|    clip_fraction        | 0.000164      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.53         |\n",
      "|    explained_variance   | 5.13e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 110           |\n",
      "|    n_updates            | 1660          |\n",
      "|    policy_gradient_loss | -1.21e-06     |\n",
      "|    reward               | 236.30164     |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 220           |\n",
      "-------------------------------------------\n",
      "Episode: 172\n",
      "row: 3654, episode: 172\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3477419.72\n",
      "total_reward: 2477419.72\n",
      "total_cost: 1738198.72\n",
      "total_trades: 3346\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 122           |\n",
      "|    iterations           | 168           |\n",
      "|    time_elapsed         | 4992          |\n",
      "|    total_timesteps      | 614040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045210758 |\n",
      "|    clip_fraction        | 0.000328      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.53         |\n",
      "|    explained_variance   | 0.000141      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 248           |\n",
      "|    n_updates            | 1670          |\n",
      "|    policy_gradient_loss | -0.000216     |\n",
      "|    reward               | 247.74197     |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 497           |\n",
      "-------------------------------------------\n",
      "Episode: 173\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 5025         |\n",
      "|    total_timesteps      | 617695       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015356157 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | 0.000194     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 273          |\n",
      "|    n_updates            | 1680         |\n",
      "|    policy_gradient_loss | -0.000415    |\n",
      "|    reward               | 105.10905    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 545          |\n",
      "------------------------------------------\n",
      "Episode: 174\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 170          |\n",
      "|    time_elapsed         | 5060         |\n",
      "|    total_timesteps      | 621350       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001895133 |\n",
      "|    clip_fraction        | 5.47e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | -0.000772    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 47.5         |\n",
      "|    n_updates            | 1690         |\n",
      "|    policy_gradient_loss | -0.000172    |\n",
      "|    reward               | 172.05913    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 95           |\n",
      "------------------------------------------\n",
      "Episode: 175\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 171          |\n",
      "|    time_elapsed         | 5092         |\n",
      "|    total_timesteps      | 625005       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053405752 |\n",
      "|    clip_fraction        | 0.158        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | 0.00141      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 130          |\n",
      "|    n_updates            | 1700         |\n",
      "|    policy_gradient_loss | -0.000735    |\n",
      "|    reward               | 104.05869    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 259          |\n",
      "------------------------------------------\n",
      "Episode: 176\n",
      "row: 3654, episode: 176\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2233372.26\n",
      "total_reward: 1233372.26\n",
      "total_cost: 1116583.45\n",
      "total_trades: 3367\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 122         |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 5128        |\n",
      "|    total_timesteps      | 628660      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.17973e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.000432    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 46.5        |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | 0.000128    |\n",
      "|    reward               | 123.33723   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 93.2        |\n",
      "-----------------------------------------\n",
      "Episode: 177\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 5160         |\n",
      "|    total_timesteps      | 632315       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015699188 |\n",
      "|    clip_fraction        | 0.0181       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | -0.000362    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 65.7         |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -0.000262    |\n",
      "|    reward               | 258.37842    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 132          |\n",
      "------------------------------------------\n",
      "Episode: 178\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 174          |\n",
      "|    time_elapsed         | 5194         |\n",
      "|    total_timesteps      | 635970       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001289772 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | 0.000917     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 297          |\n",
      "|    n_updates            | 1730         |\n",
      "|    policy_gradient_loss | -0.00012     |\n",
      "|    reward               | 175.46223    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 595          |\n",
      "------------------------------------------\n",
      "Episode: 179\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 122           |\n",
      "|    iterations           | 175           |\n",
      "|    time_elapsed         | 5229          |\n",
      "|    total_timesteps      | 639625        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024589457 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.53         |\n",
      "|    explained_variance   | 0.000256      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 135           |\n",
      "|    n_updates            | 1740          |\n",
      "|    policy_gradient_loss | -0.000203     |\n",
      "|    reward               | 150.64583     |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 271           |\n",
      "-------------------------------------------\n",
      "Episode: 180\n",
      "row: 3654, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3249989.25\n",
      "total_reward: 2249989.25\n",
      "total_cost: 1517192.57\n",
      "total_trades: 3266\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 176          |\n",
      "|    time_elapsed         | 5262         |\n",
      "|    total_timesteps      | 643280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016085467 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | -0.000189    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 99.1         |\n",
      "|    n_updates            | 1750         |\n",
      "|    policy_gradient_loss | -0.000115    |\n",
      "|    reward               | 224.99893    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 198          |\n",
      "------------------------------------------\n",
      "Episode: 181\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 122          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 5298         |\n",
      "|    total_timesteps      | 646935       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005549098 |\n",
      "|    clip_fraction        | 0.00126      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.53        |\n",
      "|    explained_variance   | 0.000571     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 225          |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -4.17e-05    |\n",
      "|    reward               | 40.238636    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 449          |\n",
      "------------------------------------------\n",
      "Episode: 182\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 121           |\n",
      "|    iterations           | 178           |\n",
      "|    time_elapsed         | 5333          |\n",
      "|    total_timesteps      | 650590        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011921661 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | -0.00436      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.84          |\n",
      "|    n_updates            | 1770          |\n",
      "|    policy_gradient_loss | -1.72e-06     |\n",
      "|    reward               | 68.92954      |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 13.9          |\n",
      "-------------------------------------------\n",
      "Episode: 183\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 121           |\n",
      "|    iterations           | 179           |\n",
      "|    time_elapsed         | 5364          |\n",
      "|    total_timesteps      | 654245        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015730629 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | -0.000901     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 19.8          |\n",
      "|    n_updates            | 1780          |\n",
      "|    policy_gradient_loss | -5.51e-05     |\n",
      "|    reward               | 251.21182     |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 39.9          |\n",
      "-------------------------------------------\n",
      "Episode: 184\n",
      "row: 3654, episode: 184\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1639040.05\n",
      "total_reward: 639040.05\n",
      "total_cost: 1024579.94\n",
      "total_trades: 3399\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 121           |\n",
      "|    iterations           | 180           |\n",
      "|    time_elapsed         | 5401          |\n",
      "|    total_timesteps      | 657900        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025677728 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | 0.00143       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 281           |\n",
      "|    n_updates            | 1790          |\n",
      "|    policy_gradient_loss | -0.000232     |\n",
      "|    reward               | 63.904007     |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 563           |\n",
      "-------------------------------------------\n",
      "Episode: 185\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 121          |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 5438         |\n",
      "|    total_timesteps      | 661555       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014318448 |\n",
      "|    clip_fraction        | 0.0139       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | -0.000224    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 17.1         |\n",
      "|    n_updates            | 1800         |\n",
      "|    policy_gradient_loss | -0.000332    |\n",
      "|    reward               | 165.60365    |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 34.5         |\n",
      "------------------------------------------\n",
      "Episode: 186\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 121          |\n",
      "|    iterations           | 182          |\n",
      "|    time_elapsed         | 5472         |\n",
      "|    total_timesteps      | 665210       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034302024 |\n",
      "|    clip_fraction        | 0.0771       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.00176      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 121          |\n",
      "|    n_updates            | 1810         |\n",
      "|    policy_gradient_loss | -0.000563    |\n",
      "|    reward               | 8.851728     |\n",
      "|    std                  | 1.12         |\n",
      "|    value_loss           | 241          |\n",
      "------------------------------------------\n",
      "Episode: 187\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 121           |\n",
      "|    iterations           | 183           |\n",
      "|    time_elapsed         | 5504          |\n",
      "|    total_timesteps      | 668865        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023337097 |\n",
      "|    clip_fraction        | 0.000109      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | -0.091        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 0.834         |\n",
      "|    n_updates            | 1820          |\n",
      "|    policy_gradient_loss | -0.000208     |\n",
      "|    reward               | 24.763573     |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 1.96          |\n",
      "-------------------------------------------\n",
      "Episode: 188\n",
      "row: 3654, episode: 188\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3574030.84\n",
      "total_reward: 2574030.84\n",
      "total_cost: 1381550.35\n",
      "total_trades: 3291\n",
      "=================================\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 121            |\n",
      "|    iterations           | 184            |\n",
      "|    time_elapsed         | 5535           |\n",
      "|    total_timesteps      | 672520         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000113150585 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.54          |\n",
      "|    explained_variance   | -0.00809       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 2.32           |\n",
      "|    n_updates            | 1830           |\n",
      "|    policy_gradient_loss | -7.1e-05       |\n",
      "|    reward               | 257.40308      |\n",
      "|    std                  | 1.12           |\n",
      "|    value_loss           | 5.18           |\n",
      "--------------------------------------------\n",
      "Episode: 189\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 121           |\n",
      "|    iterations           | 185           |\n",
      "|    time_elapsed         | 5569          |\n",
      "|    total_timesteps      | 676175        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00013750023 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | -3.84e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 297           |\n",
      "|    n_updates            | 1840          |\n",
      "|    policy_gradient_loss | 0.000429      |\n",
      "|    reward               | 161.51562     |\n",
      "|    std                  | 1.12          |\n",
      "|    value_loss           | 595           |\n",
      "-------------------------------------------\n",
      "Episode: 190\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 121          |\n",
      "|    iterations           | 186          |\n",
      "|    time_elapsed         | 5602         |\n",
      "|    total_timesteps      | 679830       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010249186 |\n",
      "|    clip_fraction        | 0.00506      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 3.86e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 115          |\n",
      "|    n_updates            | 1850         |\n",
      "|    policy_gradient_loss | -9.32e-05    |\n",
      "|    reward               | 142.4886     |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 231          |\n",
      "------------------------------------------\n",
      "Episode: 191\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 121          |\n",
      "|    iterations           | 187          |\n",
      "|    time_elapsed         | 5636         |\n",
      "|    total_timesteps      | 683485       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033206136 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | -4.29e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 89.2         |\n",
      "|    n_updates            | 1860         |\n",
      "|    policy_gradient_loss | -1.25e-06    |\n",
      "|    reward               | 243.6384     |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 178          |\n",
      "------------------------------------------\n",
      "Episode: 192\n",
      "row: 3654, episode: 192\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2742670.95\n",
      "total_reward: 1742670.95\n",
      "total_cost: 1231744.80\n",
      "total_trades: 3380\n",
      "=================================\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 121            |\n",
      "|    iterations           | 188            |\n",
      "|    time_elapsed         | 5675           |\n",
      "|    total_timesteps      | 687140         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000104039216 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.54          |\n",
      "|    explained_variance   | 0.00105        |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 265            |\n",
      "|    n_updates            | 1870           |\n",
      "|    policy_gradient_loss | 0.000377       |\n",
      "|    reward               | 174.26709      |\n",
      "|    std                  | 1.13           |\n",
      "|    value_loss           | 530            |\n",
      "--------------------------------------------\n",
      "Episode: 193\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 189          |\n",
      "|    time_elapsed         | 5712         |\n",
      "|    total_timesteps      | 690795       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042555444 |\n",
      "|    clip_fraction        | 0.0792       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.000244     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 134          |\n",
      "|    n_updates            | 1880         |\n",
      "|    policy_gradient_loss | -0.000197    |\n",
      "|    reward               | 238.97029    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 268          |\n",
      "------------------------------------------\n",
      "Episode: 194\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 190          |\n",
      "|    time_elapsed         | 5745         |\n",
      "|    total_timesteps      | 694450       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014305132 |\n",
      "|    clip_fraction        | 0.0183       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.000366     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 254          |\n",
      "|    n_updates            | 1890         |\n",
      "|    policy_gradient_loss | -5.97e-05    |\n",
      "|    reward               | 72.75914     |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 508          |\n",
      "------------------------------------------\n",
      "Episode: 195\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 191          |\n",
      "|    time_elapsed         | 5777         |\n",
      "|    total_timesteps      | 698105       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009494601 |\n",
      "|    clip_fraction        | 0.00304      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | -0.00207     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 22.4         |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -0.00051     |\n",
      "|    reward               | 99.66201     |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 45           |\n",
      "------------------------------------------\n",
      "Episode: 196\n",
      "row: 3654, episode: 196\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3247062.72\n",
      "total_reward: 2247062.72\n",
      "total_cost: 1494677.72\n",
      "total_trades: 3286\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 120           |\n",
      "|    iterations           | 192           |\n",
      "|    time_elapsed         | 5809          |\n",
      "|    total_timesteps      | 701760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027257882 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | -0.00111      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 42.5          |\n",
      "|    n_updates            | 1910          |\n",
      "|    policy_gradient_loss | 6.06e-05      |\n",
      "|    reward               | 224.70627     |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 85.1          |\n",
      "-------------------------------------------\n",
      "Episode: 197\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 193          |\n",
      "|    time_elapsed         | 5840         |\n",
      "|    total_timesteps      | 705415       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007177233 |\n",
      "|    clip_fraction        | 0.00298      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.000399     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 225          |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -8.15e-05    |\n",
      "|    reward               | 149.90497    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 449          |\n",
      "------------------------------------------\n",
      "Episode: 198\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 194          |\n",
      "|    time_elapsed         | 5870         |\n",
      "|    total_timesteps      | 709070       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011962209 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | -6.72e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 98.6         |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -0.000287    |\n",
      "|    reward               | 369.21878    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 197          |\n",
      "------------------------------------------\n",
      "Episode: 199\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 195          |\n",
      "|    time_elapsed         | 5900         |\n",
      "|    total_timesteps      | 712725       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005250708 |\n",
      "|    clip_fraction        | 0.000821     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.00229      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 613          |\n",
      "|    n_updates            | 1940         |\n",
      "|    policy_gradient_loss | 5.1e-05      |\n",
      "|    reward               | 142.00148    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 1.23e+03     |\n",
      "------------------------------------------\n",
      "Episode: 200\n",
      "row: 3654, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2530190.60\n",
      "total_reward: 1530190.60\n",
      "total_cost: 1181460.05\n",
      "total_trades: 3318\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 196          |\n",
      "|    time_elapsed         | 5929         |\n",
      "|    total_timesteps      | 716380       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006850572 |\n",
      "|    clip_fraction        | 0.000876     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -8.98e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 88.2         |\n",
      "|    n_updates            | 1950         |\n",
      "|    policy_gradient_loss | -0.000141    |\n",
      "|    reward               | 153.01906    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 176          |\n",
      "------------------------------------------\n",
      "Episode: 201\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 5966         |\n",
      "|    total_timesteps      | 720035       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013674793 |\n",
      "|    clip_fraction        | 0.129        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.000237     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 103          |\n",
      "|    n_updates            | 1960         |\n",
      "|    policy_gradient_loss | -0.000867    |\n",
      "|    reward               | 99.52423     |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 205          |\n",
      "------------------------------------------\n",
      "Episode: 202\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 198          |\n",
      "|    time_elapsed         | 5999         |\n",
      "|    total_timesteps      | 723690       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046189656 |\n",
      "|    clip_fraction        | 0.121        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.000777    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 42.6         |\n",
      "|    n_updates            | 1970         |\n",
      "|    policy_gradient_loss | -0.000656    |\n",
      "|    reward               | 299.46417    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 85.4         |\n",
      "------------------------------------------\n",
      "Episode: 203\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 120           |\n",
      "|    iterations           | 199           |\n",
      "|    time_elapsed         | 6033          |\n",
      "|    total_timesteps      | 727345        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023508712 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.000453      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 402           |\n",
      "|    n_updates            | 1980          |\n",
      "|    policy_gradient_loss | -0.000174     |\n",
      "|    reward               | 415.98822     |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 804           |\n",
      "-------------------------------------------\n",
      "Episode: 204\n",
      "row: 3654, episode: 204\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3859415.74\n",
      "total_reward: 2859415.74\n",
      "total_cost: 1453231.18\n",
      "total_trades: 3343\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 120           |\n",
      "|    iterations           | 200           |\n",
      "|    time_elapsed         | 6065          |\n",
      "|    total_timesteps      | 731000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014339318 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.000837      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 779           |\n",
      "|    n_updates            | 1990          |\n",
      "|    policy_gradient_loss | 4.14e-06      |\n",
      "|    reward               | 285.9416      |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 1.56e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 205\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 120           |\n",
      "|    iterations           | 201           |\n",
      "|    time_elapsed         | 6101          |\n",
      "|    total_timesteps      | 734655        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023653483 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.000206      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 364           |\n",
      "|    n_updates            | 2000          |\n",
      "|    policy_gradient_loss | -0.00014      |\n",
      "|    reward               | 240.73357     |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 729           |\n",
      "-------------------------------------------\n",
      "Episode: 206\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 202         |\n",
      "|    time_elapsed         | 6139        |\n",
      "|    total_timesteps      | 738310      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002203269 |\n",
      "|    clip_fraction        | 0.0293      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.000113   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 257         |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.000168   |\n",
      "|    reward               | 502.06134   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 513         |\n",
      "-----------------------------------------\n",
      "Episode: 207\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 120           |\n",
      "|    iterations           | 203           |\n",
      "|    time_elapsed         | 6172          |\n",
      "|    total_timesteps      | 741965        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054238393 |\n",
      "|    clip_fraction        | 0.00041       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.000136      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.13e+03      |\n",
      "|    n_updates            | 2020          |\n",
      "|    policy_gradient_loss | -0.000234     |\n",
      "|    reward               | 320.74405     |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 2.27e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 208\n",
      "row: 3654, episode: 208\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1442846.25\n",
      "total_reward: 442846.25\n",
      "total_cost: 937300.21\n",
      "total_trades: 3422\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 204          |\n",
      "|    time_elapsed         | 6204         |\n",
      "|    total_timesteps      | 745620       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013646996 |\n",
      "|    clip_fraction        | 0.00941      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 8.47e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 457          |\n",
      "|    n_updates            | 2030         |\n",
      "|    policy_gradient_loss | -0.000711    |\n",
      "|    reward               | 44.284626    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 914          |\n",
      "------------------------------------------\n",
      "Episode: 209\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 205          |\n",
      "|    time_elapsed         | 6235         |\n",
      "|    total_timesteps      | 749275       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007668004 |\n",
      "|    clip_fraction        | 0.00197      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.021       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.72         |\n",
      "|    n_updates            | 2040         |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    reward               | 226.17194    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 17.5         |\n",
      "------------------------------------------\n",
      "Episode: 210\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 206          |\n",
      "|    time_elapsed         | 6266         |\n",
      "|    total_timesteps      | 752930       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042686323 |\n",
      "|    clip_fraction        | 0.107        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.000323    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 224          |\n",
      "|    n_updates            | 2050         |\n",
      "|    policy_gradient_loss | -0.000753    |\n",
      "|    reward               | 101.231445   |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 449          |\n",
      "------------------------------------------\n",
      "Episode: 211\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 207          |\n",
      "|    time_elapsed         | 6298         |\n",
      "|    total_timesteps      | 756585       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020692274 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.00252     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 43.4         |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -0.000615    |\n",
      "|    reward               | 306.59613    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 87.1         |\n",
      "------------------------------------------\n",
      "Episode: 212\n",
      "row: 3654, episode: 212\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1921960.53\n",
      "total_reward: 921960.53\n",
      "total_cost: 932729.67\n",
      "total_trades: 3347\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 6333         |\n",
      "|    total_timesteps      | 760240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002213257 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 6.68e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 417          |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | 7.47e-05     |\n",
      "|    reward               | 92.19605     |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 834          |\n",
      "------------------------------------------\n",
      "Episode: 213\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 120           |\n",
      "|    iterations           | 209           |\n",
      "|    time_elapsed         | 6365          |\n",
      "|    total_timesteps      | 763895        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014201291 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | -0.00202      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 35.8          |\n",
      "|    n_updates            | 2080          |\n",
      "|    policy_gradient_loss | -0.000148     |\n",
      "|    reward               | 99.13204      |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 71.9          |\n",
      "-------------------------------------------\n",
      "Episode: 214\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 210           |\n",
      "|    time_elapsed         | 6401          |\n",
      "|    total_timesteps      | 767550        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8502017e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | -0.00115      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 41.2          |\n",
      "|    n_updates            | 2090          |\n",
      "|    policy_gradient_loss | 6.51e-05      |\n",
      "|    reward               | 305.1928      |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 82.9          |\n",
      "-------------------------------------------\n",
      "Episode: 215\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 211           |\n",
      "|    time_elapsed         | 6434          |\n",
      "|    total_timesteps      | 771205        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017323352 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.000116      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 414           |\n",
      "|    n_updates            | 2100          |\n",
      "|    policy_gradient_loss | -0.000135     |\n",
      "|    reward               | 31.606903     |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 829           |\n",
      "-------------------------------------------\n",
      "Episode: 216\n",
      "row: 3654, episode: 216\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4448276.09\n",
      "total_reward: 3448276.09\n",
      "total_cost: 1499202.32\n",
      "total_trades: 3406\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 212           |\n",
      "|    time_elapsed         | 6467          |\n",
      "|    total_timesteps      | 774860        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.2875114e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | -0.00919      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.45          |\n",
      "|    n_updates            | 2110          |\n",
      "|    policy_gradient_loss | 1.01e-06      |\n",
      "|    reward               | 344.8276      |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 9.31          |\n",
      "-------------------------------------------\n",
      "Episode: 217\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 213           |\n",
      "|    time_elapsed         | 6499          |\n",
      "|    total_timesteps      | 778515        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5634193e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.00018       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 531           |\n",
      "|    n_updates            | 2120          |\n",
      "|    policy_gradient_loss | 9.4e-07       |\n",
      "|    reward               | 280.80023     |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 1.06e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 218\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 214           |\n",
      "|    time_elapsed         | 6529          |\n",
      "|    total_timesteps      | 782170        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5956204e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.000458      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 350           |\n",
      "|    n_updates            | 2130          |\n",
      "|    policy_gradient_loss | -1.55e-05     |\n",
      "|    reward               | 136.52469     |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 699           |\n",
      "-------------------------------------------\n",
      "Episode: 219\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 215          |\n",
      "|    time_elapsed         | 6553         |\n",
      "|    total_timesteps      | 785825       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002602747 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.000431     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 80.3         |\n",
      "|    n_updates            | 2140         |\n",
      "|    policy_gradient_loss | -0.000132    |\n",
      "|    reward               | 145.39368    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 161          |\n",
      "------------------------------------------\n",
      "Episode: 220\n",
      "row: 3654, episode: 220\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1903118.49\n",
      "total_reward: 903118.49\n",
      "total_cost: 1469018.58\n",
      "total_trades: 3282\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 216           |\n",
      "|    time_elapsed         | 6582          |\n",
      "|    total_timesteps      | 789480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060406694 |\n",
      "|    clip_fraction        | 0.000547      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | -9.62e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 91.1          |\n",
      "|    n_updates            | 2150          |\n",
      "|    policy_gradient_loss | -0.000106     |\n",
      "|    reward               | 90.31185      |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 182           |\n",
      "-------------------------------------------\n",
      "Episode: 221\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 217           |\n",
      "|    time_elapsed         | 6611          |\n",
      "|    total_timesteps      | 793135        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6619884e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | -0.000296     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 34.2          |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | 0.00034       |\n",
      "|    reward               | 201.23006     |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 68.7          |\n",
      "-------------------------------------------\n",
      "Episode: 222\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 218           |\n",
      "|    time_elapsed         | 6640          |\n",
      "|    total_timesteps      | 796790        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00061364693 |\n",
      "|    clip_fraction        | 0.000876      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | 0.000824      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 178           |\n",
      "|    n_updates            | 2170          |\n",
      "|    policy_gradient_loss | -3.96e-05     |\n",
      "|    reward               | 156.7144      |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 356           |\n",
      "-------------------------------------------\n",
      "Episode: 223\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 219          |\n",
      "|    time_elapsed         | 6669         |\n",
      "|    total_timesteps      | 800445       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008109834 |\n",
      "|    clip_fraction        | 0.00145      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.000318     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 107          |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -0.00015     |\n",
      "|    reward               | 119.82985    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 214          |\n",
      "------------------------------------------\n",
      "Episode: 224\n",
      "row: 3654, episode: 224\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1564352.58\n",
      "total_reward: 564352.58\n",
      "total_cost: 882039.37\n",
      "total_trades: 3294\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 6697        |\n",
      "|    total_timesteps      | 804100      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003410208 |\n",
      "|    clip_fraction        | 0.0824      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -4.17e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 61.7        |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.000569   |\n",
      "|    reward               | 56.435257   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "Episode: 225\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 221          |\n",
      "|    time_elapsed         | 6726         |\n",
      "|    total_timesteps      | 807755       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.168705e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.00123     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 13           |\n",
      "|    n_updates            | 2200         |\n",
      "|    policy_gradient_loss | -2.39e-05    |\n",
      "|    reward               | 68.10137     |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 26.5         |\n",
      "------------------------------------------\n",
      "Episode: 226\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 222         |\n",
      "|    time_elapsed         | 6758        |\n",
      "|    total_timesteps      | 811410      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001636527 |\n",
      "|    clip_fraction        | 0.0217      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.00264    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 19.6        |\n",
      "|    n_updates            | 2210        |\n",
      "|    policy_gradient_loss | -0.000121   |\n",
      "|    reward               | 93.19353    |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 39.2        |\n",
      "-----------------------------------------\n",
      "Episode: 227\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 223          |\n",
      "|    time_elapsed         | 6790         |\n",
      "|    total_timesteps      | 815065       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.537717e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.000821    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 37.7         |\n",
      "|    n_updates            | 2220         |\n",
      "|    policy_gradient_loss | -6.47e-06    |\n",
      "|    reward               | 84.02421     |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 75.4         |\n",
      "------------------------------------------\n",
      "Episode: 228\n",
      "row: 3654, episode: 228\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1900359.93\n",
      "total_reward: 900359.93\n",
      "total_cost: 1007287.83\n",
      "total_trades: 3305\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 120           |\n",
      "|    iterations           | 224           |\n",
      "|    time_elapsed         | 6819          |\n",
      "|    total_timesteps      | 818720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018269313 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | -0.000829     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 30.6          |\n",
      "|    n_updates            | 2230          |\n",
      "|    policy_gradient_loss | -3.35e-05     |\n",
      "|    reward               | 90.035995     |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 61.3          |\n",
      "-------------------------------------------\n",
      "Episode: 229\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 225          |\n",
      "|    time_elapsed         | 6848         |\n",
      "|    total_timesteps      | 822375       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013586018 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | -0.00134     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 35.4         |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -2.84e-05    |\n",
      "|    reward               | 151.57129    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 70.8         |\n",
      "------------------------------------------\n",
      "Episode: 230\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 226          |\n",
      "|    time_elapsed         | 6877         |\n",
      "|    total_timesteps      | 826030       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007238832 |\n",
      "|    clip_fraction        | 0.00101      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.00156      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 102          |\n",
      "|    n_updates            | 2250         |\n",
      "|    policy_gradient_loss | -9.93e-05    |\n",
      "|    reward               | 186.59608    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 204          |\n",
      "------------------------------------------\n",
      "Episode: 231\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 227          |\n",
      "|    time_elapsed         | 6911         |\n",
      "|    total_timesteps      | 829685       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022569918 |\n",
      "|    clip_fraction        | 0.0368       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.000319    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 155          |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -0.000312    |\n",
      "|    reward               | 75.82214     |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 311          |\n",
      "------------------------------------------\n",
      "Episode: 232\n",
      "row: 3654, episode: 232\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1669097.69\n",
      "total_reward: 669097.69\n",
      "total_cost: 1023539.32\n",
      "total_trades: 3308\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 120          |\n",
      "|    iterations           | 228          |\n",
      "|    time_elapsed         | 6943         |\n",
      "|    total_timesteps      | 833340       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.016621e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.000678    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 24.7         |\n",
      "|    n_updates            | 2270         |\n",
      "|    policy_gradient_loss | -1.46e-05    |\n",
      "|    reward               | 66.90977     |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 49.5         |\n",
      "------------------------------------------\n",
      "Episode: 233\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 229          |\n",
      "|    time_elapsed         | 6978         |\n",
      "|    total_timesteps      | 836995       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008797667 |\n",
      "|    clip_fraction        | 0.00194      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.00117     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 19.1         |\n",
      "|    n_updates            | 2280         |\n",
      "|    policy_gradient_loss | -8.48e-05    |\n",
      "|    reward               | 228.386      |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 38.3         |\n",
      "------------------------------------------\n",
      "Episode: 234\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 230           |\n",
      "|    time_elapsed         | 7011          |\n",
      "|    total_timesteps      | 840650        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028416808 |\n",
      "|    clip_fraction        | 0.000109      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 8.12e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 234           |\n",
      "|    n_updates            | 2290          |\n",
      "|    policy_gradient_loss | 0.00071       |\n",
      "|    reward               | 137.45784     |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 468           |\n",
      "-------------------------------------------\n",
      "Episode: 235\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 231          |\n",
      "|    time_elapsed         | 7045         |\n",
      "|    total_timesteps      | 844305       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031433934 |\n",
      "|    clip_fraction        | 0.126        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.000276    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 83.4         |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -0.000142    |\n",
      "|    reward               | 218.09486    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 167          |\n",
      "------------------------------------------\n",
      "Episode: 236\n",
      "row: 3654, episode: 236\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1557108.31\n",
      "total_reward: 557108.31\n",
      "total_cost: 863794.49\n",
      "total_trades: 3351\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 7080         |\n",
      "|    total_timesteps      | 847960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010289236 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.000129     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 212          |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | 0.00046      |\n",
      "|    reward               | 55.71083     |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 425          |\n",
      "------------------------------------------\n",
      "Episode: 237\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 233           |\n",
      "|    time_elapsed         | 7116          |\n",
      "|    total_timesteps      | 851615        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028171082 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | -0.00264      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 13.2          |\n",
      "|    n_updates            | 2320          |\n",
      "|    policy_gradient_loss | -0.000175     |\n",
      "|    reward               | 59.14467      |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 26.5          |\n",
      "-------------------------------------------\n",
      "Episode: 238\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 234           |\n",
      "|    time_elapsed         | 7148          |\n",
      "|    total_timesteps      | 855270        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5115674e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | -0.00223      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 14.7          |\n",
      "|    n_updates            | 2330          |\n",
      "|    policy_gradient_loss | 3.42e-05      |\n",
      "|    reward               | 280.1337      |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 29.6          |\n",
      "-------------------------------------------\n",
      "Episode: 239\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 235          |\n",
      "|    time_elapsed         | 7183         |\n",
      "|    total_timesteps      | 858925       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011846193 |\n",
      "|    clip_fraction        | 0.00785      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.000223     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 353          |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -0.000951    |\n",
      "|    reward               | 106.56355    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 706          |\n",
      "------------------------------------------\n",
      "Episode: 240\n",
      "row: 3654, episode: 240\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2127621.48\n",
      "total_reward: 1127621.48\n",
      "total_cost: 1209714.98\n",
      "total_trades: 3311\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 236           |\n",
      "|    time_elapsed         | 7214          |\n",
      "|    total_timesteps      | 862580        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085260085 |\n",
      "|    clip_fraction        | 0.00241       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 8.31e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 49.7          |\n",
      "|    n_updates            | 2350          |\n",
      "|    policy_gradient_loss | -0.000134     |\n",
      "|    reward               | 112.762146    |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 99.5          |\n",
      "-------------------------------------------\n",
      "Episode: 241\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 237          |\n",
      "|    time_elapsed         | 7252         |\n",
      "|    total_timesteps      | 866235       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010206882 |\n",
      "|    clip_fraction        | 0.1          |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.000559     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 55.9         |\n",
      "|    n_updates            | 2360         |\n",
      "|    policy_gradient_loss | -0.000389    |\n",
      "|    reward               | 256.65164    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 112          |\n",
      "------------------------------------------\n",
      "Episode: 242\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 238          |\n",
      "|    time_elapsed         | 7288         |\n",
      "|    total_timesteps      | 869890       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025234788 |\n",
      "|    clip_fraction        | 0.0671       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.000431     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 296          |\n",
      "|    n_updates            | 2370         |\n",
      "|    policy_gradient_loss | 0.000734     |\n",
      "|    reward               | 40.98213     |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 592          |\n",
      "------------------------------------------\n",
      "Episode: 243\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 239           |\n",
      "|    time_elapsed         | 7327          |\n",
      "|    total_timesteps      | 873545        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4670929e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | -0.000873     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.17          |\n",
      "|    n_updates            | 2380          |\n",
      "|    policy_gradient_loss | -8.87e-07     |\n",
      "|    reward               | 74.38125      |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 14.4          |\n",
      "-------------------------------------------\n",
      "Episode: 244\n",
      "row: 3654, episode: 244\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1938921.96\n",
      "total_reward: 938921.96\n",
      "total_cost: 1278051.87\n",
      "total_trades: 3356\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 119           |\n",
      "|    iterations           | 240           |\n",
      "|    time_elapsed         | 7364          |\n",
      "|    total_timesteps      | 877200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3329674e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.54         |\n",
      "|    explained_variance   | -0.00085      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 23.7          |\n",
      "|    n_updates            | 2390          |\n",
      "|    policy_gradient_loss | -1.97e-05     |\n",
      "|    reward               | 93.8922       |\n",
      "|    std                  | 1.13          |\n",
      "|    value_loss           | 47.5          |\n",
      "-------------------------------------------\n",
      "Episode: 245\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 241          |\n",
      "|    time_elapsed         | 7396         |\n",
      "|    total_timesteps      | 880855       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035075098 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | -3.37e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 38.5         |\n",
      "|    n_updates            | 2400         |\n",
      "|    policy_gradient_loss | -0.000817    |\n",
      "|    reward               | 247.74556    |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 77           |\n",
      "------------------------------------------\n",
      "Episode: 246\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 119          |\n",
      "|    iterations           | 242          |\n",
      "|    time_elapsed         | 7426         |\n",
      "|    total_timesteps      | 884510       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019526224 |\n",
      "|    clip_fraction        | 0.0334       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.54        |\n",
      "|    explained_variance   | 0.000594     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 276          |\n",
      "|    n_updates            | 2410         |\n",
      "|    policy_gradient_loss | -0.000154    |\n",
      "|    reward               | 249.5336     |\n",
      "|    std                  | 1.13         |\n",
      "|    value_loss           | 552          |\n",
      "------------------------------------------\n",
      "Episode: 247\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 119         |\n",
      "|    iterations           | 243         |\n",
      "|    time_elapsed         | 7456        |\n",
      "|    total_timesteps      | 888165      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.32141e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.00019     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 279         |\n",
      "|    n_updates            | 2420        |\n",
      "|    policy_gradient_loss | 3.46e-05    |\n",
      "|    reward               | 147.81012   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 558         |\n",
      "-----------------------------------------\n",
      "Episode: 248\n",
      "row: 3654, episode: 248\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1508788.91\n",
      "total_reward: 508788.91\n",
      "total_cost: 788336.04\n",
      "total_trades: 3254\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 119        |\n",
      "|    iterations           | 244        |\n",
      "|    time_elapsed         | 7490       |\n",
      "|    total_timesteps      | 891820     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00766119 |\n",
      "|    clip_fraction        | 0.231      |\n",
      "|    clip_range           | 0.1        |\n",
      "|    entropy_loss         | -1.55      |\n",
      "|    explained_variance   | -8e-05     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 96.4       |\n",
      "|    n_updates            | 2430       |\n",
      "|    policy_gradient_loss | -0.000274  |\n",
      "|    reward               | 50.87889   |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 193        |\n",
      "----------------------------------------\n",
      "Episode: 249\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 245         |\n",
      "|    time_elapsed         | 7528        |\n",
      "|    total_timesteps      | 895475      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004161936 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.00236    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 2440        |\n",
      "|    policy_gradient_loss | -0.000704   |\n",
      "|    reward               | 106.66292   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 21.9        |\n",
      "-----------------------------------------\n",
      "Episode: 250\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 118          |\n",
      "|    iterations           | 246          |\n",
      "|    time_elapsed         | 7563         |\n",
      "|    total_timesteps      | 899130       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016844982 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.000369    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 49.6         |\n",
      "|    n_updates            | 2450         |\n",
      "|    policy_gradient_loss | -0.000128    |\n",
      "|    reward               | 48.468803    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 99.3         |\n",
      "------------------------------------------\n",
      "Episode: 251\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 247         |\n",
      "|    time_elapsed         | 7599        |\n",
      "|    total_timesteps      | 902785      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001784501 |\n",
      "|    clip_fraction        | 0.0224      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.0013     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.92        |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.000465   |\n",
      "|    reward               | 81.70772    |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 20          |\n",
      "-----------------------------------------\n",
      "Episode: 252\n",
      "row: 3654, episode: 252\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3015216.44\n",
      "total_reward: 2015216.44\n",
      "total_cost: 1493083.75\n",
      "total_trades: 3340\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 118          |\n",
      "|    iterations           | 248          |\n",
      "|    time_elapsed         | 7634         |\n",
      "|    total_timesteps      | 906440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019581914 |\n",
      "|    clip_fraction        | 0.0239       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | 0.000787     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 28.8         |\n",
      "|    n_updates            | 2470         |\n",
      "|    policy_gradient_loss | -0.000216    |\n",
      "|    reward               | 201.52164    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 57.7         |\n",
      "------------------------------------------\n",
      "Episode: 253\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 118           |\n",
      "|    iterations           | 249           |\n",
      "|    time_elapsed         | 7666          |\n",
      "|    total_timesteps      | 910095        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00034231966 |\n",
      "|    clip_fraction        | 5.47e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.000344      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 182           |\n",
      "|    n_updates            | 2480          |\n",
      "|    policy_gradient_loss | -0.000199     |\n",
      "|    reward               | 108.380295    |\n",
      "|    std                  | 1.14          |\n",
      "|    value_loss           | 364           |\n",
      "-------------------------------------------\n",
      "Episode: 254\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 118          |\n",
      "|    iterations           | 250          |\n",
      "|    time_elapsed         | 7700         |\n",
      "|    total_timesteps      | 913750       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011927066 |\n",
      "|    clip_fraction        | 0.152        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.55        |\n",
      "|    explained_variance   | -0.000233    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 51.6         |\n",
      "|    n_updates            | 2490         |\n",
      "|    policy_gradient_loss | -0.00066     |\n",
      "|    reward               | 118.66134    |\n",
      "|    std                  | 1.14         |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "Episode: 255\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 118           |\n",
      "|    iterations           | 251           |\n",
      "|    time_elapsed         | 7735          |\n",
      "|    total_timesteps      | 917405        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5029512e-05 |\n",
      "|    clip_fraction        | 0.071         |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.55         |\n",
      "|    explained_variance   | 0.000353      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 62.2          |\n",
      "|    n_updates            | 2500          |\n",
      "|    policy_gradient_loss | -0.000286     |\n",
      "|    reward               | 376.34335     |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 124           |\n",
      "-------------------------------------------\n",
      "Episode: 256\n",
      "row: 3654, episode: 256\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5048338.90\n",
      "total_reward: 4048338.90\n",
      "total_cost: 1882510.16\n",
      "total_trades: 3222\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 118           |\n",
      "|    iterations           | 252           |\n",
      "|    time_elapsed         | 7768          |\n",
      "|    total_timesteps      | 921060        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00027552748 |\n",
      "|    clip_fraction        | 0.000383      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.56         |\n",
      "|    explained_variance   | 0.00228       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 640           |\n",
      "|    n_updates            | 2510          |\n",
      "|    policy_gradient_loss | 0.000182      |\n",
      "|    reward               | 404.8339      |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 1.28e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 257\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 118           |\n",
      "|    iterations           | 253           |\n",
      "|    time_elapsed         | 7803          |\n",
      "|    total_timesteps      | 924715        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3210983e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.56         |\n",
      "|    explained_variance   | 0.00022       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 742           |\n",
      "|    n_updates            | 2520          |\n",
      "|    policy_gradient_loss | 1.28e-05      |\n",
      "|    reward               | 60.462227     |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 1.48e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 258\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 118           |\n",
      "|    iterations           | 254           |\n",
      "|    time_elapsed         | 7837          |\n",
      "|    total_timesteps      | 928370        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019457516 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.56         |\n",
      "|    explained_variance   | -0.0025       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 15.7          |\n",
      "|    n_updates            | 2530          |\n",
      "|    policy_gradient_loss | -0.000165     |\n",
      "|    reward               | 50.894936     |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 31.4          |\n",
      "-------------------------------------------\n",
      "Episode: 259\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 118           |\n",
      "|    iterations           | 255           |\n",
      "|    time_elapsed         | 7870          |\n",
      "|    total_timesteps      | 932025        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020793425 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.56         |\n",
      "|    explained_variance   | -0.00357      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 10.9          |\n",
      "|    n_updates            | 2540          |\n",
      "|    policy_gradient_loss | -1.9e-05      |\n",
      "|    reward               | 65.04286      |\n",
      "|    std                  | 1.15          |\n",
      "|    value_loss           | 22            |\n",
      "-------------------------------------------\n",
      "Episode: 260\n",
      "row: 3654, episode: 260\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3303834.63\n",
      "total_reward: 2303834.63\n",
      "total_cost: 1389565.83\n",
      "total_trades: 3323\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 118          |\n",
      "|    iterations           | 256          |\n",
      "|    time_elapsed         | 7905         |\n",
      "|    total_timesteps      | 935680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027634671 |\n",
      "|    clip_fraction        | 0.0353       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.56        |\n",
      "|    explained_variance   | -0.000472    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 18           |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -0.000863    |\n",
      "|    reward               | 230.38347    |\n",
      "|    std                  | 1.15         |\n",
      "|    value_loss           | 36.1         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent_ppo.train_model(model=model_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trained_ppo = PPO(\"MlpPolicy\", env_train,n_steps=episode_length,ent_coef=0.01,learning_rate=0.00025,batch_size=episode_length,clip_range=0.1,\n",
    "#                   policy_kwargs=policy_kwargs,tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_ppo/\",verbose=10)\n",
    "# trained_ppo.learn(total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_ddpg = DRLAgent(env = env_train)\n",
    "# DDPG_PARAMS = {\"batch_size\": 128, \"buffer_size\": episode_length, \"learning_rate\": 0.001}\n",
    "# model_ddpg = agent_ddpg.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS,tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_ddpg_tensorboard/\")\n",
    "\n",
    "# # set up logger\n",
    "# tmp_path = TENSORBOARD_LOG_DIR + '/ddpg'\n",
    "# new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# # Set new logger\n",
    "# model_sac.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the training log just show up when total_timesteps higher than 15.000\n",
    "# trained_ddpg = agent_ddpg.train_model(model=model_ddpg,\n",
    "#                              tb_log_name='ddpg',\n",
    "#                              total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "# # The noise objects for DDPG\n",
    "# n_actions = env_train.action_space.shape[-1]\n",
    "# action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# # Train the agent\n",
    "# trained_ddpg = DDPG(\"MlpPolicy\", env_train, action_noise=action_noise, verbose=1)\n",
    "# trained_ddpg.learn(total_timesteps=total_timesteps, log_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_length = len(e_train_gym.df) + 1\n",
    "# episode_amount = 4\n",
    "# total_training_step = episode_length*episode_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 2048, 'buffer_size': 58204, 'learning_rate': 0.0003, 'learning_starts': 58204, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# agent_sac = DRLAgent(env = env_train)\n",
    "# SAC_PARAMS = {\n",
    "#     \"batch_size\": 2048,\n",
    "#     \"buffer_size\": episode_length,\n",
    "#     \"learning_rate\": 0.0003,\n",
    "#     \"learning_starts\": episode_length,\n",
    "#     \"ent_coef\": \"auto_0.1\",\n",
    "# }\n",
    "\n",
    "# model_sac = agent_sac.get_model(\"sac\",model_kwargs = SAC_PARAMS,tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_sac/\")\n",
    "\n",
    "# # # set up logger\n",
    "# # tmp_path = TENSORBOARD_LOG_DIR + '/test_sac/sac_12'\n",
    "# # new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# # # Set new logger\n",
    "# # model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "# trained_sac = agent_sac.train_model(model=model_sac, \n",
    "#                              tb_log_name='sac',\n",
    "#                              total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_td3 = DRLAgent(env = env_train)\n",
    "# TD3_PARAMS = {\"batch_size\": 100, \n",
    "#               \"buffer_size\": 1000000, \n",
    "#               \"learning_rate\": 0.001}\n",
    "\n",
    "# model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_td3 = agent.train_model(model=model_td3, \n",
    "#                              tb_log_name='td3',\n",
    "#                              total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_a2c = DRLAgent(env = env_train)\n",
    "\n",
    "# A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002}\n",
    "# model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_a2c = agent.train_model(model=model_a2c, \n",
    "#                                 tb_log_name='a2c',\n",
    "#                                 total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading\n",
    "Assume that we have $1,000,000 initial capital at the begining. We use the PPO model to trade Dow jones 30 stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade\n",
    "\n",
    "DRL model needs to update periodically in order to take full advantage of the data, ideally we need to retrain our model yearly, quarterly, or monthly. We also need to tune the parameters along the way, in this notebook I only use the in-sample data from 2009-01 to 2018-12 to tune the parameters once, so there is some alpha decay here as the length of trade date extends. \n",
    "\n",
    "Numerous hyperparameters – e.g. the learning rate, the total number of samples to train on – influence the learning process and are usually determined by testing some variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "trade_data = trade_data.reset_index(drop=True)\n",
    "\n",
    "# trade_data = data_split(processed_full, TEST_START_DATE, TEST_END_DATE)\n",
    "e_trade_gym = StockTradingEnv(df = trade_data, **env_kwargs)\n",
    "# env_trade, obs_trade = e_trade_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRL_prediction function allow testing the model using trading_data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DRL_prediction(model, environment, deterministic=False):\n",
    "        \"\"\"make a prediction and get results\"\"\"\n",
    "        # test_env, test_obs = environment.get_sb_env()\n",
    "        # account_memory = None  # This help avoid unnecessary list creation\n",
    "        # actions_memory = None  # optimize memory consumption\n",
    "\n",
    "        test_obs = environment.reset()[0]\n",
    "        # max_steps = len(environment.df.index.unique()) - 1\n",
    "\n",
    "        for i in range(0,len(environment.df)):\n",
    "            action = model.predict(np.asarray(test_obs), deterministic=deterministic)\n",
    "            test_obs,reward,terminal,truncated,info = environment.step(action)\n",
    "            # test_obs, rewards, dones, info = test_env.step(action)\n",
    "\n",
    "            # if (i == max_steps - 1):  # more descriptive condition for early termination to clarify the logic\n",
    "            #     account_memory = environment.env_method(method_name=\"save_asset_memory\")\n",
    "            #     actions_memory = environment.env_method(method_name=\"save_action_memory\")\n",
    "\n",
    "            if terminal:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "        return pd.DataFrame(e_trade_gym.asset_memory, columns=['account_value']), pd.DataFrame(e_trade_gym.actions_memory,columns=['actions','none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 4\n",
      "row: 1139, episode: 4\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1175255.67\n",
      "total_reward: 175255.67\n",
      "total_cost: 245417.74\n",
      "total_trades: 873\n",
      "=================================\n",
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_ppo, df_actions_ppo = DRL_prediction(model=trained_ppo, environment = e_trade_gym)\n",
    "\n",
    "# df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "#     model=trained_ddpg, \n",
    "#     environment = e_trade_gym)\n",
    "\n",
    "# df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "#     model=trained_sac, \n",
    "#     environment = e_trade_gym)\n",
    "\n",
    "# df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "#     model=trained_a2c, \n",
    "#     environment = e_trade_gym) if if_using_a2c else [None, None]\n",
    "\n",
    "# df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "#     model=trained_td3, \n",
    "#     environment = e_trade_gym) if if_using_td3 else [None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1140, 1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_ppo.shape\n",
    "# df_account_value_ddpg.shape\n",
    "# df_account_value_sac.shape\n",
    "# df_account_value_td3.shape\n",
    "# df_account_value_a2c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_ppo['date'] = trade_data.date\n",
    "# df_account_value_ddpg['date'] = trade_data.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>1.175256e+06</td>\n",
       "      <td>2024-02-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>1.175256e+06</td>\n",
       "      <td>2024-02-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>1.175256e+06</td>\n",
       "      <td>2024-02-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>1.175256e+06</td>\n",
       "      <td>2024-02-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1.175256e+06</td>\n",
       "      <td>2024-02-29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      account_value       date\n",
       "1135   1.175256e+06 2024-02-29\n",
       "1136   1.175256e+06 2024-02-29\n",
       "1137   1.175256e+06 2024-02-29\n",
       "1138   1.175256e+06 2024-02-29\n",
       "1139   1.175256e+06 2024-02-29"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_ppo.tail()\n",
    "# df_account_value_ddpg.tail()\n",
    "# df_account_value_sac.tail()\n",
    "# df_account_value_td3.tail()\n",
    "# df_account_value_a2c.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_actions_ppo.actions.unique()\n",
    "# df_actions_ddpg.head()\n",
    "# df_actions_sac.head()\n",
    "# df_actions_td3.head()\n",
    "# df_actions_a2c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actions</th>\n",
       "      <th>none</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-1.0]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.45090783]</td>\n",
       "      <td>None</td>\n",
       "      <td>2021-01-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         actions  none       date\n",
       "0         [-1.0]  None 2021-01-31\n",
       "1         [-1.0]  None 2021-01-31\n",
       "2         [-1.0]  None 2021-01-31\n",
       "3         [-1.0]  None 2021-01-31\n",
       "4  [-0.45090783]  None 2021-01-31"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_actions_ppo['date'] = trade_data.date\n",
    "df_actions_ppo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>9.990004e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>9.950660e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>1.004002e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>9.718640e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>9.731815e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  account_value\n",
       "0 2021-01-31   9.990004e+05\n",
       "1 2021-02-28   9.950660e+05\n",
       "2 2021-03-31   1.004002e+06\n",
       "3 2021-04-30   9.718640e+05\n",
       "4 2021-05-31   9.731815e+05"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value = pd.DataFrame(df_account_value_ppo.date.unique())\n",
    "df_account_value.columns = ['date']\n",
    "df_account_value['account_value'] = df_account_value.apply(lambda x: \\\n",
    "                                    df_account_value_ppo[df_account_value_ppo.date == x.date].iloc[-1].account_value, axis=1)\n",
    "df_account_value.head()\n",
    "\n",
    "# df_account_value = pd.DataFrame(df_account_value_ddpg.date.unique())\n",
    "# df_account_value.columns = ['date']\n",
    "# df_account_value['account_value'] = df_account_value.apply(lambda x: \\\n",
    "#                                     df_account_value_ddpg[df_account_value_ddpg.date == x.date].iloc[-1].account_value, axis=1)\n",
    "# df_account_value.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a missmatch between the quarterly computation of df_account_value and the daily frequency of the comparison datasets. We need to transform the df_account_value to a daily basis.\n",
    "To fill up the missing data, th **ffill** function effectively imputes values, providing the continous picture of account value trends until the next recorded change. However, for the entries preceding the first recorded change, we will use the **initial_amount**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>9.990004e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>9.950660e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>1.004002e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>9.718640e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>9.731815e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>9.238103e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>9.638080e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>9.988999e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>9.742424e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>1.046619e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>9.712435e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>1.055135e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>1.032284e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>9.929022e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>1.001517e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>9.567293e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>9.922257e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>9.693989e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>9.477010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>9.256988e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>8.766309e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>8.996251e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>1.029237e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>9.377796e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>9.863210e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>9.565460e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>9.354670e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>9.251242e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>8.750034e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>8.983531e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>9.438329e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>9.255804e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>9.250213e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>9.316725e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>1.015504e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>1.026975e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>1.021773e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>1.175256e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>1.175256e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  account_value\n",
       "0  2021-01-31   9.990004e+05\n",
       "1  2021-02-28   9.950660e+05\n",
       "2  2021-03-31   1.004002e+06\n",
       "3  2021-04-30   9.718640e+05\n",
       "4  2021-05-31   9.731815e+05\n",
       "5  2021-06-30   9.238103e+05\n",
       "6  2021-07-31   9.638080e+05\n",
       "7  2021-08-31   9.988999e+05\n",
       "8  2021-09-30   9.742424e+05\n",
       "9  2021-10-31   1.046619e+06\n",
       "10 2021-11-30   9.712435e+05\n",
       "11 2021-12-31   1.055135e+06\n",
       "12 2022-01-31   1.032284e+06\n",
       "13 2022-02-28   9.929022e+05\n",
       "14 2022-03-31   1.001517e+06\n",
       "15 2022-04-30   9.567293e+05\n",
       "16 2022-05-31   9.922257e+05\n",
       "17 2022-06-30   9.693989e+05\n",
       "18 2022-07-31   9.477010e+05\n",
       "19 2022-08-31   9.256988e+05\n",
       "20 2022-09-30   8.766309e+05\n",
       "21 2022-10-31   8.996251e+05\n",
       "22 2022-11-30   1.029237e+06\n",
       "23 2022-12-31   9.377796e+05\n",
       "24 2023-01-31   9.863210e+05\n",
       "25 2023-02-28   9.565460e+05\n",
       "26 2023-03-31   9.354670e+05\n",
       "27 2023-04-30   9.251242e+05\n",
       "28 2023-05-31   8.750034e+05\n",
       "29 2023-06-30   8.983531e+05\n",
       "30 2023-07-31   9.438329e+05\n",
       "31 2023-08-31   9.255804e+05\n",
       "32 2023-09-30   9.250213e+05\n",
       "33 2023-10-31   9.316725e+05\n",
       "34 2023-11-30   1.015504e+06\n",
       "35 2023-12-31   1.026975e+06\n",
       "36 2024-01-31   1.021773e+06\n",
       "37 2024-02-29   1.175256e+06\n",
       "38 2024-03-31   1.175256e+06"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_account_value = pd.DataFrame(df.date.unique())\n",
    "daily_account_value.columns = ['date']\n",
    "daily_account_value['date'] = pd.to_datetime(daily_account_value.date)\n",
    "daily_account_value = daily_account_value.merge(df_account_value,how='left')\n",
    "daily_account_value.ffill(inplace=True)\n",
    "daily_account_value.fillna(env_kwargs[\"initial_amount\"],inplace=True)\n",
    "daily_account_value = daily_account_value[daily_account_value.date >= TEST_START_DATE]\n",
    "daily_account_value.reset_index(inplace=True,drop=True)\n",
    "daily_account_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "\n",
      " ppo:\n",
      "Annual return          0.036342\n",
      "Cumulative returns     0.175256\n",
      "Annual volatility      0.142017\n",
      "Sharpe ratio           0.319261\n",
      "Calmar ratio           0.210240\n",
      "Stability              0.019618\n",
      "Max drawdown          -0.172858\n",
      "Omega ratio            1.229746\n",
      "Sortino ratio          0.716963\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.058288\n",
      "Daily value at risk   -0.017713\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "# now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "print(\"\\n ppo:\")\n",
    "perf_stats_all_ppo = backtest_stats(account_value=df_account_value_ppo)\n",
    "perf_stats_all_ppo = pd.DataFrame(perf_stats_all_ppo)\n",
    "# perf_stats_all_ppo.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_ppo_\"+now+'.csv')\n",
    "\n",
    "# print(\"\\n ddpg:\")\n",
    "# perf_stats_all_ddpg = backtest_stats(account_value=df_account_value_ddpg)\n",
    "# perf_stats_all_ddpg = pd.DataFrame(perf_stats_all_ddpg)\n",
    "# perf_stats_all_ddpg.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_ddpg_\"+now+'.csv')\n",
    "\n",
    "# print(\"\\n sac:\")\n",
    "# perf_stats_all_sac = backtest_stats(account_value=df_account_value_sac)\n",
    "# perf_stats_all_sac = pd.DataFrame(perf_stats_all_sac)\n",
    "#   perf_stats_all_sac.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_sac_\"+now+'.csv')\n",
    "\n",
    "#   print(\"\\n atd3:\")\n",
    "#   perf_stats_all_td3 = backtest_stats(account_value=df_account_value_td3)\n",
    "#   perf_stats_all_td3 = pd.DataFrame(perf_stats_all_td3)\n",
    "#   perf_stats_all_td3.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_td3_\"+now+'.csv')\n",
    "\n",
    "#   print(\"\\n a2c:\")\n",
    "#   perf_stats_all_a2c = backtest_stats(account_value=df_account_value_a2c)\n",
    "#   perf_stats_all_a2c = pd.DataFrame(perf_stats_all_a2c)\n",
    "#   perf_stats_all_a2c.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_a2c_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Annual return          0.089358\n",
      "Cumulative returns     0.316223\n",
      "Annual volatility      0.146547\n",
      "Sharpe ratio           0.658110\n",
      "Calmar ratio           0.407268\n",
      "Stability              0.108092\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            1.118655\n",
      "Sortino ratio          0.940026\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.003905\n",
      "Daily value at risk   -0.018081\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = TEST_START_DATE,\n",
    "        end = TEST_END_DATE)\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTest with DJIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Compare to DJIA===========\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\"><th>Start date</th><td colspan=2>2021-01-31</td></tr>\n",
       "    <tr style=\"text-align: right;\"><th>End date</th><td colspan=2>2024-03-31</td></tr>\n",
       "    <tr style=\"text-align: right;\"><th>Total months</th><td colspan=2>1</td></tr>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Backtest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Annual return</th>\n",
       "      <td>185.739%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cumulative returns</th>\n",
       "      <td>17.643%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual volatility</th>\n",
       "      <td>84.652%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe ratio</th>\n",
       "      <td>1.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Calmar ratio</th>\n",
       "      <td>10.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stability</th>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max drawdown</th>\n",
       "      <td>-17.072%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omega ratio</th>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sortino ratio</th>\n",
       "      <td>3.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skew</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kurtosis</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tail ratio</th>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daily value at risk</th>\n",
       "      <td>-10.103%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha</th>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta</th>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Worst drawdown periods</th>\n",
       "      <th>Net drawdown in %</th>\n",
       "      <th>Peak date</th>\n",
       "      <th>Valley date</th>\n",
       "      <th>Recovery date</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.07</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.99</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.20</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.39</td>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConversionError",
     "evalue": "Failed to convert value(s) to axis units: (NaT, Timestamp('2024-03-31 00:00:00+0000', tz='UTC'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axis.py:1769\u001b[0m, in \u001b[0;36mAxis.convert_units\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1769\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pandas/plotting/_matplotlib/converter.py:237\u001b[0m, in \u001b[0;36mPeriodConverter.convert\u001b[0;34m(values, units, axis)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mPeriodConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pandas/plotting/_matplotlib/converter.py:267\u001b[0m, in \u001b[0;36mPeriodConverter._convert_1d\u001b[0;34m(values, units, axis)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, Index)):\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [get_datevalue(x, axis\u001b[38;5;241m.\u001b[39mfreq) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pandas/plotting/_matplotlib/converter.py:267\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, Index)):\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mget_datevalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pandas/plotting/_matplotlib/converter.py:275\u001b[0m, in \u001b[0;36mget_datevalue\u001b[0;34m(date, freq)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(date, (\u001b[38;5;28mstr\u001b[39m, datetime, pydt\u001b[38;5;241m.\u001b[39mdate, pydt\u001b[38;5;241m.\u001b[39mtime, np\u001b[38;5;241m.\u001b[39mdatetime64)):\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeriod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mordinal\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    277\u001b[0m     is_integer(date)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_float(date)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(date, (np\u001b[38;5;241m.\u001b[39mndarray, Index)) \u001b[38;5;129;01mand\u001b[39;00m (date\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    280\u001b[0m ):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NaTType' object has no attribute 'ordinal'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConversionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# S&P 500: ^GSPC\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Dow Jones Index: ^DJI\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# NASDAQ 100: ^NDX\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mbacktest_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily_account_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_ticker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m^DJI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTEST_START_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTEST_END_DATE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/finrl/plot.py:71\u001b[0m, in \u001b[0;36mbacktest_plot\u001b[0;34m(account_value, baseline_start, baseline_end, baseline_ticker, value_col_name)\u001b[0m\n\u001b[1;32m     68\u001b[0m baseline_returns \u001b[38;5;241m=\u001b[39m get_daily_return(baseline_df, value_col_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyfolio\u001b[38;5;241m.\u001b[39mplotting\u001b[38;5;241m.\u001b[39mplotting_context(font_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mpyfolio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_full_tear_sheet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_returns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_rets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_returns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pyfolio/tears.py:180\u001b[0m, in \u001b[0;36mcreate_full_tear_sheet\u001b[0;34m(returns, positions, transactions, market_data, benchmark_rets, slippage, live_start_date, sector_mappings, round_trips, estimate_intraday, hide_positions, cone_std, bootstrap, unadjusted_returns, turnover_denom, set_context, factor_returns, factor_loadings, pos_in_dollars, header_rows, factor_partitions)\u001b[0m\n\u001b[1;32m    174\u001b[0m     returns \u001b[38;5;241m=\u001b[39m txn\u001b[38;5;241m.\u001b[39madjust_returns_for_slippage(returns, positions,\n\u001b[1;32m    175\u001b[0m                                               transactions, slippage)\n\u001b[1;32m    177\u001b[0m positions \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mcheck_intraday(estimate_intraday, returns,\n\u001b[1;32m    178\u001b[0m                                  positions, transactions)\n\u001b[0;32m--> 180\u001b[0m \u001b[43mcreate_returns_tear_sheet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_start_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_start_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcone_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcone_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark_rets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark_rets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mturnover_denom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturnover_denom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mset_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m create_interesting_times_tear_sheet(returns,\n\u001b[1;32m    193\u001b[0m                                     benchmark_rets\u001b[38;5;241m=\u001b[39mbenchmark_rets,\n\u001b[1;32m    194\u001b[0m                                     set_context\u001b[38;5;241m=\u001b[39mset_context)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pyfolio/plotting.py:54\u001b[0m, in \u001b[0;36mcustomize.<locals>.call_w_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pyfolio/tears.py:569\u001b[0m, in \u001b[0;36mcreate_returns_tear_sheet\u001b[0;34m(returns, positions, transactions, live_start_date, cone_std, benchmark_rets, bootstrap, turnover_denom, header_rows, return_fig)\u001b[0m\n\u001b[1;32m    565\u001b[0m plotting\u001b[38;5;241m.\u001b[39mplot_rolling_sharpe(\n\u001b[1;32m    566\u001b[0m     returns, ax\u001b[38;5;241m=\u001b[39max_rolling_sharpe)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# Drawdowns\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[43mplotting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_drawdown_periods\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max_drawdown\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m plotting\u001b[38;5;241m.\u001b[39mplot_drawdown_underwater(\n\u001b[1;32m    573\u001b[0m     returns\u001b[38;5;241m=\u001b[39mreturns, ax\u001b[38;5;241m=\u001b[39max_underwater)\n\u001b[1;32m    575\u001b[0m plotting\u001b[38;5;241m.\u001b[39mplot_monthly_returns_heatmap(returns, ax\u001b[38;5;241m=\u001b[39max_monthly_heatmap)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pyfolio/plotting.py:442\u001b[0m, in \u001b[0;36mplot_drawdown_periods\u001b[0;34m(returns, top, ax, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(recovery):\n\u001b[1;32m    441\u001b[0m         recovery \u001b[38;5;241m=\u001b[39m returns\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 442\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_between\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecovery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlim\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlim\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m                    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(lim)\n\u001b[1;32m    448\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m drawdown periods\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m top)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5512\u001b[0m, in \u001b[0;36mAxes.fill_between\u001b[0;34m(self, x, y1, y2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfill_between\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y1, y2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5511\u001b[0m                  step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 5512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_between_x_or_y\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5413\u001b[0m, in \u001b[0;36mAxes._fill_between_x_or_y\u001b[0;34m(self, ind_dir, ind, dep1, dep2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5408\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m   5409\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_patches_for_fill\u001b[38;5;241m.\u001b[39mget_next_color()\n\u001b[1;32m   5411\u001b[0m \u001b[38;5;66;03m# Handle united data, such as dates\u001b[39;00m\n\u001b[1;32m   5412\u001b[0m ind, dep1, dep2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m-> 5413\u001b[0m     ma\u001b[38;5;241m.\u001b[39mmasked_invalid, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_unit_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5414\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdep_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdep1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdep_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdep2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   5416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, array \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m   5417\u001b[0m         (ind_dir, ind), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, dep1), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, dep2)]:\n\u001b[1;32m   5418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axes/_base.py:2573\u001b[0m, in \u001b[0;36m_AxesBase._process_unit_info\u001b[0;34m(self, datasets, kwargs, convert)\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m dataset_axis_name \u001b[38;5;241m==\u001b[39m axis_name \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2572\u001b[0m                 axis\u001b[38;5;241m.\u001b[39mupdate_units(data)\n\u001b[0;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [axis_map[axis_name]\u001b[38;5;241m.\u001b[39mconvert_units(data)\n\u001b[1;32m   2574\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m convert \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data\n\u001b[1;32m   2575\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m axis_name, data \u001b[38;5;129;01min\u001b[39;00m datasets]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axes/_base.py:2573\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m dataset_axis_name \u001b[38;5;241m==\u001b[39m axis_name \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2572\u001b[0m                 axis\u001b[38;5;241m.\u001b[39mupdate_units(data)\n\u001b[0;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43maxis_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m convert \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data\n\u001b[1;32m   2575\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m axis_name, data \u001b[38;5;129;01min\u001b[39;00m datasets]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axis.py:1771\u001b[0m, in \u001b[0;36mAxis.convert_units\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1769\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter\u001b[38;5;241m.\u001b[39mconvert(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m munits\u001b[38;5;241m.\u001b[39mConversionError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to convert value(s) to axis \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1772\u001b[0m                                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mConversionError\u001b[0m: Failed to convert value(s) to axis units: (NaT, Timestamp('2024-03-31 00:00:00+0000', tz='UTC'))"
     ]
    }
   ],
   "source": [
    "print(\"==============Compare to DJIA===========\")\n",
    "%matplotlib inline\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "\n",
    "backtest_plot(daily_account_value, \n",
    "            baseline_ticker = '^DJI', \n",
    "            baseline_start = TEST_START_DATE,\n",
    "            baseline_end = TEST_END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Buy&Hold Strategy\n",
    "pass in df_account_value, this information is stored in env class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTest with Buy&Hold Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(DOW_30_TICKER)\n",
    "test_portfolio = DOW_30_TICKER\n",
    "modify_fields = ['open','high','low','close']\n",
    "used_columns = ['date','close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stock_test_ = get_baseline(\n",
    "#         ticker='AXP', \n",
    "#         start = TEST_START_DATE,\n",
    "#         end = TEST_END_DATE)\n",
    "# df_stock_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for field in modify_fields:\n",
    "#     df_stock_test_[field] = df_stock_test_[field]/df_stock_test_.iloc[0][field]/len(test_portfolio)\n",
    "# df_stock_test_ = df_stock_test_[used_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "oBQx4bVQFi-a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (809, 8)\n",
      "Shape of DataFrame:  (809, 8)\n",
      "Annual return           0.415405\n",
      "Cumulative returns      0.055238\n",
      "Annual volatility       0.142013\n",
      "Sharpe ratio            2.581802\n",
      "Calmar ratio           10.349352\n",
      "Stability               0.407411\n",
      "Max drawdown           -0.040138\n",
      "Omega ratio             1.575704\n",
      "Sortino ratio           3.763793\n",
      "Skew                         NaN\n",
      "Kurtosis                     NaN\n",
      "Tail ratio              0.848872\n",
      "Daily value at risk    -0.016437\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "shape = daily_account_value.shape\n",
    "df_hold_ = pd.DataFrame(0,index=range(shape[0]), columns=range(shape[1]))\n",
    "df_hold_.columns = used_columns\n",
    "df_hold_['date'] = daily_account_value.date\n",
    "\n",
    "for stock in test_portfolio:\n",
    "    df_stock_ = get_baseline(\n",
    "        ticker=stock, \n",
    "        start = TEST_START_DATE,\n",
    "        end = TEST_END_DATE)\n",
    "    # for field in modify_fields:\n",
    "    if len(df_stock_) < len(df_hold_):\n",
    "        final_row = df_stock_.iloc[-1]\n",
    "        new_rows = pd.DataFrame([final_row] * (len(df_hold_) - len(df_stock_)))\n",
    "        df_stock_ = pd.concat([df_stock_, new_rows], ignore_index=True)\n",
    "        \n",
    "        for i in range(0,len(df_stock_) < len(df_hold_)):\n",
    "            df_stock_.iloc[en(df_stock_)] = df_stock_.iloc[len(df_stock_-1)]\n",
    "    df_stock_['close'] = df_stock_['close']/df_stock_.iloc[0]['close']/len(test_portfolio)\n",
    "    df_hold_['close'] = df_hold_.close + df_stock_.close\n",
    "\n",
    "stats = backtest_stats(df_hold_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model' calculation encompass the entire timeframe, including non-working days. To address this, we need to determine the most appropriate method to fill up the weekend plots, for example backfilling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hold_.date = pd.to_datetime(df_hold_['date'])\n",
    "# df_hold_ = pd.merge(df_account_value_ppo['date'],df_hold_,how='left')\n",
    "# df_hold_.bfill(inplace=True)\n",
    "# df_hold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_hold:                      hold\n",
      "date                    \n",
      "2021-01-31  1.000000e+06\n",
      "2021-02-28  1.006314e+06\n",
      "2021-03-31  1.021052e+06\n",
      "2021-04-30  1.029017e+06\n",
      "2021-05-31  1.030249e+06\n",
      "2021-06-30  1.029785e+06\n",
      "2021-07-31  1.033666e+06\n",
      "2021-08-31  1.035076e+06\n",
      "2021-09-30  1.035051e+06\n",
      "2021-10-31  1.026937e+06\n",
      "2021-11-30  1.030746e+06\n",
      "2021-12-31  1.039045e+06\n",
      "2022-01-31  1.038597e+06\n",
      "2022-02-28  1.029507e+06\n",
      "2022-03-31  1.028421e+06\n",
      "2022-04-30  1.027227e+06\n",
      "2022-05-31  1.009060e+06\n",
      "2022-06-30  1.018215e+06\n",
      "2022-07-31  9.973398e+05\n",
      "2022-08-31  1.004524e+06\n",
      "2022-09-30  1.018989e+06\n",
      "2022-10-31  1.020445e+06\n",
      "2022-11-30  1.033378e+06\n",
      "2022-12-31  1.038320e+06\n",
      "2023-01-31  1.046430e+06\n",
      "2023-02-28  1.044933e+06\n",
      "2023-03-31  1.047083e+06\n",
      "2023-04-30  1.047593e+06\n",
      "2023-05-31  1.049621e+06\n",
      "2023-06-30  1.051232e+06\n",
      "2023-07-31  1.054914e+06\n",
      "2023-08-31  1.050851e+06\n",
      "2023-09-30  1.052054e+06\n",
      "2023-10-31  1.054023e+06\n",
      "2023-11-30  1.055057e+06\n",
      "2023-12-31  1.069562e+06\n",
      "2024-01-31  1.051501e+06\n",
      "2024-02-29  1.034710e+06\n",
      "2024-03-31  1.055238e+06\n"
     ]
    }
   ],
   "source": [
    "df_hold = pd.DataFrame()\n",
    "df_hold['date'] = daily_account_value['date']\n",
    "df_hold['hold'] = df_hold_['close'] / df_hold_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "# print(\"df_hold: \", df_hold)\n",
    "# df_dji.to_csv(\"df_dji.csv\")\n",
    "df_hold = df_hold.set_index(df_hold.columns[0])\n",
    "print(\"df_hold: \", df_hold)\n",
    "# df_hold.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "# df_account_value.to_csv('df_account_value.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Compare to Buy&Hold===========\n",
      "result:                     stock          hold\n",
      "date                                  \n",
      "2021-01-31  1.000000e+06  1.000000e+06\n",
      "2021-01-31  1.000000e+06  1.000000e+06\n",
      "2021-01-31  1.000000e+06  1.000000e+06\n",
      "2021-01-31  1.000000e+06  1.000000e+06\n",
      "2021-01-31  1.000000e+06  1.000000e+06\n",
      "...                  ...           ...\n",
      "2024-02-29  1.349998e+06  1.034710e+06\n",
      "2024-02-29  1.349998e+06  1.034710e+06\n",
      "2024-02-29  1.322213e+06  1.034710e+06\n",
      "2024-02-29  1.317441e+06  1.034710e+06\n",
      "2024-02-29  1.317441e+06  1.034710e+06\n",
      "\n",
      "[1140 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"==============Compare to Buy&Hold===========\")\n",
    "df_result = pd.DataFrame({'date': df_account_value_ppo['date'], 'stock': df_account_value_ppo['account_value']})\n",
    "df_result = df_result.set_index('date')\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result = pd.merge(df_result, df_hold, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "# result.to_csv(\"result.csv\")\n",
    "result.columns = ['model', 'buy_and_hold']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "# 8. Save and load model #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.1'></a>\n",
    "## 8.1 Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Desktop/GameProjects/DataScience/DeepReinforcementLearning/FinRL-Tutorials-master/6-Binhlai_Testing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./trained_models/dow30_endRecord_128neu_ppo_768episode'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"./\"+TRAINED_MODEL_DIR+\"/dow30_endRecord_128neu_ppo_768episode\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Save model as ONNX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.onnx\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxableSB3Policy(th.nn.Module):\n",
    "    def __init__(self, policy: BasePolicy):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        # NOTE: Preprocessing is included, but postprocessing\n",
    "        # (clipping/inscaling actions) is not,\n",
    "        # If needed, you also need to transpose the images so that they are channel first\n",
    "        # use deterministic=False if you want to export the stochastic policy\n",
    "        # policy() returns `actions, values, log_prob` for PPO\n",
    "        return self.policy(observation, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_policy = OnnxableSB3Policy(trained_ppo.policy)\n",
    "observation_size = trained_ppo.observation_space.shape\n",
    "dummy_input = th.randn(1, *observation_size)\n",
    "\n",
    "th.onnx.export(\n",
    "    onnx_policy,\n",
    "    dummy_input,\n",
    "    \"sac_model_1200k_sac9.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.2'></a>\n",
    "## 8.2 Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.2.1'></a>\n",
    "### 8.2.1 Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Desktop/GameProjects/DataScience/DeepReinforcementLearning/FinRL-Tutorials-master/6-Binhlai_Testing\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./trained_models/dow30_endRecord_128neu_ppo_512episode'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"./\"+TRAINED_MODEL_DIR+\"/dow30_endRecord_128neu_ppo_512episode\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = PPO.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continous training based on the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establish the training environment using StockTradingEnv() class\n",
    "agent = DRLAgent(env = env_train)\n",
    "trained_model.env = env_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to tensorboard_log/test_ppo/ppo_36\n",
      "Episode: 5\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 183       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 19        |\n",
      "|    total_timesteps | 3655      |\n",
      "| train/             |           |\n",
      "|    reward          | 370.40125 |\n",
      "----------------------------------\n",
      "Episode: 6\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 178           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 40            |\n",
      "|    total_timesteps      | 7310          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2750546e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000246      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 614           |\n",
      "|    n_updates            | 5130          |\n",
      "|    policy_gradient_loss | -6.22e-06     |\n",
      "|    reward               | 381.21042     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.23e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 7\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 179          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 61           |\n",
      "|    total_timesteps      | 10965        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.378103e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -3.66e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 650          |\n",
      "|    n_updates            | 5140         |\n",
      "|    policy_gradient_loss | -2.62e-06    |\n",
      "|    reward               | 175.82585    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 1.3e+03      |\n",
      "------------------------------------------\n",
      "Episode: 8\n",
      "row: 3654, episode: 8\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2543956.33\n",
      "total_reward: 1543956.33\n",
      "total_cost: 720615.56\n",
      "total_trades: 2641\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 177           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 82            |\n",
      "|    total_timesteps      | 14620         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1132223e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -0.003        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 136           |\n",
      "|    n_updates            | 5150          |\n",
      "|    policy_gradient_loss | -4.31e-05     |\n",
      "|    reward               | 154.39563     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 273           |\n",
      "-------------------------------------------\n",
      "Episode: 9\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 177           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 102           |\n",
      "|    total_timesteps      | 18275         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1686544e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -0.000981     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 104           |\n",
      "|    n_updates            | 5160          |\n",
      "|    policy_gradient_loss | -3.02e-05     |\n",
      "|    reward               | 210.98985     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 209           |\n",
      "-------------------------------------------\n",
      "Episode: 10\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 176          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 21930        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.373216e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -0.000269    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 197          |\n",
      "|    n_updates            | 5170         |\n",
      "|    policy_gradient_loss | -3.55e-06    |\n",
      "|    reward               | 347.89273    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 394          |\n",
      "------------------------------------------\n",
      "Episode: 11\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 175          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 145          |\n",
      "|    total_timesteps      | 25585        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.563986e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -6.32e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 536          |\n",
      "|    n_updates            | 5180         |\n",
      "|    policy_gradient_loss | -6.76e-05    |\n",
      "|    reward               | 271.9629     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 1.07e+03     |\n",
      "------------------------------------------\n",
      "Episode: 12\n",
      "row: 3654, episode: 12\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2818930.44\n",
      "total_reward: 1818930.44\n",
      "total_cost: 906470.93\n",
      "total_trades: 2531\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 176          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 165          |\n",
      "|    total_timesteps      | 29240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0008955149 |\n",
      "|    clip_fraction        | 0.00219      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -0.000101    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 327          |\n",
      "|    n_updates            | 5190         |\n",
      "|    policy_gradient_loss | -0.000131    |\n",
      "|    reward               | 181.89305    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 654          |\n",
      "------------------------------------------\n",
      "Episode: 13\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 177          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 185          |\n",
      "|    total_timesteps      | 32895        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010724232 |\n",
      "|    clip_fraction        | 0.0084       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -0.000196    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 143          |\n",
      "|    n_updates            | 5200         |\n",
      "|    policy_gradient_loss | 0.000492     |\n",
      "|    reward               | 231.06143    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 286          |\n",
      "------------------------------------------\n",
      "Episode: 14\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 178           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 205           |\n",
      "|    total_timesteps      | 36550         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015708599 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 3.23e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 239           |\n",
      "|    n_updates            | 5210          |\n",
      "|    policy_gradient_loss | -6.47e-05     |\n",
      "|    reward               | 425.00323     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 478           |\n",
      "-------------------------------------------\n",
      "Episode: 15\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 178           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 225           |\n",
      "|    total_timesteps      | 40205         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026216588 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 0.000197      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 803           |\n",
      "|    n_updates            | 5220          |\n",
      "|    policy_gradient_loss | -2.33e-05     |\n",
      "|    reward               | 272.77826     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.61e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 16\n",
      "row: 3654, episode: 16\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4657998.95\n",
      "total_reward: 3657998.95\n",
      "total_cost: 982635.99\n",
      "total_trades: 2586\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 178           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 245           |\n",
      "|    total_timesteps      | 43860         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00056166097 |\n",
      "|    clip_fraction        | 0.000328      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 0.000142      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 328           |\n",
      "|    n_updates            | 5230          |\n",
      "|    policy_gradient_loss | -0.000134     |\n",
      "|    reward               | 365.7999      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 656           |\n",
      "-------------------------------------------\n",
      "Episode: 17\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 178           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 266           |\n",
      "|    total_timesteps      | 47515         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00041639325 |\n",
      "|    clip_fraction        | 8.21e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 0.000183      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 623           |\n",
      "|    n_updates            | 5240          |\n",
      "|    policy_gradient_loss | 6.91e-05      |\n",
      "|    reward               | 205.34314     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.25e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 18\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 169           |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 301           |\n",
      "|    total_timesteps      | 51170         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00015363414 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 8.59e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 189           |\n",
      "|    n_updates            | 5250          |\n",
      "|    policy_gradient_loss | -0.000126     |\n",
      "|    reward               | 177.61417     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 377           |\n",
      "-------------------------------------------\n",
      "Episode: 19\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 168           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 325           |\n",
      "|    total_timesteps      | 54825         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040498644 |\n",
      "|    clip_fraction        | 8.21e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 0.000145      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 140           |\n",
      "|    n_updates            | 5260          |\n",
      "|    policy_gradient_loss | -0.000425     |\n",
      "|    reward               | 266.10764     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 280           |\n",
      "-------------------------------------------\n",
      "Episode: 20\n",
      "row: 3654, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3685299.11\n",
      "total_reward: 2685299.11\n",
      "total_cost: 1030721.29\n",
      "total_trades: 2844\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 167          |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 348          |\n",
      "|    total_timesteps      | 58480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047605587 |\n",
      "|    clip_fraction        | 0.117        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 8.26e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 307          |\n",
      "|    n_updates            | 5270         |\n",
      "|    policy_gradient_loss | -0.000938    |\n",
      "|    reward               | 268.5299     |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 615          |\n",
      "------------------------------------------\n",
      "Episode: 21\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 372         |\n",
      "|    total_timesteps      | 62135       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001970398 |\n",
      "|    clip_fraction        | 0.0347      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 0.000286    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 320         |\n",
      "|    n_updates            | 5280        |\n",
      "|    policy_gradient_loss | -0.000262   |\n",
      "|    reward               | 362.90616   |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 640         |\n",
      "-----------------------------------------\n",
      "Episode: 22\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 165           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 398           |\n",
      "|    total_timesteps      | 65790         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021864996 |\n",
      "|    clip_fraction        | 0.000109      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000176      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 605           |\n",
      "|    n_updates            | 5290          |\n",
      "|    policy_gradient_loss | 0.00035       |\n",
      "|    reward               | 376.8018      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.21e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 23\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 423         |\n",
      "|    total_timesteps      | 69445       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.71603e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 2.93e-05    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 633         |\n",
      "|    n_updates            | 5300        |\n",
      "|    policy_gradient_loss | -1.65e-05   |\n",
      "|    reward               | 58.70528    |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 1.27e+03    |\n",
      "-----------------------------------------\n",
      "Episode: 24\n",
      "row: 3654, episode: 24\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3449056.41\n",
      "total_reward: 2449056.41\n",
      "total_cost: 1223403.75\n",
      "total_trades: 2947\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 163           |\n",
      "|    iterations           | 20            |\n",
      "|    time_elapsed         | 446           |\n",
      "|    total_timesteps      | 73100         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2139467e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -0.00579      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 18.6          |\n",
      "|    n_updates            | 5310          |\n",
      "|    policy_gradient_loss | 1.15e-05      |\n",
      "|    reward               | 244.90564     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 37.3          |\n",
      "-------------------------------------------\n",
      "Episode: 25\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 163           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 469           |\n",
      "|    total_timesteps      | 76755         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9093006e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -0.000223     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 272           |\n",
      "|    n_updates            | 5320          |\n",
      "|    policy_gradient_loss | -2.3e-05      |\n",
      "|    reward               | 223.24867     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 544           |\n",
      "-------------------------------------------\n",
      "Episode: 26\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 162           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 493           |\n",
      "|    total_timesteps      | 80410         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5696324e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000386      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 224           |\n",
      "|    n_updates            | 5330          |\n",
      "|    policy_gradient_loss | -4.84e-06     |\n",
      "|    reward               | 168.19089     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 449           |\n",
      "-------------------------------------------\n",
      "Episode: 27\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 163           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 515           |\n",
      "|    total_timesteps      | 84065         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8067689e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -0.00107      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 123           |\n",
      "|    n_updates            | 5340          |\n",
      "|    policy_gradient_loss | -5.4e-05      |\n",
      "|    reward               | 308.27158     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 245           |\n",
      "-------------------------------------------\n",
      "Episode: 28\n",
      "row: 3654, episode: 28\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4440278.36\n",
      "total_reward: 3440278.36\n",
      "total_cost: 1382122.94\n",
      "total_trades: 2955\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 162           |\n",
      "|    iterations           | 24            |\n",
      "|    time_elapsed         | 540           |\n",
      "|    total_timesteps      | 87720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2502951e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -2.56e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 419           |\n",
      "|    n_updates            | 5350          |\n",
      "|    policy_gradient_loss | -1.95e-07     |\n",
      "|    reward               | 344.02783     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 838           |\n",
      "-------------------------------------------\n",
      "Episode: 29\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 161           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 565           |\n",
      "|    total_timesteps      | 91375         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1961463e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000118      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 528           |\n",
      "|    n_updates            | 5360          |\n",
      "|    policy_gradient_loss | 1.92e-05      |\n",
      "|    reward               | 199.58153     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.06e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 30\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 160           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 590           |\n",
      "|    total_timesteps      | 95030         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6762848e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -0.000364     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 176           |\n",
      "|    n_updates            | 5370          |\n",
      "|    policy_gradient_loss | -2.32e-06     |\n",
      "|    reward               | 200.81206     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 352           |\n",
      "-------------------------------------------\n",
      "Episode: 31\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 160           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 615           |\n",
      "|    total_timesteps      | 98685         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1968559e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000289      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 185           |\n",
      "|    n_updates            | 5380          |\n",
      "|    policy_gradient_loss | -2.21e-05     |\n",
      "|    reward               | 177.25407     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 371           |\n",
      "-------------------------------------------\n",
      "Episode: 32\n",
      "row: 3654, episode: 32\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5106823.75\n",
      "total_reward: 4106823.75\n",
      "total_cost: 1275962.20\n",
      "total_trades: 2918\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 159           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 640           |\n",
      "|    total_timesteps      | 102340        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1072537e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000218      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 140           |\n",
      "|    n_updates            | 5390          |\n",
      "|    policy_gradient_loss | -3.47e-05     |\n",
      "|    reward               | 410.68237     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 280           |\n",
      "-------------------------------------------\n",
      "Episode: 33\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 668          |\n",
      "|    total_timesteps      | 105995       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.896002e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 5.74e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 753          |\n",
      "|    n_updates            | 5400         |\n",
      "|    policy_gradient_loss | 4.59e-05     |\n",
      "|    reward               | 110.92182    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 1.51e+03     |\n",
      "------------------------------------------\n",
      "Episode: 34\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 158          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 691          |\n",
      "|    total_timesteps      | 109650       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.950594e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -7.5e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 59.3         |\n",
      "|    n_updates            | 5410         |\n",
      "|    policy_gradient_loss | -0.00014     |\n",
      "|    reward               | 218.77496    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 119          |\n",
      "------------------------------------------\n",
      "Episode: 35\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 157          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 719          |\n",
      "|    total_timesteps      | 113305       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015858146 |\n",
      "|    clip_fraction        | 0.0115       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -1.74e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 215          |\n",
      "|    n_updates            | 5420         |\n",
      "|    policy_gradient_loss | -0.000279    |\n",
      "|    reward               | 261.84625    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 431          |\n",
      "------------------------------------------\n",
      "Episode: 36\n",
      "row: 3654, episode: 36\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1903006.26\n",
      "total_reward: 903006.26\n",
      "total_cost: 815489.55\n",
      "total_trades: 2893\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 157           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 743           |\n",
      "|    total_timesteps      | 116960        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00029203252 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 4.14e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 303           |\n",
      "|    n_updates            | 5430          |\n",
      "|    policy_gradient_loss | 0.000111      |\n",
      "|    reward               | 90.30063      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 606           |\n",
      "-------------------------------------------\n",
      "Episode: 37\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 769           |\n",
      "|    total_timesteps      | 120615        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00053652807 |\n",
      "|    clip_fraction        | 8.21e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -0.000875     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 35.1          |\n",
      "|    n_updates            | 5440          |\n",
      "|    policy_gradient_loss | -0.000672     |\n",
      "|    reward               | 264.77625     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 71            |\n",
      "-------------------------------------------\n",
      "Episode: 38\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 792           |\n",
      "|    total_timesteps      | 124270        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1646625e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -0.000214     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 313           |\n",
      "|    n_updates            | 5450          |\n",
      "|    policy_gradient_loss | 0.000108      |\n",
      "|    reward               | 143.33946     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 627           |\n",
      "-------------------------------------------\n",
      "Episode: 39\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 815           |\n",
      "|    total_timesteps      | 127925        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7609538e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000308      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 94.6          |\n",
      "|    n_updates            | 5460          |\n",
      "|    policy_gradient_loss | -2.58e-05     |\n",
      "|    reward               | 299.0406      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 190           |\n",
      "-------------------------------------------\n",
      "Episode: 40\n",
      "row: 3654, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4884797.65\n",
      "total_reward: 3884797.65\n",
      "total_cost: 1636057.76\n",
      "total_trades: 2983\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 840           |\n",
      "|    total_timesteps      | 131580        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017497307 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000152      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 402           |\n",
      "|    n_updates            | 5470          |\n",
      "|    policy_gradient_loss | -0.000198     |\n",
      "|    reward               | 388.47977     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 804           |\n",
      "-------------------------------------------\n",
      "Episode: 41\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 863           |\n",
      "|    total_timesteps      | 135235        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026755576 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000124      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 696           |\n",
      "|    n_updates            | 5480          |\n",
      "|    policy_gradient_loss | -0.00017      |\n",
      "|    reward               | 120.66589     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.39e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 42\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 156           |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 887           |\n",
      "|    total_timesteps      | 138890        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012107138 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 0.000134      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 63.4          |\n",
      "|    n_updates            | 5490          |\n",
      "|    policy_gradient_loss | -5.47e-05     |\n",
      "|    reward               | 175.48636     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 127           |\n",
      "-------------------------------------------\n",
      "Episode: 43\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 156            |\n",
      "|    iterations           | 39             |\n",
      "|    time_elapsed         | 911            |\n",
      "|    total_timesteps      | 142545         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000110973604 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.64          |\n",
      "|    explained_variance   | -0.000162      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 141            |\n",
      "|    n_updates            | 5500           |\n",
      "|    policy_gradient_loss | -6.2e-05       |\n",
      "|    reward               | 133.23883      |\n",
      "|    std                  | 1.25           |\n",
      "|    value_loss           | 283            |\n",
      "--------------------------------------------\n",
      "Episode: 44\n",
      "row: 3654, episode: 44\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3121555.42\n",
      "total_reward: 2121555.42\n",
      "total_cost: 1306113.10\n",
      "total_trades: 2967\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 156          |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 935          |\n",
      "|    total_timesteps      | 146200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003733151 |\n",
      "|    clip_fraction        | 2.74e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -0.000144    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 81.4         |\n",
      "|    n_updates            | 5510         |\n",
      "|    policy_gradient_loss | -0.000284    |\n",
      "|    reward               | 212.15555    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 164          |\n",
      "------------------------------------------\n",
      "Episode: 45\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 155          |\n",
      "|    iterations           | 41           |\n",
      "|    time_elapsed         | 961          |\n",
      "|    total_timesteps      | 149855       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014069306 |\n",
      "|    clip_fraction        | 0.0146       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -0.000186    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 197          |\n",
      "|    n_updates            | 5520         |\n",
      "|    policy_gradient_loss | -0.000264    |\n",
      "|    reward               | 124.38887    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 394          |\n",
      "------------------------------------------\n",
      "Episode: 46\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 155          |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 989          |\n",
      "|    total_timesteps      | 153510       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005752085 |\n",
      "|    clip_fraction        | 0.000301     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -9.06e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 70           |\n",
      "|    n_updates            | 5530         |\n",
      "|    policy_gradient_loss | -0.000373    |\n",
      "|    reward               | 124.2151     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 141          |\n",
      "------------------------------------------\n",
      "Episode: 47\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 1016         |\n",
      "|    total_timesteps      | 157165       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022099332 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -0.000226    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 70.8         |\n",
      "|    n_updates            | 5540         |\n",
      "|    policy_gradient_loss | -0.00077     |\n",
      "|    reward               | 103.57037    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 142          |\n",
      "------------------------------------------\n",
      "Episode: 48\n",
      "row: 3654, episode: 48\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2057156.61\n",
      "total_reward: 1057156.61\n",
      "total_cost: 950949.59\n",
      "total_trades: 3124\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 154          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 1041         |\n",
      "|    total_timesteps      | 160820       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015397143 |\n",
      "|    clip_fraction        | 0.00903      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -0.00149     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 47           |\n",
      "|    n_updates            | 5550         |\n",
      "|    policy_gradient_loss | -0.000472    |\n",
      "|    reward               | 105.71566    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 94.2         |\n",
      "------------------------------------------\n",
      "Episode: 49\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 153           |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 1068          |\n",
      "|    total_timesteps      | 164475        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00040442933 |\n",
      "|    clip_fraction        | 0.0043        |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -0.00397      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 50.2          |\n",
      "|    n_updates            | 5560          |\n",
      "|    policy_gradient_loss | 0.000693      |\n",
      "|    reward               | 219.83673     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 101           |\n",
      "-------------------------------------------\n",
      "Episode: 50\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 153         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 1098        |\n",
      "|    total_timesteps      | 168130      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000346998 |\n",
      "|    clip_fraction        | 8.21e-05    |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | -0.00169    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 218         |\n",
      "|    n_updates            | 5570        |\n",
      "|    policy_gradient_loss | 1.3e-05     |\n",
      "|    reward               | 204.083     |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 436         |\n",
      "-----------------------------------------\n",
      "Episode: 51\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 151          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 1132         |\n",
      "|    total_timesteps      | 171785       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.415163e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -0.000217    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 190          |\n",
      "|    n_updates            | 5580         |\n",
      "|    policy_gradient_loss | -1.18e-05    |\n",
      "|    reward               | 145.76653    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 379          |\n",
      "------------------------------------------\n",
      "Episode: 52\n",
      "row: 3654, episode: 52\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2533205.30\n",
      "total_reward: 1533205.30\n",
      "total_cost: 1175551.12\n",
      "total_trades: 3193\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 150           |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 1163          |\n",
      "|    total_timesteps      | 175440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00084480946 |\n",
      "|    clip_fraction        | 0.00252       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -0.00033      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 96.4          |\n",
      "|    n_updates            | 5590          |\n",
      "|    policy_gradient_loss | -0.000167     |\n",
      "|    reward               | 153.32053     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 193           |\n",
      "-------------------------------------------\n",
      "Episode: 53\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 149           |\n",
      "|    iterations           | 49            |\n",
      "|    time_elapsed         | 1197          |\n",
      "|    total_timesteps      | 179095        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00043627853 |\n",
      "|    clip_fraction        | 0.000848      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -4.01e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 105           |\n",
      "|    n_updates            | 5600          |\n",
      "|    policy_gradient_loss | -4.16e-05     |\n",
      "|    reward               | 134.09637     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 211           |\n",
      "-------------------------------------------\n",
      "Episode: 54\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 148           |\n",
      "|    iterations           | 50            |\n",
      "|    time_elapsed         | 1230          |\n",
      "|    total_timesteps      | 182750        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00054181274 |\n",
      "|    clip_fraction        | 0.000438      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -0.000203     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 79.8          |\n",
      "|    n_updates            | 5610          |\n",
      "|    policy_gradient_loss | -0.000154     |\n",
      "|    reward               | 160.59222     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 160           |\n",
      "-------------------------------------------\n",
      "Episode: 55\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 147           |\n",
      "|    iterations           | 51            |\n",
      "|    time_elapsed         | 1261          |\n",
      "|    total_timesteps      | 186405        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4483997e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -0.000941     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 116           |\n",
      "|    n_updates            | 5620          |\n",
      "|    policy_gradient_loss | 0.000185      |\n",
      "|    reward               | 174.99208     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 232           |\n",
      "-------------------------------------------\n",
      "Episode: 56\n",
      "row: 3654, episode: 56\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2709686.65\n",
      "total_reward: 1709686.65\n",
      "total_cost: 1319349.24\n",
      "total_trades: 2859\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 147         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 1291        |\n",
      "|    total_timesteps      | 190060      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002481747 |\n",
      "|    clip_fraction        | 0.0228      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | -0.000377   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 140         |\n",
      "|    n_updates            | 5630        |\n",
      "|    policy_gradient_loss | -0.000366   |\n",
      "|    reward               | 170.96866   |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 279         |\n",
      "-----------------------------------------\n",
      "Episode: 57\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 1321        |\n",
      "|    total_timesteps      | 193715      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002165081 |\n",
      "|    clip_fraction        | 0.0486      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | -9.05e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 134         |\n",
      "|    n_updates            | 5640        |\n",
      "|    policy_gradient_loss | -0.000205   |\n",
      "|    reward               | 331.17053   |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 269         |\n",
      "-----------------------------------------\n",
      "Episode: 58\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 145         |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 1353        |\n",
      "|    total_timesteps      | 197370      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.86239e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0.000231    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 498         |\n",
      "|    n_updates            | 5650        |\n",
      "|    policy_gradient_loss | 6.06e-05    |\n",
      "|    reward               | 312.7928    |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 998         |\n",
      "-----------------------------------------\n",
      "Episode: 59\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 144          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 1389         |\n",
      "|    total_timesteps      | 201025       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.656764e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 0.000287     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 439          |\n",
      "|    n_updates            | 5660         |\n",
      "|    policy_gradient_loss | -3.78e-05    |\n",
      "|    reward               | 160.78743    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 880          |\n",
      "------------------------------------------\n",
      "Episode: 60\n",
      "row: 3654, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2335519.23\n",
      "total_reward: 1335519.23\n",
      "total_cost: 822949.14\n",
      "total_trades: 2824\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 144           |\n",
      "|    iterations           | 56            |\n",
      "|    time_elapsed         | 1418          |\n",
      "|    total_timesteps      | 204680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00072220556 |\n",
      "|    clip_fraction        | 0.00178       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -0.00217      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 117           |\n",
      "|    n_updates            | 5670          |\n",
      "|    policy_gradient_loss | -0.000489     |\n",
      "|    reward               | 133.55193     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 234           |\n",
      "-------------------------------------------\n",
      "Episode: 61\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 1447        |\n",
      "|    total_timesteps      | 208335      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.43836e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | -0.00208    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 80.7        |\n",
      "|    n_updates            | 5680        |\n",
      "|    policy_gradient_loss | 0.000149    |\n",
      "|    reward               | 196.78694   |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "Episode: 62\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 143          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 1476         |\n",
      "|    total_timesteps      | 211990       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005802579 |\n",
      "|    clip_fraction        | 0.000356     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.000408    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 173          |\n",
      "|    n_updates            | 5690         |\n",
      "|    policy_gradient_loss | -0.000247    |\n",
      "|    reward               | 147.54474    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 346          |\n",
      "------------------------------------------\n",
      "Episode: 63\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 143           |\n",
      "|    iterations           | 59            |\n",
      "|    time_elapsed         | 1505          |\n",
      "|    total_timesteps      | 215645        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016180822 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -7.69e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 94            |\n",
      "|    n_updates            | 5700          |\n",
      "|    policy_gradient_loss | 9.38e-05      |\n",
      "|    reward               | 260.71875     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 188           |\n",
      "-------------------------------------------\n",
      "Episode: 64\n",
      "row: 3654, episode: 64\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5116848.54\n",
      "total_reward: 4116848.54\n",
      "total_cost: 1364877.81\n",
      "total_trades: 3000\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 142           |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 1535          |\n",
      "|    total_timesteps      | 219300        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8846364e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 0.000104      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 307           |\n",
      "|    n_updates            | 5710          |\n",
      "|    policy_gradient_loss | -3.25e-05     |\n",
      "|    reward               | 411.68484     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 614           |\n",
      "-------------------------------------------\n",
      "Episode: 65\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 142           |\n",
      "|    iterations           | 61            |\n",
      "|    time_elapsed         | 1563          |\n",
      "|    total_timesteps      | 222955        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4235693e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 7.22e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 761           |\n",
      "|    n_updates            | 5720          |\n",
      "|    policy_gradient_loss | -2.59e-05     |\n",
      "|    reward               | 331.43723     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.52e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 66\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 142           |\n",
      "|    iterations           | 62            |\n",
      "|    time_elapsed         | 1591          |\n",
      "|    total_timesteps      | 226610        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.2037235e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 6.88e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 497           |\n",
      "|    n_updates            | 5730          |\n",
      "|    policy_gradient_loss | -2.89e-05     |\n",
      "|    reward               | 290.45007     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 994           |\n",
      "-------------------------------------------\n",
      "Episode: 67\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 1620        |\n",
      "|    total_timesteps      | 230265      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 5.33919e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | -0.00031    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 392         |\n",
      "|    n_updates            | 5740        |\n",
      "|    policy_gradient_loss | 1.47e-05    |\n",
      "|    reward               | 211.28127   |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 784         |\n",
      "-----------------------------------------\n",
      "Episode: 68\n",
      "row: 3654, episode: 68\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5112288.72\n",
      "total_reward: 4112288.72\n",
      "total_cost: 1394169.09\n",
      "total_trades: 2938\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 141           |\n",
      "|    iterations           | 64            |\n",
      "|    time_elapsed         | 1649          |\n",
      "|    total_timesteps      | 233920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019713069 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -0.000692     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 202           |\n",
      "|    n_updates            | 5750          |\n",
      "|    policy_gradient_loss | -0.000142     |\n",
      "|    reward               | 411.22888     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 405           |\n",
      "-------------------------------------------\n",
      "Episode: 69\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 141          |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 1678         |\n",
      "|    total_timesteps      | 237575       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0001913187 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -7.99e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 791          |\n",
      "|    n_updates            | 5760         |\n",
      "|    policy_gradient_loss | -0.000145    |\n",
      "|    reward               | 170.96667    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 1.58e+03     |\n",
      "------------------------------------------\n",
      "Episode: 70\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 141           |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 1706          |\n",
      "|    total_timesteps      | 241230        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1701427e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -0.00149      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 134           |\n",
      "|    n_updates            | 5770          |\n",
      "|    policy_gradient_loss | 3.29e-05      |\n",
      "|    reward               | 203.53773     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 268           |\n",
      "-------------------------------------------\n",
      "Episode: 71\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 141          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 1735         |\n",
      "|    total_timesteps      | 244885       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.894889e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.000409    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 182          |\n",
      "|    n_updates            | 5780         |\n",
      "|    policy_gradient_loss | -4.39e-05    |\n",
      "|    reward               | 313.0373     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 364          |\n",
      "------------------------------------------\n",
      "Episode: 72\n",
      "row: 3654, episode: 72\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5116846.62\n",
      "total_reward: 4116846.62\n",
      "total_cost: 1322692.80\n",
      "total_trades: 2968\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 140           |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 1764          |\n",
      "|    total_timesteps      | 248540        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020508084 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 6.65e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 435           |\n",
      "|    n_updates            | 5790          |\n",
      "|    policy_gradient_loss | -0.000113     |\n",
      "|    reward               | 411.68466     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 871           |\n",
      "-------------------------------------------\n",
      "Episode: 73\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 1792         |\n",
      "|    total_timesteps      | 252195       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.709351e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 0.000106     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 753          |\n",
      "|    n_updates            | 5800         |\n",
      "|    policy_gradient_loss | 9.01e-05     |\n",
      "|    reward               | 127.00209    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 1.51e+03     |\n",
      "------------------------------------------\n",
      "Episode: 74\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 1821         |\n",
      "|    total_timesteps      | 255850       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.063947e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.00103     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 71.5         |\n",
      "|    n_updates            | 5810         |\n",
      "|    policy_gradient_loss | -3.35e-05    |\n",
      "|    reward               | 352.502      |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 143          |\n",
      "------------------------------------------\n",
      "Episode: 75\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 140           |\n",
      "|    iterations           | 71            |\n",
      "|    time_elapsed         | 1850          |\n",
      "|    total_timesteps      | 259505        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8608501e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0.000148      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 570           |\n",
      "|    n_updates            | 5820          |\n",
      "|    policy_gradient_loss | -1.65e-05     |\n",
      "|    reward               | 310.3878      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 1.14e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 76\n",
      "row: 3654, episode: 76\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3973464.78\n",
      "total_reward: 2973464.78\n",
      "total_cost: 1388886.45\n",
      "total_trades: 2738\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 140          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 1877         |\n",
      "|    total_timesteps      | 263160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.726765e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 0.000174     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 436          |\n",
      "|    n_updates            | 5830         |\n",
      "|    policy_gradient_loss | -9.98e-05    |\n",
      "|    reward               | 297.34647    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 871          |\n",
      "------------------------------------------\n",
      "Episode: 77\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 139           |\n",
      "|    iterations           | 73            |\n",
      "|    time_elapsed         | 1905          |\n",
      "|    total_timesteps      | 266815        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00080383476 |\n",
      "|    clip_fraction        | 0.00189       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -5.72e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 409           |\n",
      "|    n_updates            | 5840          |\n",
      "|    policy_gradient_loss | -0.000731     |\n",
      "|    reward               | 113.37674     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 818           |\n",
      "-------------------------------------------\n",
      "Episode: 78\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 139          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 1934         |\n",
      "|    total_timesteps      | 270470       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006788368 |\n",
      "|    clip_fraction        | 0.00156      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.00168     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 57.1         |\n",
      "|    n_updates            | 5850         |\n",
      "|    policy_gradient_loss | 0.000246     |\n",
      "|    reward               | 197.3302     |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "Episode: 79\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 139           |\n",
      "|    iterations           | 75            |\n",
      "|    time_elapsed         | 1964          |\n",
      "|    total_timesteps      | 274125        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2207805e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -0.00044      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 171           |\n",
      "|    n_updates            | 5860          |\n",
      "|    policy_gradient_loss | -2.98e-06     |\n",
      "|    reward               | 205.30879     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 341           |\n",
      "-------------------------------------------\n",
      "Episode: 80\n",
      "row: 3654, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2327138.86\n",
      "total_reward: 1327138.86\n",
      "total_cost: 1091175.43\n",
      "total_trades: 3031\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 139           |\n",
      "|    iterations           | 76            |\n",
      "|    time_elapsed         | 1992          |\n",
      "|    total_timesteps      | 277780        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00047182437 |\n",
      "|    clip_fraction        | 0.000192      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0.000104      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 185           |\n",
      "|    n_updates            | 5870          |\n",
      "|    policy_gradient_loss | -0.000171     |\n",
      "|    reward               | 132.71388     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 370           |\n",
      "-------------------------------------------\n",
      "Episode: 81\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 139          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 2021         |\n",
      "|    total_timesteps      | 281435       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004866977 |\n",
      "|    clip_fraction        | 0.00052      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.00094     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 78.8         |\n",
      "|    n_updates            | 5880         |\n",
      "|    policy_gradient_loss | 0.000362     |\n",
      "|    reward               | 262.23932    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 158          |\n",
      "------------------------------------------\n",
      "Episode: 82\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 138           |\n",
      "|    iterations           | 78            |\n",
      "|    time_elapsed         | 2052          |\n",
      "|    total_timesteps      | 285090        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024750148 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 8.9e-05       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 335           |\n",
      "|    n_updates            | 5890          |\n",
      "|    policy_gradient_loss | -0.000226     |\n",
      "|    reward               | 207.74393     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 670           |\n",
      "-------------------------------------------\n",
      "Episode: 83\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 138           |\n",
      "|    iterations           | 79            |\n",
      "|    time_elapsed         | 2082          |\n",
      "|    total_timesteps      | 288745        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00067463453 |\n",
      "|    clip_fraction        | 0.00104       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0.000231      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 188           |\n",
      "|    n_updates            | 5900          |\n",
      "|    policy_gradient_loss | -0.000216     |\n",
      "|    reward               | 119.47376     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 376           |\n",
      "-------------------------------------------\n",
      "Episode: 84\n",
      "row: 3654, episode: 84\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3890200.31\n",
      "total_reward: 2890200.31\n",
      "total_cost: 1203293.30\n",
      "total_trades: 2985\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 138           |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 2111          |\n",
      "|    total_timesteps      | 292400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016176423 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0.000177      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 62.2          |\n",
      "|    n_updates            | 5910          |\n",
      "|    policy_gradient_loss | 9.57e-05      |\n",
      "|    reward               | 289.02002     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 125           |\n",
      "-------------------------------------------\n",
      "Episode: 85\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 138           |\n",
      "|    iterations           | 81            |\n",
      "|    time_elapsed         | 2139          |\n",
      "|    total_timesteps      | 296055        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1731922e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0.00015       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 375           |\n",
      "|    n_updates            | 5920          |\n",
      "|    policy_gradient_loss | 4.04e-06      |\n",
      "|    reward               | 111.328995    |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 751           |\n",
      "-------------------------------------------\n",
      "Episode: 86\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 138           |\n",
      "|    iterations           | 82            |\n",
      "|    time_elapsed         | 2168          |\n",
      "|    total_timesteps      | 299710        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2379846e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -0.000194     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 56            |\n",
      "|    n_updates            | 5930          |\n",
      "|    policy_gradient_loss | -1.08e-05     |\n",
      "|    reward               | 92.22762      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 112           |\n",
      "-------------------------------------------\n",
      "Episode: 87\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 137           |\n",
      "|    iterations           | 83            |\n",
      "|    time_elapsed         | 2198          |\n",
      "|    total_timesteps      | 303365        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0864382e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -0.000318     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 37.8          |\n",
      "|    n_updates            | 5940          |\n",
      "|    policy_gradient_loss | -8.25e-05     |\n",
      "|    reward               | 177.85387     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 76.5          |\n",
      "-------------------------------------------\n",
      "Episode: 88\n",
      "row: 3654, episode: 88\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3377831.60\n",
      "total_reward: 2377831.60\n",
      "total_cost: 1365611.74\n",
      "total_trades: 3072\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 137           |\n",
      "|    iterations           | 84            |\n",
      "|    time_elapsed         | 2228          |\n",
      "|    total_timesteps      | 307020        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055767084 |\n",
      "|    clip_fraction        | 0.00041       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0.00012       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 143           |\n",
      "|    n_updates            | 5950          |\n",
      "|    policy_gradient_loss | -0.000105     |\n",
      "|    reward               | 237.78316     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 286           |\n",
      "-------------------------------------------\n",
      "Episode: 89\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 137          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 2257         |\n",
      "|    total_timesteps      | 310675       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022332228 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 7.81e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 254          |\n",
      "|    n_updates            | 5960         |\n",
      "|    policy_gradient_loss | -0.000444    |\n",
      "|    reward               | 155.49196    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 508          |\n",
      "------------------------------------------\n",
      "Episode: 90\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 137          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 2286         |\n",
      "|    total_timesteps      | 314330       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011116943 |\n",
      "|    clip_fraction        | 0.00465      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 2.47e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 113          |\n",
      "|    n_updates            | 5970         |\n",
      "|    policy_gradient_loss | -0.000628    |\n",
      "|    reward               | 130.9845     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 226          |\n",
      "------------------------------------------\n",
      "Episode: 91\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 137           |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 2315          |\n",
      "|    total_timesteps      | 317985        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1615531e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0.000611      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 74.7          |\n",
      "|    n_updates            | 5980          |\n",
      "|    policy_gradient_loss | 0.000279      |\n",
      "|    reward               | 49.538185     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 150           |\n",
      "-------------------------------------------\n",
      "Episode: 92\n",
      "row: 3654, episode: 92\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3548851.25\n",
      "total_reward: 2548851.25\n",
      "total_cost: 1854977.88\n",
      "total_trades: 3135\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 137          |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 2345         |\n",
      "|    total_timesteps      | 321640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014500052 |\n",
      "|    clip_fraction        | 0.0314       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.00111     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 11.4         |\n",
      "|    n_updates            | 5990         |\n",
      "|    policy_gradient_loss | 6.81e-05     |\n",
      "|    reward               | 254.88513    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 24.1         |\n",
      "------------------------------------------\n",
      "Episode: 93\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 137          |\n",
      "|    iterations           | 89           |\n",
      "|    time_elapsed         | 2374         |\n",
      "|    total_timesteps      | 325295       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004345922 |\n",
      "|    clip_fraction        | 2.74e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.000638    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 296          |\n",
      "|    n_updates            | 6000         |\n",
      "|    policy_gradient_loss | 0.000385     |\n",
      "|    reward               | 135.98062    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 594          |\n",
      "------------------------------------------\n",
      "Episode: 94\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 136           |\n",
      "|    iterations           | 90            |\n",
      "|    time_elapsed         | 2402          |\n",
      "|    total_timesteps      | 328950        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7001076e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -0.000869     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 84            |\n",
      "|    n_updates            | 6010          |\n",
      "|    policy_gradient_loss | -1.51e-05     |\n",
      "|    reward               | 100.437256    |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 168           |\n",
      "-------------------------------------------\n",
      "Episode: 95\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 136          |\n",
      "|    iterations           | 91           |\n",
      "|    time_elapsed         | 2431         |\n",
      "|    total_timesteps      | 332605       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011006895 |\n",
      "|    clip_fraction        | 0.00476      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.000985    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 46.5         |\n",
      "|    n_updates            | 6020         |\n",
      "|    policy_gradient_loss | -0.000351    |\n",
      "|    reward               | 210.40855    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 93.1         |\n",
      "------------------------------------------\n",
      "Episode: 96\n",
      "row: 3654, episode: 96\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3250112.91\n",
      "total_reward: 2250112.91\n",
      "total_cost: 1328692.81\n",
      "total_trades: 3074\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 136          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 2459         |\n",
      "|    total_timesteps      | 336260       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006272846 |\n",
      "|    clip_fraction        | 0.00202      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -0.000907    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 200          |\n",
      "|    n_updates            | 6030         |\n",
      "|    policy_gradient_loss | 0.000166     |\n",
      "|    reward               | 225.01129    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 399          |\n",
      "------------------------------------------\n",
      "Episode: 97\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 136           |\n",
      "|    iterations           | 93            |\n",
      "|    time_elapsed         | 2488          |\n",
      "|    total_timesteps      | 339915        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6637995e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.41e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 242           |\n",
      "|    n_updates            | 6040          |\n",
      "|    policy_gradient_loss | -4.35e-07     |\n",
      "|    reward               | 127.297005    |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 484           |\n",
      "-------------------------------------------\n",
      "Episode: 98\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 136           |\n",
      "|    iterations           | 94            |\n",
      "|    time_elapsed         | 2517          |\n",
      "|    total_timesteps      | 343570        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.4255485e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -0.000368     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 73.7          |\n",
      "|    n_updates            | 6050          |\n",
      "|    policy_gradient_loss | 5.22e-06      |\n",
      "|    reward               | 149.62645     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 148           |\n",
      "-------------------------------------------\n",
      "Episode: 99\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 136          |\n",
      "|    iterations           | 95           |\n",
      "|    time_elapsed         | 2546         |\n",
      "|    total_timesteps      | 347225       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016378222 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | 1.76e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 99           |\n",
      "|    n_updates            | 6060         |\n",
      "|    policy_gradient_loss | -0.000464    |\n",
      "|    reward               | 282.7476     |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 198          |\n",
      "------------------------------------------\n",
      "Episode: 100\n",
      "row: 3654, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1904954.93\n",
      "total_reward: 904954.93\n",
      "total_cost: 1112117.72\n",
      "total_trades: 3185\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 136           |\n",
      "|    iterations           | 96            |\n",
      "|    time_elapsed         | 2576          |\n",
      "|    total_timesteps      | 350880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00060305564 |\n",
      "|    clip_fraction        | 0.00101       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.67         |\n",
      "|    explained_variance   | 0.000113      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 364           |\n",
      "|    n_updates            | 6070          |\n",
      "|    policy_gradient_loss | 0.000172      |\n",
      "|    reward               | 90.49549      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 729           |\n",
      "-------------------------------------------\n",
      "Episode: 101\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 136           |\n",
      "|    iterations           | 97            |\n",
      "|    time_elapsed         | 2605          |\n",
      "|    total_timesteps      | 354535        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5586411e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.67         |\n",
      "|    explained_variance   | -0.00011      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 39.1          |\n",
      "|    n_updates            | 6080          |\n",
      "|    policy_gradient_loss | 3.5e-06       |\n",
      "|    reward               | 89.11288      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 78.2          |\n",
      "-------------------------------------------\n",
      "Episode: 102\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 135           |\n",
      "|    iterations           | 98            |\n",
      "|    time_elapsed         | 2634          |\n",
      "|    total_timesteps      | 358190        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021645894 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.67         |\n",
      "|    explained_variance   | -0.000821     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 35.8          |\n",
      "|    n_updates            | 6090          |\n",
      "|    policy_gradient_loss | -0.000199     |\n",
      "|    reward               | 86.61255      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 72.4          |\n",
      "-------------------------------------------\n",
      "Episode: 103\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 2664         |\n",
      "|    total_timesteps      | 361845       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011090531 |\n",
      "|    clip_fraction        | 0.00555      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | 5.99e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 33.6         |\n",
      "|    n_updates            | 6100         |\n",
      "|    policy_gradient_loss | -0.00035     |\n",
      "|    reward               | 106.092995   |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 67.5         |\n",
      "------------------------------------------\n",
      "Episode: 104\n",
      "row: 3654, episode: 104\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3263182.94\n",
      "total_reward: 2263182.94\n",
      "total_cost: 1092848.70\n",
      "total_trades: 3105\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 135          |\n",
      "|    iterations           | 100          |\n",
      "|    time_elapsed         | 2702         |\n",
      "|    total_timesteps      | 365500       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013428766 |\n",
      "|    clip_fraction        | 0.00539      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | -0.000593    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 51.2         |\n",
      "|    n_updates            | 6110         |\n",
      "|    policy_gradient_loss | -0.00026     |\n",
      "|    reward               | 226.3183     |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "Episode: 105\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 134         |\n",
      "|    iterations           | 101         |\n",
      "|    time_elapsed         | 2735        |\n",
      "|    total_timesteps      | 369155      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003226403 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | -0.000471   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 238         |\n",
      "|    n_updates            | 6120        |\n",
      "|    policy_gradient_loss | 5.38e-06    |\n",
      "|    reward               | 132.8083    |\n",
      "|    std                  | 1.28        |\n",
      "|    value_loss           | 476         |\n",
      "-----------------------------------------\n",
      "Episode: 106\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 134          |\n",
      "|    iterations           | 102          |\n",
      "|    time_elapsed         | 2777         |\n",
      "|    total_timesteps      | 372810       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023397985 |\n",
      "|    clip_fraction        | 0.0613       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | -0.000206    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 79.7         |\n",
      "|    n_updates            | 6130         |\n",
      "|    policy_gradient_loss | -4.28e-05    |\n",
      "|    reward               | 168.44803    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 160          |\n",
      "------------------------------------------\n",
      "Episode: 107\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 133          |\n",
      "|    iterations           | 103          |\n",
      "|    time_elapsed         | 2816         |\n",
      "|    total_timesteps      | 376465       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.491609e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | 0.000321     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 129          |\n",
      "|    n_updates            | 6140         |\n",
      "|    policy_gradient_loss | -3.77e-05    |\n",
      "|    reward               | 195.04938    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 258          |\n",
      "------------------------------------------\n",
      "Episode: 108\n",
      "row: 3654, episode: 108\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3737527.25\n",
      "total_reward: 2737527.25\n",
      "total_cost: 1609521.05\n",
      "total_trades: 2981\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 133          |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 2845         |\n",
      "|    total_timesteps      | 380120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012320823 |\n",
      "|    clip_fraction        | 0.00774      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | -5.6e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 170          |\n",
      "|    n_updates            | 6150         |\n",
      "|    policy_gradient_loss | -0.000169    |\n",
      "|    reward               | 273.75272    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 339          |\n",
      "------------------------------------------\n",
      "Episode: 109\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 133           |\n",
      "|    iterations           | 105           |\n",
      "|    time_elapsed         | 2877          |\n",
      "|    total_timesteps      | 383775        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00039443077 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.67         |\n",
      "|    explained_variance   | 0.000568      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 358           |\n",
      "|    n_updates            | 6160          |\n",
      "|    policy_gradient_loss | 8.71e-05      |\n",
      "|    reward               | 189.34663     |\n",
      "|    std                  | 1.29          |\n",
      "|    value_loss           | 717           |\n",
      "-------------------------------------------\n",
      "Episode: 110\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 133          |\n",
      "|    iterations           | 106          |\n",
      "|    time_elapsed         | 2907         |\n",
      "|    total_timesteps      | 387430       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010131373 |\n",
      "|    clip_fraction        | 0.00345      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | -0.000127    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 172          |\n",
      "|    n_updates            | 6170         |\n",
      "|    policy_gradient_loss | -0.000176    |\n",
      "|    reward               | 175.56616    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 343          |\n",
      "------------------------------------------\n",
      "Episode: 111\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 133          |\n",
      "|    iterations           | 107          |\n",
      "|    time_elapsed         | 2936         |\n",
      "|    total_timesteps      | 391085       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012011634 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | 2.62e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 139          |\n",
      "|    n_updates            | 6180         |\n",
      "|    policy_gradient_loss | -0.000175    |\n",
      "|    reward               | 37.357178    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 278          |\n",
      "------------------------------------------\n",
      "Episode: 112\n",
      "row: 3654, episode: 112\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2838953.80\n",
      "total_reward: 1838953.80\n",
      "total_cost: 1277590.63\n",
      "total_trades: 3082\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 133          |\n",
      "|    iterations           | 108          |\n",
      "|    time_elapsed         | 2967         |\n",
      "|    total_timesteps      | 394740       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014564609 |\n",
      "|    clip_fraction        | 0.0177       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | -0.000874    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.83         |\n",
      "|    n_updates            | 6190         |\n",
      "|    policy_gradient_loss | -8.74e-05    |\n",
      "|    reward               | 183.89539    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 20.3         |\n",
      "------------------------------------------\n",
      "Episode: 113\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 2998         |\n",
      "|    total_timesteps      | 398395       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005371372 |\n",
      "|    clip_fraction        | 0.000328     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | -5.73e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 157          |\n",
      "|    n_updates            | 6200         |\n",
      "|    policy_gradient_loss | -0.000134    |\n",
      "|    reward               | 171.64738    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 314          |\n",
      "------------------------------------------\n",
      "Episode: 114\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 110          |\n",
      "|    time_elapsed         | 3031         |\n",
      "|    total_timesteps      | 402050       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.413347e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | 5.66e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 135          |\n",
      "|    n_updates            | 6210         |\n",
      "|    policy_gradient_loss | 2.67e-05     |\n",
      "|    reward               | 173.56874    |\n",
      "|    std                  | 1.29         |\n",
      "|    value_loss           | 270          |\n",
      "------------------------------------------\n",
      "Episode: 115\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 132          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 3061         |\n",
      "|    total_timesteps      | 405705       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009870065 |\n",
      "|    clip_fraction        | 0.0338       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.67        |\n",
      "|    explained_variance   | 9.82e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 134          |\n",
      "|    n_updates            | 6220         |\n",
      "|    policy_gradient_loss | -0.00021     |\n",
      "|    reward               | 242.61555    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 269          |\n",
      "------------------------------------------\n",
      "Episode: 116\n",
      "row: 3654, episode: 116\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2948606.71\n",
      "total_reward: 1948606.71\n",
      "total_cost: 1321844.37\n",
      "total_trades: 3171\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 112         |\n",
      "|    time_elapsed         | 3096        |\n",
      "|    total_timesteps      | 409360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001713351 |\n",
      "|    clip_fraction        | 0.0262      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.00017     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 263         |\n",
      "|    n_updates            | 6230        |\n",
      "|    policy_gradient_loss | 0.000107    |\n",
      "|    reward               | 194.86067   |\n",
      "|    std                  | 1.28        |\n",
      "|    value_loss           | 526         |\n",
      "-----------------------------------------\n",
      "Episode: 117\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 132         |\n",
      "|    iterations           | 113         |\n",
      "|    time_elapsed         | 3127        |\n",
      "|    total_timesteps      | 413015      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003060816 |\n",
      "|    clip_fraction        | 0.0456      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | -0.000293   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 173         |\n",
      "|    n_updates            | 6240        |\n",
      "|    policy_gradient_loss | -0.000487   |\n",
      "|    reward               | 290.31665   |\n",
      "|    std                  | 1.28        |\n",
      "|    value_loss           | 345         |\n",
      "-----------------------------------------\n",
      "Episode: 118\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 114           |\n",
      "|    time_elapsed         | 3157          |\n",
      "|    total_timesteps      | 416670        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00049282867 |\n",
      "|    clip_fraction        | 0.000356      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.67         |\n",
      "|    explained_variance   | -2.19e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 396           |\n",
      "|    n_updates            | 6250          |\n",
      "|    policy_gradient_loss | -6.88e-05     |\n",
      "|    reward               | 215.04192     |\n",
      "|    std                  | 1.29          |\n",
      "|    value_loss           | 793           |\n",
      "-------------------------------------------\n",
      "Episode: 119\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 115         |\n",
      "|    time_elapsed         | 3187        |\n",
      "|    total_timesteps      | 420325      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002968168 |\n",
      "|    clip_fraction        | 0.0415      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 4.17e-05    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 207         |\n",
      "|    n_updates            | 6260        |\n",
      "|    policy_gradient_loss | -0.000494   |\n",
      "|    reward               | 375.38187   |\n",
      "|    std                  | 1.29        |\n",
      "|    value_loss           | 414         |\n",
      "-----------------------------------------\n",
      "Episode: 120\n",
      "row: 3654, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2590802.79\n",
      "total_reward: 1590802.79\n",
      "total_cost: 1242917.29\n",
      "total_trades: 3015\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 116           |\n",
      "|    time_elapsed         | 3216          |\n",
      "|    total_timesteps      | 423980        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016672171 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.68         |\n",
      "|    explained_variance   | 7.82e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 630           |\n",
      "|    n_updates            | 6270          |\n",
      "|    policy_gradient_loss | 0.0001        |\n",
      "|    reward               | 159.08028     |\n",
      "|    std                  | 1.3           |\n",
      "|    value_loss           | 1.26e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 121\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 117           |\n",
      "|    time_elapsed         | 3244          |\n",
      "|    total_timesteps      | 427635        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023212293 |\n",
      "|    clip_fraction        | 5.47e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.68         |\n",
      "|    explained_variance   | -0.000445     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 110           |\n",
      "|    n_updates            | 6280          |\n",
      "|    policy_gradient_loss | -0.000218     |\n",
      "|    reward               | 107.45922     |\n",
      "|    std                  | 1.3           |\n",
      "|    value_loss           | 219           |\n",
      "-------------------------------------------\n",
      "Episode: 122\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 131            |\n",
      "|    iterations           | 118            |\n",
      "|    time_elapsed         | 3273           |\n",
      "|    total_timesteps      | 431290         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000107658285 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.68          |\n",
      "|    explained_variance   | -0.000459      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 54             |\n",
      "|    n_updates            | 6290           |\n",
      "|    policy_gradient_loss | 0.00014        |\n",
      "|    reward               | 112.76829      |\n",
      "|    std                  | 1.3            |\n",
      "|    value_loss           | 109            |\n",
      "--------------------------------------------\n",
      "Episode: 123\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 119           |\n",
      "|    time_elapsed         | 3302          |\n",
      "|    total_timesteps      | 434945        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.0931735e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.68         |\n",
      "|    explained_variance   | -3.93e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 54.5          |\n",
      "|    n_updates            | 6300          |\n",
      "|    policy_gradient_loss | -7.33e-06     |\n",
      "|    reward               | 92.62548      |\n",
      "|    std                  | 1.3           |\n",
      "|    value_loss           | 110           |\n",
      "-------------------------------------------\n",
      "Episode: 124\n",
      "row: 3654, episode: 124\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2486222.93\n",
      "total_reward: 1486222.93\n",
      "total_cost: 1030353.25\n",
      "total_trades: 2983\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 120           |\n",
      "|    time_elapsed         | 3331          |\n",
      "|    total_timesteps      | 438600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3315725e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.68         |\n",
      "|    explained_variance   | 0.000112      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 38.6          |\n",
      "|    n_updates            | 6310          |\n",
      "|    policy_gradient_loss | -4.7e-05      |\n",
      "|    reward               | 148.6223      |\n",
      "|    std                  | 1.3           |\n",
      "|    value_loss           | 77.9          |\n",
      "-------------------------------------------\n",
      "Episode: 125\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 3359         |\n",
      "|    total_timesteps      | 442255       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006644858 |\n",
      "|    clip_fraction        | 0.000739     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.69        |\n",
      "|    explained_variance   | -0.000103    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 100          |\n",
      "|    n_updates            | 6320         |\n",
      "|    policy_gradient_loss | -0.000202    |\n",
      "|    reward               | 175.38506    |\n",
      "|    std                  | 1.31         |\n",
      "|    value_loss           | 201          |\n",
      "------------------------------------------\n",
      "Episode: 126\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 3388         |\n",
      "|    total_timesteps      | 445910       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032401092 |\n",
      "|    clip_fraction        | 0.0444       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.69        |\n",
      "|    explained_variance   | -0.000224    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 141          |\n",
      "|    n_updates            | 6330         |\n",
      "|    policy_gradient_loss | -0.000508    |\n",
      "|    reward               | 208.42055    |\n",
      "|    std                  | 1.31         |\n",
      "|    value_loss           | 282          |\n",
      "------------------------------------------\n",
      "Episode: 127\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 3418         |\n",
      "|    total_timesteps      | 449565       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013627878 |\n",
      "|    clip_fraction        | 0.0135       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.69        |\n",
      "|    explained_variance   | 1.35e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 200          |\n",
      "|    n_updates            | 6340         |\n",
      "|    policy_gradient_loss | -0.00032     |\n",
      "|    reward               | 143.73415    |\n",
      "|    std                  | 1.31         |\n",
      "|    value_loss           | 401          |\n",
      "------------------------------------------\n",
      "Episode: 128\n",
      "row: 3654, episode: 128\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3142962.00\n",
      "total_reward: 2142962.00\n",
      "total_cost: 1527629.13\n",
      "total_trades: 3050\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 124           |\n",
      "|    time_elapsed         | 3447          |\n",
      "|    total_timesteps      | 453220        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1505409e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.69         |\n",
      "|    explained_variance   | -0.000391     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 91.3          |\n",
      "|    n_updates            | 6350          |\n",
      "|    policy_gradient_loss | 5.52e-05      |\n",
      "|    reward               | 214.2962      |\n",
      "|    std                  | 1.31          |\n",
      "|    value_loss           | 183           |\n",
      "-------------------------------------------\n",
      "Episode: 129\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 125           |\n",
      "|    time_elapsed         | 3475          |\n",
      "|    total_timesteps      | 456875        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032816862 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.69         |\n",
      "|    explained_variance   | 0.000153      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 206           |\n",
      "|    n_updates            | 6360          |\n",
      "|    policy_gradient_loss | -0.000155     |\n",
      "|    reward               | 194.01111     |\n",
      "|    std                  | 1.31          |\n",
      "|    value_loss           | 412           |\n",
      "-------------------------------------------\n",
      "Episode: 130\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 126          |\n",
      "|    time_elapsed         | 3504         |\n",
      "|    total_timesteps      | 460530       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010935869 |\n",
      "|    clip_fraction        | 0.00569      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.69        |\n",
      "|    explained_variance   | 0.000122     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 176          |\n",
      "|    n_updates            | 6370         |\n",
      "|    policy_gradient_loss | -0.000314    |\n",
      "|    reward               | 173.7132     |\n",
      "|    std                  | 1.32         |\n",
      "|    value_loss           | 352          |\n",
      "------------------------------------------\n",
      "Episode: 131\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 127          |\n",
      "|    time_elapsed         | 3533         |\n",
      "|    total_timesteps      | 464185       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010743304 |\n",
      "|    clip_fraction        | 0.00646      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.7         |\n",
      "|    explained_variance   | 8.43e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 138          |\n",
      "|    n_updates            | 6380         |\n",
      "|    policy_gradient_loss | -0.000143    |\n",
      "|    reward               | 268.034      |\n",
      "|    std                  | 1.32         |\n",
      "|    value_loss           | 277          |\n",
      "------------------------------------------\n",
      "Episode: 132\n",
      "row: 3654, episode: 132\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2735924.89\n",
      "total_reward: 1735924.89\n",
      "total_cost: 932094.64\n",
      "total_trades: 2893\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 3562          |\n",
      "|    total_timesteps      | 467840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026400713 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.7          |\n",
      "|    explained_variance   | 0.000145      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 336           |\n",
      "|    n_updates            | 6390          |\n",
      "|    policy_gradient_loss | 9.95e-05      |\n",
      "|    reward               | 173.59248     |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 672           |\n",
      "-------------------------------------------\n",
      "Episode: 133\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 129          |\n",
      "|    time_elapsed         | 3591         |\n",
      "|    total_timesteps      | 471495       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015516488 |\n",
      "|    clip_fraction        | 0.0123       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.7         |\n",
      "|    explained_variance   | 4.39e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 138          |\n",
      "|    n_updates            | 6400         |\n",
      "|    policy_gradient_loss | -0.000388    |\n",
      "|    reward               | 72.34094     |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 276          |\n",
      "------------------------------------------\n",
      "Episode: 134\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 3620          |\n",
      "|    total_timesteps      | 475150        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028128727 |\n",
      "|    clip_fraction        | 8.21e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.7          |\n",
      "|    explained_variance   | -0.000611     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 25            |\n",
      "|    n_updates            | 6410          |\n",
      "|    policy_gradient_loss | 0.000499      |\n",
      "|    reward               | 315.18204     |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 50.7          |\n",
      "-------------------------------------------\n",
      "Episode: 135\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 131          |\n",
      "|    time_elapsed         | 3650         |\n",
      "|    total_timesteps      | 478805       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.814706e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.7         |\n",
      "|    explained_variance   | 8.94e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 452          |\n",
      "|    n_updates            | 6420         |\n",
      "|    policy_gradient_loss | 5.82e-06     |\n",
      "|    reward               | 251.31392    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 905          |\n",
      "------------------------------------------\n",
      "Episode: 136\n",
      "row: 3654, episode: 136\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2984955.61\n",
      "total_reward: 1984955.61\n",
      "total_cost: 1322906.44\n",
      "total_trades: 2901\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 132           |\n",
      "|    time_elapsed         | 3678          |\n",
      "|    total_timesteps      | 482460        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00024121096 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.7          |\n",
      "|    explained_variance   | 0.000135      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 283           |\n",
      "|    n_updates            | 6430          |\n",
      "|    policy_gradient_loss | -0.000197     |\n",
      "|    reward               | 198.49556     |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 566           |\n",
      "-------------------------------------------\n",
      "Episode: 137\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 131         |\n",
      "|    iterations           | 133         |\n",
      "|    time_elapsed         | 3707        |\n",
      "|    total_timesteps      | 486115      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.16028e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.7        |\n",
      "|    explained_variance   | 0.000315    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 184         |\n",
      "|    n_updates            | 6440        |\n",
      "|    policy_gradient_loss | 2.79e-05    |\n",
      "|    reward               | 239.0343    |\n",
      "|    std                  | 1.33        |\n",
      "|    value_loss           | 367         |\n",
      "-----------------------------------------\n",
      "Episode: 138\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 134          |\n",
      "|    time_elapsed         | 3735         |\n",
      "|    total_timesteps      | 489770       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003657906 |\n",
      "|    clip_fraction        | 5.47e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.7         |\n",
      "|    explained_variance   | -0.000183    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 266          |\n",
      "|    n_updates            | 6450         |\n",
      "|    policy_gradient_loss | -0.000102    |\n",
      "|    reward               | 111.81254    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 533          |\n",
      "------------------------------------------\n",
      "Episode: 139\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 135           |\n",
      "|    time_elapsed         | 3763          |\n",
      "|    total_timesteps      | 493425        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020061668 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.000478     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 55.6          |\n",
      "|    n_updates            | 6460          |\n",
      "|    policy_gradient_loss | -0.00013      |\n",
      "|    reward               | 288.36395     |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 111           |\n",
      "-------------------------------------------\n",
      "Episode: 140\n",
      "row: 3654, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3832014.48\n",
      "total_reward: 2832014.48\n",
      "total_cost: 1559691.63\n",
      "total_trades: 2988\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 131          |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 3792         |\n",
      "|    total_timesteps      | 497080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.254614e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.7         |\n",
      "|    explained_variance   | 0.000109     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 371          |\n",
      "|    n_updates            | 6470         |\n",
      "|    policy_gradient_loss | 3.97e-05     |\n",
      "|    reward               | 283.20145    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 743          |\n",
      "------------------------------------------\n",
      "Episode: 141\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 137           |\n",
      "|    time_elapsed         | 3821          |\n",
      "|    total_timesteps      | 500735        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7695314e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.7          |\n",
      "|    explained_variance   | 0.00026       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 369           |\n",
      "|    n_updates            | 6480          |\n",
      "|    policy_gradient_loss | -5.54e-05     |\n",
      "|    reward               | 142.88936     |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 738           |\n",
      "-------------------------------------------\n",
      "Episode: 142\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 131           |\n",
      "|    iterations           | 138           |\n",
      "|    time_elapsed         | 3850          |\n",
      "|    total_timesteps      | 504390        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00055764196 |\n",
      "|    clip_fraction        | 0.00041       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.7          |\n",
      "|    explained_variance   | -0.000541     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 90.9          |\n",
      "|    n_updates            | 6490          |\n",
      "|    policy_gradient_loss | -0.000406     |\n",
      "|    reward               | 235.2774      |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 182           |\n",
      "-------------------------------------------\n",
      "Episode: 143\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 130            |\n",
      "|    iterations           | 139            |\n",
      "|    time_elapsed         | 3879           |\n",
      "|    total_timesteps      | 508045         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000103254664 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.71          |\n",
      "|    explained_variance   | -3.74e-05      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 247            |\n",
      "|    n_updates            | 6500           |\n",
      "|    policy_gradient_loss | 7.92e-05       |\n",
      "|    reward               | 205.44427      |\n",
      "|    std                  | 1.33           |\n",
      "|    value_loss           | 494            |\n",
      "--------------------------------------------\n",
      "Episode: 144\n",
      "row: 3654, episode: 144\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1420189.27\n",
      "total_reward: 420189.27\n",
      "total_cost: 837058.36\n",
      "total_trades: 2928\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 3907         |\n",
      "|    total_timesteps      | 511700       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014514023 |\n",
      "|    clip_fraction        | 0.00906      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 7.14e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 198          |\n",
      "|    n_updates            | 6510         |\n",
      "|    policy_gradient_loss | -0.000249    |\n",
      "|    reward               | 42.018925    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 396          |\n",
      "------------------------------------------\n",
      "Episode: 145\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 141           |\n",
      "|    time_elapsed         | 3934          |\n",
      "|    total_timesteps      | 515355        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021484852 |\n",
      "|    clip_fraction        | 8.21e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.00963      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 10.3          |\n",
      "|    n_updates            | 6520          |\n",
      "|    policy_gradient_loss | 0.000548      |\n",
      "|    reward               | 89.351524     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 21.1          |\n",
      "-------------------------------------------\n",
      "Episode: 146\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 142           |\n",
      "|    time_elapsed         | 3963          |\n",
      "|    total_timesteps      | 519010        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4265536e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.00172      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 45.4          |\n",
      "|    n_updates            | 6530          |\n",
      "|    policy_gradient_loss | -1.04e-05     |\n",
      "|    reward               | 105.64497     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 91.5          |\n",
      "-------------------------------------------\n",
      "Episode: 147\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 143         |\n",
      "|    time_elapsed         | 3992        |\n",
      "|    total_timesteps      | 522665      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.84465e-05 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 3.8e-05     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 49.6        |\n",
      "|    n_updates            | 6540        |\n",
      "|    policy_gradient_loss | -6.64e-05   |\n",
      "|    reward               | 223.07506   |\n",
      "|    std                  | 1.34        |\n",
      "|    value_loss           | 99.8        |\n",
      "-----------------------------------------\n",
      "Episode: 148\n",
      "row: 3654, episode: 148\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2231166.74\n",
      "total_reward: 1231166.74\n",
      "total_cost: 957203.91\n",
      "total_trades: 2959\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 144           |\n",
      "|    time_elapsed         | 4022          |\n",
      "|    total_timesteps      | 526320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1587518e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 3.03e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 226           |\n",
      "|    n_updates            | 6550          |\n",
      "|    policy_gradient_loss | 1.1e-05       |\n",
      "|    reward               | 123.11668     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 452           |\n",
      "-------------------------------------------\n",
      "Episode: 149\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 145          |\n",
      "|    time_elapsed         | 4051         |\n",
      "|    total_timesteps      | 529975       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.643428e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 9.55e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 67.3         |\n",
      "|    n_updates            | 6560         |\n",
      "|    policy_gradient_loss | -3.23e-05    |\n",
      "|    reward               | 232.94643    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 135          |\n",
      "------------------------------------------\n",
      "Episode: 150\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 146           |\n",
      "|    time_elapsed         | 4080          |\n",
      "|    total_timesteps      | 533630        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8998056e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.000133      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 242           |\n",
      "|    n_updates            | 6570          |\n",
      "|    policy_gradient_loss | -3.77e-05     |\n",
      "|    reward               | 91.19329      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 484           |\n",
      "-------------------------------------------\n",
      "Episode: 151\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 147           |\n",
      "|    time_elapsed         | 4109          |\n",
      "|    total_timesteps      | 537285        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7215082e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.000379     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 41.7          |\n",
      "|    n_updates            | 6580          |\n",
      "|    policy_gradient_loss | 5.69e-05      |\n",
      "|    reward               | 228.2443      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 83.5          |\n",
      "-------------------------------------------\n",
      "Episode: 152\n",
      "row: 3654, episode: 152\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2938476.49\n",
      "total_reward: 1938476.49\n",
      "total_cost: 1250774.43\n",
      "total_trades: 2925\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 148           |\n",
      "|    time_elapsed         | 4138          |\n",
      "|    total_timesteps      | 540940        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2147438e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.000127      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 238           |\n",
      "|    n_updates            | 6590          |\n",
      "|    policy_gradient_loss | -3.27e-05     |\n",
      "|    reward               | 193.84766     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 476           |\n",
      "-------------------------------------------\n",
      "Episode: 153\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 149          |\n",
      "|    time_elapsed         | 4166         |\n",
      "|    total_timesteps      | 544595       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014764529 |\n",
      "|    clip_fraction        | 0.0101       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.000123     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 171          |\n",
      "|    n_updates            | 6600         |\n",
      "|    policy_gradient_loss | -0.000382    |\n",
      "|    reward               | 421.11084    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 342          |\n",
      "------------------------------------------\n",
      "Episode: 154\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 150         |\n",
      "|    time_elapsed         | 4193        |\n",
      "|    total_timesteps      | 548250      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001533279 |\n",
      "|    clip_fraction        | 0.0176      |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 8.86e-05    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 796         |\n",
      "|    n_updates            | 6610        |\n",
      "|    policy_gradient_loss | -0.00029    |\n",
      "|    reward               | 247.04355   |\n",
      "|    std                  | 1.34        |\n",
      "|    value_loss           | 1.59e+03    |\n",
      "-----------------------------------------\n",
      "Episode: 155\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 151           |\n",
      "|    time_elapsed         | 4221          |\n",
      "|    total_timesteps      | 551905        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00085635984 |\n",
      "|    clip_fraction        | 0.00274       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.00016       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 272           |\n",
      "|    n_updates            | 6620          |\n",
      "|    policy_gradient_loss | -0.000456     |\n",
      "|    reward               | 208.2955      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 544           |\n",
      "-------------------------------------------\n",
      "Episode: 156\n",
      "row: 3654, episode: 156\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2097490.73\n",
      "total_reward: 1097490.73\n",
      "total_cost: 833190.97\n",
      "total_trades: 2914\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 152           |\n",
      "|    time_elapsed         | 4248          |\n",
      "|    total_timesteps      | 555560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8079342e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.000103      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 193           |\n",
      "|    n_updates            | 6630          |\n",
      "|    policy_gradient_loss | 3.06e-06      |\n",
      "|    reward               | 109.74907     |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 386           |\n",
      "-------------------------------------------\n",
      "Episode: 157\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 153           |\n",
      "|    time_elapsed         | 4276          |\n",
      "|    total_timesteps      | 559215        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00048407243 |\n",
      "|    clip_fraction        | 0.000356      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -1.78e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 52.5          |\n",
      "|    n_updates            | 6640          |\n",
      "|    policy_gradient_loss | -0.000182     |\n",
      "|    reward               | 211.17665     |\n",
      "|    std                  | 1.33          |\n",
      "|    value_loss           | 105           |\n",
      "-------------------------------------------\n",
      "Episode: 158\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 130         |\n",
      "|    iterations           | 154         |\n",
      "|    time_elapsed         | 4306        |\n",
      "|    total_timesteps      | 562870      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000674014 |\n",
      "|    clip_fraction        | 0.00126     |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.000166    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 208         |\n",
      "|    n_updates            | 6650        |\n",
      "|    policy_gradient_loss | -0.00025    |\n",
      "|    reward               | 206.8549    |\n",
      "|    std                  | 1.34        |\n",
      "|    value_loss           | 416         |\n",
      "-----------------------------------------\n",
      "Episode: 159\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 155          |\n",
      "|    time_elapsed         | 4334         |\n",
      "|    total_timesteps      | 566525       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039983783 |\n",
      "|    clip_fraction        | 0.0934       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.00014      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 194          |\n",
      "|    n_updates            | 6660         |\n",
      "|    policy_gradient_loss | -0.00123     |\n",
      "|    reward               | 257.65778    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 387          |\n",
      "------------------------------------------\n",
      "Episode: 160\n",
      "row: 3654, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3534204.58\n",
      "total_reward: 2534204.58\n",
      "total_cost: 1158438.53\n",
      "total_trades: 2969\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 156          |\n",
      "|    time_elapsed         | 4362         |\n",
      "|    total_timesteps      | 570180       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.865341e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.000138     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 301          |\n",
      "|    n_updates            | 6670         |\n",
      "|    policy_gradient_loss | 0.00013      |\n",
      "|    reward               | 253.42046    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 602          |\n",
      "------------------------------------------\n",
      "Episode: 161\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 157           |\n",
      "|    time_elapsed         | 4391          |\n",
      "|    total_timesteps      | 573835        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6571396e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.000147      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 293           |\n",
      "|    n_updates            | 6680          |\n",
      "|    policy_gradient_loss | 3.95e-06      |\n",
      "|    reward               | 87.66817      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 585           |\n",
      "-------------------------------------------\n",
      "Episode: 162\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 158          |\n",
      "|    time_elapsed         | 4417         |\n",
      "|    total_timesteps      | 577490       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.204485e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -0.00159     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 37.6         |\n",
      "|    n_updates            | 6690         |\n",
      "|    policy_gradient_loss | -1.72e-05    |\n",
      "|    reward               | 194.16331    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 75.3         |\n",
      "------------------------------------------\n",
      "Episode: 163\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 159          |\n",
      "|    time_elapsed         | 4446         |\n",
      "|    total_timesteps      | 581145       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009870478 |\n",
      "|    clip_fraction        | 0.00274      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 1.73e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 168          |\n",
      "|    n_updates            | 6700         |\n",
      "|    policy_gradient_loss | -0.0004      |\n",
      "|    reward               | 90.998215    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 336          |\n",
      "------------------------------------------\n",
      "Episode: 164\n",
      "row: 3654, episode: 164\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3041713.52\n",
      "total_reward: 2041713.52\n",
      "total_cost: 1370250.00\n",
      "total_trades: 3031\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 160          |\n",
      "|    time_elapsed         | 4474         |\n",
      "|    total_timesteps      | 584800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038904557 |\n",
      "|    clip_fraction        | 0.0974       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -0.000477    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 36.1         |\n",
      "|    n_updates            | 6710         |\n",
      "|    policy_gradient_loss | -0.000188    |\n",
      "|    reward               | 204.17136    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 72.6         |\n",
      "------------------------------------------\n",
      "Episode: 165\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 161          |\n",
      "|    time_elapsed         | 4505         |\n",
      "|    total_timesteps      | 588455       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.591467e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.000166     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 187          |\n",
      "|    n_updates            | 6720         |\n",
      "|    policy_gradient_loss | 7.91e-06     |\n",
      "|    reward               | 168.94992    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 374          |\n",
      "------------------------------------------\n",
      "Episode: 166\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 4538         |\n",
      "|    total_timesteps      | 592110       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.852761e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.000292     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 129          |\n",
      "|    n_updates            | 6730         |\n",
      "|    policy_gradient_loss | 3.22e-05     |\n",
      "|    reward               | 238.91875    |\n",
      "|    std                  | 1.33         |\n",
      "|    value_loss           | 257          |\n",
      "------------------------------------------\n",
      "Episode: 167\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 163          |\n",
      "|    time_elapsed         | 4567         |\n",
      "|    total_timesteps      | 595765       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.639034e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -0.000187    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 260          |\n",
      "|    n_updates            | 6740         |\n",
      "|    policy_gradient_loss | 9.55e-05     |\n",
      "|    reward               | 186.88728    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 520          |\n",
      "------------------------------------------\n",
      "Episode: 168\n",
      "row: 3654, episode: 168\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2091417.67\n",
      "total_reward: 1091417.67\n",
      "total_cost: 1048471.36\n",
      "total_trades: 3121\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 164          |\n",
      "|    time_elapsed         | 4598         |\n",
      "|    total_timesteps      | 599420       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031513814 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -5.35e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 156          |\n",
      "|    n_updates            | 6750         |\n",
      "|    policy_gradient_loss | -0.000967    |\n",
      "|    reward               | 109.14177    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 312          |\n",
      "------------------------------------------\n",
      "Episode: 169\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 165          |\n",
      "|    time_elapsed         | 4628         |\n",
      "|    total_timesteps      | 603075       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.267777e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.000351     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 53.1         |\n",
      "|    n_updates            | 6760         |\n",
      "|    policy_gradient_loss | 0.000305     |\n",
      "|    reward               | 30.611374    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "Episode: 170\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 166           |\n",
      "|    time_elapsed         | 4659          |\n",
      "|    total_timesteps      | 606730        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6551697e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.0028       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.75          |\n",
      "|    n_updates            | 6770          |\n",
      "|    policy_gradient_loss | -3.17e-05     |\n",
      "|    reward               | 262.9178      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 14.4          |\n",
      "-------------------------------------------\n",
      "Episode: 171\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 130          |\n",
      "|    iterations           | 167          |\n",
      "|    time_elapsed         | 4688         |\n",
      "|    total_timesteps      | 610385       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.712078e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -0.000108    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 336          |\n",
      "|    n_updates            | 6780         |\n",
      "|    policy_gradient_loss | -0.000105    |\n",
      "|    reward               | 70.546326    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 672          |\n",
      "------------------------------------------\n",
      "Episode: 172\n",
      "row: 3654, episode: 172\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2012872.61\n",
      "total_reward: 1012872.61\n",
      "total_cost: 1136052.49\n",
      "total_trades: 3126\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 130           |\n",
      "|    iterations           | 168           |\n",
      "|    time_elapsed         | 4722          |\n",
      "|    total_timesteps      | 614040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00019657018 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.000185      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 24.3          |\n",
      "|    n_updates            | 6790          |\n",
      "|    policy_gradient_loss | -0.000266     |\n",
      "|    reward               | 101.28726     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 48.9          |\n",
      "-------------------------------------------\n",
      "Episode: 173\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 4756         |\n",
      "|    total_timesteps      | 617695       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.596403e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.000142     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 47.3         |\n",
      "|    n_updates            | 6800         |\n",
      "|    policy_gradient_loss | 1.37e-05     |\n",
      "|    reward               | 95.67575     |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 95           |\n",
      "------------------------------------------\n",
      "Episode: 174\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 170          |\n",
      "|    time_elapsed         | 4787         |\n",
      "|    total_timesteps      | 621350       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0010020459 |\n",
      "|    clip_fraction        | 0.00304      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 4.43e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 43.7         |\n",
      "|    n_updates            | 6810         |\n",
      "|    policy_gradient_loss | -0.000402    |\n",
      "|    reward               | 250.23758    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 87.4         |\n",
      "------------------------------------------\n",
      "Episode: 175\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 171          |\n",
      "|    time_elapsed         | 4817         |\n",
      "|    total_timesteps      | 625005       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015504888 |\n",
      "|    clip_fraction        | 0.0182       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -0.000463    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 290          |\n",
      "|    n_updates            | 6820         |\n",
      "|    policy_gradient_loss | -0.000469    |\n",
      "|    reward               | 193.70638    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 580          |\n",
      "------------------------------------------\n",
      "Episode: 176\n",
      "row: 3654, episode: 176\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2651519.53\n",
      "total_reward: 1651519.53\n",
      "total_cost: 1334121.21\n",
      "total_trades: 3291\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 172          |\n",
      "|    time_elapsed         | 4847         |\n",
      "|    total_timesteps      | 628660       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065080132 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 6.12e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 178          |\n",
      "|    n_updates            | 6830         |\n",
      "|    policy_gradient_loss | -0.000879    |\n",
      "|    reward               | 165.15195    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 356          |\n",
      "------------------------------------------\n",
      "Episode: 177\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 4876         |\n",
      "|    total_timesteps      | 632315       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021708058 |\n",
      "|    clip_fraction        | 0.0286       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 8.02e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 122          |\n",
      "|    n_updates            | 6840         |\n",
      "|    policy_gradient_loss | -0.00029     |\n",
      "|    reward               | 70.63536     |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 243          |\n",
      "------------------------------------------\n",
      "Episode: 178\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 129           |\n",
      "|    iterations           | 174           |\n",
      "|    time_elapsed         | 4906          |\n",
      "|    total_timesteps      | 635970        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9846236e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.000358     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 24.2          |\n",
      "|    n_updates            | 6850          |\n",
      "|    policy_gradient_loss | 1.68e-06      |\n",
      "|    reward               | 181.47784     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 48.8          |\n",
      "-------------------------------------------\n",
      "Episode: 179\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 129         |\n",
      "|    iterations           | 175         |\n",
      "|    time_elapsed         | 4936        |\n",
      "|    total_timesteps      | 639625      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.000305525 |\n",
      "|    clip_fraction        | 5.47e-05    |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 6.6e-05     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 153         |\n",
      "|    n_updates            | 6860        |\n",
      "|    policy_gradient_loss | -0.00015    |\n",
      "|    reward               | 127.73086   |\n",
      "|    std                  | 1.34        |\n",
      "|    value_loss           | 306         |\n",
      "-----------------------------------------\n",
      "Episode: 180\n",
      "row: 3654, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1794368.71\n",
      "total_reward: 794368.71\n",
      "total_cost: 1194224.52\n",
      "total_trades: 3188\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 129           |\n",
      "|    iterations           | 176           |\n",
      "|    time_elapsed         | 4965          |\n",
      "|    total_timesteps      | 643280        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7330017e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | 0.000196      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 74.7          |\n",
      "|    n_updates            | 6870          |\n",
      "|    policy_gradient_loss | -4.29e-05     |\n",
      "|    reward               | 79.436874     |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 149           |\n",
      "-------------------------------------------\n",
      "Episode: 181\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 4995         |\n",
      "|    total_timesteps      | 646935       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0005477491 |\n",
      "|    clip_fraction        | 0.000821     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | 0.000352     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 32           |\n",
      "|    n_updates            | 6880         |\n",
      "|    policy_gradient_loss | -0.000122    |\n",
      "|    reward               | 235.71019    |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 64.5         |\n",
      "------------------------------------------\n",
      "Episode: 182\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 178          |\n",
      "|    time_elapsed         | 5025         |\n",
      "|    total_timesteps      | 650590       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.449531e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | -0.000337    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 253          |\n",
      "|    n_updates            | 6890         |\n",
      "|    policy_gradient_loss | 5.03e-05     |\n",
      "|    reward               | 209.72002    |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 506          |\n",
      "------------------------------------------\n",
      "Episode: 183\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 179          |\n",
      "|    time_elapsed         | 5055         |\n",
      "|    total_timesteps      | 654245       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003127759 |\n",
      "|    clip_fraction        | 2.74e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | -6.53e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 203          |\n",
      "|    n_updates            | 6900         |\n",
      "|    policy_gradient_loss | -0.000196    |\n",
      "|    reward               | 64.38998     |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 407          |\n",
      "------------------------------------------\n",
      "Episode: 184\n",
      "row: 3654, episode: 184\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2030130.02\n",
      "total_reward: 1030130.02\n",
      "total_cost: 951004.46\n",
      "total_trades: 3266\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 129          |\n",
      "|    iterations           | 180          |\n",
      "|    time_elapsed         | 5090         |\n",
      "|    total_timesteps      | 657900       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0007130216 |\n",
      "|    clip_fraction        | 0.0012       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | -0.000188    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 19.6         |\n",
      "|    n_updates            | 6910         |\n",
      "|    policy_gradient_loss | 0.000204     |\n",
      "|    reward               | 103.013      |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 39.3         |\n",
      "------------------------------------------\n",
      "Episode: 185\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 129           |\n",
      "|    iterations           | 181           |\n",
      "|    time_elapsed         | 5125          |\n",
      "|    total_timesteps      | 661555        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021646626 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | 0.000115      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 47.6          |\n",
      "|    n_updates            | 6920          |\n",
      "|    policy_gradient_loss | -0.000158     |\n",
      "|    reward               | 114.62774     |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 95.4          |\n",
      "-------------------------------------------\n",
      "Episode: 186\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 182           |\n",
      "|    time_elapsed         | 5159          |\n",
      "|    total_timesteps      | 665210        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012530082 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | -0.000142     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 62.2          |\n",
      "|    n_updates            | 6930          |\n",
      "|    policy_gradient_loss | -1.09e-05     |\n",
      "|    reward               | 101.14263     |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 124           |\n",
      "-------------------------------------------\n",
      "Episode: 187\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 5187         |\n",
      "|    total_timesteps      | 668865       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.971127e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | -0.000412    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 46.8         |\n",
      "|    n_updates            | 6940         |\n",
      "|    policy_gradient_loss | -4.35e-05    |\n",
      "|    reward               | 103.3466     |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 93.6         |\n",
      "------------------------------------------\n",
      "Episode: 188\n",
      "row: 3654, episode: 188\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4010777.97\n",
      "total_reward: 3010777.97\n",
      "total_cost: 1542980.45\n",
      "total_trades: 3131\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 184          |\n",
      "|    time_elapsed         | 5216         |\n",
      "|    total_timesteps      | 672520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003906617 |\n",
      "|    clip_fraction        | 5.47e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -8.46e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 48.8         |\n",
      "|    n_updates            | 6950         |\n",
      "|    policy_gradient_loss | -0.000128    |\n",
      "|    reward               | 301.0778     |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 97.7         |\n",
      "------------------------------------------\n",
      "Episode: 189\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 185           |\n",
      "|    time_elapsed         | 5244          |\n",
      "|    total_timesteps      | 676175        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00032401516 |\n",
      "|    clip_fraction        | 5.47e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.000235     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 412           |\n",
      "|    n_updates            | 6960          |\n",
      "|    policy_gradient_loss | -0.000131     |\n",
      "|    reward               | 263.9438      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 825           |\n",
      "-------------------------------------------\n",
      "Episode: 190\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 186           |\n",
      "|    time_elapsed         | 5274          |\n",
      "|    total_timesteps      | 679830        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1758044e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 6.5e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 315           |\n",
      "|    n_updates            | 6970          |\n",
      "|    policy_gradient_loss | 2.59e-05      |\n",
      "|    reward               | 162.80371     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 630           |\n",
      "-------------------------------------------\n",
      "Episode: 191\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 187          |\n",
      "|    time_elapsed         | 5303         |\n",
      "|    total_timesteps      | 683485       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032530432 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -0.000144    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 120          |\n",
      "|    n_updates            | 6980         |\n",
      "|    policy_gradient_loss | -0.000655    |\n",
      "|    reward               | 255.06898    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 241          |\n",
      "------------------------------------------\n",
      "Episode: 192\n",
      "row: 3654, episode: 192\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2362726.49\n",
      "total_reward: 1362726.49\n",
      "total_cost: 1146689.94\n",
      "total_trades: 3086\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 188          |\n",
      "|    time_elapsed         | 5333         |\n",
      "|    total_timesteps      | 687140       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0004324212 |\n",
      "|    clip_fraction        | 8.21e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 4.86e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 295          |\n",
      "|    n_updates            | 6990         |\n",
      "|    policy_gradient_loss | -0.000377    |\n",
      "|    reward               | 136.27264    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 591          |\n",
      "------------------------------------------\n",
      "Episode: 193\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 189          |\n",
      "|    time_elapsed         | 5363         |\n",
      "|    total_timesteps      | 690795       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0011853417 |\n",
      "|    clip_fraction        | 0.00648      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | 0.000284     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 84.5         |\n",
      "|    n_updates            | 7000         |\n",
      "|    policy_gradient_loss | -0.000521    |\n",
      "|    reward               | 69.247574    |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 169          |\n",
      "------------------------------------------\n",
      "Episode: 194\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 190           |\n",
      "|    time_elapsed         | 5393          |\n",
      "|    total_timesteps      | 694450        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00037175967 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.000466     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 24            |\n",
      "|    n_updates            | 7010          |\n",
      "|    policy_gradient_loss | 0.000246      |\n",
      "|    reward               | 49.096424     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 48.5          |\n",
      "-------------------------------------------\n",
      "Episode: 195\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 191           |\n",
      "|    time_elapsed         | 5423          |\n",
      "|    total_timesteps      | 698105        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00044574816 |\n",
      "|    clip_fraction        | 0.000246      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 2.22e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 12.3          |\n",
      "|    n_updates            | 7020          |\n",
      "|    policy_gradient_loss | -0.000586     |\n",
      "|    reward               | 138.7001      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 25.2          |\n",
      "-------------------------------------------\n",
      "Episode: 196\n",
      "row: 3654, episode: 196\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1404721.56\n",
      "total_reward: 404721.56\n",
      "total_cost: 964183.01\n",
      "total_trades: 3236\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 192           |\n",
      "|    time_elapsed         | 5453          |\n",
      "|    total_timesteps      | 701760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016886137 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.000113      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 97.3          |\n",
      "|    n_updates            | 7030          |\n",
      "|    policy_gradient_loss | 4.87e-05      |\n",
      "|    reward               | 40.472157     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 195           |\n",
      "-------------------------------------------\n",
      "Episode: 197\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 193           |\n",
      "|    time_elapsed         | 5485          |\n",
      "|    total_timesteps      | 705415        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00020262296 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | -0.000722     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.49          |\n",
      "|    n_updates            | 7040          |\n",
      "|    policy_gradient_loss | -0.000297     |\n",
      "|    reward               | 230.64426     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 19.3          |\n",
      "-------------------------------------------\n",
      "Episode: 198\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 194           |\n",
      "|    time_elapsed         | 5513          |\n",
      "|    total_timesteps      | 709070        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.2017036e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 3.85e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 245           |\n",
      "|    n_updates            | 7050          |\n",
      "|    policy_gradient_loss | 0.00012       |\n",
      "|    reward               | 47.422905     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 491           |\n",
      "-------------------------------------------\n",
      "Episode: 199\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 195          |\n",
      "|    time_elapsed         | 5542         |\n",
      "|    total_timesteps      | 712725       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.531606e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.71        |\n",
      "|    explained_variance   | -7.89e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 14           |\n",
      "|    n_updates            | 7060         |\n",
      "|    policy_gradient_loss | -3.25e-05    |\n",
      "|    reward               | 175.1302     |\n",
      "|    std                  | 1.34         |\n",
      "|    value_loss           | 28.3         |\n",
      "------------------------------------------\n",
      "Episode: 200\n",
      "row: 3654, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1598949.82\n",
      "total_reward: 598949.82\n",
      "total_cost: 1256201.23\n",
      "total_trades: 3275\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 196           |\n",
      "|    time_elapsed         | 5571          |\n",
      "|    total_timesteps      | 716380        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.3199027e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.00015       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 140           |\n",
      "|    n_updates            | 7070          |\n",
      "|    policy_gradient_loss | -2.17e-05     |\n",
      "|    reward               | 59.89498      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 280           |\n",
      "-------------------------------------------\n",
      "Episode: 201\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 197           |\n",
      "|    time_elapsed         | 5602          |\n",
      "|    total_timesteps      | 720035        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.0133086e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 0.000448      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 22.1          |\n",
      "|    n_updates            | 7080          |\n",
      "|    policy_gradient_loss | -8.34e-05     |\n",
      "|    reward               | 307.1194      |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 44.4          |\n",
      "-------------------------------------------\n",
      "Episode: 202\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 198           |\n",
      "|    time_elapsed         | 5632          |\n",
      "|    total_timesteps      | 723690        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00014240589 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.71         |\n",
      "|    explained_variance   | 6.44e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 447           |\n",
      "|    n_updates            | 7090          |\n",
      "|    policy_gradient_loss | -0.000219     |\n",
      "|    reward               | 104.45469     |\n",
      "|    std                  | 1.34          |\n",
      "|    value_loss           | 894           |\n",
      "-------------------------------------------\n",
      "Episode: 203\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 128           |\n",
      "|    iterations           | 199           |\n",
      "|    time_elapsed         | 5662          |\n",
      "|    total_timesteps      | 727345        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016764956 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | -0.000275     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 50.2          |\n",
      "|    n_updates            | 7100          |\n",
      "|    policy_gradient_loss | -0.000151     |\n",
      "|    reward               | 74.17227      |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 101           |\n",
      "-------------------------------------------\n",
      "Episode: 204\n",
      "row: 3654, episode: 204\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1382832.44\n",
      "total_reward: 382832.44\n",
      "total_cost: 969061.84\n",
      "total_trades: 3233\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 128          |\n",
      "|    iterations           | 200          |\n",
      "|    time_elapsed         | 5693         |\n",
      "|    total_timesteps      | 731000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006148644 |\n",
      "|    clip_fraction        | 0.000848     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | 0.000243     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 26.4         |\n",
      "|    n_updates            | 7110         |\n",
      "|    policy_gradient_loss | -0.000163    |\n",
      "|    reward               | 38.283245    |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 53.2         |\n",
      "------------------------------------------\n",
      "Episode: 205\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 201         |\n",
      "|    time_elapsed         | 5734        |\n",
      "|    total_timesteps      | 734655      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.76196e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.72       |\n",
      "|    explained_variance   | -0.00112    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.55        |\n",
      "|    n_updates            | 7120        |\n",
      "|    policy_gradient_loss | 0.000117    |\n",
      "|    reward               | 132.31985   |\n",
      "|    std                  | 1.35        |\n",
      "|    value_loss           | 17.8        |\n",
      "-----------------------------------------\n",
      "Episode: 206\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 127            |\n",
      "|    iterations           | 202            |\n",
      "|    time_elapsed         | 5770           |\n",
      "|    total_timesteps      | 738310         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000109792294 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.72          |\n",
      "|    explained_variance   | -0.000618      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 88.3           |\n",
      "|    n_updates            | 7130           |\n",
      "|    policy_gradient_loss | 1.68e-05       |\n",
      "|    reward               | 110.20116      |\n",
      "|    std                  | 1.35           |\n",
      "|    value_loss           | 177            |\n",
      "--------------------------------------------\n",
      "Episode: 207\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 203          |\n",
      "|    time_elapsed         | 5800         |\n",
      "|    total_timesteps      | 741965       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029361714 |\n",
      "|    clip_fraction        | 0.0641       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | -0.000318    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 57.7         |\n",
      "|    n_updates            | 7140         |\n",
      "|    policy_gradient_loss | -0.000666    |\n",
      "|    reward               | 148.05786    |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 116          |\n",
      "------------------------------------------\n",
      "Episode: 208\n",
      "row: 3654, episode: 208\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1579958.41\n",
      "total_reward: 579958.41\n",
      "total_cost: 991951.64\n",
      "total_trades: 3209\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 204           |\n",
      "|    time_elapsed         | 5830          |\n",
      "|    total_timesteps      | 745620        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00089135236 |\n",
      "|    clip_fraction        | 0.00588       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | -0.000237     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 107           |\n",
      "|    n_updates            | 7150          |\n",
      "|    policy_gradient_loss | 4.99e-05      |\n",
      "|    reward               | 57.995842     |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 214           |\n",
      "-------------------------------------------\n",
      "Episode: 209\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 205           |\n",
      "|    time_elapsed         | 5860          |\n",
      "|    total_timesteps      | 749275        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.3326064e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | 0.00023       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 16.1          |\n",
      "|    n_updates            | 7160          |\n",
      "|    policy_gradient_loss | -3.03e-05     |\n",
      "|    reward               | 39.31448      |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 32.4          |\n",
      "-------------------------------------------\n",
      "Episode: 210\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 206          |\n",
      "|    time_elapsed         | 5890         |\n",
      "|    total_timesteps      | 752930       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006312515 |\n",
      "|    clip_fraction        | 0.000766     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | 0.000141     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.06         |\n",
      "|    n_updates            | 7170         |\n",
      "|    policy_gradient_loss | -0.000481    |\n",
      "|    reward               | 81.27882     |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 18.6         |\n",
      "------------------------------------------\n",
      "Episode: 211\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 207           |\n",
      "|    time_elapsed         | 5919          |\n",
      "|    total_timesteps      | 756585        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00064937404 |\n",
      "|    clip_fraction        | 0.00353       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | -0.00106      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 31.3          |\n",
      "|    n_updates            | 7180          |\n",
      "|    policy_gradient_loss | 0.000209      |\n",
      "|    reward               | 60.98294      |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 62.7          |\n",
      "-------------------------------------------\n",
      "Episode: 212\n",
      "row: 3654, episode: 212\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2077384.85\n",
      "total_reward: 1077384.85\n",
      "total_cost: 1115541.22\n",
      "total_trades: 3261\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 208           |\n",
      "|    time_elapsed         | 5948          |\n",
      "|    total_timesteps      | 760240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7277796e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | 0.00013       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 22.1          |\n",
      "|    n_updates            | 7190          |\n",
      "|    policy_gradient_loss | -1.51e-05     |\n",
      "|    reward               | 107.73849     |\n",
      "|    std                  | 1.35          |\n",
      "|    value_loss           | 44.4          |\n",
      "-------------------------------------------\n",
      "Episode: 213\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 209          |\n",
      "|    time_elapsed         | 5977         |\n",
      "|    total_timesteps      | 763895       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.146475e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | -0.0017      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 52.1         |\n",
      "|    n_updates            | 7200         |\n",
      "|    policy_gradient_loss | -3.14e-05    |\n",
      "|    reward               | 47.862637    |\n",
      "|    std                  | 1.35         |\n",
      "|    value_loss           | 104          |\n",
      "------------------------------------------\n",
      "Episode: 214\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 210           |\n",
      "|    time_elapsed         | 6008          |\n",
      "|    total_timesteps      | 767550        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023266893 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | -0.000574     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 16.1          |\n",
      "|    n_updates            | 7210          |\n",
      "|    policy_gradient_loss | -4.36e-05     |\n",
      "|    reward               | 25.003973     |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 32.6          |\n",
      "-------------------------------------------\n",
      "Episode: 215\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 211           |\n",
      "|    time_elapsed         | 6038          |\n",
      "|    total_timesteps      | 771205        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7466065e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | -0.00195      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.45          |\n",
      "|    n_updates            | 7220          |\n",
      "|    policy_gradient_loss | -2.24e-05     |\n",
      "|    reward               | 258.42407     |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 11            |\n",
      "-------------------------------------------\n",
      "Episode: 216\n",
      "row: 3654, episode: 216\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3309448.95\n",
      "total_reward: 2309448.95\n",
      "total_cost: 1546910.14\n",
      "total_trades: 3170\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 212           |\n",
      "|    time_elapsed         | 6068          |\n",
      "|    total_timesteps      | 774860        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00011064259 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | 0.000131      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 319           |\n",
      "|    n_updates            | 7230          |\n",
      "|    policy_gradient_loss | -3.76e-05     |\n",
      "|    reward               | 230.9449      |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 641           |\n",
      "-------------------------------------------\n",
      "Episode: 217\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 213           |\n",
      "|    time_elapsed         | 6098          |\n",
      "|    total_timesteps      | 778515        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6729994e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.72         |\n",
      "|    explained_variance   | -0.000147     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 266           |\n",
      "|    n_updates            | 7240          |\n",
      "|    policy_gradient_loss | -9.24e-05     |\n",
      "|    reward               | 345.0141      |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 533           |\n",
      "-------------------------------------------\n",
      "Episode: 218\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 214          |\n",
      "|    time_elapsed         | 6127         |\n",
      "|    total_timesteps      | 782170       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0003493953 |\n",
      "|    clip_fraction        | 0.000109     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.72        |\n",
      "|    explained_variance   | -5.94e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 553          |\n",
      "|    n_updates            | 7250         |\n",
      "|    policy_gradient_loss | -0.000234    |\n",
      "|    reward               | 138.11684    |\n",
      "|    std                  | 1.36         |\n",
      "|    value_loss           | 1.11e+03     |\n",
      "------------------------------------------\n",
      "Episode: 219\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 215          |\n",
      "|    time_elapsed         | 6157         |\n",
      "|    total_timesteps      | 785825       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029055811 |\n",
      "|    clip_fraction        | 0.0462       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | -0.000653    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 97.1         |\n",
      "|    n_updates            | 7260         |\n",
      "|    policy_gradient_loss | -0.000532    |\n",
      "|    reward               | 105.39196    |\n",
      "|    std                  | 1.36         |\n",
      "|    value_loss           | 194          |\n",
      "------------------------------------------\n",
      "Episode: 220\n",
      "row: 3654, episode: 220\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1320142.29\n",
      "total_reward: 320142.29\n",
      "total_cost: 1175331.45\n",
      "total_trades: 3338\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 216           |\n",
      "|    time_elapsed         | 6187          |\n",
      "|    total_timesteps      | 789480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2959338e-05 |\n",
      "|    clip_fraction        | 0.000356      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.73         |\n",
      "|    explained_variance   | -0.00137      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 53.2          |\n",
      "|    n_updates            | 7270          |\n",
      "|    policy_gradient_loss | -9.31e-05     |\n",
      "|    reward               | 32.01423      |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 106           |\n",
      "-------------------------------------------\n",
      "Episode: 221\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 217           |\n",
      "|    time_elapsed         | 6216          |\n",
      "|    total_timesteps      | 793135        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7216818e-05 |\n",
      "|    clip_fraction        | 0.000164      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.73         |\n",
      "|    explained_variance   | -0.00176      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.74          |\n",
      "|    n_updates            | 7280          |\n",
      "|    policy_gradient_loss | 1.12e-05      |\n",
      "|    reward               | 79.041466     |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 15.9          |\n",
      "-------------------------------------------\n",
      "Episode: 222\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 218           |\n",
      "|    time_elapsed         | 6246          |\n",
      "|    total_timesteps      | 796790        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00045132774 |\n",
      "|    clip_fraction        | 0.000219      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.73         |\n",
      "|    explained_variance   | 0.000311      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 30            |\n",
      "|    n_updates            | 7290          |\n",
      "|    policy_gradient_loss | -0.000151     |\n",
      "|    reward               | 89.57299      |\n",
      "|    std                  | 1.36          |\n",
      "|    value_loss           | 60.1          |\n",
      "-------------------------------------------\n",
      "Episode: 223\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 219          |\n",
      "|    time_elapsed         | 6277         |\n",
      "|    total_timesteps      | 800445       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027203024 |\n",
      "|    clip_fraction        | 0.0689       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | -0.000921    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 42.3         |\n",
      "|    n_updates            | 7300         |\n",
      "|    policy_gradient_loss | -0.000281    |\n",
      "|    reward               | 136.39226    |\n",
      "|    std                  | 1.37         |\n",
      "|    value_loss           | 84.8         |\n",
      "------------------------------------------\n",
      "Episode: 224\n",
      "row: 3654, episode: 224\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3396654.46\n",
      "total_reward: 2396654.46\n",
      "total_cost: 1276334.82\n",
      "total_trades: 3317\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 220          |\n",
      "|    time_elapsed         | 6307         |\n",
      "|    total_timesteps      | 804100       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029439342 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | -1.07e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 90           |\n",
      "|    n_updates            | 7310         |\n",
      "|    policy_gradient_loss | -0.000317    |\n",
      "|    reward               | 239.66545    |\n",
      "|    std                  | 1.37         |\n",
      "|    value_loss           | 180          |\n",
      "------------------------------------------\n",
      "Episode: 225\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 221          |\n",
      "|    time_elapsed         | 6336         |\n",
      "|    total_timesteps      | 807755       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030092692 |\n",
      "|    clip_fraction        | 0.066        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.73        |\n",
      "|    explained_variance   | -0.00023     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 261          |\n",
      "|    n_updates            | 7320         |\n",
      "|    policy_gradient_loss | -0.000992    |\n",
      "|    reward               | 99.92148     |\n",
      "|    std                  | 1.37         |\n",
      "|    value_loss           | 523          |\n",
      "------------------------------------------\n",
      "Episode: 226\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 222           |\n",
      "|    time_elapsed         | 6366          |\n",
      "|    total_timesteps      | 811410        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018674272 |\n",
      "|    clip_fraction        | 0.000137      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.73         |\n",
      "|    explained_variance   | -0.000286     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 53.6          |\n",
      "|    n_updates            | 7330          |\n",
      "|    policy_gradient_loss | 8.68e-07      |\n",
      "|    reward               | 372.5061      |\n",
      "|    std                  | 1.37          |\n",
      "|    value_loss           | 107           |\n",
      "-------------------------------------------\n",
      "Episode: 227\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 223           |\n",
      "|    time_elapsed         | 6395          |\n",
      "|    total_timesteps      | 815065        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7339424e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.73         |\n",
      "|    explained_variance   | -5.48e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 657           |\n",
      "|    n_updates            | 7340          |\n",
      "|    policy_gradient_loss | -6.49e-05     |\n",
      "|    reward               | 209.5111      |\n",
      "|    std                  | 1.37          |\n",
      "|    value_loss           | 1.32e+03      |\n",
      "-------------------------------------------\n",
      "Episode: 228\n",
      "row: 3373, episode: 228\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 694930.48\n",
      "total_reward: -305069.52\n",
      "total_cost: 701843.61\n",
      "total_trades: 2976\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 224           |\n",
      "|    time_elapsed         | 6423          |\n",
      "|    total_timesteps      | 818720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00018571586 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.73         |\n",
      "|    explained_variance   | 0.000372      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 205           |\n",
      "|    n_updates            | 7350          |\n",
      "|    policy_gradient_loss | -0.00023      |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.37          |\n",
      "|    value_loss           | 410           |\n",
      "-------------------------------------------\n",
      "Episode: 229\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 225           |\n",
      "|    time_elapsed         | 6452          |\n",
      "|    total_timesteps      | 822375        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00023229928 |\n",
      "|    clip_fraction        | 2.74e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.74         |\n",
      "|    explained_variance   | -0.0179       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.42          |\n",
      "|    n_updates            | 7360          |\n",
      "|    policy_gradient_loss | 0.000137      |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.37          |\n",
      "|    value_loss           | 10.9          |\n",
      "-------------------------------------------\n",
      "Episode: 230\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 226          |\n",
      "|    time_elapsed         | 6482         |\n",
      "|    total_timesteps      | 826030       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012227718 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.74        |\n",
      "|    explained_variance   | -0.000409    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 73.2         |\n",
      "|    n_updates            | 7370         |\n",
      "|    policy_gradient_loss | -0.0012      |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.38         |\n",
      "|    value_loss           | 146          |\n",
      "------------------------------------------\n",
      "Episode: 231\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 227          |\n",
      "|    time_elapsed         | 6511         |\n",
      "|    total_timesteps      | 829685       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023364248 |\n",
      "|    clip_fraction        | 0.0545       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.74        |\n",
      "|    explained_variance   | -0.00166     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 14.3         |\n",
      "|    n_updates            | 7380         |\n",
      "|    policy_gradient_loss | 3.01e-05     |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.38         |\n",
      "|    value_loss           | 28.7         |\n",
      "------------------------------------------\n",
      "Episode: 232\n",
      "row: 3654, episode: 232\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2440832.97\n",
      "total_reward: 1440832.97\n",
      "total_cost: 1110264.88\n",
      "total_trades: 3259\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 228           |\n",
      "|    time_elapsed         | 6541          |\n",
      "|    total_timesteps      | 833340        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2821962e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.74         |\n",
      "|    explained_variance   | 0.00022       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 11.9          |\n",
      "|    n_updates            | 7390          |\n",
      "|    policy_gradient_loss | 8.19e-06      |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.38          |\n",
      "|    value_loss           | 24.1          |\n",
      "-------------------------------------------\n",
      "Episode: 233\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 229           |\n",
      "|    time_elapsed         | 6572          |\n",
      "|    total_timesteps      | 836995        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00026959513 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.74         |\n",
      "|    explained_variance   | -7.51e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 93.6          |\n",
      "|    n_updates            | 7400          |\n",
      "|    policy_gradient_loss | -0.000188     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.38          |\n",
      "|    value_loss           | 187           |\n",
      "-------------------------------------------\n",
      "Episode: 234\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 230           |\n",
      "|    time_elapsed         | 6603          |\n",
      "|    total_timesteps      | 840650        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6549446e-05 |\n",
      "|    clip_fraction        | 5.47e-05      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.74         |\n",
      "|    explained_variance   | -0.000112     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 52.8          |\n",
      "|    n_updates            | 7410          |\n",
      "|    policy_gradient_loss | -2.97e-05     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.38          |\n",
      "|    value_loss           | 106           |\n",
      "-------------------------------------------\n",
      "Episode: 235\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 231          |\n",
      "|    time_elapsed         | 6633         |\n",
      "|    total_timesteps      | 844305       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.764415e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.74        |\n",
      "|    explained_variance   | -0.00218     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.86         |\n",
      "|    n_updates            | 7420         |\n",
      "|    policy_gradient_loss | 0.000169     |\n",
      "|    reward               | -0.1         |\n",
      "|    std                  | 1.38         |\n",
      "|    value_loss           | 14.2         |\n",
      "------------------------------------------\n",
      "Episode: 236\n",
      "row: 3654, episode: 236\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2514465.35\n",
      "total_reward: 1514465.35\n",
      "total_cost: 1516736.25\n",
      "total_trades: 3334\n",
      "=================================\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 127            |\n",
      "|    iterations           | 232            |\n",
      "|    time_elapsed         | 6664           |\n",
      "|    total_timesteps      | 847960         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000107470616 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.74          |\n",
      "|    explained_variance   | -0.00012       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 86.8           |\n",
      "|    n_updates            | 7430           |\n",
      "|    policy_gradient_loss | -2.38e-05      |\n",
      "|    reward               | 0.0            |\n",
      "|    std                  | 1.38           |\n",
      "|    value_loss           | 174            |\n",
      "--------------------------------------------\n",
      "Episode: 237\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 233           |\n",
      "|    time_elapsed         | 6695          |\n",
      "|    total_timesteps      | 851615        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012126588 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.74         |\n",
      "|    explained_variance   | -0.000348     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 113           |\n",
      "|    n_updates            | 7440          |\n",
      "|    policy_gradient_loss | -6.95e-05     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.39          |\n",
      "|    value_loss           | 226           |\n",
      "-------------------------------------------\n",
      "Episode: 238\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 234           |\n",
      "|    time_elapsed         | 6726          |\n",
      "|    total_timesteps      | 855270        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2583597e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.75         |\n",
      "|    explained_variance   | -0.0107       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.5           |\n",
      "|    n_updates            | 7450          |\n",
      "|    policy_gradient_loss | -9.37e-05     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.39          |\n",
      "|    value_loss           | 5.04          |\n",
      "-------------------------------------------\n",
      "Episode: 239\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 235           |\n",
      "|    time_elapsed         | 6755          |\n",
      "|    total_timesteps      | 858925        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7823745e-05 |\n",
      "|    clip_fraction        | 0.000164      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.75         |\n",
      "|    explained_variance   | 0.00061       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 15.1          |\n",
      "|    n_updates            | 7460          |\n",
      "|    policy_gradient_loss | 3.23e-05      |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.39          |\n",
      "|    value_loss           | 30.5          |\n",
      "-------------------------------------------\n",
      "Episode: 240\n",
      "row: 3654, episode: 240\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2692766.09\n",
      "total_reward: 1692766.09\n",
      "total_cost: 1507462.26\n",
      "total_trades: 3237\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 236          |\n",
      "|    time_elapsed         | 6786         |\n",
      "|    total_timesteps      | 862580       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.262523e-05 |\n",
      "|    clip_fraction        | 5.47e-05     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | -0.00155     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.6          |\n",
      "|    n_updates            | 7470         |\n",
      "|    policy_gradient_loss | -7.04e-06    |\n",
      "|    reward               | 0.022187     |\n",
      "|    std                  | 1.39         |\n",
      "|    value_loss           | 17.5         |\n",
      "------------------------------------------\n",
      "Episode: 241\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 237          |\n",
      "|    time_elapsed         | 6815         |\n",
      "|    total_timesteps      | 866235       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.818075e-05 |\n",
      "|    clip_fraction        | 0.000164     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | 8.98e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 141          |\n",
      "|    n_updates            | 7480         |\n",
      "|    policy_gradient_loss | -0.000222    |\n",
      "|    reward               | 0.008974517  |\n",
      "|    std                  | 1.39         |\n",
      "|    value_loss           | 283          |\n",
      "------------------------------------------\n",
      "Episode: 242\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 127           |\n",
      "|    iterations           | 238           |\n",
      "|    time_elapsed         | 6844          |\n",
      "|    total_timesteps      | 869890        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.8296766e-05 |\n",
      "|    clip_fraction        | 0.000438      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.75         |\n",
      "|    explained_variance   | 5.94e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 55            |\n",
      "|    n_updates            | 7490          |\n",
      "|    policy_gradient_loss | 1.89e-06      |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.39          |\n",
      "|    value_loss           | 110           |\n",
      "-------------------------------------------\n",
      "Episode: 243\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 127          |\n",
      "|    iterations           | 239          |\n",
      "|    time_elapsed         | 6877         |\n",
      "|    total_timesteps      | 873545       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.090741e-05 |\n",
      "|    clip_fraction        | 0.000438     |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | 0.000139     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 312          |\n",
      "|    n_updates            | 7500         |\n",
      "|    policy_gradient_loss | 9.03e-05     |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.39         |\n",
      "|    value_loss           | 624          |\n",
      "------------------------------------------\n",
      "Episode: 244\n",
      "row: 3654, episode: 244\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2761532.86\n",
      "total_reward: 1761532.86\n",
      "total_cost: 1551671.31\n",
      "total_trades: 3299\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 240          |\n",
      "|    time_elapsed         | 6909         |\n",
      "|    total_timesteps      | 877200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014577196 |\n",
      "|    clip_fraction        | 0.045        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | 9.68e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 60.2         |\n",
      "|    n_updates            | 7510         |\n",
      "|    policy_gradient_loss | -0.000398    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.39         |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "Episode: 245\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 241          |\n",
      "|    time_elapsed         | 6938         |\n",
      "|    total_timesteps      | 880855       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0013957989 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | 0.000141     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 148          |\n",
      "|    n_updates            | 7520         |\n",
      "|    policy_gradient_loss | -0.000665    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.4          |\n",
      "|    value_loss           | 296          |\n",
      "------------------------------------------\n",
      "Episode: 246\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 242          |\n",
      "|    time_elapsed         | 6967         |\n",
      "|    total_timesteps      | 884510       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0002716295 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | -0.00321     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.72         |\n",
      "|    n_updates            | 7530         |\n",
      "|    policy_gradient_loss | 4.76e-05     |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.4          |\n",
      "|    value_loss           | 11.5         |\n",
      "------------------------------------------\n",
      "Episode: 247\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 243           |\n",
      "|    time_elapsed         | 6997          |\n",
      "|    total_timesteps      | 888165        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1573738e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.75         |\n",
      "|    explained_variance   | -9.2e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 159           |\n",
      "|    n_updates            | 7540          |\n",
      "|    policy_gradient_loss | 7.68e-06      |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 317           |\n",
      "-------------------------------------------\n",
      "Episode: 248\n",
      "row: 3566, episode: 248\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 697161.21\n",
      "total_reward: -302838.79\n",
      "total_cost: 822833.71\n",
      "total_trades: 3147\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 244          |\n",
      "|    time_elapsed         | 7026         |\n",
      "|    total_timesteps      | 891820       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.545632e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | -0.000473    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 26.2         |\n",
      "|    n_updates            | 7550         |\n",
      "|    policy_gradient_loss | -1.23e-05    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.4          |\n",
      "|    value_loss           | 52.5         |\n",
      "------------------------------------------\n",
      "Episode: 249\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 245          |\n",
      "|    time_elapsed         | 7054         |\n",
      "|    total_timesteps      | 895475       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.719859e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.75        |\n",
      "|    explained_variance   | -0.0104      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.96         |\n",
      "|    n_updates            | 7560         |\n",
      "|    policy_gradient_loss | -1.64e-05    |\n",
      "|    reward               | 0.03973523   |\n",
      "|    std                  | 1.4          |\n",
      "|    value_loss           | 10.5         |\n",
      "------------------------------------------\n",
      "Episode: 250\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 246           |\n",
      "|    time_elapsed         | 7083          |\n",
      "|    total_timesteps      | 899130        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0071424e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.75         |\n",
      "|    explained_variance   | -0.00189      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.32          |\n",
      "|    n_updates            | 7570          |\n",
      "|    policy_gradient_loss | -3.14e-05     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 17.1          |\n",
      "-------------------------------------------\n",
      "Episode: 251\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 247           |\n",
      "|    time_elapsed         | 7112          |\n",
      "|    total_timesteps      | 902785        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4169663e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.75         |\n",
      "|    explained_variance   | -2.19e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 214           |\n",
      "|    n_updates            | 7580          |\n",
      "|    policy_gradient_loss | -6.36e-06     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 428           |\n",
      "-------------------------------------------\n",
      "Episode: 252\n",
      "row: 3654, episode: 252\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1834257.08\n",
      "total_reward: 834257.08\n",
      "total_cost: 1263263.64\n",
      "total_trades: 3224\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 248           |\n",
      "|    time_elapsed         | 7142          |\n",
      "|    total_timesteps      | 906440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00025453398 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.75         |\n",
      "|    explained_variance   | -0.00162      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.53          |\n",
      "|    n_updates            | 7590          |\n",
      "|    policy_gradient_loss | -0.000273     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 11.2          |\n",
      "-------------------------------------------\n",
      "Episode: 253\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 249           |\n",
      "|    time_elapsed         | 7172          |\n",
      "|    total_timesteps      | 910095        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00050508016 |\n",
      "|    clip_fraction        | 0.000301      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.75         |\n",
      "|    explained_variance   | -0.000124     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 35.9          |\n",
      "|    n_updates            | 7600          |\n",
      "|    policy_gradient_loss | -7.66e-05     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 71.8          |\n",
      "-------------------------------------------\n",
      "Episode: 254\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 250           |\n",
      "|    time_elapsed         | 7201          |\n",
      "|    total_timesteps      | 913750        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00096350344 |\n",
      "|    clip_fraction        | 0.00498       |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.76         |\n",
      "|    explained_variance   | -0.000204     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 78.1          |\n",
      "|    n_updates            | 7610          |\n",
      "|    policy_gradient_loss | -0.000319     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 156           |\n",
      "-------------------------------------------\n",
      "Episode: 255\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 251          |\n",
      "|    time_elapsed         | 7230         |\n",
      "|    total_timesteps      | 917405       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014087787 |\n",
      "|    clip_fraction        | 0.0113       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.76        |\n",
      "|    explained_variance   | -0.00058     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 23.9         |\n",
      "|    n_updates            | 7620         |\n",
      "|    policy_gradient_loss | -0.000353    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.4          |\n",
      "|    value_loss           | 47.9         |\n",
      "------------------------------------------\n",
      "Episode: 256\n",
      "row: 3654, episode: 256\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2236162.46\n",
      "total_reward: 1236162.46\n",
      "total_cost: 1272798.57\n",
      "total_trades: 3223\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 252           |\n",
      "|    time_elapsed         | 7260          |\n",
      "|    total_timesteps      | 921060        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021272284 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.76         |\n",
      "|    explained_variance   | 0.000278      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 18.7          |\n",
      "|    n_updates            | 7630          |\n",
      "|    policy_gradient_loss | -4.29e-05     |\n",
      "|    reward               | -0.1          |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 37.5          |\n",
      "-------------------------------------------\n",
      "Episode: 257\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 253           |\n",
      "|    time_elapsed         | 7290          |\n",
      "|    total_timesteps      | 924715        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5155295e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.76         |\n",
      "|    explained_variance   | 0.000271      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 73.8          |\n",
      "|    n_updates            | 7640          |\n",
      "|    policy_gradient_loss | 0.000222      |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 148           |\n",
      "-------------------------------------------\n",
      "Episode: 258\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 126           |\n",
      "|    iterations           | 254           |\n",
      "|    time_elapsed         | 7320          |\n",
      "|    total_timesteps      | 928370        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00046735123 |\n",
      "|    clip_fraction        | 0.000109      |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.76         |\n",
      "|    explained_variance   | 0.000249      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 251           |\n",
      "|    n_updates            | 7650          |\n",
      "|    policy_gradient_loss | -9.54e-05     |\n",
      "|    reward               | 0.0           |\n",
      "|    std                  | 1.4           |\n",
      "|    value_loss           | 503           |\n",
      "-------------------------------------------\n",
      "Episode: 259\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 255          |\n",
      "|    time_elapsed         | 7350         |\n",
      "|    total_timesteps      | 932025       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016216844 |\n",
      "|    clip_fraction        | 0.0243       |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.76        |\n",
      "|    explained_variance   | 0.00103      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 44           |\n",
      "|    n_updates            | 7660         |\n",
      "|    policy_gradient_loss | -0.000397    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.4          |\n",
      "|    value_loss           | 88           |\n",
      "------------------------------------------\n",
      "Episode: 260\n",
      "row: 3654, episode: 260\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 950556.63\n",
      "total_reward: -49443.37\n",
      "total_cost: 1043665.45\n",
      "total_trades: 3316\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 126          |\n",
      "|    iterations           | 256          |\n",
      "|    time_elapsed         | 7379         |\n",
      "|    total_timesteps      | 935680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0009010483 |\n",
      "|    clip_fraction        | 0.00233      |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.76        |\n",
      "|    explained_variance   | -0.00243     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.87         |\n",
      "|    n_updates            | 7670         |\n",
      "|    policy_gradient_loss | -0.000449    |\n",
      "|    reward               | 0.0          |\n",
      "|    std                  | 1.4          |\n",
      "|    value_loss           | 20           |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=trained_model, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = StockTradingEnv(df = trade_data, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_step = 1000\n",
    "test_result = []\n",
    "test_env.reset()\n",
    "for i in range(0,test_step):\n",
    "    observation = [test_env.state]\n",
    "    observation = np.array(observation).astype(np.float32)\n",
    "    actions, values, log_prob = ort_sess.run(None, {\"input\": observation})\n",
    "    result = test_env.step(actions)\n",
    "    test_result.append(result[1])\n",
    "    if result[2] == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Load ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"all_in_one_ppo.onnx\"\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# # Check that the predictions are the same\n",
    "# with th.no_grad():\n",
    "#     print(model.policy(th.as_tensor(observation), deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = np.zeros((1, state_space)).astype(np.float32)\n",
    "ort_sess = ort.InferenceSession(onnx_path)\n",
    "actions, values, log_prob = ort_sess.run(None, {\"input\": observation})\n",
    "print(actions, values, log_prob)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
