{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Train Custom_Env\
\
\pard\tx220\tx720\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\li720\fi-720\pardirnatural\partightenfactor0
\ls1\ilvl0\cf0 {\listtext	\uc0\u8226 	}
\f1\b ppo_384episodes(ppo_1)
\f0\b0 : Still not convergent, required to add punishments to unavailable buying and selling. I charge 0.1% of initial capital on any unavailable action.\
{\listtext	\uc0\u8226 	}
\f1\b ppo_512episodes(ppo_2): 
\f0\b0 Add punishments and the model converges but the agent tries to buy just a small amount of stock to avoid the punishments. Must take NAV\'92s changes into account as a part of the rewards\
{\listtext	\uc0\u8226 	}
\f1\b ppo_sp500_90iter(ppo_3)
\f0\b0 : Use sp500_ready_data_monthly dataset. It seems not to be good for training while it takes more than 60.000 steps to finish an iteration. In addition, I got a bug here with -inf computed ratios from zero gross revenue when do not process feature engineer rigorously. Try to use the yearly dataset, add prices & current_amount, take total_reward at the end of each iteration into account (or use accumulated reward=currentNAV - initialCapital), and remove all tickers containing any zero gross revenue\
{\listtext	\uc0\u8226 	}
\f1\b sp500_ppo_yearly_1024episode(ppo_4)
\f0\b0 : While expanding the dataset to include more companies using SP500 instead of Dow30 did not improce training performance, the issue might lie in the yearly frequency. The agent\'92s limitation of trading opportunity (close deals only ten time along each episode) can be hindered by this frequency. To address it, let\'92s experiment with a smaller set of companies, Dow30, but use montly dataset instead of yearly one.\
{\listtext	\uc0\u8226 	}
\f1\b dow30_ppo_256episode(ppo_30)
\f0\b0 : Use the different at each step as reward seem not to be a good design while the model work so bad in testing dataset.\
{\listtext	\uc0\u8226 	}
\f1\b dow30_endRecord_ppo_256episode(ppo_5)
\f0\b0 : Turn back to the previous environment design with total asset surplus recoding at the end of each episode. The testing result represent some advantage in comparison with DJ index or buy-and-hold strategy. Let test it with more network layers and set the reward of the step n-1 as total surplus\
{\listtext	\uc0\u8226 	}
\f1\b dow30_endRecord_128neu_ppo_512episode(ppo_6)
\f0\b0 : The testing result deviated from my desired outcome. The reward reached zero, indicating no positive reinforcement for the agent's action. While the loss function stabilized, it doesn't translate to the result. Our agent underperformed comparing to Dow Jone Industrial Average, with its total assets falling significantly short. We need to investigate the reward policy and potentially refined the environment with random selected tickers for daily training to improve the agent's performance\
{\listtext	\uc0\u8226 	}
\f1\b main_dow30_randomTic_discrete_128neu_ppo_500k_steps(ppo_7)
\f0\b0 : I got a mistake when reset the reward at the begining of each step. The Q-network use accumulated rewards to learn from a chain of actions.\
{\listtext	\uc0\u8226 	}
\f1\b main_dow30_randomTic_discrete_128neu_ppo_1500k_steps(ppo_8):
\f0\b0  The design of random selecting a random ticker for training seem to be a logical design. However, it\'92s training performance deviated from my desired outcome again. Try to train it many more steps or return the Main_Portfolio_Box_Dow30_Monthly with new reward design\
{\listtext	\uc0\u8226 	}
\f1\b main_portfolio_box_ppo_dow30_monthly_2048episode(ppo_14)
\f0\b0 : It is deviated from my expected outcome\
{\listtext	\uc0\u8226 	}
\f1\b main_dow30_randomTic_discrete_256neu_ppo_3m_steps(ppo_16)
\f0\b0 : While the training addressed a clear tendency to converge, it became distracted from the improvement and turn away from getting better at the end of the training. I think the problem come from the number of trades that increase cost of trading when the agent encounter with a downward trend. Try to  add short-sell (and the number of trades as an item of the state).\
{\listtext	\uc0\u8226 	}
\f1\b main_dow30_randomTic_discrete_256neu_ppo_5m_steps(ppo_17)
\f0\b0 : Wanna try more from the previous but the model still not converge. Turn to the short-sell version\
{\listtext	\uc0\u8226 	}
\f1\b main_dow30_randomTic_discrete_shortSell_256neu_ppo_6m_steps(ppo_18&19&20&21)
\f0\b0 : The short-sell version does work not really well. Try to standadize the data (consider to use SP500 or Dow30) and add one more hidden layer\
\pard\tx220\tx720\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\li720\fi-720\pardirnatural\partightenfactor0
\ls1\ilvl0
\f1\b \cf0 {\listtext	\uc0\u8226 	}main_dow30_randomTic_discrete_standadized_shortSell_64x3_ppo_6m_steps (ppo_22,23,24,25)
\f0\b0 : After all, I realize the environment work incorrect due to the any() function of pandas.Series work inaccuracy in buy and sell actions, so I turn it to item() function.\
}