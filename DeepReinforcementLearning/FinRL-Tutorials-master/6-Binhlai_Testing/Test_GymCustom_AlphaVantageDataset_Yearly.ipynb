{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/1-Introduction/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch\n",
    "## Alpha_Dataset_Yearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Check Additional Packages](#1.1)\n",
    "    * [2.2. Import Packages](#1.2)\n",
    "    * [2.3. Create Folders & Relevant Configurations¶](#1.3)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess fundamental data](#3)        \n",
    "    * [4.1. Import the financial data](#3.1)\n",
    "    * [4.2. Specify items needed to calculate financial ratios](#3.2)\n",
    "    * [4.3. Sort data based on date and tic columns](#3.3)\n",
    "    * [4.4. Finish data preparation](#3.4)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. Set up the training environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Train DRL Agent](#5)  \n",
    "* [7.Backtest Our Strategy](#6)  \n",
    "    * [7.1. BackTest with DJIA](#6.1)\n",
    "    * [7.2. BackTest with Buy&Hold Strategy](#6.2)\n",
    "* [8.Save & load model](#7)\n",
    "    * [8.1. Save model](#7.1)\n",
    "    * [8.2. Load model](#7.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "\n",
    "<a id='1.1'></a>\n",
    "## 2.1. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.2'></a>\n",
    "## 2.2. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "import math\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "%matplotlib inline\n",
    "# from finrl.config_tickers import SP_500_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "# from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.meta.env_portfolio_allocation.env_portfolio import StockPortfolioEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Create Folders & Relevant Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "TEST_END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "TRAIN_START_DATE = '2000-01-01'\n",
    "TRAIN_END_DATE = '2021-01-01'\n",
    "TEST_START_DATE = '2021-01-01'\n",
    "TEST_END_DATE = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "# Load price data from csv file\n",
    "tic_dir = './' + DATA_SAVE_DIR + '/sp500_price_monthly.csv'\n",
    "df = pd.read_csv(tic_dir,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is available locally, we can skip downloading steps and jump directly to part [**4.4.Finish data preparation**](#3.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Price Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download data from YFinance\n",
    "# tic_dir = './' + DATA_SAVE_DIR + '/sp500_ticker.csv'\n",
    "# tic_list = pd.read_csv(tic_dir,index_col=0)\n",
    "# SP_500_TICKER = np.array(tic_list.tic).tolist()\n",
    "# df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                      end_date = TEST_END_DATE,\n",
    "#                      ticker_list = SP_500_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "GiRuFOTOtj1Y",
    "outputId": "402874c0-b13f-437b-a67f-a83f88de66eb"
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "DSw4ZEzVtj1Z",
    "outputId": "94617d16-432c-40eb-f758-16d2fdab09e0"
   },
   "outputs": [],
   "source": [
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CV3HrZHLh1hy",
    "outputId": "73944c23-5a4e-49f8-b9e5-da382b4fc7f5"
   },
   "outputs": [],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "4hYkeaPiICHS",
    "outputId": "87cca0b1-8d3c-4a61-e061-ea0d9989daa1"
   },
   "outputs": [],
   "source": [
    "# df.sort_values(['date','tic']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2vryMsdNL9H",
    "outputId": "6691ba9b-e613-412b-dba5-dee592bb0ff2"
   },
   "outputs": [],
   "source": [
    "# len(df.tic.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcNyXa7RNPrF",
    "outputId": "edb04575-9b82-4d5e-f13a-55c884214725"
   },
   "outputs": [],
   "source": [
    "# df.tic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "<a id='3'></a>\n",
    "# Part 4: Preprocess fundamental data\n",
    "- Import finanical data downloaded from Alpha Vantage\n",
    "- Preprocess the dataset and calculate financial ratios\n",
    "- Turn yearly ratio into daily basis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.1'></a>\n",
    "## 4.1 Import the financial data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define configurations of the collecting data & download data via Alpha Vantage API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# api_key = 'GZWRUSYXT18ZTR6C'\n",
    "# features_cols = ['fiscalDateEnding','totalRevenue','costOfRevenue','sellingGeneralAndAdministrative','researchAndDevelopment','depreciation','interestExpense','totalCurrentLiabilities','incomeTaxExpense','netIncome','commonStockSharesOutstanding','cashAndCashEquivalentsAtCarryingValue','cashAndShortTermInvestments','operatingCashflow','totalLiabilities','inventory','currentNetReceivables','propertyPlantEquipment','capitalExpenditures','longTermInvestments','totalShareholderEquity','longTermDebt','retainedEarnings','dividendPayoutCommonStock','paymentsForRepurchaseOfCommonStock','treasuryStock','currentLongTermDebt']\n",
    "# price_cols = ['open','high','low','close','volume']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to download fundamental data from financial reports by ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_fundamental(ticket):\n",
    "#     # Download income statement\n",
    "#     url = f'https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol={ticket}&apikey={api_key}'\n",
    "#     r = requests.get(url)\n",
    "#     data_is = r.json()\n",
    "\n",
    "#     # Download balance sheet\n",
    "#     url = f'https://www.alphavantage.co/query?function=BALANCE_SHEET&symbol={ticket}&apikey={api_key}'\n",
    "#     r = requests.get(url)\n",
    "#     data_bs = r.json()\n",
    "\n",
    "#     # Download cash flow\n",
    "#     url = f'https://www.alphavantage.co/query?function=CASH_FLOW&symbol={ticket}&apikey={api_key}'\n",
    "#     r = requests.get(url)\n",
    "#     data_cf = r.json()\n",
    "\n",
    "#     df_is = pd.json_normalize(data_is['annualReports'])\n",
    "#     df_bs = pd.json_normalize(data_bs['annualReports'])\n",
    "#     df_cf = pd.json_normalize(data_cf['annualReports'])\n",
    "\n",
    "#     merged_df = df_is.merge(df_bs).merge(df_cf)\n",
    "#     merged_df['tic'] = ticket\n",
    "#     merged_df = merged_df[['tic']+features_cols]\n",
    "#     merged_df['fiscalDateEnding'] = pd.to_datetime(merged_df.fiscalDateEnding,format='mixed')\n",
    "\n",
    "#     return merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to download stock price by ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def collect_price(ticket):\n",
    "#     url = f'https://www.alphavantage.co/query?function=TIME_SERIES_MONTHLY&symbol={ticket}&apikey={api_key}'\n",
    "#     r = requests.get(url)\n",
    "#     data_monthly_price = r.json()\n",
    "\n",
    "#     price_monthly = pd.DataFrame.from_dict(data_monthly_price['Monthly Time Series'], orient='index')\n",
    "#     price_monthly.columns = price_cols\n",
    "#     price_monthly['fiscalDateEnding'] = pd.to_datetime(price_monthly.index,format='mixed')\n",
    "#     price_monthly.reset_index(inplace=True,drop=True)\n",
    "#     price_monthly = price_monthly[['fiscalDateEnding','open','high','low','close','volume']]\n",
    "#     return price_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to merge monthly stock price into yearly fundamental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_price_to_df(merged_df,price_monthly):\n",
    "#     merged_df['year'] = merged_df.fiscalDateEnding.dt.year\n",
    "#     merged_df['month'] = merged_df.fiscalDateEnding.dt.month\n",
    "#     price_monthly['year'] = price_monthly.fiscalDateEnding.dt.year\n",
    "#     price_monthly['month'] = price_monthly.fiscalDateEnding.dt.month\n",
    "#     merged_final = pd.merge(merged_df,price_monthly,how=\"left\",on=['year','month'])\n",
    "#     merged_final.drop(columns=['year','month','fiscalDateEnding_y'],inplace=True)\n",
    "#     merged_final = df_final.rename(columns={'fiscalDateEnding_x': 'date'})\n",
    "#     merged_final['tic'] = ticket\n",
    "\n",
    "#     merged_columns = [merged_final.columns[-1]]\n",
    "#     for i in range(0,len(merged_final.columns)-1):\n",
    "#         merged_columns.append(merged_final.columns[i])\n",
    "#     merged_final = merged_final[merged_columns]\n",
    "    \n",
    "#     return merged_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_tics = SP_500_TICKER[453:]\n",
    "# print(download_tics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_data = pd.DataFrame()\n",
    "\n",
    "# for ticket in download_tics:\n",
    "#     df_fund = collect_fundamental(ticket)\n",
    "#     fund_data = pd.concat([fund_data, df_fund], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_data.to_csv('sp500_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dropped tickers which are not available data to download from Alpha Vantage\n",
    "\n",
    "# tics_1 = pd.DataFrame()\n",
    "# tics_2 = pd.DataFrame()\n",
    "# tics_1['tic'] = df.tic.unique()\n",
    "# tics_2['tic'] = fund_data.tic.unique()\n",
    "\n",
    "# merged_df = tics_1.merge(tics_2, how='outer', indicator=True)\n",
    "# unique_in_df = merged_df[merged_df['_merge'] == 'left_only']\n",
    "# unique_in_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check reach download limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# url = f'https://www.alphavantage.co/query?function=INCOME_STATEMENT&symbol=IBM&apikey=GZWRUSYXT18ZTR6C'\n",
    "# r = requests.get(url)\n",
    "# data_is = r.json()\n",
    "# data_is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fund_data = pd.read_csv('./' + DATA_SAVE_DIR + '/sp500_fundamental.csv',index_col=0)\n",
    "# # fund_data = fund_data.rename(columns={'fiscalDateEnding_x':'date'})\n",
    "# fund_data['date'] = pd.to_datetime(fund_data.date,format='mixed')\n",
    "# fund_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTIL TOOL: merge yearly fundamental data to daily stock price\n",
    "\n",
    "# start_date = df.iloc[0].date\n",
    "# end_date = df.iloc[-1].date\n",
    "# date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "# date_range = pd.DataFrame(date_range)\n",
    "# date_range.columns = ['date']\n",
    "\n",
    "# fund_data = fund_data.rename(columns={'fiscalDateEnding':'date'})\n",
    "# fund_data['date'] = pd.to_datetime(fund_data.date,format='mixed')\n",
    "\n",
    "# fund_data_with_price = pd.DataFrame()\n",
    "# for ticket in fund_data.tic.unique():\n",
    "#     price_by_ticket = df[df.tic == ticket]\n",
    "#     price_by_ticket['date'] = pd.to_datetime(price_by_ticket['date'],format='mixed')\n",
    "#     price_by_ticket = pd.merge(date_range,price_by_ticket,how='left')\n",
    "#     price_by_ticket.bfill(inplace=True)\n",
    "#     price_by_ticket = pd.merge(fund_data.loc[fund_data.tic==ticket],price_by_ticket,how='left',on=['date'])\n",
    "#     price_by_ticket.drop(columns=['tic_y','day'],inplace=True,axis=0)\n",
    "#     price_by_ticket = price_by_ticket.rename(columns={'tic_x':'tic'})\n",
    "#     fund_data_with_price = pd.concat([fund_data_with_price, price_by_ticket], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning ####\n",
    "Refine the data before computing fundamental ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fund_data.info(),'\\n')\n",
    "# print(fund_data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The company with zero totalRevenue might cause problems for computing ratios while it is the denominator in some formulars.<br>\n",
    "Deleting all tickers containing this issue is a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero_revenue_tics = fund_data[fund_data.totalRevenue == 0].tic.unique()\n",
    "# fund_data = fund_data[~fund_data.tic.isin(zero_revenue_tics)]\n",
    "# fund_data[fund_data.totalRevenue == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While removing zero revenue data was neccessary, other columns with zero values might indicate some potential issues that require further investigations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column_nan = fund_data.eq(0).any()\n",
    "# column_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fund_data = fund_data.fillna(0)\n",
    "# for i in fund_data.columns:\n",
    "#     print(i,'\\n',fund_data[i].unique(),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'None' value is replaced with '0' and correcting datatype can now being apply on the features. In this case, all data points must be turned into float instead of object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float_columns = fund_data.columns\n",
    "# float_columns = float_columns.drop(['date','tic'])\n",
    "\n",
    "# for column in float_columns:\n",
    "#     fund_data[column] = fund_data[column].astype(float)\n",
    "\n",
    "# fund_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Compute fundamental ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define support functions before computing fundamental ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Define a function to count positive values within a window\n",
    "# def count_positives(window):\n",
    "#   return window[window > 0].count()\n",
    "\n",
    "# def count_positives_window(data,k):\n",
    "#     df = pd.DataFrame(data, columns=['values'])\n",
    "\n",
    "#     # Create a new column with the adjusted positive count (using a shifted window)\n",
    "#     df['positive_count'] = df['values'].rolling(window=k, min_periods=1).apply(count_positives)\n",
    "\n",
    "#     # Set positive_count of the first k rows to positive_count of row k\n",
    "#     df.loc[:k-1,'positive_count'] = df.loc[k-1].positive_count\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Define a function to min value within a window\n",
    "# def get_min(window):\n",
    "#   return window.min()\n",
    "\n",
    "# def min_in_window(data,k):\n",
    "#     df = pd.DataFrame(data, columns=['values'])\n",
    "\n",
    "#     # Create a new column with the min in the window (using a shifted window)\n",
    "#     df['min'] = df['values'].rolling(window=k, min_periods=1).apply(get_min)\n",
    "\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, data and supporting functions are ready for computing fundamental ratios required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_fund_ratios(df_final):\n",
    "#     pos_count_window = 10  # number of years to look back for count_positive_profit\n",
    "#     min_ebit_window = 4  # number of years to look back for min_ebit\n",
    "    \n",
    "#     gross_income = df_final.totalRevenue - df_final.costOfRevenue\n",
    "#     ebit = gross_income - df_final.sellingGeneralAndAdministrative\n",
    "#     profit = ebit - df_final.interestExpense - df_final.incomeTaxExpense\n",
    "#     market_equity = df_final.commonStockSharesOutstanding * df_final.close\n",
    "#     market_asset = df_final.totalLiabilities + market_equity\n",
    "    \n",
    "#     # Gross profit margin\n",
    "#     gross_profit_margin = (gross_income/df_final.totalRevenue).to_frame('gross_profit_margin')\n",
    "    \n",
    "#     # SGA Expense / Gross Profit\n",
    "#     sga_ratio = (df_final.sellingGeneralAndAdministrative/gross_income).to_frame('sga_ratio')\n",
    "    \n",
    "#     # Depreciation / Gross Profit\n",
    "#     dep_ratio = (df_final.depreciation/gross_income).to_frame('dep_ratio')\n",
    "    \n",
    "#     # EBIT / Bond interest\n",
    "#     ebit_on_int = (ebit/df_final.interestExpense).to_frame('ebit_on_int')\n",
    "    \n",
    "#     # Profit margin\n",
    "#     profit_margin = (profit/df_final.totalRevenue).to_frame('profit_margin')\n",
    "    \n",
    "#     # Amount of positive profit within a window\n",
    "#     count_positive_profit = count_positives_window(profit,pos_count_window)\n",
    "#     count_positive_profit = count_positive_profit['positive_count'].to_frame('count_positive_profit')\n",
    "    \n",
    "#     # Cash And Short Term Investments / Total Liabilities\n",
    "#     csti_on_liabilities = (df_final.cashAndShortTermInvestments/df_final.totalLiabilities).to_frame('csti_on_liabilities')\n",
    "    \n",
    "#     # Inventory / EBIT\n",
    "#     inventory_on_ebit = (df_final.inventory / ebit).to_frame('inventory_on_ebit')\n",
    "    \n",
    "#     # Receivable / Revenue\n",
    "#     receivable_on_rev = (df_final.currentNetReceivables / df_final.totalRevenue).to_frame('receivable_on_rev')\n",
    "    \n",
    "#     # ROA\n",
    "#     roa = (profit / market_asset).to_frame('roa')\n",
    "    \n",
    "#     # ROE\n",
    "#     roe = (profit / market_equity).to_frame('roe')\n",
    "    \n",
    "#     # Long-term debt / minEBIT\n",
    "#     min_ebit = min_in_window(ebit,min_ebit_window)['min']\n",
    "#     debt_on_min_ebit = (df_final.longTermDebt / min_ebit).to_frame('debt_on_min_ebit')\n",
    "    \n",
    "#     # Total Liabilities / Total Equity\n",
    "#     liabilities_on_equity = (df_final.totalLiabilities / market_equity).to_frame('liabilities_on_equity')\n",
    "    \n",
    "#     # Capital Expenditures / EBIT\n",
    "#     capital_cost_on_ebit = (df_final.capitalExpenditures / ebit).to_frame('capital_cost_on_ebit')\n",
    "    \n",
    "#     # EPS / MP\n",
    "#     eps = profit / df_final.commonStockSharesOutstanding\n",
    "#     eps_on_mp = (eps / df_final.close).to_frame('eps_on_mp')\n",
    "    \n",
    "#     # Cash and Stock Dividend & Repurchase Common / MP\n",
    "#     dividend_on_mp = ((df_final.dividendPayoutCommonStock + df_final.paymentsForRepurchaseOfCommonStock) / df_final.close).to_frame('dividend_on_mp')\n",
    "    \n",
    "#     # MP / BV\n",
    "#     mp_on_bv = (df_final.close / (df_final.cashAndShortTermInvestments + df_final.currentNetReceivables*0.8 +df_final.inventory*0.5 + df_final.propertyPlantEquipment*0.2 + df_final.longTermInvestments - df_final.totalLiabilities)).to_frame('mp_on_bv')\n",
    "\n",
    "#     # Create a dataframe that merges all the ratios\n",
    "#     ratios = pd.concat([df_final.date,df_final.tic,df_final.close,gross_profit_margin,sga_ratio,dep_ratio,\n",
    "#                     ebit_on_int,profit_margin,count_positive_profit,csti_on_liabilities,\n",
    "#                     inventory_on_ebit,receivable_on_rev,roa,roe,debt_on_min_ebit,\n",
    "#                    liabilities_on_equity,capital_cost_on_ebit,eps_on_mp,dividend_on_mp,mp_on_bv], axis=1)\n",
    "\n",
    "#     return ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios = compute_fund_ratios(fund_data)\n",
    "# ratios.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check columns with inf values\n",
    "\n",
    "# check_ratios = ratios[ratio_list]\n",
    "# inf_cols = check_ratios.columns[~np.isfinite(check_ratios).all()]\n",
    "# inf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check rows with inf values\n",
    "\n",
    "# print(f'{ratios.info()}\\n')\n",
    "# print(ratios.isna().sum())\n",
    "\n",
    "# ratio_inf = ratios.isin([np.inf,-np.inf])\n",
    "# filtered_inf = ratio_inf[(ratio_inf == True).any(axis=1)]\n",
    "# print(len(filtered_inf))\n",
    "\n",
    "# ratio_list = ratios.columns.drop(['date','tic','close'])\n",
    "# count_inf = np.isinf(ratios[ratio_list]).values.sum()\n",
    "\n",
    "# ratio_inf = ratios[((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]\n",
    "# print(len(ratios[((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **ebit_on_int** has infinity values for companies without banking support. We need to address these cases separately: <br>\n",
    " * Replace positive inf with maximum value in ebit_on_int\n",
    " * Replace negative inf with minimum value in ebit_on_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio_finite = ratios[~((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]\n",
    "# ratios['ebit_on_int'] = ratios.ebit_on_int.replace(np.inf,ratio_finite.ebit_on_int.max())\n",
    "# ratios['ebit_on_int'] = ratios.ebit_on_int.replace(-np.inf,ratio_finite.ebit_on_int.min())\n",
    "# print(len(ratios[((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While data processing greatly reduces the number of inf values, some still remain.<br>\n",
    "Since the remain inf values represent a small portion of the data, we can safely remove the corresponding rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_ratios = ratios[ratio_list]\n",
    "# inf_cols = check_ratios.columns[~np.isfinite(check_ratios).all()]\n",
    "# inf_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratios = ratios[~((ratios == np.inf)|(ratios == -np.inf)).any(axis=1)]\n",
    "# ratios.reset_index(inplace=True,drop=True)\n",
    "# ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.3'></a>\n",
    "## 4.3 Sort dataset based on date and tic columns\n",
    "The data is prepared for training, however the data points are not ordered chronologically.<br>\n",
    "This step ensure the agent travel through the data in a year-by-year order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_full = ratios.sort_values(by=['date','tic']).reset_index(drop=True)\n",
    "# processed_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_full.to_csv('./' + DATA_SAVE_DIR + '/sp500_ready_data_yearly.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3.4'></a>\n",
    "## 4.4 Finish data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# processed_full = final_ratios.copy()\n",
    "\n",
    "# If the data is available in the data storage, load processed_full from readied data\n",
    "processed_full = pd.read_csv('./' + DATA_SAVE_DIR + '/sp500_ready_data_yearly.csv',index_col=0)\n",
    "processed_full['date'] = pd.to_datetime(processed_full.date,format='mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date                     0\n",
      "tic                      0\n",
      "close                    0\n",
      "gross_profit_margin      0\n",
      "sga_ratio                0\n",
      "dep_ratio                0\n",
      "ebit_on_int              0\n",
      "profit_margin            0\n",
      "count_positive_profit    0\n",
      "csti_on_liabilities      0\n",
      "inventory_on_ebit        0\n",
      "receivable_on_rev        0\n",
      "roa                      0\n",
      "roe                      0\n",
      "debt_on_min_ebit         0\n",
      "liabilities_on_equity    0\n",
      "capital_cost_on_ebit     0\n",
      "eps_on_mp                0\n",
      "dividend_on_mp           0\n",
      "mp_on_bv                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "processed_full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "processed_full = processed_full.fillna(0)\n",
    "print(processed_full.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset **TRAIN_START_DATE** and **TEST_END_DATE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN_START_DATE:  2009-09-30 \n",
      "\n",
      "TRAIN_END_DATE:  2021-01-01 \n",
      "\n",
      "TEST_START_DATE:  2021-01-01 \n",
      "\n",
      "TEST_END_DATE:  2024-02-03 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "TRAIN_START_DATE = processed_full.date.min().strftime(\"%Y-%m-%d\")\n",
    "TEST_END_DATE = processed_full.date.max().strftime(\"%Y-%m-%d\")\n",
    "print('TRAIN_START_DATE: ',TRAIN_START_DATE,'\\n')\n",
    "print('TRAIN_END_DATE: ',TRAIN_END_DATE,'\\n')\n",
    "print('TEST_START_DATE: ',TEST_START_DATE,'\\n')\n",
    "print('TEST_END_DATE: ',TEST_END_DATE,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.1'></a>\n",
    "## 5.1 Training & Trade Data Split\n",
    "- Training data period: 2009-01-01 to 2020-01-01\n",
    "- Trade data period: 2020-01-01 to 2023-12-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5251\n",
      "1483\n"
     ]
    }
   ],
   "source": [
    "train_data = data_split(processed_full, TRAIN_START_DATE, TRAIN_END_DATE)\n",
    "trade_data = data_split(processed_full, TEST_START_DATE, TEST_END_DATE)\n",
    "# Check the length of the two datasets\n",
    "print(len(train_data))\n",
    "print(len(trade_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>gross_profit_margin</th>\n",
       "      <th>sga_ratio</th>\n",
       "      <th>dep_ratio</th>\n",
       "      <th>ebit_on_int</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>count_positive_profit</th>\n",
       "      <th>csti_on_liabilities</th>\n",
       "      <th>inventory_on_ebit</th>\n",
       "      <th>receivable_on_rev</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>debt_on_min_ebit</th>\n",
       "      <th>liabilities_on_equity</th>\n",
       "      <th>capital_cost_on_ebit</th>\n",
       "      <th>eps_on_mp</th>\n",
       "      <th>dividend_on_mp</th>\n",
       "      <th>mp_on_bv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>5.604187</td>\n",
       "      <td>0.304696</td>\n",
       "      <td>0.317372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.374233</td>\n",
       "      <td>0.111106</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.479352</td>\n",
       "      <td>0.050986</td>\n",
       "      <td>0.078336</td>\n",
       "      <td>0.228046</td>\n",
       "      <td>0.945331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.145352</td>\n",
       "      <td>0.128194</td>\n",
       "      <td>0.945331</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.618375e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>APD</td>\n",
       "      <td>50.297554</td>\n",
       "      <td>0.103254</td>\n",
       "      <td>1.123229</td>\n",
       "      <td>0.971425</td>\n",
       "      <td>-0.849057</td>\n",
       "      <td>-0.050490</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.060278</td>\n",
       "      <td>-4.923671</td>\n",
       "      <td>0.167587</td>\n",
       "      <td>-0.021933</td>\n",
       "      <td>-0.038651</td>\n",
       "      <td>-10.575235</td>\n",
       "      <td>0.762205</td>\n",
       "      <td>-11.392271</td>\n",
       "      <td>-0.038651</td>\n",
       "      <td>7.421832e+06</td>\n",
       "      <td>-1.249467e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>BDX</td>\n",
       "      <td>53.350754</td>\n",
       "      <td>0.283421</td>\n",
       "      <td>0.832864</td>\n",
       "      <td>0.154761</td>\n",
       "      <td>8.351160</td>\n",
       "      <td>-0.016058</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.467527</td>\n",
       "      <td>3.429526</td>\n",
       "      <td>0.164127</td>\n",
       "      <td>-0.006757</td>\n",
       "      <td>-0.008962</td>\n",
       "      <td>-14.966166</td>\n",
       "      <td>0.326203</td>\n",
       "      <td>1.752481</td>\n",
       "      <td>-0.008962</td>\n",
       "      <td>1.624875e+07</td>\n",
       "      <td>-4.873523e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>BEN</td>\n",
       "      <td>20.057692</td>\n",
       "      <td>0.296393</td>\n",
       "      <td>0.864485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.671970</td>\n",
       "      <td>-0.052366</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.580349</td>\n",
       "      <td>3.733393</td>\n",
       "      <td>0.138721</td>\n",
       "      <td>-0.034486</td>\n",
       "      <td>-0.047748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384568</td>\n",
       "      <td>0.268215</td>\n",
       "      <td>-0.047748</td>\n",
       "      <td>2.840262e+07</td>\n",
       "      <td>4.786678e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-09-30</td>\n",
       "      <td>EMR</td>\n",
       "      <td>26.482727</td>\n",
       "      <td>0.151422</td>\n",
       "      <td>1.394380</td>\n",
       "      <td>0.171456</td>\n",
       "      <td>-5.677273</td>\n",
       "      <td>-0.103132</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.139186</td>\n",
       "      <td>-1.485188</td>\n",
       "      <td>0.173225</td>\n",
       "      <td>-0.069313</td>\n",
       "      <td>-0.108329</td>\n",
       "      <td>-3.654123</td>\n",
       "      <td>0.562887</td>\n",
       "      <td>0.425140</td>\n",
       "      <td>-0.108329</td>\n",
       "      <td>6.479695e+07</td>\n",
       "      <td>-5.170287e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5246</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>YUM</td>\n",
       "      <td>102.310020</td>\n",
       "      <td>0.462668</td>\n",
       "      <td>0.488337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.385027</td>\n",
       "      <td>0.116950</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.053118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100672</td>\n",
       "      <td>0.014807</td>\n",
       "      <td>0.021393</td>\n",
       "      <td>15.989537</td>\n",
       "      <td>0.444791</td>\n",
       "      <td>0.119581</td>\n",
       "      <td>0.021393</td>\n",
       "      <td>7.868242e+06</td>\n",
       "      <td>-8.331571e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>ZBH</td>\n",
       "      <td>146.273346</td>\n",
       "      <td>-0.028587</td>\n",
       "      <td>-13.975786</td>\n",
       "      <td>-1.990211</td>\n",
       "      <td>-13.711321</td>\n",
       "      <td>-0.445204</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.065647</td>\n",
       "      <td>-0.843092</td>\n",
       "      <td>0.213956</td>\n",
       "      <td>-0.070983</td>\n",
       "      <td>-0.099544</td>\n",
       "      <td>-3.137127</td>\n",
       "      <td>0.402363</td>\n",
       "      <td>-0.040422</td>\n",
       "      <td>-0.099544</td>\n",
       "      <td>1.357048e+06</td>\n",
       "      <td>-1.697074e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>ZBRA</td>\n",
       "      <td>384.329987</td>\n",
       "      <td>0.217252</td>\n",
       "      <td>0.831045</td>\n",
       "      <td>0.071806</td>\n",
       "      <td>2.105263</td>\n",
       "      <td>0.006423</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.075828</td>\n",
       "      <td>3.193750</td>\n",
       "      <td>0.120211</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>151.272727</td>\n",
       "      <td>0.157482</td>\n",
       "      <td>0.418750</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>5.203861e+05</td>\n",
       "      <td>-1.703213e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>ZION</td>\n",
       "      <td>38.849308</td>\n",
       "      <td>-2.805100</td>\n",
       "      <td>-0.147403</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-11.625000</td>\n",
       "      <td>-3.737705</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.021972</td>\n",
       "      <td>4.474250</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.025660</td>\n",
       "      <td>-0.321893</td>\n",
       "      <td>-0.677485</td>\n",
       "      <td>11.544393</td>\n",
       "      <td>-0.096774</td>\n",
       "      <td>-0.321893</td>\n",
       "      <td>8.623062e+06</td>\n",
       "      <td>-6.680166e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5250</th>\n",
       "      <td>2020-12-31</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>161.550873</td>\n",
       "      <td>0.388464</td>\n",
       "      <td>0.665638</td>\n",
       "      <td>0.081373</td>\n",
       "      <td>3.753247</td>\n",
       "      <td>0.041348</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.366409</td>\n",
       "      <td>1.877739</td>\n",
       "      <td>0.158951</td>\n",
       "      <td>0.003186</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>10.692042</td>\n",
       "      <td>0.128093</td>\n",
       "      <td>0.522491</td>\n",
       "      <td>0.003594</td>\n",
       "      <td>3.899700e+06</td>\n",
       "      <td>-3.912780e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5251 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   tic       close  gross_profit_margin  sga_ratio  dep_ratio  \\\n",
       "0    2009-09-30  AAPL    5.604187             0.304696   0.317372   0.000000   \n",
       "1    2009-09-30   APD   50.297554             0.103254   1.123229   0.971425   \n",
       "2    2009-09-30   BDX   53.350754             0.283421   0.832864   0.154761   \n",
       "3    2009-09-30   BEN   20.057692             0.296393   0.864485   0.000000   \n",
       "4    2009-09-30   EMR   26.482727             0.151422   1.394380   0.171456   \n",
       "...         ...   ...         ...                  ...        ...        ...   \n",
       "5246 2020-12-31   YUM  102.310020             0.462668   0.488337   0.000000   \n",
       "5247 2020-12-31   ZBH  146.273346            -0.028587 -13.975786  -1.990211   \n",
       "5248 2020-12-31  ZBRA  384.329987             0.217252   0.831045   0.071806   \n",
       "5249 2020-12-31  ZION   38.849308            -2.805100  -0.147403  -0.000000   \n",
       "5250 2020-12-31   ZTS  161.550873             0.388464   0.665638   0.081373   \n",
       "\n",
       "      ebit_on_int  profit_margin  count_positive_profit  csti_on_liabilities  \\\n",
       "0       27.374233       0.111106                   10.0             1.479352   \n",
       "1       -0.849057      -0.050490                    5.0             0.060278   \n",
       "2        8.351160      -0.016058                    3.0             0.467527   \n",
       "3       44.671970      -0.052366                    7.0             2.580349   \n",
       "4       -5.677273      -0.103132                    3.0             0.139186   \n",
       "...           ...            ...                    ...                  ...   \n",
       "5246     2.385027       0.116950                   10.0             0.053118   \n",
       "5247   -13.711321      -0.445204                    6.0             0.065647   \n",
       "5248     2.105263       0.006423                    9.0             0.075828   \n",
       "5249   -11.625000      -3.737705                    1.0             0.021972   \n",
       "5250     3.753247       0.041348                   10.0             0.366409   \n",
       "\n",
       "      inventory_on_ebit  receivable_on_rev       roa       roe  \\\n",
       "0              0.050986           0.078336  0.228046  0.945331   \n",
       "1             -4.923671           0.167587 -0.021933 -0.038651   \n",
       "2              3.429526           0.164127 -0.006757 -0.008962   \n",
       "3              3.733393           0.138721 -0.034486 -0.047748   \n",
       "4             -1.485188           0.173225 -0.069313 -0.108329   \n",
       "...                 ...                ...       ...       ...   \n",
       "5246           0.000000           0.100672  0.014807  0.021393   \n",
       "5247          -0.843092           0.213956 -0.070983 -0.099544   \n",
       "5248           3.193750           0.120211  0.001179  0.001365   \n",
       "5249           4.474250           0.000000 -0.025660 -0.321893   \n",
       "5250           1.877739           0.158951  0.003186  0.003594   \n",
       "\n",
       "      debt_on_min_ebit  liabilities_on_equity  capital_cost_on_ebit  \\\n",
       "0             0.000000               3.145352              0.128194   \n",
       "1           -10.575235               0.762205            -11.392271   \n",
       "2           -14.966166               0.326203              1.752481   \n",
       "3             0.000000               0.384568              0.268215   \n",
       "4            -3.654123               0.562887              0.425140   \n",
       "...                ...                    ...                   ...   \n",
       "5246         15.989537               0.444791              0.119581   \n",
       "5247         -3.137127               0.402363             -0.040422   \n",
       "5248        151.272727               0.157482              0.418750   \n",
       "5249         -0.677485              11.544393             -0.096774   \n",
       "5250         10.692042               0.128093              0.522491   \n",
       "\n",
       "      eps_on_mp  dividend_on_mp      mp_on_bv  \n",
       "0      0.945331    0.000000e+00  2.618375e-10  \n",
       "1     -0.038651    7.421832e+06 -1.249467e-08  \n",
       "2     -0.008962    1.624875e+07 -4.873523e-07  \n",
       "3     -0.047748    2.840262e+07  4.786678e-09  \n",
       "4     -0.108329    6.479695e+07 -5.170287e-09  \n",
       "...         ...             ...           ...  \n",
       "5246   0.021393    7.868242e+06 -8.331571e-09  \n",
       "5247  -0.099544    1.357048e+06 -1.697074e-08  \n",
       "5248   0.001365    5.203861e+05 -1.703213e-07  \n",
       "5249  -0.321893    8.623062e+06 -6.680166e-10  \n",
       "5250   0.003594    3.899700e+06 -3.912780e-08  \n",
       "\n",
       "[5251 rows x 20 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.reset_index(drop=True)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>close</th>\n",
       "      <th>gross_profit_margin</th>\n",
       "      <th>sga_ratio</th>\n",
       "      <th>dep_ratio</th>\n",
       "      <th>ebit_on_int</th>\n",
       "      <th>profit_margin</th>\n",
       "      <th>count_positive_profit</th>\n",
       "      <th>csti_on_liabilities</th>\n",
       "      <th>inventory_on_ebit</th>\n",
       "      <th>receivable_on_rev</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>debt_on_min_ebit</th>\n",
       "      <th>liabilities_on_equity</th>\n",
       "      <th>capital_cost_on_ebit</th>\n",
       "      <th>eps_on_mp</th>\n",
       "      <th>dividend_on_mp</th>\n",
       "      <th>mp_on_bv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>DG</td>\n",
       "      <td>188.258728</td>\n",
       "      <td>0.101251</td>\n",
       "      <td>2.106047</td>\n",
       "      <td>0.167358</td>\n",
       "      <td>-25.018566</td>\n",
       "      <td>-0.138769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.071692</td>\n",
       "      <td>-1.394709</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>-0.072246</td>\n",
       "      <td>-0.102849</td>\n",
       "      <td>-0.911556</td>\n",
       "      <td>0.423592</td>\n",
       "      <td>-0.273219</td>\n",
       "      <td>-0.102849</td>\n",
       "      <td>1.499192e+07</td>\n",
       "      <td>-1.312048e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>LOW</td>\n",
       "      <td>157.945999</td>\n",
       "      <td>0.088005</td>\n",
       "      <td>2.401296</td>\n",
       "      <td>0.194426</td>\n",
       "      <td>-12.397936</td>\n",
       "      <td>-0.154988</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.114707</td>\n",
       "      <td>-1.497826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.084519</td>\n",
       "      <td>-0.117679</td>\n",
       "      <td>-3.926001</td>\n",
       "      <td>0.392331</td>\n",
       "      <td>-0.165665</td>\n",
       "      <td>-0.117679</td>\n",
       "      <td>4.226128e+07</td>\n",
       "      <td>-5.646070e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-30</td>\n",
       "      <td>BBWI</td>\n",
       "      <td>32.094421</td>\n",
       "      <td>0.073265</td>\n",
       "      <td>1.793269</td>\n",
       "      <td>0.626202</td>\n",
       "      <td>-1.527778</td>\n",
       "      <td>-0.118792</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.319055</td>\n",
       "      <td>-0.866667</td>\n",
       "      <td>0.023688</td>\n",
       "      <td>-0.063767</td>\n",
       "      <td>-0.151195</td>\n",
       "      <td>-7.480611</td>\n",
       "      <td>1.371067</td>\n",
       "      <td>-0.345455</td>\n",
       "      <td>-0.151195</td>\n",
       "      <td>2.586119e+06</td>\n",
       "      <td>-4.208884e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-30</td>\n",
       "      <td>BBY</td>\n",
       "      <td>95.633957</td>\n",
       "      <td>0.223710</td>\n",
       "      <td>0.749834</td>\n",
       "      <td>0.071787</td>\n",
       "      <td>50.865385</td>\n",
       "      <td>0.042614</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.379420</td>\n",
       "      <td>2.121739</td>\n",
       "      <td>0.022449</td>\n",
       "      <td>0.051564</td>\n",
       "      <td>0.081943</td>\n",
       "      <td>0.733642</td>\n",
       "      <td>0.589147</td>\n",
       "      <td>0.269565</td>\n",
       "      <td>0.081943</td>\n",
       "      <td>9.201752e+06</td>\n",
       "      <td>-1.960034e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-30</td>\n",
       "      <td>DLTR</td>\n",
       "      <td>100.860001</td>\n",
       "      <td>0.305277</td>\n",
       "      <td>0.757685</td>\n",
       "      <td>0.081041</td>\n",
       "      <td>589.687500</td>\n",
       "      <td>0.058249</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.105640</td>\n",
       "      <td>1.907631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040214</td>\n",
       "      <td>0.063125</td>\n",
       "      <td>1.792333</td>\n",
       "      <td>0.569722</td>\n",
       "      <td>0.476312</td>\n",
       "      <td>0.063125</td>\n",
       "      <td>4.134444e+06</td>\n",
       "      <td>-1.076312e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>ZTS</td>\n",
       "      <td>196.125183</td>\n",
       "      <td>0.415964</td>\n",
       "      <td>0.605234</td>\n",
       "      <td>0.086100</td>\n",
       "      <td>5.870293</td>\n",
       "      <td>0.066479</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.219580</td>\n",
       "      <td>1.827512</td>\n",
       "      <td>0.159410</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>6.550249</td>\n",
       "      <td>0.103396</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.006318</td>\n",
       "      <td>9.096231e+06</td>\n",
       "      <td>-4.623849e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>HD</td>\n",
       "      <td>353.585266</td>\n",
       "      <td>0.333794</td>\n",
       "      <td>0.521939</td>\n",
       "      <td>0.059262</td>\n",
       "      <td>12.538343</td>\n",
       "      <td>0.115531</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.049811</td>\n",
       "      <td>0.861013</td>\n",
       "      <td>0.021799</td>\n",
       "      <td>0.041380</td>\n",
       "      <td>0.050286</td>\n",
       "      <td>4.355689</td>\n",
       "      <td>0.215209</td>\n",
       "      <td>0.132419</td>\n",
       "      <td>0.050286</td>\n",
       "      <td>4.619536e+07</td>\n",
       "      <td>-6.628299e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>624.620728</td>\n",
       "      <td>0.727176</td>\n",
       "      <td>0.059908</td>\n",
       "      <td>0.020180</td>\n",
       "      <td>162.050584</td>\n",
       "      <td>0.612784</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.142154</td>\n",
       "      <td>0.126828</td>\n",
       "      <td>0.164128</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.024256</td>\n",
       "      <td>3.124879</td>\n",
       "      <td>0.014782</td>\n",
       "      <td>0.025668</td>\n",
       "      <td>0.024256</td>\n",
       "      <td>1.589445e+07</td>\n",
       "      <td>3.854970e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>CRM</td>\n",
       "      <td>280.723297</td>\n",
       "      <td>0.171212</td>\n",
       "      <td>2.581407</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-325.551724</td>\n",
       "      <td>-0.294932</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.355278</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.327339</td>\n",
       "      <td>-0.032882</td>\n",
       "      <td>-0.037728</td>\n",
       "      <td>-0.998411</td>\n",
       "      <td>0.147394</td>\n",
       "      <td>-0.077958</td>\n",
       "      <td>-0.037728</td>\n",
       "      <td>2.989421e+07</td>\n",
       "      <td>-2.463176e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>WMT</td>\n",
       "      <td>54.896366</td>\n",
       "      <td>0.243754</td>\n",
       "      <td>0.829020</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.449516</td>\n",
       "      <td>0.029082</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.108676</td>\n",
       "      <td>2.032134</td>\n",
       "      <td>0.013571</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>0.014171</td>\n",
       "      <td>-6.715134</td>\n",
       "      <td>0.126712</td>\n",
       "      <td>0.762846</td>\n",
       "      <td>0.014171</td>\n",
       "      <td>1.624698e+08</td>\n",
       "      <td>-5.866425e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1483 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date   tic       close  gross_profit_margin  sga_ratio  dep_ratio  \\\n",
       "0    2021-01-29    DG  188.258728             0.101251   2.106047   0.167358   \n",
       "1    2021-01-29   LOW  157.945999             0.088005   2.401296   0.194426   \n",
       "2    2021-01-30  BBWI   32.094421             0.073265   1.793269   0.626202   \n",
       "3    2021-01-30   BBY   95.633957             0.223710   0.749834   0.071787   \n",
       "4    2021-01-30  DLTR  100.860001             0.305277   0.757685   0.081041   \n",
       "...         ...   ...         ...                  ...        ...        ...   \n",
       "1478 2023-12-31   ZTS  196.125183             0.415964   0.605234   0.086100   \n",
       "1479 2024-01-28    HD  353.585266             0.333794   0.521939   0.059262   \n",
       "1480 2024-01-28  NVDA  624.620728             0.727176   0.059908   0.020180   \n",
       "1481 2024-01-31   CRM  280.723297             0.171212   2.581407   0.000000   \n",
       "1482 2024-01-31   WMT   54.896366             0.243754   0.829020   0.000000   \n",
       "\n",
       "      ebit_on_int  profit_margin  count_positive_profit  csti_on_liabilities  \\\n",
       "0      -25.018566      -0.138769                    1.0             0.071692   \n",
       "1      -12.397936      -0.154988                    8.0             0.114707   \n",
       "2       -1.527778      -0.118792                    0.0             0.319055   \n",
       "3       50.865385       0.042614                   10.0             0.379420   \n",
       "4      589.687500       0.058249                   10.0             0.105640   \n",
       "...           ...            ...                    ...                  ...   \n",
       "1478     5.870293       0.066479                   10.0             0.219580   \n",
       "1479    12.538343       0.115531                    7.0             0.049811   \n",
       "1480   162.050584       0.612784                   10.0             1.142154   \n",
       "1481  -325.551724      -0.294932                    9.0             0.355278   \n",
       "1482    10.449516       0.029082                    1.0             0.108676   \n",
       "\n",
       "      inventory_on_ebit  receivable_on_rev       roa       roe  \\\n",
       "0             -1.394709           0.002701 -0.072246 -0.102849   \n",
       "1             -1.497826           0.000000 -0.084519 -0.117679   \n",
       "2             -0.866667           0.023688 -0.063767 -0.151195   \n",
       "3              2.121739           0.022449  0.051564  0.081943   \n",
       "4              1.907631           0.000000  0.040214  0.063125   \n",
       "...                 ...                ...       ...       ...   \n",
       "1478           1.827512           0.159410  0.005726  0.006318   \n",
       "1479           0.861013           0.021799  0.041380  0.050286   \n",
       "1480           0.126828           0.164128  0.023903  0.024256   \n",
       "1481          -0.000000           0.327339 -0.032882 -0.037728   \n",
       "1482           2.032134           0.013571  0.012578  0.014171   \n",
       "\n",
       "      debt_on_min_ebit  liabilities_on_equity  capital_cost_on_ebit  \\\n",
       "0            -0.911556               0.423592             -0.273219   \n",
       "1            -3.926001               0.392331             -0.165665   \n",
       "2            -7.480611               1.371067             -0.345455   \n",
       "3             0.733642               0.589147              0.269565   \n",
       "4             1.792333               0.569722              0.476312   \n",
       "...                ...                    ...                   ...   \n",
       "1478          6.550249               0.103396              0.521739   \n",
       "1479          4.355689               0.215209              0.132419   \n",
       "1480          3.124879               0.014782              0.025668   \n",
       "1481         -0.998411               0.147394             -0.077958   \n",
       "1482         -6.715134               0.126712              0.762846   \n",
       "\n",
       "      eps_on_mp  dividend_on_mp      mp_on_bv  \n",
       "0     -0.102849    1.499192e+07 -1.312048e-08  \n",
       "1     -0.117679    4.226128e+07 -5.646070e-09  \n",
       "2     -0.151195    2.586119e+06 -4.208884e-09  \n",
       "3      0.081943    9.201752e+06 -1.960034e-08  \n",
       "4      0.063125    4.134444e+06 -1.076312e-08  \n",
       "...         ...             ...           ...  \n",
       "1478   0.006318    9.096231e+06 -4.623849e-08  \n",
       "1479   0.050286    4.619536e+07 -6.628299e-09  \n",
       "1480   0.024256    1.589445e+07  3.854970e-08  \n",
       "1481  -0.037728    2.989421e+07 -2.463176e-08  \n",
       "1482   0.014171    1.624698e+08 -5.866425e-10  \n",
       "\n",
       "[1483 rows x 20 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trade_data = trade_data.reset_index(drop=True)\n",
    "trade_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.2'></a>\n",
    "## 5.2 Set up the training environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import seeding\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "# from gym.spaces import Box\n",
    "\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# from stable_baselines3.common import logger\n",
    "\n",
    "\n",
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"A stock trading environment for OpenAI gym\"\"\"\n",
    "\n",
    "    metadata = {\"render.modes\": [\"human\"]}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df,\n",
    "        # stock_dim,\n",
    "        hmax,\n",
    "        initial_amount,\n",
    "        buy_cost_pct,\n",
    "        sell_cost_pct,\n",
    "        reward_scaling,\n",
    "        state_space,\n",
    "        action_space,\n",
    "        tech_indicator_list,\n",
    "        stop_loss,\n",
    "        punishment_rate,\n",
    "        make_plots=False,\n",
    "        print_verbosity=10,\n",
    "        row=0,\n",
    "        initial=True,\n",
    "        previous_state=[],\n",
    "        model_name=\"\",\n",
    "        mode=\"\",\n",
    "        iteration=\"\",\n",
    "    ):\n",
    "        # self.row = row\n",
    "        self.df = df\n",
    "        # self.stock_dim = stock_dim\n",
    "        self.hmax = hmax\n",
    "        self.initial_amount = initial_amount\n",
    "        self.buy_cost_pct = buy_cost_pct\n",
    "        self.sell_cost_pct = sell_cost_pct\n",
    "        self.reward_scaling = reward_scaling\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.tech_indicator_list = tech_indicator_list\n",
    "        self.punishment_rate = punishment_rate\n",
    "        self.stop_loss = stop_loss # the game stops when the asset loses more than stop_loss percent\n",
    "        self.action_space = spaces.Box(low=-1, high=1, shape=(self.action_space,))\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(self.state_space,)\n",
    "        )\n",
    "        # self.data = self.df.loc[self.row, :]\n",
    "        self.terminal = False\n",
    "        self.make_plots = make_plots\n",
    "        self.print_verbosity = print_verbosity\n",
    "        # self.turbulence_threshold = turbulence_threshold\n",
    "        # self.risk_indicator_col = risk_indicator_col\n",
    "        self.initial = initial\n",
    "        self.previous_state = previous_state\n",
    "        self.model_name = model_name\n",
    "        self.mode = mode\n",
    "        self.iteration = iteration\n",
    "        self.portfolio_columns = ['tic','price','buy_price','amount','weight']\n",
    "        self.row = 0\n",
    "        \n",
    "        # initalize state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # initialize reward\n",
    "        self.reward = 0\n",
    "        self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.episode = 0\n",
    "        \n",
    "        # memorize all the total balance change\n",
    "        self.asset_memory = [self.initial_amount]\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "        # self.reset()\n",
    "        self._seed()\n",
    "\n",
    "    def _buy_stock(self, action):\n",
    "        def _do_buy():\n",
    "            if self.data.close > 0: # Buy only if the price is > 0 (no missing data in this particular date)\n",
    "                buy_amount = (self.portfolio.loc[0].price * action)\n",
    "                buy_num_shares = math.floor(buy_amount/self.data.close)\n",
    "                if buy_num_shares > 0:\n",
    "                    if self.portfolio[self.portfolio.tic == self.data.tic].empty:\n",
    "                        selected_index = len(self.portfolio)\n",
    "                        selected_row = [self.data.tic,0,self.data.close,0,0]\n",
    "                    else:\n",
    "                        selected_index = self.portfolio[self.portfolio.tic == self.data.tic].index[0]\n",
    "                        selected_row = self.portfolio.loc[selected_index]\n",
    "                        selected_row[2] = (buy_num_shares*self.data.close + selected_row[2]*selected_row[3]) \\\n",
    "                                    /(buy_num_shares + selected_row[3])\n",
    "                    selected_row[1] = self.data.close\n",
    "                    selected_row[3] += buy_num_shares\n",
    "                    self.portfolio.loc[selected_index] = selected_row\n",
    "    \n",
    "                    # Update remain capital\n",
    "                    capital_row = self.portfolio.loc[0]\n",
    "                    capital_change = buy_num_shares * self.data.close * (1 + self.buy_cost_pct)\n",
    "                    capital_row[1] -= capital_change\n",
    "                    capital_row[2] -= capital_change\n",
    "                    self.portfolio.loc[0] = capital_row\n",
    "                    \n",
    "                    self._compute_weight()\n",
    "                    self.cost += selected_row[1] * buy_num_shares * self.buy_cost_pct\n",
    "                    self.trades += 1\n",
    "                else:\n",
    "                    # Punish a certain amount of money if buying without avaiable capital\n",
    "                    self.reward = -1 * self.initial_amount * self.punishment_rate\n",
    "                    # print(f'Set punishment for unavailable buying: {self.reward}')\n",
    "                    \n",
    "            else:\n",
    "                buy_num_shares = 0\n",
    "\n",
    "            return buy_num_shares\n",
    "\n",
    "        buy_num_shares = _do_buy()\n",
    "        return buy_num_shares\n",
    "    \n",
    "    def _sell_stock(self, action):\n",
    "        def _do_sell_normal():\n",
    "            # TODO: Punish a certain amount of money if selling without avaiable stock in the inventory\n",
    "            if self.data.close > 0: # Sell only if the price is > 0 (no missing data in this particular date)\n",
    "                # perform sell action based on the sign of the action\n",
    "                if self.portfolio[self.portfolio.tic == self.data.tic].empty:            \n",
    "                    sell_num_shares = 0\n",
    "                    \n",
    "                    # Punish a certain amount of money if selling without avaiable stock in the inventory\n",
    "                    self.reward = -1 * self.initial_amount * self.punishment_rate\n",
    "                \n",
    "                else:\n",
    "                    sell_num_shares = math.floor(abs(action) * self.portfolio[self.portfolio.tic == self.data.tic].amount) \n",
    "                    sell_amount = self.data.close * sell_num_shares * (1 - self.sell_cost_pct)\n",
    "\n",
    "                    # Update reward only when closing a deal\n",
    "                    # buy_amount = self.portfolio[self.portfolio.tic == self.data.tic].buy_price * sell_num_shares\n",
    "                    # self.reward = (sell_amount - buy_amount).values[0].item()\n",
    "                    \n",
    "                    # update stock row in the portfolio\n",
    "                    selected_index = self.portfolio[self.portfolio.tic == self.data.tic].index[0]\n",
    "                    selected_row = self.portfolio.loc[selected_index]\n",
    "                    selected_row[1] = self.data.close\n",
    "                    selected_row[3] -= sell_num_shares\n",
    "\n",
    "                    # Update remain capital\n",
    "                    capital_row = self.portfolio.loc[0]\n",
    "                    capital_row[1] += sell_amount\n",
    "                    capital_row[2] += sell_amount\n",
    "\n",
    "                    # Update changes to portfolio\n",
    "                    self.portfolio.loc[selected_index] = selected_row\n",
    "                    self.portfolio.loc[0] = capital_row\n",
    "                    self._compute_weight()\n",
    "                    self.cost += selected_row[1] * sell_num_shares * self.sell_cost_pct\n",
    "                    self.trades += 1\n",
    "            else:\n",
    "                sell_num_shares = 0\n",
    "\n",
    "            return sell_num_shares\n",
    "\n",
    "        sell_num_shares = _do_sell_normal()\n",
    "        return sell_num_shares\n",
    "\n",
    "    def _compute_weight(self):\n",
    "        nav = sum(self.portfolio.price*self.portfolio.amount)\n",
    "        self.portfolio['weight'] = self.portfolio.apply(lambda x: x.price * x.amount / nav, axis=1)\n",
    "    \n",
    "    # def _make_plot(self):\n",
    "    #     plt.plot(self.asset_memory, \"r\")\n",
    "    #     plt.savefig(\"results/account_value_trade_{}.png\".format(self.episode))\n",
    "    #     plt.close()\n",
    "\n",
    "    def step(self, actions):\n",
    "\n",
    "        current_total_asset = sum(self.portfolio.price * self.portfolio.amount)\n",
    "        self.terminal = (self.row >= len(self.df.index.unique()) - 1) | (current_total_asset < self.initial_amount*(1-self.stop_loss))\n",
    "        # print(f'Action of step {self.row} is {actions[0]}')\n",
    "        \n",
    "        # --> IN CASE THE STEP IS THE TERMINATED STEP\n",
    "        if self.terminal: \n",
    "            print(f\"Episode: {self.episode}\")\n",
    "            # if self.make_plots:\n",
    "            #     self._make_plot()\n",
    "                \n",
    "            # Summary the training performance after an episode\n",
    "            end_total_asset = sum(self.portfolio.price*self.portfolio.amount)\n",
    "            tot_reward = end_total_asset - self.initial_amount\n",
    "\n",
    "            # Summary total_value\n",
    "            # df_total_value = pd.DataFrame(self.asset_memory)\n",
    "            # df_total_value.columns = [\"account_value\"]\n",
    "            # df_total_value[\"date\"] = self.date_memory\n",
    "            # df_total_value[\"daily_return\"] = df_total_value[\"account_value\"].pct_change(1)\n",
    "            # if df_total_value[\"daily_return\"].std() != 0:\n",
    "            #     sharpe = ((252 ** 0.5)* df_total_value[\"daily_return\"].mean()/ df_total_value[\"daily_return\"].std())\n",
    "\n",
    "            # Take tot_reward into account\n",
    "            self.reward = tot_reward\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            self.reward = self.reward * self.reward_scaling\n",
    "            \n",
    "            # Summary rewards\n",
    "            # df_rewards = pd.DataFrame(self.rewards_memory)\n",
    "            # df_rewards.columns = [\"account_rewards\"]\n",
    "            # df_rewards[\"date\"] = self.date_memory[:-1]\n",
    "\n",
    "            # Print out training results after a certain amount of episodes\n",
    "            if self.episode % self.print_verbosity == 0:\n",
    "                print(f\"row: {self.row}, episode: {self.episode}\")\n",
    "                print(f\"begin_total_asset: {self.asset_memory[0]:0.2f}\")\n",
    "                print(f\"end_total_asset: {end_total_asset:0.2f}\")\n",
    "                print(f\"total_reward: {tot_reward:0.2f}\")\n",
    "                print(f\"total_cost: {self.cost:0.2f}\")\n",
    "                print(f\"total_trades: {self.trades}\")\n",
    "                # if df_total_value[\"daily_return\"].std() != 0:\n",
    "                #     print(f\"Sharpe: {sharpe:0.3f}\")\n",
    "                print(\"=================================\")\n",
    "\n",
    "            # if (self.model_name != \"\") and (self.mode != \"\"):\n",
    "                # df_actions = self.save_action_memory()\n",
    "                # df_actions.to_csv(\n",
    "                #     \"results/actions_{}_{}_{}.csv\".format(\n",
    "                #         self.mode, self.model_name, self.iteration\n",
    "                #     )\n",
    "                # )\n",
    "                # df_total_value.to_csv(\n",
    "                #     \"results/account_value_{}_{}_{}.csv\".format(\n",
    "                #         self.mode, self.model_name, self.iteration\n",
    "                #     ),\n",
    "                #     index=False,\n",
    "                # )\n",
    "                # df_rewards.to_csv(\n",
    "                #     \"results/account_rewards_{}_{}_{}.csv\".format(\n",
    "                #         self.mode, self.model_name, self.iteration\n",
    "                #     ),\n",
    "                #     index=False,\n",
    "                # )\n",
    "                # plt.plot(self.asset_memory, \"r\")\n",
    "                # plt.savefig(\n",
    "                #     \"results/account_value_{}_{}_{}.png\".format(\n",
    "                #         self.mode, self.model_name, self.iteration\n",
    "                #     ),\n",
    "                #     index=False,\n",
    "                # )\n",
    "                # plt.close()\n",
    "            \n",
    "            truncated = False  # we do not limit the number of steps here\n",
    "            # Optionally we can pass additional info, we are not using that for now\n",
    "            info = {}\n",
    "\n",
    "\n",
    "            return (\n",
    "                np.array(self.state).astype(np.float32),\n",
    "                self.reward,\n",
    "                self.terminal,\n",
    "                truncated,\n",
    "                info,\n",
    "            )\n",
    "\n",
    "        # --> IN A NORMAL STEP\n",
    "        else: \n",
    "\n",
    "            # begin_total_asset = sum(self.portfolio.price * self.portfolio.amount)\n",
    "            \n",
    "            # update previous_portfolio\n",
    "            self.previous_port = self.portfolio.copy()\n",
    "        \n",
    "            # sell_number_share = 0 # Default value at each transaction to detect reward value\n",
    "            self.reward = 0\n",
    "            if actions[0] > 0:\n",
    "                self._buy_stock(actions[0])\n",
    "            else:\n",
    "                sell_number_share = self._sell_stock(actions[0])\n",
    "\n",
    "            # actions = actions * self.hmax  # actions initially is scaled between 0 to 1\n",
    "            # actions = actions.astype(int)  # convert into integer because we can't by fraction of shares\n",
    "            # print(type(actions))\n",
    "            self.actions_memory.append(actions)\n",
    "\n",
    "            # Update selected row in the dataset based on state: s -> s+1\n",
    "            self.row += 1\n",
    "            self.data = self.df.loc[self.row]\n",
    "            self.state = self._update_state()\n",
    "    \n",
    "            end_total_asset = sum(self.portfolio.price * self.portfolio.amount)\n",
    "            # print(f'Begin asset: {begin_total_asset}, End asset: {end_total_asset}')\n",
    "\n",
    "            # Update asset memory\n",
    "            self.asset_memory.append(end_total_asset)\n",
    "            self.date_memory.append(self._get_date())\n",
    "\n",
    "            # Update reward\n",
    "            self.reward = end_total_asset - self.initial_amount\n",
    "            self.rewards_memory.append(self.reward)\n",
    "            self.reward = self.reward * self.reward_scaling\n",
    "\n",
    "        truncated = False  # we do not limit the number of steps here\n",
    "        # Optionally we can pass additional info, we are not using that for now\n",
    "        info = {}\n",
    "        \n",
    "        # return self.state, self.reward, self.terminal, {}\n",
    "    \n",
    "        return (\n",
    "            np.array(self.state).astype(np.float32),\n",
    "            self.reward,\n",
    "            self.terminal,\n",
    "            truncated,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # initiate state\n",
    "        self.state = self._initiate_state()\n",
    "\n",
    "        # Reset asset_memory\n",
    "        if self.initial:\n",
    "            self.asset_memory = [self.initial_amount]\n",
    "        else:\n",
    "            previous_total_asset = sum(self.previous_port.price * self.previous_port.amount)\n",
    "            self.asset_memory = [previous_total_asset]\n",
    "\n",
    "        # Reset support variables\n",
    "        # self.row = 0\n",
    "        # self.data = self.df.loc[self.row, :]\n",
    "        # self.turbulence = 0\n",
    "        self.cost = 0\n",
    "        self.trades = 0\n",
    "        self.terminal = False\n",
    "        self.rewards_memory = []\n",
    "        self.actions_memory = []\n",
    "        self.date_memory = [self._get_date()]\n",
    "        self.episode += 1\n",
    "\n",
    "        return np.array(self.state).astype(np.float32), {}\n",
    "        # return self.state\n",
    "\n",
    "    def render(self, mode=\"human\", close=False):\n",
    "        return self.state\n",
    "\n",
    "    def _initiate_state(self):\n",
    "        \n",
    "        # Reset portfolio & previous_portfolio\n",
    "        if self.initial:\n",
    "            self.portfolio = pd.DataFrame([['cap',self.initial_amount,self.initial_amount,1,1]])\n",
    "            self.portfolio.columns = self.portfolio_columns\n",
    "            self.previous_port = self.portfolio.copy()\n",
    "        else:\n",
    "            self.portfolio = self.previous_port.copy()\n",
    "\n",
    "        # Reset data\n",
    "        self.row = 0\n",
    "        self.data = self.df.loc[self.row]\n",
    "        \n",
    "         # Reset state\n",
    "        state = self._update_state()\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def _update_state(self):\n",
    "\n",
    "        # if the stock appear in the portfolio already\n",
    "        if self.portfolio[self.portfolio.tic == self.data.tic].empty:\n",
    "            state = ([self.portfolio.loc[0].price] + [self.data['close']] + [0] + [0] + [0] + sum([[self.data[tech]] for tech in self.tech_indicator_list], []))\n",
    "            \n",
    "        else:\n",
    "            # Update stock's prices in portfolio before updating state\n",
    "            selected_index = self.portfolio[self.portfolio.tic == self.data.tic].index[0]\n",
    "            selected_row = self.portfolio.loc[selected_index]\n",
    "            selected_row['price'] = self.data.close\n",
    "            self.portfolio.loc[selected_index] = selected_row\n",
    "            self._compute_weight()\n",
    "            selected_row = self.portfolio.loc[selected_index]\n",
    "            # print(\"Update portfolio at \",self.data.tic,\" price: \", self.data.close,\"; with weight: \", selected_row.weight)\n",
    "            \n",
    "            state = (\n",
    "                    [self.portfolio.iloc[0].price]\n",
    "                    + [(selected_row.buy_price/selected_row.price-1)]\n",
    "                    + [self.data.close]\n",
    "                    + [selected_row.amount]\n",
    "                    + [selected_row.weight]\n",
    "                    + sum([[self.data[tech]] for tech in self.tech_indicator_list], [])\n",
    "                )\n",
    "    \n",
    "        return state\n",
    "\n",
    "    def _get_date(self):\n",
    "        # return self.data.date\n",
    "        return self.row\n",
    "\n",
    "    def save_asset_memory(self):\n",
    "        date_list = self.date_memory\n",
    "        asset_list = self.asset_memory\n",
    "        # print(len(date_list))\n",
    "        # print(len(asset_list))\n",
    "        df_account_value = pd.DataFrame(\n",
    "            {\"date\": date_list, \"account_value\": asset_list}\n",
    "        )\n",
    "        return df_account_value\n",
    "\n",
    "    def save_action_memory(self):\n",
    "        \n",
    "        date_list = self.date_memory[:-1]\n",
    "        action_list = self.actions_memory\n",
    "        df_actions = pd.DataFrame({\"date\": date_list, \"actions\": action_list})\n",
    "        return df_actions\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def get_sb_env(self):\n",
    "        e = DummyVecEnv([lambda: self])\n",
    "        obs = e.reset()\n",
    "        return e, obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='4.3'></a>\n",
    "## 5.3 Initialize Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State's space include current capital amount, current stock price, current amount, the return of the current stock, the weight of this stock in the portfolio, and the indicators decided in ratio_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Dimension: 1, State Space: 22\n"
     ]
    }
   ],
   "source": [
    "ratio_list = train_data.columns.drop(['date','tic','close'])\n",
    "\n",
    "action_dimension = 1 # k float in range (-1,1) to decide sell (k<0) or buy (k>0) decisions\n",
    "state_space = 1 + 4*action_dimension + len(ratio_list)\n",
    "print(f\"Action Dimension: {action_dimension}, State Space: {state_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "# Parameters for the environment\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.01,\n",
    "    \"state_space\": state_space, \n",
    "    \"tech_indicator_list\": ratio_list, \n",
    "    \"action_space\": action_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"stop_loss\": 0.3,\n",
    "    \"print_verbosity\":4,\n",
    "    \"punishment_rate\": 0.01\n",
    "}\n",
    "\n",
    "#Establish the training environment using StockTradingEnv() class\n",
    "e_train_gym = StockTradingEnv(df = train_data, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52694166], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = e_train_gym.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(e_train_gym, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, A2C, DQN, DDPG\n",
    "\n",
    "# Instantiate the env\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 1.00000000e+06,  1.00000000e+01,  0.00000000e+00,  0.00000000e+00,\n",
      "        0.00000000e+00,  1.01251163e-01,  2.10604668e+00,  1.67358488e-01,\n",
      "       -2.50185661e+01, -1.38768569e-01,  1.00000000e+00,  7.16915429e-02,\n",
      "       -1.39470899e+00,  2.70147552e-03, -7.22460970e-02, -1.02848984e-01,\n",
      "       -9.11556423e-01,  4.23592269e-01, -2.73218781e-01, -1.02848984e-01,\n",
      "        1.49919210e+07, -1.31204807e-08], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "# Get samples from train_data\n",
    "test_env_data = trade_data.iloc[:20]\n",
    "test_env_data.close = [10,100,15,90,20,80,25,85,18,92,10,100,15,90,20,80,25,85,18,92]\n",
    "\n",
    "# Parameters for the environment\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.001,\n",
    "    \"state_space\": state_space, \n",
    "    \"tech_indicator_list\": ratio_list, \n",
    "    \"action_space\": action_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"stop_loss\": 0.3,\n",
    "    \"print_verbosity\":4,\n",
    "    \"punishment_rate\": 0.01\n",
    "}\n",
    "\n",
    "#Establish the training environment using StockTradingEnv() class\n",
    "test_train_gym = StockTradingEnv(df = test_env_data, **env_kwargs)\n",
    "\n",
    "# test reset state\n",
    "print(test_train_gym.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.027759999999997672  type  <class 'float'> \n",
      "\n",
      "False \n",
      "\n",
      "[ 7.2212238e+05  1.0000000e+02  0.0000000e+00  0.0000000e+00\n",
      "  0.0000000e+00  8.8005476e-02  2.4012961e+00  1.9442645e-01\n",
      " -1.2397936e+01 -1.5498774e-01  8.0000000e+00  1.1470705e-01\n",
      " -1.4978263e+00  0.0000000e+00 -8.4519118e-02 -1.1767862e-01\n",
      " -3.9260013e+00  3.9233136e-01 -1.6566460e-01 -1.1767862e-01\n",
      "  4.2261280e+07 -5.6460703e-09] \n",
      "\n",
      "   tic     price  buy_price  amount    weight\n",
      "0  cap  722122.4   722122.4       1  0.722323\n",
      "1   DG      10.0       10.0   27760  0.277677 \n",
      "\n",
      "   tic    price  buy_price  amount  weight\n",
      "0  cap  1000000    1000000       1       1\n"
     ]
    }
   ],
   "source": [
    "state,reward,terminal,truncated,info = test_train_gym.step(test_train_gym.action_space.sample())\n",
    "# print(test_train_gym.save_action_memory())\n",
    "print(reward,' type ',type(reward),'\\n')\n",
    "print(terminal,'\\n')\n",
    "# print(test_train_gym.data,'\\n')\n",
    "print(state,'\\n')\n",
    "print(test_train_gym.portfolio,'\\n')\n",
    "print(test_train_gym.previous_port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Train DRL Agents\n",
    "* The DRL algorithms are from **Stable Baselines 3**.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "672128"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_length = len(e_train_gym.df)\n",
    "episode_amount = 128\n",
    "total_training_step = episode_length*episode_amount\n",
    "total_training_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 5251, 'clip_range': 0.1}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "agent_ppo = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": episode_length,\n",
    "    \"clip_range\":0.1\n",
    "}\n",
    "\n",
    "model_ppo = agent_ppo.get_model(\"ppo\",model_kwargs = PPO_PARAMS,tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_ppo/\",verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to tensorboard_log/test_ppo/ppo_25\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 469       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 4         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 41.756084 |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 445          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.576538e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.00109     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.44e+04     |\n",
      "|    n_updates            | 3300         |\n",
      "|    policy_gradient_loss | -0.000105    |\n",
      "|    reward               | 152.68568    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.09e+05     |\n",
      "------------------------------------------\n",
      "Episode: 134\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 466          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.097995e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.000119    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.66e+05     |\n",
      "|    n_updates            | 3310         |\n",
      "|    policy_gradient_loss | -2.13e-05    |\n",
      "|    reward               | 9.348838     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.73e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 477          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.447975e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.000407     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.17e+06     |\n",
      "|    n_updates            | 3320         |\n",
      "|    policy_gradient_loss | 3.24e-05     |\n",
      "|    reward               | 99.33816     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 4.34e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 481           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 21            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6563724e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.000538     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.07e+05      |\n",
      "|    n_updates            | 3330          |\n",
      "|    policy_gradient_loss | -2.05e-05     |\n",
      "|    reward               | 117.56652     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.21e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 135\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 487           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 25            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3639448e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -8.61e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.37e+06      |\n",
      "|    n_updates            | 3340          |\n",
      "|    policy_gradient_loss | -6.72e-06     |\n",
      "|    reward               | 58.638447     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 6.74e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 478          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.824688e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.000396     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.9e+05      |\n",
      "|    n_updates            | 3350         |\n",
      "|    policy_gradient_loss | -6.21e-05    |\n",
      "|    reward               | 161.20251    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 5.81e+05     |\n",
      "------------------------------------------\n",
      "Episode: 136\n",
      "row: 5250, episode: 136\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3594185.71\n",
      "total_reward: 2594185.71\n",
      "total_cost: 58168.26\n",
      "total_trades: 230\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 472           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7175825e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 8.37e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.38e+06      |\n",
      "|    n_updates            | 3360          |\n",
      "|    policy_gradient_loss | -9.03e-05     |\n",
      "|    reward               | 8.29132       |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.76e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 488           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 37            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1777272e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.00022       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.89e+06      |\n",
      "|    n_updates            | 3370          |\n",
      "|    policy_gradient_loss | -2.86e-05     |\n",
      "|    reward               | 103.38593     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.79e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 499           |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 40            |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4421391e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.00013      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.94e+05      |\n",
      "|    n_updates            | 3380          |\n",
      "|    policy_gradient_loss | -2.52e-05     |\n",
      "|    reward               | 227.62886     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.39e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 137\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 507           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 44            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3319019e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000526      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.1e+06       |\n",
      "|    n_updates            | 3390          |\n",
      "|    policy_gradient_loss | -1.76e-05     |\n",
      "|    reward               | 71.53992      |\n",
      "|    std                  | 1.02          |\n",
      "|    value_loss           | 8.19e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 509           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 48            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1196396e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000283      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.69e+06      |\n",
      "|    n_updates            | 3400          |\n",
      "|    policy_gradient_loss | 7.72e-06      |\n",
      "|    reward               | 341.82928     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 5.37e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 138\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 511          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.622002e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -2.62e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.9e+06      |\n",
      "|    n_updates            | 3410         |\n",
      "|    policy_gradient_loss | -1.37e-05    |\n",
      "|    reward               | 8.29132      |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.18e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.131572e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 8.71e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.54e+07     |\n",
      "|    n_updates            | 3420         |\n",
      "|    policy_gradient_loss | 4.47e-06     |\n",
      "|    reward               | 96.64856     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 5.08e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 59            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6386115e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000106      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5e+05         |\n",
      "|    n_updates            | 3430          |\n",
      "|    policy_gradient_loss | -2.21e-05     |\n",
      "|    reward               | 487.90097     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.99e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 139\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 523           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 62            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1456002e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.00017       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.09e+07      |\n",
      "|    n_updates            | 3440          |\n",
      "|    policy_gradient_loss | 1.22e-05      |\n",
      "|    reward               | 78.001274     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.18e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 66            |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1237938e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000123      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.69e+07      |\n",
      "|    n_updates            | 3450          |\n",
      "|    policy_gradient_loss | -3.19e-05     |\n",
      "|    reward               | 243.18672     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.37e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 140\n",
      "row: 5250, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5416152.71\n",
      "total_reward: 4416152.71\n",
      "total_cost: 129109.45\n",
      "total_trades: 149\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 69            |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4007826e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.000133     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.03e+06      |\n",
      "|    n_updates            | 3460          |\n",
      "|    policy_gradient_loss | 6.22e-06      |\n",
      "|    reward               | -0.099903576  |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 8.06e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 529           |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 73            |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5518162e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000296      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.94e+07      |\n",
      "|    n_updates            | 3470          |\n",
      "|    policy_gradient_loss | -2.24e-06     |\n",
      "|    reward               | 140.77173     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.89e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 530          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 77           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.258465e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.000221    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.74e+05     |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -2.16e-06    |\n",
      "|    reward               | 493.77762    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.55e+06     |\n",
      "------------------------------------------\n",
      "Episode: 141\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 531           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 80            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4424163e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -7.86e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.29e+07      |\n",
      "|    n_updates            | 3490          |\n",
      "|    policy_gradient_loss | 2.3e-06       |\n",
      "|    reward               | 52.093933     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.58e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 532           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 84            |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0707452e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000167      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.35e+07      |\n",
      "|    n_updates            | 3500          |\n",
      "|    policy_gradient_loss | -7.6e-06      |\n",
      "|    reward               | 367.26074     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 4.7e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 530           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 88            |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2305252e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.00021      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.56e+06      |\n",
      "|    n_updates            | 3510          |\n",
      "|    policy_gradient_loss | 2.04e-06      |\n",
      "|    reward               | 651.4094      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.31e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 142\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 533          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 92           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.059119e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.000234     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.48e+07     |\n",
      "|    n_updates            | 3520         |\n",
      "|    policy_gradient_loss | -1.53e-05    |\n",
      "|    reward               | 238.85762    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 6.95e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 534           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 95            |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3774261e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 6.01e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.42e+06      |\n",
      "|    n_updates            | 3530          |\n",
      "|    policy_gradient_loss | -1.65e-05     |\n",
      "|    reward               | 494.7437      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.28e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 143\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 533          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.957868e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.000138     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.79e+07     |\n",
      "|    n_updates            | 3540         |\n",
      "|    policy_gradient_loss | 2.88e-08     |\n",
      "|    reward               | -1.5216457   |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 3.57e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 530           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 104           |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3760291e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000192      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.39e+07      |\n",
      "|    n_updates            | 3550          |\n",
      "|    policy_gradient_loss | -8.3e-07      |\n",
      "|    reward               | 62.776905     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 6.78e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 523           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 109           |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7977588e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.000662     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.38e+05      |\n",
      "|    n_updates            | 3560          |\n",
      "|    policy_gradient_loss | -8.96e-06     |\n",
      "|    reward               | 255.20343     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 6.75e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 144\n",
      "row: 5250, episode: 144\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4059533.98\n",
      "total_reward: 3059533.98\n",
      "total_cost: 56830.77\n",
      "total_trades: 554\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 113           |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7427374e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000177      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.1e+06       |\n",
      "|    n_updates            | 3570          |\n",
      "|    policy_gradient_loss | 7.36e-06      |\n",
      "|    reward               | 152.17082     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.22e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 30            |\n",
      "|    time_elapsed         | 117           |\n",
      "|    total_timesteps      | 61440         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0710868e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000249      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.11e+06      |\n",
      "|    n_updates            | 3580          |\n",
      "|    policy_gradient_loss | -2.54e-05     |\n",
      "|    reward               | 352.5448      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 6.23e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 145\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 521          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 121          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.723371e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.000198     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.94e+06     |\n",
      "|    n_updates            | 3590         |\n",
      "|    policy_gradient_loss | -8.34e-05    |\n",
      "|    reward               | 9.3762865    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.59e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 523          |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 125          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.941619e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 0.000257     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.89e+07     |\n",
      "|    n_updates            | 3600         |\n",
      "|    policy_gradient_loss | -1.9e-05     |\n",
      "|    reward               | 192.33589    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 5.79e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 128           |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.6330616e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000647      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.78e+06      |\n",
      "|    n_updates            | 3610          |\n",
      "|    policy_gradient_loss | -2.57e-06     |\n",
      "|    reward               | 289.52075     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.55e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 146\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 132           |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.6915755e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000112      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.02e+07      |\n",
      "|    n_updates            | 3620          |\n",
      "|    policy_gradient_loss | 2.63e-06      |\n",
      "|    reward               | 20.995567     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.05e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 136           |\n",
      "|    total_timesteps      | 71680         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0109292e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000316      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.22e+06      |\n",
      "|    n_updates            | 3630          |\n",
      "|    policy_gradient_loss | -2.19e-05     |\n",
      "|    reward               | 211.91565     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.04e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 147\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 141           |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6454876e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.000596     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.23e+06      |\n",
      "|    n_updates            | 3640          |\n",
      "|    policy_gradient_loss | -8.7e-06      |\n",
      "|    reward               | -0.0999       |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.47e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 523           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 144           |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0535967e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000615      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.08e+07      |\n",
      "|    n_updates            | 3650          |\n",
      "|    policy_gradient_loss | 6.95e-06      |\n",
      "|    reward               | 77.03004      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.15e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 38            |\n",
      "|    time_elapsed         | 149           |\n",
      "|    total_timesteps      | 77824         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00017289334 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.000713     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.73e+05      |\n",
      "|    n_updates            | 3660          |\n",
      "|    policy_gradient_loss | -0.000571     |\n",
      "|    reward               | 348.59708     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 5.46e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 148\n",
      "row: 5250, episode: 148\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5313864.18\n",
      "total_reward: 4313864.18\n",
      "total_cost: 111204.72\n",
      "total_trades: 315\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 153           |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00021563133 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 4.67e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.56e+06      |\n",
      "|    n_updates            | 3670          |\n",
      "|    policy_gradient_loss | -0.000238     |\n",
      "|    reward               | 55.510986     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.31e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 523           |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 156           |\n",
      "|    total_timesteps      | 81920         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0240259e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | 0.000181      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.92e+06      |\n",
      "|    n_updates            | 3680          |\n",
      "|    policy_gradient_loss | 0.000138      |\n",
      "|    reward               | 205.90004     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.98e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 159           |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8184226e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -0.000631     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.84e+06      |\n",
      "|    n_updates            | 3690          |\n",
      "|    policy_gradient_loss | -1.36e-06     |\n",
      "|    reward               | 340.3591      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.67e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 149\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 528           |\n",
      "|    iterations           | 42            |\n",
      "|    time_elapsed         | 162           |\n",
      "|    total_timesteps      | 86016         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8361683e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.44         |\n",
      "|    explained_variance   | -7.41e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.13e+07      |\n",
      "|    n_updates            | 3700          |\n",
      "|    policy_gradient_loss | -3.27e-07     |\n",
      "|    reward               | 57.405006     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.26e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 530          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.429466e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | 7.36e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.09e+05     |\n",
      "|    n_updates            | 3710         |\n",
      "|    policy_gradient_loss | -3.32e-05    |\n",
      "|    reward               | 66.97021     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 6.18e+05     |\n",
      "------------------------------------------\n",
      "Episode: 150\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 532          |\n",
      "|    iterations           | 44           |\n",
      "|    time_elapsed         | 169          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.861243e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.000368    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.14e+05     |\n",
      "|    n_updates            | 3720         |\n",
      "|    policy_gradient_loss | 2.49e-05     |\n",
      "|    reward               | 25.78565     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 8.28e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 532          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 173          |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.887705e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.00267      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.36e+05     |\n",
      "|    n_updates            | 3730         |\n",
      "|    policy_gradient_loss | -1.16e-05    |\n",
      "|    reward               | 199.90666    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 6.72e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 178           |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6721315e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000236     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.43e+06      |\n",
      "|    n_updates            | 3740          |\n",
      "|    policy_gradient_loss | 1.46e-05      |\n",
      "|    reward               | 611.84247     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 4.86e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 151\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 528           |\n",
      "|    iterations           | 47            |\n",
      "|    time_elapsed         | 182           |\n",
      "|    total_timesteps      | 96256         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3178214e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000269      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.31e+07      |\n",
      "|    n_updates            | 3750          |\n",
      "|    policy_gradient_loss | -2.68e-06     |\n",
      "|    reward               | 159.71475     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 4.61e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 529           |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 185           |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1347736e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000144      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.3e+06       |\n",
      "|    n_updates            | 3760          |\n",
      "|    policy_gradient_loss | -3.96e-06     |\n",
      "|    reward               | 529.2371      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.86e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 152\n",
      "row: 5250, episode: 152\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8928250.39\n",
      "total_reward: 7928250.39\n",
      "total_cost: 153581.27\n",
      "total_trades: 81\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 525          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 190          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.589107e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -1.97e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.01e+07     |\n",
      "|    n_updates            | 3770         |\n",
      "|    policy_gradient_loss | 1.39e-06     |\n",
      "|    reward               | 51.30615     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 4.03e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 50            |\n",
      "|    time_elapsed         | 194           |\n",
      "|    total_timesteps      | 102400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4930646e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 9.97e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.62e+07      |\n",
      "|    n_updates            | 3780          |\n",
      "|    policy_gradient_loss | -3.73e-06     |\n",
      "|    reward               | 213.79901     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.25e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 51            |\n",
      "|    time_elapsed         | 198           |\n",
      "|    total_timesteps      | 104448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1056621e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000412     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.13e+06      |\n",
      "|    n_updates            | 3790          |\n",
      "|    policy_gradient_loss | -2.17e-05     |\n",
      "|    reward               | 490.59848     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 4.27e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 153\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 52            |\n",
      "|    time_elapsed         | 202           |\n",
      "|    total_timesteps      | 106496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6872364e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000184      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.9e+07       |\n",
      "|    n_updates            | 3800          |\n",
      "|    policy_gradient_loss | -9.75e-06     |\n",
      "|    reward               | 20.555933     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.8e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 53            |\n",
      "|    time_elapsed         | 205           |\n",
      "|    total_timesteps      | 108544        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9217335e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000184      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.32e+07      |\n",
      "|    n_updates            | 3810          |\n",
      "|    policy_gradient_loss | -2.97e-07     |\n",
      "|    reward               | 264.33926     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.63e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 154\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 54            |\n",
      "|    time_elapsed         | 209           |\n",
      "|    total_timesteps      | 110592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8895662e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 6.27e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.41e+06      |\n",
      "|    n_updates            | 3820          |\n",
      "|    policy_gradient_loss | -2.04e-05     |\n",
      "|    reward               | 51.526264     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 4.82e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 529          |\n",
      "|    iterations           | 55           |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 112640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.389556e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000178     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.15e+06     |\n",
      "|    n_updates            | 3830         |\n",
      "|    policy_gradient_loss | -2.84e-05    |\n",
      "|    reward               | 128.83568    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 8.3e+06      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 530          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 216          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.420558e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.000546    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.22e+06     |\n",
      "|    n_updates            | 3840         |\n",
      "|    policy_gradient_loss | 4.2e-06      |\n",
      "|    reward               | 496.68686    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 2.44e+06     |\n",
      "------------------------------------------\n",
      "Episode: 155\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 531           |\n",
      "|    iterations           | 57            |\n",
      "|    time_elapsed         | 219           |\n",
      "|    total_timesteps      | 116736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0640665e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000323      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.08e+07      |\n",
      "|    n_updates            | 3850          |\n",
      "|    policy_gradient_loss | 2.42e-06      |\n",
      "|    reward               | 115.458824    |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.16e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 532          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 222          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.542541e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000182     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.31e+07     |\n",
      "|    n_updates            | 3860         |\n",
      "|    policy_gradient_loss | -1.97e-06    |\n",
      "|    reward               | 358.8421     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 2.61e+07     |\n",
      "------------------------------------------\n",
      "Episode: 156\n",
      "row: 5250, episode: 156\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8604324.35\n",
      "total_reward: 7604324.35\n",
      "total_cost: 183552.64\n",
      "total_trades: 201\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 533           |\n",
      "|    iterations           | 59            |\n",
      "|    time_elapsed         | 226           |\n",
      "|    total_timesteps      | 120832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1842425e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000317     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.49e+06      |\n",
      "|    n_updates            | 3870          |\n",
      "|    policy_gradient_loss | -7.24e-06     |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.5e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 534           |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 229           |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.3015614e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000177      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.87e+07      |\n",
      "|    n_updates            | 3880          |\n",
      "|    policy_gradient_loss | -1e-05        |\n",
      "|    reward               | 295.38345     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.73e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 61           |\n",
      "|    time_elapsed         | 233          |\n",
      "|    total_timesteps      | 124928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.888578e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -6.84e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.88e+06     |\n",
      "|    n_updates            | 3890         |\n",
      "|    policy_gradient_loss | -1.21e-05    |\n",
      "|    reward               | 488.29498    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 7.76e+06     |\n",
      "------------------------------------------\n",
      "Episode: 157\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 236          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.531007e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -6.19e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.08e+07     |\n",
      "|    n_updates            | 3900         |\n",
      "|    policy_gradient_loss | -1.46e-05    |\n",
      "|    reward               | 14.802065    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 4.16e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 536           |\n",
      "|    iterations           | 63            |\n",
      "|    time_elapsed         | 240           |\n",
      "|    total_timesteps      | 129024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3795216e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.00021       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.94e+07      |\n",
      "|    n_updates            | 3910          |\n",
      "|    policy_gradient_loss | 4.64e-06      |\n",
      "|    reward               | 127.461266    |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 5.89e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 536           |\n",
      "|    iterations           | 64            |\n",
      "|    time_elapsed         | 244           |\n",
      "|    total_timesteps      | 131072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2192253e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000649     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.55e+05      |\n",
      "|    n_updates            | 3920          |\n",
      "|    policy_gradient_loss | -1.72e-05     |\n",
      "|    reward               | 274.33942     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.71e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 158\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 537           |\n",
      "|    iterations           | 65            |\n",
      "|    time_elapsed         | 247           |\n",
      "|    total_timesteps      | 133120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7783833e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000335      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.45e+06      |\n",
      "|    n_updates            | 3930          |\n",
      "|    policy_gradient_loss | -4.25e-05     |\n",
      "|    reward               | 73.08514      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.29e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 537           |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 251           |\n",
      "|    total_timesteps      | 135168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9255766e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000293      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.09e+06      |\n",
      "|    n_updates            | 3940          |\n",
      "|    policy_gradient_loss | -2.5e-05      |\n",
      "|    reward               | 279.42767     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.19e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 159\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 537          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 255          |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.099086e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000129     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.86e+06     |\n",
      "|    n_updates            | 3950         |\n",
      "|    policy_gradient_loss | -1.56e-05    |\n",
      "|    reward               | 10.057639    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 7.72e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 538           |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 258           |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2686955e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.0002        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.14e+07      |\n",
      "|    n_updates            | 3960          |\n",
      "|    policy_gradient_loss | -5.92e-05     |\n",
      "|    reward               | 121.36112     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.29e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 538           |\n",
      "|    iterations           | 69            |\n",
      "|    time_elapsed         | 262           |\n",
      "|    total_timesteps      | 141312        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5489363e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000285     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.38e+05      |\n",
      "|    n_updates            | 3970          |\n",
      "|    policy_gradient_loss | 3.41e-06      |\n",
      "|    reward               | 173.51524     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.28e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 160\n",
      "row: 5250, episode: 160\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3262599.81\n",
      "total_reward: 2262599.81\n",
      "total_cost: 46621.28\n",
      "total_trades: 154\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 539           |\n",
      "|    iterations           | 70            |\n",
      "|    time_elapsed         | 265           |\n",
      "|    total_timesteps      | 143360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7444912e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000699      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.1e+06       |\n",
      "|    n_updates            | 3980          |\n",
      "|    policy_gradient_loss | -2.74e-06     |\n",
      "|    reward               | 117.45331     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 8.2e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 538           |\n",
      "|    iterations           | 71            |\n",
      "|    time_elapsed         | 269           |\n",
      "|    total_timesteps      | 145408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4159672e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000407      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.74e+06      |\n",
      "|    n_updates            | 3990          |\n",
      "|    policy_gradient_loss | -2.18e-05     |\n",
      "|    reward               | 211.74469     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.49e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 161\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 536           |\n",
      "|    iterations           | 72            |\n",
      "|    time_elapsed         | 274           |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1363604e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000392      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.05e+06      |\n",
      "|    n_updates            | 4000          |\n",
      "|    policy_gradient_loss | -2.07e-05     |\n",
      "|    reward               | 25.068436     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 6.11e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 536           |\n",
      "|    iterations           | 73            |\n",
      "|    time_elapsed         | 278           |\n",
      "|    total_timesteps      | 149504        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7805876e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000419      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.17e+07      |\n",
      "|    n_updates            | 4010          |\n",
      "|    policy_gradient_loss | 5.53e-06      |\n",
      "|    reward               | 184.47499     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.34e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 535           |\n",
      "|    iterations           | 74            |\n",
      "|    time_elapsed         | 283           |\n",
      "|    total_timesteps      | 151552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4660043e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.00029      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.57e+06      |\n",
      "|    n_updates            | 4020          |\n",
      "|    policy_gradient_loss | -3.91e-05     |\n",
      "|    reward               | 385.46237     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.13e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 162\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 535           |\n",
      "|    iterations           | 75            |\n",
      "|    time_elapsed         | 286           |\n",
      "|    total_timesteps      | 153600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2651878e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000418      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.05e+07      |\n",
      "|    n_updates            | 4030          |\n",
      "|    policy_gradient_loss | 4.6e-05       |\n",
      "|    reward               | 33.63966      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.1e+07       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 290          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.739536e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000252     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.28e+06     |\n",
      "|    n_updates            | 4040         |\n",
      "|    policy_gradient_loss | -1.45e-06    |\n",
      "|    reward               | 205.09595    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.66e+07     |\n",
      "------------------------------------------\n",
      "Episode: 163\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 535           |\n",
      "|    iterations           | 77            |\n",
      "|    time_elapsed         | 294           |\n",
      "|    total_timesteps      | 157696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4564166e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000172     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.32e+06      |\n",
      "|    n_updates            | 4050          |\n",
      "|    policy_gradient_loss | -1.16e-05     |\n",
      "|    reward               | -0.09995949   |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 4.64e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 536           |\n",
      "|    iterations           | 78            |\n",
      "|    time_elapsed         | 297           |\n",
      "|    total_timesteps      | 159744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2437598e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.00025       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.97e+07      |\n",
      "|    n_updates            | 4060          |\n",
      "|    policy_gradient_loss | -3.94e-05     |\n",
      "|    reward               | 111.56809     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.94e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 534           |\n",
      "|    iterations           | 79            |\n",
      "|    time_elapsed         | 302           |\n",
      "|    total_timesteps      | 161792        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0539522e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000594     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.82e+05      |\n",
      "|    n_updates            | 4070          |\n",
      "|    policy_gradient_loss | -9.48e-06     |\n",
      "|    reward               | 226.62148     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.64e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 164\n",
      "row: 5250, episode: 164\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3870312.01\n",
      "total_reward: 2870312.01\n",
      "total_cost: 87279.13\n",
      "total_trades: 212\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 533          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 306          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.321343e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 3.14e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.71e+06     |\n",
      "|    n_updates            | 4080         |\n",
      "|    policy_gradient_loss | -2.4e-06     |\n",
      "|    reward               | 15.609455    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 7.43e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 535           |\n",
      "|    iterations           | 81            |\n",
      "|    time_elapsed         | 310           |\n",
      "|    total_timesteps      | 165888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7107384e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000325      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.74e+06      |\n",
      "|    n_updates            | 4090          |\n",
      "|    policy_gradient_loss | 1.34e-06      |\n",
      "|    reward               | 145.54045     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.48e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 313          |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.088506e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.000855    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.24e+05     |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -4.15e-06    |\n",
      "|    reward               | 347.23486    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.45e+06     |\n",
      "------------------------------------------\n",
      "Episode: 165\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 537           |\n",
      "|    iterations           | 83            |\n",
      "|    time_elapsed         | 316           |\n",
      "|    total_timesteps      | 169984        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3850665e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 7.68e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.47e+06      |\n",
      "|    n_updates            | 4110          |\n",
      "|    policy_gradient_loss | 3.57e-06      |\n",
      "|    reward               | 51.599174     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.69e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 84           |\n",
      "|    time_elapsed         | 319          |\n",
      "|    total_timesteps      | 172032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.731949e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.00012      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.8e+05      |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | -2.3e-05     |\n",
      "|    reward               | 271.1181     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.36e+06     |\n",
      "------------------------------------------\n",
      "Episode: 166\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 538          |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 322          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.290764e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000168     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.09e+06     |\n",
      "|    n_updates            | 4130         |\n",
      "|    policy_gradient_loss | 3.71e-05     |\n",
      "|    reward               | 84.03557     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 6.18e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 540           |\n",
      "|    iterations           | 86            |\n",
      "|    time_elapsed         | 326           |\n",
      "|    total_timesteps      | 176128        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1408854e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000196      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.03e+06      |\n",
      "|    n_updates            | 4140          |\n",
      "|    policy_gradient_loss | -3.34e-06     |\n",
      "|    reward               | 164.54276     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.81e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 540           |\n",
      "|    iterations           | 87            |\n",
      "|    time_elapsed         | 329           |\n",
      "|    total_timesteps      | 178176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2989268e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000126      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.62e+06      |\n",
      "|    n_updates            | 4150          |\n",
      "|    policy_gradient_loss | -5.43e-06     |\n",
      "|    reward               | 232.1734      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.24e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 167\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 332          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.945047e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000185     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.18e+06     |\n",
      "|    n_updates            | 4160         |\n",
      "|    policy_gradient_loss | -1.85e-05    |\n",
      "|    reward               | 61.313946    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.64e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 541           |\n",
      "|    iterations           | 89            |\n",
      "|    time_elapsed         | 336           |\n",
      "|    total_timesteps      | 182272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7404964e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000375      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.29e+06      |\n",
      "|    n_updates            | 4170          |\n",
      "|    policy_gradient_loss | -1.3e-05      |\n",
      "|    reward               | 102.44925     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.57e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 168\n",
      "row: 5250, episode: 168\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1914971.98\n",
      "total_reward: 914971.98\n",
      "total_cost: 49328.37\n",
      "total_trades: 325\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 340          |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.784771e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000866     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.67e+05     |\n",
      "|    n_updates            | 4180         |\n",
      "|    policy_gradient_loss | 8.89e-06     |\n",
      "|    reward               | 15.890608    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.53e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 542           |\n",
      "|    iterations           | 91            |\n",
      "|    time_elapsed         | 343           |\n",
      "|    total_timesteps      | 186368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4086836e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000359      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.18e+06      |\n",
      "|    n_updates            | 4190          |\n",
      "|    policy_gradient_loss | -1.47e-05     |\n",
      "|    reward               | 149.08598     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.37e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 543           |\n",
      "|    iterations           | 92            |\n",
      "|    time_elapsed         | 346           |\n",
      "|    total_timesteps      | 188416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5878759e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000154      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.82e+05      |\n",
      "|    n_updates            | 4200          |\n",
      "|    policy_gradient_loss | -1.42e-05     |\n",
      "|    reward               | 467.0339      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.97e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 169\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 543           |\n",
      "|    iterations           | 93            |\n",
      "|    time_elapsed         | 350           |\n",
      "|    total_timesteps      | 190464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4676771e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000201      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.55e+07      |\n",
      "|    n_updates            | 4210          |\n",
      "|    policy_gradient_loss | 2.23e-05      |\n",
      "|    reward               | 104.1603      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.09e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 355          |\n",
      "|    total_timesteps      | 192512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.892354e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.00017      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.15e+07     |\n",
      "|    n_updates            | 4220         |\n",
      "|    policy_gradient_loss | -8.2e-06     |\n",
      "|    reward               | 331.47025    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 2.31e+07     |\n",
      "------------------------------------------\n",
      "Episode: 170\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 540           |\n",
      "|    iterations           | 95            |\n",
      "|    time_elapsed         | 360           |\n",
      "|    total_timesteps      | 194560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5867637e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 1.75e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.71e+06      |\n",
      "|    n_updates            | 4230          |\n",
      "|    policy_gradient_loss | 3.9e-06       |\n",
      "|    reward               | 10.271843     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.34e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 540           |\n",
      "|    iterations           | 96            |\n",
      "|    time_elapsed         | 363           |\n",
      "|    total_timesteps      | 196608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6641723e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000157      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.54e+07      |\n",
      "|    n_updates            | 4240          |\n",
      "|    policy_gradient_loss | -8.19e-07     |\n",
      "|    reward               | 68.46987      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.08e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 541           |\n",
      "|    iterations           | 97            |\n",
      "|    time_elapsed         | 367           |\n",
      "|    total_timesteps      | 198656        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1280645e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.00018      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.29e+05      |\n",
      "|    n_updates            | 4250          |\n",
      "|    policy_gradient_loss | -2.69e-06     |\n",
      "|    reward               | 329.762       |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 4.58e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 171\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 541          |\n",
      "|    iterations           | 98           |\n",
      "|    time_elapsed         | 370          |\n",
      "|    total_timesteps      | 200704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.912451e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000183     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.23e+06     |\n",
      "|    n_updates            | 4260         |\n",
      "|    policy_gradient_loss | -2.03e-05    |\n",
      "|    reward               | 32.24341     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.05e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 374          |\n",
      "|    total_timesteps      | 202752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.862683e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000198     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.26e+06     |\n",
      "|    n_updates            | 4270         |\n",
      "|    policy_gradient_loss | -8.23e-06    |\n",
      "|    reward               | 121.64809    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.45e+07     |\n",
      "------------------------------------------\n",
      "Episode: 172\n",
      "row: 5250, episode: 172\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3259898.25\n",
      "total_reward: 2259898.25\n",
      "total_cost: 130938.52\n",
      "total_trades: 147\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 542           |\n",
      "|    iterations           | 100           |\n",
      "|    time_elapsed         | 377           |\n",
      "|    total_timesteps      | 204800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0559161e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.00108      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.23e+05      |\n",
      "|    n_updates            | 4280          |\n",
      "|    policy_gradient_loss | -1.87e-05     |\n",
      "|    reward               | -0.099995315  |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.65e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 542           |\n",
      "|    iterations           | 101           |\n",
      "|    time_elapsed         | 381           |\n",
      "|    total_timesteps      | 206848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0500662e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -9.06e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6e+06         |\n",
      "|    n_updates            | 4290          |\n",
      "|    policy_gradient_loss | 1.19e-05      |\n",
      "|    reward               | 124.19968     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.2e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 542           |\n",
      "|    iterations           | 102           |\n",
      "|    time_elapsed         | 384           |\n",
      "|    total_timesteps      | 208896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3716635e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000206     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.69e+05      |\n",
      "|    n_updates            | 4300          |\n",
      "|    policy_gradient_loss | -8.24e-06     |\n",
      "|    reward               | 295.7391      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.38e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 173\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 543           |\n",
      "|    iterations           | 103           |\n",
      "|    time_elapsed         | 388           |\n",
      "|    total_timesteps      | 210944        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8227729e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 3.6e-05       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.68e+06      |\n",
      "|    n_updates            | 4310          |\n",
      "|    policy_gradient_loss | 5.62e-06      |\n",
      "|    reward               | 104.92392     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 9.37e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 543           |\n",
      "|    iterations           | 104           |\n",
      "|    time_elapsed         | 392           |\n",
      "|    total_timesteps      | 212992        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8859282e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000186      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.02e+07      |\n",
      "|    n_updates            | 4320          |\n",
      "|    policy_gradient_loss | 4.25e-07      |\n",
      "|    reward               | 416.01004     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.04e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 542           |\n",
      "|    iterations           | 105           |\n",
      "|    time_elapsed         | 396           |\n",
      "|    total_timesteps      | 215040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3236498e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -2.37e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.85e+06      |\n",
      "|    n_updates            | 4330          |\n",
      "|    policy_gradient_loss | -7.16e-06     |\n",
      "|    reward               | 1013.7492     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.97e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 174\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 106          |\n",
      "|    time_elapsed         | 399          |\n",
      "|    total_timesteps      | 217088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.843467e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.00012      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.12e+07     |\n",
      "|    n_updates            | 4340         |\n",
      "|    policy_gradient_loss | -7.25e-06    |\n",
      "|    reward               | 26.446577    |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 1.22e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 543           |\n",
      "|    iterations           | 107           |\n",
      "|    time_elapsed         | 403           |\n",
      "|    total_timesteps      | 219136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6973582e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 8.08e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.61e+07      |\n",
      "|    n_updates            | 4350          |\n",
      "|    policy_gradient_loss | -1.19e-06     |\n",
      "|    reward               | 174.49724     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 3.22e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 175\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 543           |\n",
      "|    iterations           | 108           |\n",
      "|    time_elapsed         | 406           |\n",
      "|    total_timesteps      | 221184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2913446e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000207      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.35e+06      |\n",
      "|    n_updates            | 4360          |\n",
      "|    policy_gradient_loss | -1.55e-06     |\n",
      "|    reward               | 7.5419374     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 2.7e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 544           |\n",
      "|    iterations           | 109           |\n",
      "|    time_elapsed         | 410           |\n",
      "|    total_timesteps      | 223232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3332541e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000168      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.13e+06      |\n",
      "|    n_updates            | 4370          |\n",
      "|    policy_gradient_loss | -3.62e-06     |\n",
      "|    reward               | 47.882442     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 6.27e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 544           |\n",
      "|    iterations           | 110           |\n",
      "|    time_elapsed         | 414           |\n",
      "|    total_timesteps      | 225280        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2090036e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000165      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.84e+05      |\n",
      "|    n_updates            | 4380          |\n",
      "|    policy_gradient_loss | -9.76e-06     |\n",
      "|    reward               | 57.264595     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 7.68e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 176\n",
      "row: 5250, episode: 176\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2022251.63\n",
      "total_reward: 1022251.63\n",
      "total_cost: 54137.21\n",
      "total_trades: 224\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 543          |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 417          |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.283241e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.00363     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.5e+05      |\n",
      "|    n_updates            | 4390         |\n",
      "|    policy_gradient_loss | -9.05e-06    |\n",
      "|    reward               | 91.97537     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 7e+05        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 541           |\n",
      "|    iterations           | 112           |\n",
      "|    time_elapsed         | 423           |\n",
      "|    total_timesteps      | 229376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0109128e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000397      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.12e+05      |\n",
      "|    n_updates            | 4400          |\n",
      "|    policy_gradient_loss | -6.17e-05     |\n",
      "|    reward               | 341.23267     |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.22e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 177\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 538           |\n",
      "|    iterations           | 113           |\n",
      "|    time_elapsed         | 429           |\n",
      "|    total_timesteps      | 231424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1403608e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -4.86e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.04e+06      |\n",
      "|    n_updates            | 4410          |\n",
      "|    policy_gradient_loss | 1.9e-05       |\n",
      "|    reward               | 50.76254      |\n",
      "|    std                  | 1.03          |\n",
      "|    value_loss           | 1.21e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 538           |\n",
      "|    iterations           | 114           |\n",
      "|    time_elapsed         | 433           |\n",
      "|    total_timesteps      | 233472        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1269232e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000165      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.49e+07      |\n",
      "|    n_updates            | 4420          |\n",
      "|    policy_gradient_loss | 6.61e-06      |\n",
      "|    reward               | 303.5328      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.98e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 537         |\n",
      "|    iterations           | 115         |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 235520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.82952e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 9.89e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.05e+06    |\n",
      "|    n_updates            | 4430        |\n",
      "|    policy_gradient_loss | -1.59e-05   |\n",
      "|    reward               | 519.68713   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 1.01e+07    |\n",
      "-----------------------------------------\n",
      "Episode: 178\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 535          |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 443          |\n",
      "|    total_timesteps      | 237568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.474729e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000258     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.71e+07     |\n",
      "|    n_updates            | 4440         |\n",
      "|    policy_gradient_loss | 5.42e-06     |\n",
      "|    reward               | 29.828571    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.41e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 534           |\n",
      "|    iterations           | 117           |\n",
      "|    time_elapsed         | 448           |\n",
      "|    total_timesteps      | 239616        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4965266e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000182      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.06e+07      |\n",
      "|    n_updates            | 4450          |\n",
      "|    policy_gradient_loss | -4.06e-06     |\n",
      "|    reward               | 119.19115     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.12e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 179\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 531           |\n",
      "|    iterations           | 118           |\n",
      "|    time_elapsed         | 454           |\n",
      "|    total_timesteps      | 241664        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0372834e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000742     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.56e+05      |\n",
      "|    n_updates            | 4460          |\n",
      "|    policy_gradient_loss | -1.03e-05     |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.51e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 530           |\n",
      "|    iterations           | 119           |\n",
      "|    time_elapsed         | 459           |\n",
      "|    total_timesteps      | 243712        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4310062e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000344      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.81e+06      |\n",
      "|    n_updates            | 4470          |\n",
      "|    policy_gradient_loss | -8.48e-06     |\n",
      "|    reward               | 285.79572     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.56e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 530           |\n",
      "|    iterations           | 120           |\n",
      "|    time_elapsed         | 463           |\n",
      "|    total_timesteps      | 245760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0419171e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000105     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.04e+06      |\n",
      "|    n_updates            | 4480          |\n",
      "|    policy_gradient_loss | 6.74e-07      |\n",
      "|    reward               | 635.9949      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 8.07e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 180\n",
      "row: 5250, episode: 180\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11736243.34\n",
      "total_reward: 10736243.34\n",
      "total_cost: 126613.91\n",
      "total_trades: 119\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 530          |\n",
      "|    iterations           | 121          |\n",
      "|    time_elapsed         | 466          |\n",
      "|    total_timesteps      | 247808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -6.6e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.57e+07     |\n",
      "|    n_updates            | 4490         |\n",
      "|    policy_gradient_loss | -6.93e-07    |\n",
      "|    reward               | 11.858948    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.13e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 529         |\n",
      "|    iterations           | 122         |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 249856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.14208e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 9.72e-05    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.99e+07    |\n",
      "|    n_updates            | 4500        |\n",
      "|    policy_gradient_loss | -1.95e-06   |\n",
      "|    reward               | 143.09392   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 9.98e+07    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 528         |\n",
      "|    iterations           | 123         |\n",
      "|    time_elapsed         | 476         |\n",
      "|    total_timesteps      | 251904      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.61239e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | -0.000182   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.79e+05    |\n",
      "|    n_updates            | 4510        |\n",
      "|    policy_gradient_loss | -2.85e-06   |\n",
      "|    reward               | 291.7338    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 1.56e+06    |\n",
      "-----------------------------------------\n",
      "Episode: 181\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 528          |\n",
      "|    iterations           | 124          |\n",
      "|    time_elapsed         | 480          |\n",
      "|    total_timesteps      | 253952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.785275e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000409     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.95e+06     |\n",
      "|    n_updates            | 4520         |\n",
      "|    policy_gradient_loss | -1.71e-06    |\n",
      "|    reward               | 161.77628    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.39e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 528           |\n",
      "|    iterations           | 125           |\n",
      "|    time_elapsed         | 484           |\n",
      "|    total_timesteps      | 256000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9848812e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 7.89e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.77e+06      |\n",
      "|    n_updates            | 4530          |\n",
      "|    policy_gradient_loss | 2.06e-07      |\n",
      "|    reward               | 569.2088      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 7.54e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 182\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 528         |\n",
      "|    iterations           | 126         |\n",
      "|    time_elapsed         | 488         |\n",
      "|    total_timesteps      | 258048      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.34233e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.000103    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.49e+07    |\n",
      "|    n_updates            | 4540        |\n",
      "|    policy_gradient_loss | -2.52e-07   |\n",
      "|    reward               | 18.61986    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 2.99e+07    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 528           |\n",
      "|    iterations           | 127           |\n",
      "|    time_elapsed         | 492           |\n",
      "|    total_timesteps      | 260096        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3940735e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000263      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.4e+07       |\n",
      "|    n_updates            | 4550          |\n",
      "|    policy_gradient_loss | -9.15e-07     |\n",
      "|    reward               | 161.90634     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.79e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 527          |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 497          |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.513437e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -8.54e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.08e+06     |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | -8.79e-06    |\n",
      "|    reward               | 487.22354    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 4.15e+06     |\n",
      "------------------------------------------\n",
      "Episode: 183\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 527          |\n",
      "|    iterations           | 129          |\n",
      "|    time_elapsed         | 500          |\n",
      "|    total_timesteps      | 264192       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.518405e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000172     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.92e+07     |\n",
      "|    n_updates            | 4570         |\n",
      "|    policy_gradient_loss | -1.45e-05    |\n",
      "|    reward               | 56.269115    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 3.85e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 504           |\n",
      "|    total_timesteps      | 266240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9740913e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000136      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.02e+06      |\n",
      "|    n_updates            | 4580          |\n",
      "|    policy_gradient_loss | -2.5e-05      |\n",
      "|    reward               | 217.09705     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.8e+07       |\n",
      "-------------------------------------------\n",
      "Episode: 184\n",
      "row: 5250, episode: 184\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4140190.48\n",
      "total_reward: 3140190.48\n",
      "total_cost: 40453.16\n",
      "total_trades: 81\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 131           |\n",
      "|    time_elapsed         | 508           |\n",
      "|    total_timesteps      | 268288        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1770538e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 4.37e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.68e+06      |\n",
      "|    n_updates            | 4590          |\n",
      "|    policy_gradient_loss | -1.02e-05     |\n",
      "|    reward               | 9.160662      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 5.37e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 528          |\n",
      "|    iterations           | 132          |\n",
      "|    time_elapsed         | 511          |\n",
      "|    total_timesteps      | 270336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.177893e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.00025      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.84e+06     |\n",
      "|    n_updates            | 4600         |\n",
      "|    policy_gradient_loss | -7.16e-06    |\n",
      "|    reward               | 46.90772     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.97e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 528           |\n",
      "|    iterations           | 133           |\n",
      "|    time_elapsed         | 515           |\n",
      "|    total_timesteps      | 272384        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6530935e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -6.32e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.76e+05      |\n",
      "|    n_updates            | 4610          |\n",
      "|    policy_gradient_loss | -0.00013      |\n",
      "|    reward               | 159.4111      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.35e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 185\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 528           |\n",
      "|    iterations           | 134           |\n",
      "|    time_elapsed         | 519           |\n",
      "|    total_timesteps      | 274432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0469684e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000412      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.26e+06      |\n",
      "|    n_updates            | 4620          |\n",
      "|    policy_gradient_loss | 3.12e-05      |\n",
      "|    reward               | 55.97776      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 4.52e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 528           |\n",
      "|    iterations           | 135           |\n",
      "|    time_elapsed         | 522           |\n",
      "|    total_timesteps      | 276480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3147073e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000476      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.4e+06       |\n",
      "|    n_updates            | 4630          |\n",
      "|    policy_gradient_loss | -6.32e-05     |\n",
      "|    reward               | 204.13347     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.81e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 186\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 528          |\n",
      "|    iterations           | 136          |\n",
      "|    time_elapsed         | 526          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.499228e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -8.12e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.84e+06     |\n",
      "|    n_updates            | 4640         |\n",
      "|    policy_gradient_loss | -2.23e-05    |\n",
      "|    reward               | -0.09989997  |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 3.68e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 528           |\n",
      "|    iterations           | 137           |\n",
      "|    time_elapsed         | 530           |\n",
      "|    total_timesteps      | 280576        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3199169e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000307      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.37e+07      |\n",
      "|    n_updates            | 4650          |\n",
      "|    policy_gradient_loss | 8.13e-06      |\n",
      "|    reward               | 229.88213     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.73e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 528         |\n",
      "|    iterations           | 138         |\n",
      "|    time_elapsed         | 535         |\n",
      "|    total_timesteps      | 282624      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.67406e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | -3.79e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.49e+06    |\n",
      "|    n_updates            | 4660        |\n",
      "|    policy_gradient_loss | -2.47e-06   |\n",
      "|    reward               | 592.4056    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 4.98e+06    |\n",
      "-----------------------------------------\n",
      "Episode: 187\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 139           |\n",
      "|    time_elapsed         | 539           |\n",
      "|    total_timesteps      | 284672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1385495e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 4.95e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.38e+07      |\n",
      "|    n_updates            | 4670          |\n",
      "|    policy_gradient_loss | -2.46e-06     |\n",
      "|    reward               | 54.38064      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 4.76e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 528          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 542          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.932793e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000138     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.94e+07     |\n",
      "|    n_updates            | 4680         |\n",
      "|    policy_gradient_loss | -7.41e-06    |\n",
      "|    reward               | 195.87357    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.87e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 141           |\n",
      "|    time_elapsed         | 547           |\n",
      "|    total_timesteps      | 288768        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4560646e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000788     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.87e+06      |\n",
      "|    n_updates            | 4690          |\n",
      "|    policy_gradient_loss | 2.31e-07      |\n",
      "|    reward               | 436.25806     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.74e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 188\n",
      "row: 5250, episode: 188\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5362580.47\n",
      "total_reward: 4362580.47\n",
      "total_cost: 105416.84\n",
      "total_trades: 224\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 142           |\n",
      "|    time_elapsed         | 551           |\n",
      "|    total_timesteps      | 290816        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7535058e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000143      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.69e+07      |\n",
      "|    n_updates            | 4700          |\n",
      "|    policy_gradient_loss | -5.81e-06     |\n",
      "|    reward               | 354.2651      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.39e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 143           |\n",
      "|    time_elapsed         | 555           |\n",
      "|    total_timesteps      | 292864        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1010284e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -1.13e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.29e+06      |\n",
      "|    n_updates            | 4710          |\n",
      "|    policy_gradient_loss | -5.02e-06     |\n",
      "|    reward               | 762.1054      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 8.59e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 189\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 144           |\n",
      "|    time_elapsed         | 559           |\n",
      "|    total_timesteps      | 294912        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7541183e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 6.96e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.76e+07      |\n",
      "|    n_updates            | 4720          |\n",
      "|    policy_gradient_loss | -1.37e-05     |\n",
      "|    reward               | 3.440533      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 7.52e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 527          |\n",
      "|    iterations           | 145          |\n",
      "|    time_elapsed         | 562          |\n",
      "|    total_timesteps      | 296960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.930166e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 8.57e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.54e+07     |\n",
      "|    n_updates            | 4730         |\n",
      "|    policy_gradient_loss | 2.03e-06     |\n",
      "|    reward               | 119.746925   |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.31e+08     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 527          |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 566          |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.468043e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.000522    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.72e+05     |\n",
      "|    n_updates            | 4740         |\n",
      "|    policy_gradient_loss | -3.69e-06    |\n",
      "|    reward               | 364.01022    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.54e+06     |\n",
      "------------------------------------------\n",
      "Episode: 190\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 527          |\n",
      "|    iterations           | 147          |\n",
      "|    time_elapsed         | 571          |\n",
      "|    total_timesteps      | 301056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.579771e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000265     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.17e+06     |\n",
      "|    n_updates            | 4750         |\n",
      "|    policy_gradient_loss | -2.56e-06    |\n",
      "|    reward               | 149.52106    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.63e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 148           |\n",
      "|    time_elapsed         | 575           |\n",
      "|    total_timesteps      | 303104        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7284183e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000255      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.28e+06      |\n",
      "|    n_updates            | 4760          |\n",
      "|    policy_gradient_loss | -1.53e-05     |\n",
      "|    reward               | 340.0819      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.56e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 191\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 149           |\n",
      "|    time_elapsed         | 579           |\n",
      "|    total_timesteps      | 305152        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3478212e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000199      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.34e+06      |\n",
      "|    n_updates            | 4770          |\n",
      "|    policy_gradient_loss | 8.6e-06       |\n",
      "|    reward               | 13.828463     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.47e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 150           |\n",
      "|    time_elapsed         | 582           |\n",
      "|    total_timesteps      | 307200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9849193e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000198      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.92e+07      |\n",
      "|    n_updates            | 4780          |\n",
      "|    policy_gradient_loss | -2.59e-07     |\n",
      "|    reward               | 137.24376     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.83e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 151           |\n",
      "|    time_elapsed         | 586           |\n",
      "|    total_timesteps      | 309248        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4472753e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.000356     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.39e+05      |\n",
      "|    n_updates            | 4790          |\n",
      "|    policy_gradient_loss | -7.91e-06     |\n",
      "|    reward               | 306.3918      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.08e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 192\n",
      "row: 5250, episode: 192\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5040868.08\n",
      "total_reward: 4040868.08\n",
      "total_cost: 107153.63\n",
      "total_trades: 168\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 152           |\n",
      "|    time_elapsed         | 590           |\n",
      "|    total_timesteps      | 311296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1855598e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000347      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.93e+06      |\n",
      "|    n_updates            | 4800          |\n",
      "|    policy_gradient_loss | -3.09e-05     |\n",
      "|    reward               | 132.90793     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.39e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 527         |\n",
      "|    iterations           | 153         |\n",
      "|    time_elapsed         | 594         |\n",
      "|    total_timesteps      | 313344      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.23473e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.000225    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.87e+06    |\n",
      "|    n_updates            | 4810        |\n",
      "|    policy_gradient_loss | -3.53e-07   |\n",
      "|    reward               | 257.38104   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 1.17e+07    |\n",
      "-----------------------------------------\n",
      "Episode: 193\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 527          |\n",
      "|    iterations           | 154          |\n",
      "|    time_elapsed         | 598          |\n",
      "|    total_timesteps      | 315392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.263711e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000235     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.8e+06      |\n",
      "|    n_updates            | 4820         |\n",
      "|    policy_gradient_loss | 4.73e-07     |\n",
      "|    reward               | 42.572605    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 9.59e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 155           |\n",
      "|    time_elapsed         | 602           |\n",
      "|    total_timesteps      | 317440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0372757e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000249      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.87e+07      |\n",
      "|    n_updates            | 4830          |\n",
      "|    policy_gradient_loss | -8.4e-06      |\n",
      "|    reward               | 276.0548      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.74e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 156           |\n",
      "|    time_elapsed         | 606           |\n",
      "|    total_timesteps      | 319488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2130477e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | -0.00013      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.07e+06      |\n",
      "|    n_updates            | 4840          |\n",
      "|    policy_gradient_loss | -1.17e-05     |\n",
      "|    reward               | 722.54755     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.01e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 194\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 526          |\n",
      "|    iterations           | 157          |\n",
      "|    time_elapsed         | 610          |\n",
      "|    total_timesteps      | 321536       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.071074e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.000163     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.54e+07     |\n",
      "|    n_updates            | 4850         |\n",
      "|    policy_gradient_loss | -6.92e-06    |\n",
      "|    reward               | 97.87275     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.08e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 527           |\n",
      "|    iterations           | 158           |\n",
      "|    time_elapsed         | 613           |\n",
      "|    total_timesteps      | 323584        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3456156e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000104      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.38e+07      |\n",
      "|    n_updates            | 4860          |\n",
      "|    policy_gradient_loss | -3.34e-05     |\n",
      "|    reward               | 208.09985     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 8.76e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 195\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 526          |\n",
      "|    iterations           | 159          |\n",
      "|    time_elapsed         | 618          |\n",
      "|    total_timesteps      | 325632       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.858892e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.000553    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.16e+06     |\n",
      "|    n_updates            | 4870         |\n",
      "|    policy_gradient_loss | -8.4e-06     |\n",
      "|    reward               | -0.09999944  |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 6.33e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 621           |\n",
      "|    total_timesteps      | 327680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4371042e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.000267      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.52e+06      |\n",
      "|    n_updates            | 4880          |\n",
      "|    policy_gradient_loss | -2.15e-05     |\n",
      "|    reward               | 218.15918     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.5e+07       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 526          |\n",
      "|    iterations           | 161          |\n",
      "|    time_elapsed         | 626          |\n",
      "|    total_timesteps      | 329728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.752191e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.000163    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.22e+06     |\n",
      "|    n_updates            | 4890         |\n",
      "|    policy_gradient_loss | -6.82e-05    |\n",
      "|    reward               | 495.44687    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 4.45e+06     |\n",
      "------------------------------------------\n",
      "Episode: 196\n",
      "row: 5250, episode: 196\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8128896.27\n",
      "total_reward: 7128896.27\n",
      "total_cost: 156074.42\n",
      "total_trades: 330\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 526          |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 630          |\n",
      "|    total_timesteps      | 331776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.401118e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | -0.000122    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.95e+07     |\n",
      "|    n_updates            | 4900         |\n",
      "|    policy_gradient_loss | 1.22e-05     |\n",
      "|    reward               | 12.602893    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 3.9e+07      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 526          |\n",
      "|    iterations           | 163          |\n",
      "|    time_elapsed         | 634          |\n",
      "|    total_timesteps      | 333824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.144393e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.45        |\n",
      "|    explained_variance   | 0.00015      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.98e+07     |\n",
      "|    n_updates            | 4910         |\n",
      "|    policy_gradient_loss | 1.09e-07     |\n",
      "|    reward               | 252.34393    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.96e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 164           |\n",
      "|    time_elapsed         | 638           |\n",
      "|    total_timesteps      | 335872        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7334546e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 1.19e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.11e+06      |\n",
      "|    n_updates            | 4920          |\n",
      "|    policy_gradient_loss | -2.36e-06     |\n",
      "|    reward               | 609.5084      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.22e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 197\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 526         |\n",
      "|    iterations           | 165         |\n",
      "|    time_elapsed         | 642         |\n",
      "|    total_timesteps      | 337920      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.36788e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.45       |\n",
      "|    explained_variance   | 0.000262    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.7e+07     |\n",
      "|    n_updates            | 4930        |\n",
      "|    policy_gradient_loss | 9.95e-07    |\n",
      "|    reward               | 145.25131   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 5.4e+07     |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 166           |\n",
      "|    time_elapsed         | 646           |\n",
      "|    total_timesteps      | 339968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1783268e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.45         |\n",
      "|    explained_variance   | 0.00012       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.96e+06      |\n",
      "|    n_updates            | 4940          |\n",
      "|    policy_gradient_loss | -1.38e-05     |\n",
      "|    reward               | 769.42725     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 9.92e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 198\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 167           |\n",
      "|    time_elapsed         | 650           |\n",
      "|    total_timesteps      | 342016        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8990326e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 5.41e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.61e+07      |\n",
      "|    n_updates            | 4950          |\n",
      "|    policy_gradient_loss | 2.64e-05      |\n",
      "|    reward               | 38.42315      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.23e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 526          |\n",
      "|    iterations           | 168          |\n",
      "|    time_elapsed         | 653          |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.334717e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 7.03e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.11e+08     |\n",
      "|    n_updates            | 4960         |\n",
      "|    policy_gradient_loss | -1.17e-05    |\n",
      "|    reward               | 162.20753    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 2.21e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 526           |\n",
      "|    iterations           | 169           |\n",
      "|    time_elapsed         | 657           |\n",
      "|    total_timesteps      | 346112        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6110425e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000197     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.37e+06      |\n",
      "|    n_updates            | 4970          |\n",
      "|    policy_gradient_loss | -1.3e-05      |\n",
      "|    reward               | 422.77936     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.74e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 199\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 170           |\n",
      "|    time_elapsed         | 662           |\n",
      "|    total_timesteps      | 348160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3722456e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000395      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.16e+07      |\n",
      "|    n_updates            | 4980          |\n",
      "|    policy_gradient_loss | -4.03e-06     |\n",
      "|    reward               | 90.036156     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.32e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 171           |\n",
      "|    time_elapsed         | 666           |\n",
      "|    total_timesteps      | 350208        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.4968557e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000218      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.41e+06      |\n",
      "|    n_updates            | 4990          |\n",
      "|    policy_gradient_loss | -4.98e-06     |\n",
      "|    reward               | 247.00926     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.08e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 200\n",
      "row: 5250, episode: 200\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6132903.20\n",
      "total_reward: 5132903.20\n",
      "total_cost: 95320.15\n",
      "total_trades: 266\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 525         |\n",
      "|    iterations           | 172         |\n",
      "|    time_elapsed         | 670         |\n",
      "|    total_timesteps      | 352256      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.28941e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 2.75e-05    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.41e+06    |\n",
      "|    n_updates            | 5000        |\n",
      "|    policy_gradient_loss | -4.96e-05   |\n",
      "|    reward               | 25.700993   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 6.82e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 173           |\n",
      "|    time_elapsed         | 674           |\n",
      "|    total_timesteps      | 354304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2698583e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000218      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.62e+07      |\n",
      "|    n_updates            | 5010          |\n",
      "|    policy_gradient_loss | 3.22e-05      |\n",
      "|    reward               | 27.84508      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.23e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 525          |\n",
      "|    iterations           | 174          |\n",
      "|    time_elapsed         | 678          |\n",
      "|    total_timesteps      | 356352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.518595e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -0.000164    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.48e+05     |\n",
      "|    n_updates            | 5020         |\n",
      "|    policy_gradient_loss | -0.000113    |\n",
      "|    reward               | 188.69812    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 2.97e+05     |\n",
      "------------------------------------------\n",
      "Episode: 201\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 525          |\n",
      "|    iterations           | 175          |\n",
      "|    time_elapsed         | 682          |\n",
      "|    total_timesteps      | 358400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.747839e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000567     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.42e+06     |\n",
      "|    n_updates            | 5030         |\n",
      "|    policy_gradient_loss | -7.67e-05    |\n",
      "|    reward               | 20.887472    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 2.84e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 176           |\n",
      "|    time_elapsed         | 686           |\n",
      "|    total_timesteps      | 360448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6293256e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000292      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.34e+06      |\n",
      "|    n_updates            | 5040          |\n",
      "|    policy_gradient_loss | -5.08e-05     |\n",
      "|    reward               | 214.87372     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.68e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 202\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 177           |\n",
      "|    time_elapsed         | 689           |\n",
      "|    total_timesteps      | 362496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7397777e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000191     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.35e+06      |\n",
      "|    n_updates            | 5050          |\n",
      "|    policy_gradient_loss | -2.05e-05     |\n",
      "|    reward               | -0.09991217   |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 4.69e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 178           |\n",
      "|    time_elapsed         | 693           |\n",
      "|    total_timesteps      | 364544        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7080456e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000187      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.7e+07       |\n",
      "|    n_updates            | 5060          |\n",
      "|    policy_gradient_loss | -2.74e-07     |\n",
      "|    reward               | 271.85092     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.39e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 525          |\n",
      "|    iterations           | 179          |\n",
      "|    time_elapsed         | 697          |\n",
      "|    total_timesteps      | 366592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.421783e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -0.000147    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.53e+06     |\n",
      "|    n_updates            | 5070         |\n",
      "|    policy_gradient_loss | -1.7e-05     |\n",
      "|    reward               | 631.2954     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 7.07e+06     |\n",
      "------------------------------------------\n",
      "Episode: 203\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 525           |\n",
      "|    iterations           | 180           |\n",
      "|    time_elapsed         | 701           |\n",
      "|    total_timesteps      | 368640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5770278e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -6.2e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.86e+07      |\n",
      "|    n_updates            | 5080          |\n",
      "|    policy_gradient_loss | -2.87e-05     |\n",
      "|    reward               | 8.984502      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 5.72e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 525          |\n",
      "|    iterations           | 181          |\n",
      "|    time_elapsed         | 705          |\n",
      "|    total_timesteps      | 370688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.388015e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 9.4e-05      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.24e+07     |\n",
      "|    n_updates            | 5090         |\n",
      "|    policy_gradient_loss | -6.42e-06    |\n",
      "|    reward               | 165.71828    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.05e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 524           |\n",
      "|    iterations           | 182           |\n",
      "|    time_elapsed         | 710           |\n",
      "|    total_timesteps      | 372736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9430666e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000934     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.07e+06      |\n",
      "|    n_updates            | 5100          |\n",
      "|    policy_gradient_loss | -2.39e-05     |\n",
      "|    reward               | 316.31686     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.13e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 204\n",
      "row: 5250, episode: 204\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4163168.52\n",
      "total_reward: 3163168.52\n",
      "total_cost: 73466.82\n",
      "total_trades: 274\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 524          |\n",
      "|    iterations           | 183          |\n",
      "|    time_elapsed         | 713          |\n",
      "|    total_timesteps      | 374784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.379624e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -2.61e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.93e+06     |\n",
      "|    n_updates            | 5110         |\n",
      "|    policy_gradient_loss | 1.7e-05      |\n",
      "|    reward               | 64.46176     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.99e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 524           |\n",
      "|    iterations           | 184           |\n",
      "|    time_elapsed         | 717           |\n",
      "|    total_timesteps      | 376832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4629913e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 5.73e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.53e+05      |\n",
      "|    n_updates            | 5120          |\n",
      "|    policy_gradient_loss | -6.53e-06     |\n",
      "|    reward               | 202.03369     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.11e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 205\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 524          |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 721          |\n",
      "|    total_timesteps      | 378880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.329754e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -5.96e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.57e+06     |\n",
      "|    n_updates            | 5130         |\n",
      "|    policy_gradient_loss | -3.79e-06    |\n",
      "|    reward               | -0.2102259   |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.14e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 524           |\n",
      "|    iterations           | 186           |\n",
      "|    time_elapsed         | 725           |\n",
      "|    total_timesteps      | 380928        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1315722e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000281      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.17e+06      |\n",
      "|    n_updates            | 5140          |\n",
      "|    policy_gradient_loss | 3.24e-06      |\n",
      "|    reward               | 79.75474      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.43e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 524           |\n",
      "|    iterations           | 187           |\n",
      "|    time_elapsed         | 730           |\n",
      "|    total_timesteps      | 382976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4551056e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.00125      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.98e+05      |\n",
      "|    n_updates            | 5150          |\n",
      "|    policy_gradient_loss | -2.79e-05     |\n",
      "|    reward               | 251.72285     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 7.96e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 206\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 524          |\n",
      "|    iterations           | 188          |\n",
      "|    time_elapsed         | 733          |\n",
      "|    total_timesteps      | 385024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.878094e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000182     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.43e+06     |\n",
      "|    n_updates            | 5160         |\n",
      "|    policy_gradient_loss | -4.37e-05    |\n",
      "|    reward               | 77.34255     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.09e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 524          |\n",
      "|    iterations           | 189          |\n",
      "|    time_elapsed         | 737          |\n",
      "|    total_timesteps      | 387072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.637899e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000341     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.49e+06     |\n",
      "|    n_updates            | 5170         |\n",
      "|    policy_gradient_loss | -3.05e-05    |\n",
      "|    reward               | 271.79855    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 2.98e+06     |\n",
      "------------------------------------------\n",
      "Episode: 207\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 524          |\n",
      "|    iterations           | 190          |\n",
      "|    time_elapsed         | 741          |\n",
      "|    total_timesteps      | 389120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.786978e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000162     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.61e+06     |\n",
      "|    n_updates            | 5180         |\n",
      "|    policy_gradient_loss | 5.15e-06     |\n",
      "|    reward               | 17.919092    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 7.23e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 524           |\n",
      "|    iterations           | 191           |\n",
      "|    time_elapsed         | 745           |\n",
      "|    total_timesteps      | 391168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5060511e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000188      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.26e+07      |\n",
      "|    n_updates            | 5190          |\n",
      "|    policy_gradient_loss | -4.64e-05     |\n",
      "|    reward               | 348.35266     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.52e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 524           |\n",
      "|    iterations           | 192           |\n",
      "|    time_elapsed         | 750           |\n",
      "|    total_timesteps      | 393216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6025594e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 3.36e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.27e+06      |\n",
      "|    n_updates            | 5200          |\n",
      "|    policy_gradient_loss | -2.24e-05     |\n",
      "|    reward               | 742.4656      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 8.54e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 208\n",
      "row: 5250, episode: 208\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8391931.52\n",
      "total_reward: 7391931.52\n",
      "total_cost: 114479.40\n",
      "total_trades: 281\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 193           |\n",
      "|    time_elapsed         | 755           |\n",
      "|    total_timesteps      | 395264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3095269e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.00024       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.47e+07      |\n",
      "|    n_updates            | 5210          |\n",
      "|    policy_gradient_loss | -3.75e-06     |\n",
      "|    reward               | 59.876255     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.94e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 194          |\n",
      "|    time_elapsed         | 759          |\n",
      "|    total_timesteps      | 397312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.492867e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000107     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.22e+07     |\n",
      "|    n_updates            | 5220         |\n",
      "|    policy_gradient_loss | -6.69e-05    |\n",
      "|    reward               | 197.90622    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 4.44e+07     |\n",
      "------------------------------------------\n",
      "Episode: 209\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 195          |\n",
      "|    time_elapsed         | 764          |\n",
      "|    total_timesteps      | 399360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.812274e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000444     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.48e+06     |\n",
      "|    n_updates            | 5230         |\n",
      "|    policy_gradient_loss | -5.88e-06    |\n",
      "|    reward               | 9.138864     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 4.96e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 196          |\n",
      "|    time_elapsed         | 767          |\n",
      "|    total_timesteps      | 401408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.191564e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.00018      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.71e+06     |\n",
      "|    n_updates            | 5240         |\n",
      "|    policy_gradient_loss | -5.09e-06    |\n",
      "|    reward               | 77.40961     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.34e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 522         |\n",
      "|    iterations           | 197         |\n",
      "|    time_elapsed         | 771         |\n",
      "|    total_timesteps      | 403456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.72425e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | -0.000191   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.26e+05    |\n",
      "|    n_updates            | 5250        |\n",
      "|    policy_gradient_loss | -1.7e-05    |\n",
      "|    reward               | 204.4494    |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 4.52e+05    |\n",
      "-----------------------------------------\n",
      "Episode: 210\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 198          |\n",
      "|    time_elapsed         | 775          |\n",
      "|    total_timesteps      | 405504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.223453e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000328     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.22e+06     |\n",
      "|    n_updates            | 5260         |\n",
      "|    policy_gradient_loss | 1.9e-06      |\n",
      "|    reward               | 22.742378    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 4.45e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 199           |\n",
      "|    time_elapsed         | 779           |\n",
      "|    total_timesteps      | 407552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7630747e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000431      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.52e+06      |\n",
      "|    n_updates            | 5270          |\n",
      "|    policy_gradient_loss | -4.82e-05     |\n",
      "|    reward               | 58.61349      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 7.04e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 211\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 200           |\n",
      "|    time_elapsed         | 784           |\n",
      "|    total_timesteps      | 409600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7542508e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.00028      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.73e+05      |\n",
      "|    n_updates            | 5280          |\n",
      "|    policy_gradient_loss | -5.29e-05     |\n",
      "|    reward               | -0.09990092   |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 7.46e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 201           |\n",
      "|    time_elapsed         | 788           |\n",
      "|    total_timesteps      | 411648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2887532e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000586      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.72e+06      |\n",
      "|    n_updates            | 5290          |\n",
      "|    policy_gradient_loss | -0.000259     |\n",
      "|    reward               | 286.76654     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.43e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 202           |\n",
      "|    time_elapsed         | 792           |\n",
      "|    total_timesteps      | 413696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8653169e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -4.78e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.28e+06      |\n",
      "|    n_updates            | 5300          |\n",
      "|    policy_gradient_loss | 2.4e-05       |\n",
      "|    reward               | 535.234       |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.57e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 212\n",
      "row: 5250, episode: 212\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6409828.16\n",
      "total_reward: 5409828.16\n",
      "total_cost: 148020.64\n",
      "total_trades: 270\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 203           |\n",
      "|    time_elapsed         | 796           |\n",
      "|    total_timesteps      | 415744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4874095e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 4.35e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.13e+07      |\n",
      "|    n_updates            | 5310          |\n",
      "|    policy_gradient_loss | 2.26e-06      |\n",
      "|    reward               | 100.826996    |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 4.26e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 204           |\n",
      "|    time_elapsed         | 800           |\n",
      "|    total_timesteps      | 417792        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4744305e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000146      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.51e+07      |\n",
      "|    n_updates            | 5320          |\n",
      "|    policy_gradient_loss | 2.17e-06      |\n",
      "|    reward               | 223.7116      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 5.02e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 205           |\n",
      "|    time_elapsed         | 804           |\n",
      "|    total_timesteps      | 419840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5727786e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.0003        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.94e+06      |\n",
      "|    n_updates            | 5330          |\n",
      "|    policy_gradient_loss | -2.32e-06     |\n",
      "|    reward               | 402.53403     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.39e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 213\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 206           |\n",
      "|    time_elapsed         | 807           |\n",
      "|    total_timesteps      | 421888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5401976e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000337      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.11e+07      |\n",
      "|    n_updates            | 5340          |\n",
      "|    policy_gradient_loss | 9.85e-07      |\n",
      "|    reward               | 90.517815     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.23e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 207          |\n",
      "|    time_elapsed         | 811          |\n",
      "|    total_timesteps      | 423936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.573085e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000221     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.67e+06     |\n",
      "|    n_updates            | 5350         |\n",
      "|    policy_gradient_loss | -3.66e-06    |\n",
      "|    reward               | 302.84244    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.34e+06     |\n",
      "------------------------------------------\n",
      "Episode: 214\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 208           |\n",
      "|    time_elapsed         | 815           |\n",
      "|    total_timesteps      | 425984        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1827873e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000104      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.19e+06      |\n",
      "|    n_updates            | 5360          |\n",
      "|    policy_gradient_loss | 1.68e-06      |\n",
      "|    reward               | 72.39517      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.04e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 209           |\n",
      "|    time_elapsed         | 819           |\n",
      "|    total_timesteps      | 428032        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4400728e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000254      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.45e+07      |\n",
      "|    n_updates            | 5370          |\n",
      "|    policy_gradient_loss | -6.31e-06     |\n",
      "|    reward               | 240.30559     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.91e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 210           |\n",
      "|    time_elapsed         | 823           |\n",
      "|    total_timesteps      | 430080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9773142e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.00021      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.49e+06      |\n",
      "|    n_updates            | 5380          |\n",
      "|    policy_gradient_loss | -5.08e-06     |\n",
      "|    reward               | 655.85895     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 8.97e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 215\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 211           |\n",
      "|    time_elapsed         | 826           |\n",
      "|    total_timesteps      | 432128        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6528294e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000277      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.34e+07      |\n",
      "|    n_updates            | 5390          |\n",
      "|    policy_gradient_loss | -5.66e-07     |\n",
      "|    reward               | 44.63634      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.67e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 212           |\n",
      "|    time_elapsed         | 831           |\n",
      "|    total_timesteps      | 434176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9278453e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000238      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.73e+07      |\n",
      "|    n_updates            | 5400          |\n",
      "|    policy_gradient_loss | -6.22e-08     |\n",
      "|    reward               | 160.31303     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.46e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 216\n",
      "row: 5250, episode: 216\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4345309.20\n",
      "total_reward: 3345309.20\n",
      "total_cost: 95727.07\n",
      "total_trades: 262\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 213           |\n",
      "|    time_elapsed         | 835           |\n",
      "|    total_timesteps      | 436224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9169972e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000279     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.6e+06       |\n",
      "|    n_updates            | 5410          |\n",
      "|    policy_gradient_loss | -4.34e-05     |\n",
      "|    reward               | 29.356539     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.2e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 214           |\n",
      "|    time_elapsed         | 838           |\n",
      "|    total_timesteps      | 438272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0414313e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000321      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.77e+06      |\n",
      "|    n_updates            | 5420          |\n",
      "|    policy_gradient_loss | -1.43e-05     |\n",
      "|    reward               | 87.009445     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.35e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 215           |\n",
      "|    time_elapsed         | 843           |\n",
      "|    total_timesteps      | 440320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5205849e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000604      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.22e+05      |\n",
      "|    n_updates            | 5430          |\n",
      "|    policy_gradient_loss | -7.23e-05     |\n",
      "|    reward               | 239.997       |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 8.44e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 217\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 216           |\n",
      "|    time_elapsed         | 847           |\n",
      "|    total_timesteps      | 442368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3429439e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.00077       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.41e+06      |\n",
      "|    n_updates            | 5440          |\n",
      "|    policy_gradient_loss | 0.000108      |\n",
      "|    reward               | 26.124914     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.82e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 217           |\n",
      "|    time_elapsed         | 850           |\n",
      "|    total_timesteps      | 444416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5454515e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000516      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.87e+06      |\n",
      "|    n_updates            | 5450          |\n",
      "|    policy_gradient_loss | -3.6e-07      |\n",
      "|    reward               | 101.303185    |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 5.73e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 218\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 521          |\n",
      "|    iterations           | 218          |\n",
      "|    time_elapsed         | 855          |\n",
      "|    total_timesteps      | 446464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.298557e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -0.00103     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.07e+05     |\n",
      "|    n_updates            | 5460         |\n",
      "|    policy_gradient_loss | -6.48e-05    |\n",
      "|    reward               | -0.09991519  |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 1.61e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 219           |\n",
      "|    time_elapsed         | 859           |\n",
      "|    total_timesteps      | 448512        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2375502e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000974      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.72e+06      |\n",
      "|    n_updates            | 5470          |\n",
      "|    policy_gradient_loss | -7.25e-05     |\n",
      "|    reward               | 69.696        |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 7.44e+06      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 521         |\n",
      "|    iterations           | 220         |\n",
      "|    time_elapsed         | 863         |\n",
      "|    total_timesteps      | 450560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.74407e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | -0.0011     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.12e+05    |\n",
      "|    n_updates            | 5480        |\n",
      "|    policy_gradient_loss | -4.93e-06   |\n",
      "|    reward               | 165.42055   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 4.24e+05    |\n",
      "-----------------------------------------\n",
      "Episode: 219\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 221           |\n",
      "|    time_elapsed         | 867           |\n",
      "|    total_timesteps      | 452608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4265028e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 2.4e-05       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.86e+06      |\n",
      "|    n_updates            | 5490          |\n",
      "|    policy_gradient_loss | -2.73e-05     |\n",
      "|    reward               | -9.239637     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.71e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 521          |\n",
      "|    iterations           | 222          |\n",
      "|    time_elapsed         | 871          |\n",
      "|    total_timesteps      | 454656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.324662e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000506     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.64e+06     |\n",
      "|    n_updates            | 5500         |\n",
      "|    policy_gradient_loss | -5.46e-06    |\n",
      "|    reward               | 163.5083     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.29e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 223           |\n",
      "|    time_elapsed         | 876           |\n",
      "|    total_timesteps      | 456704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.7235745e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000208     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.02e+06      |\n",
      "|    n_updates            | 5510          |\n",
      "|    policy_gradient_loss | 1.52e-07      |\n",
      "|    reward               | 529.00934     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.03e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 220\n",
      "row: 5250, episode: 220\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6290093.68\n",
      "total_reward: 5290093.68\n",
      "total_cost: 103681.55\n",
      "total_trades: 240\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 224           |\n",
      "|    time_elapsed         | 879           |\n",
      "|    total_timesteps      | 458752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2241223e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000177      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.65e+07      |\n",
      "|    n_updates            | 5520          |\n",
      "|    policy_gradient_loss | -7.02e-06     |\n",
      "|    reward               | 93.54551      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.29e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 521          |\n",
      "|    iterations           | 225          |\n",
      "|    time_elapsed         | 884          |\n",
      "|    total_timesteps      | 460800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.496295e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000136     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.35e+06     |\n",
      "|    n_updates            | 5530         |\n",
      "|    policy_gradient_loss | -4.12e-05    |\n",
      "|    reward               | 182.88177    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 4.69e+06     |\n",
      "------------------------------------------\n",
      "Episode: 221\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 226           |\n",
      "|    time_elapsed         | 888           |\n",
      "|    total_timesteps      | 462848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3331115e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000423      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.41e+06      |\n",
      "|    n_updates            | 5540          |\n",
      "|    policy_gradient_loss | -0.000114     |\n",
      "|    reward               | 49.61656      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.82e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 227           |\n",
      "|    time_elapsed         | 892           |\n",
      "|    total_timesteps      | 464896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8241903e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000263      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.13e+06      |\n",
      "|    n_updates            | 5550          |\n",
      "|    policy_gradient_loss | -9.5e-05      |\n",
      "|    reward               | 193.74252     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 8.25e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 228           |\n",
      "|    time_elapsed         | 896           |\n",
      "|    total_timesteps      | 466944        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0512263e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000209     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.25e+06      |\n",
      "|    n_updates            | 5560          |\n",
      "|    policy_gradient_loss | -0.000149     |\n",
      "|    reward               | 553.83734     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 4.49e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 222\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 229          |\n",
      "|    time_elapsed         | 900          |\n",
      "|    total_timesteps      | 468992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.954063e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.00023      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.76e+07     |\n",
      "|    n_updates            | 5570         |\n",
      "|    policy_gradient_loss | 5.7e-06      |\n",
      "|    reward               | 91.74099     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 3.52e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 230           |\n",
      "|    time_elapsed         | 904           |\n",
      "|    total_timesteps      | 471040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9752846e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000137      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.6e+06       |\n",
      "|    n_updates            | 5580          |\n",
      "|    policy_gradient_loss | 6.51e-06      |\n",
      "|    reward               | 313.88992     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.72e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 223\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 231           |\n",
      "|    time_elapsed         | 908           |\n",
      "|    total_timesteps      | 473088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6309863e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000164      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.5e+06       |\n",
      "|    n_updates            | 5590          |\n",
      "|    policy_gradient_loss | -1.74e-06     |\n",
      "|    reward               | 39.681057     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.3e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 232           |\n",
      "|    time_elapsed         | 912           |\n",
      "|    total_timesteps      | 475136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9950752e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000201      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.59e+07      |\n",
      "|    n_updates            | 5600          |\n",
      "|    policy_gradient_loss | -1.2e-05      |\n",
      "|    reward               | 264.2113      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 5.18e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 233           |\n",
      "|    time_elapsed         | 916           |\n",
      "|    total_timesteps      | 477184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6030542e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000278     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.23e+06      |\n",
      "|    n_updates            | 5610          |\n",
      "|    policy_gradient_loss | -1.5e-06      |\n",
      "|    reward               | 483.36707     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.45e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 224\n",
      "row: 5250, episode: 224\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6055924.42\n",
      "total_reward: 5055924.42\n",
      "total_cost: 201941.44\n",
      "total_trades: 366\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 234          |\n",
      "|    time_elapsed         | 920          |\n",
      "|    total_timesteps      | 479232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.824848e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000215     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.17e+07     |\n",
      "|    n_updates            | 5620         |\n",
      "|    policy_gradient_loss | -5.48e-06    |\n",
      "|    reward               | 50.2681      |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 4.33e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 235          |\n",
      "|    time_elapsed         | 925          |\n",
      "|    total_timesteps      | 481280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.670284e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.00019      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.08e+07     |\n",
      "|    n_updates            | 5630         |\n",
      "|    policy_gradient_loss | -4.75e-06    |\n",
      "|    reward               | 281.2454     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 2.16e+07     |\n",
      "------------------------------------------\n",
      "Episode: 225\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 236          |\n",
      "|    time_elapsed         | 929          |\n",
      "|    total_timesteps      | 483328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.228007e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000135     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.18e+06     |\n",
      "|    n_updates            | 5640         |\n",
      "|    policy_gradient_loss | -9.18e-06    |\n",
      "|    reward               | -0.099916406 |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 4.37e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 237           |\n",
      "|    time_elapsed         | 933           |\n",
      "|    total_timesteps      | 485376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8047867e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000113      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.36e+07      |\n",
      "|    n_updates            | 5650          |\n",
      "|    policy_gradient_loss | -2.95e-05     |\n",
      "|    reward               | 164.58995     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.72e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 238           |\n",
      "|    time_elapsed         | 938           |\n",
      "|    total_timesteps      | 487424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5201938e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000163     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.06e+06      |\n",
      "|    n_updates            | 5660          |\n",
      "|    policy_gradient_loss | -0.000157     |\n",
      "|    reward               | 265.0124      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.13e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 226\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 239           |\n",
      "|    time_elapsed         | 942           |\n",
      "|    total_timesteps      | 489472        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2808923e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000152      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.33e+06      |\n",
      "|    n_updates            | 5670          |\n",
      "|    policy_gradient_loss | -0.000162     |\n",
      "|    reward               | 80.360245     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.47e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 519         |\n",
      "|    iterations           | 240         |\n",
      "|    time_elapsed         | 946         |\n",
      "|    total_timesteps      | 491520      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.43098e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | 0.000243    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.67e+06    |\n",
      "|    n_updates            | 5680        |\n",
      "|    policy_gradient_loss | -2.66e-05   |\n",
      "|    reward               | 349.51605   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 1.73e+07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 241          |\n",
      "|    time_elapsed         | 950          |\n",
      "|    total_timesteps      | 493568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.605122e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -0.000397    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.63e+06     |\n",
      "|    n_updates            | 5690         |\n",
      "|    policy_gradient_loss | 8.19e-06     |\n",
      "|    reward               | 618.8744     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 9.26e+06     |\n",
      "------------------------------------------\n",
      "Episode: 227\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 242          |\n",
      "|    time_elapsed         | 953          |\n",
      "|    total_timesteps      | 495616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.598405e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000188     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.75e+07     |\n",
      "|    n_updates            | 5700         |\n",
      "|    policy_gradient_loss | -1.14e-06    |\n",
      "|    reward               | 159.13902    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 5.5e+07      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 243           |\n",
      "|    time_elapsed         | 957           |\n",
      "|    total_timesteps      | 497664        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0622898e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000176     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.19e+06      |\n",
      "|    n_updates            | 5710          |\n",
      "|    policy_gradient_loss | -1.37e-06     |\n",
      "|    reward               | 251.13712     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.39e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 228\n",
      "row: 5250, episode: 228\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4408388.32\n",
      "total_reward: 3408388.32\n",
      "total_cost: 129106.81\n",
      "total_trades: 173\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 244           |\n",
      "|    time_elapsed         | 961           |\n",
      "|    total_timesteps      | 499712        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8233626e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000427      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.01e+06      |\n",
      "|    n_updates            | 5720          |\n",
      "|    policy_gradient_loss | -5.67e-06     |\n",
      "|    reward               | 64.36434      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 8.03e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 245           |\n",
      "|    time_elapsed         | 965           |\n",
      "|    total_timesteps      | 501760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4953777e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000264      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.52e+06      |\n",
      "|    n_updates            | 5730          |\n",
      "|    policy_gradient_loss | -7.76e-07     |\n",
      "|    reward               | 208.58044     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.5e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 246           |\n",
      "|    time_elapsed         | 968           |\n",
      "|    total_timesteps      | 503808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9659866e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000511     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.4e+06       |\n",
      "|    n_updates            | 5740          |\n",
      "|    policy_gradient_loss | -9.39e-06     |\n",
      "|    reward               | 316.59982     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.81e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 229\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 247           |\n",
      "|    time_elapsed         | 972           |\n",
      "|    total_timesteps      | 505856        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3987301e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 6.82e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.18e+07      |\n",
      "|    n_updates            | 5750          |\n",
      "|    policy_gradient_loss | 1.13e-05      |\n",
      "|    reward               | 55.442406     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.35e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 248           |\n",
      "|    time_elapsed         | 976           |\n",
      "|    total_timesteps      | 507904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9278086e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000278      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.87e+06      |\n",
      "|    n_updates            | 5760          |\n",
      "|    policy_gradient_loss | -1.18e-05     |\n",
      "|    reward               | 249.0667      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 3.74e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 230\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 249           |\n",
      "|    time_elapsed         | 981           |\n",
      "|    total_timesteps      | 509952        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8755672e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 8.03e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.19e+06      |\n",
      "|    n_updates            | 5770          |\n",
      "|    policy_gradient_loss | 1.61e-05      |\n",
      "|    reward               | 52.988426     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 6.37e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 250           |\n",
      "|    time_elapsed         | 984           |\n",
      "|    total_timesteps      | 512000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8836228e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000201      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.17e+07      |\n",
      "|    n_updates            | 5780          |\n",
      "|    policy_gradient_loss | -2.72e-06     |\n",
      "|    reward               | 334.0623      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.34e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 251           |\n",
      "|    time_elapsed         | 988           |\n",
      "|    total_timesteps      | 514048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7107536e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -2.59e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.43e+06      |\n",
      "|    n_updates            | 5790          |\n",
      "|    policy_gradient_loss | -1.12e-05     |\n",
      "|    reward               | 813.90625     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.49e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 231\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 252          |\n",
      "|    time_elapsed         | 991          |\n",
      "|    total_timesteps      | 516096       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.618464e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000158     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.68e+07     |\n",
      "|    n_updates            | 5800         |\n",
      "|    policy_gradient_loss | 7.01e-06     |\n",
      "|    reward               | 69.04887     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 9.37e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 253           |\n",
      "|    time_elapsed         | 996           |\n",
      "|    total_timesteps      | 518144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2118911e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000172      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.47e+07      |\n",
      "|    n_updates            | 5810          |\n",
      "|    policy_gradient_loss | -9.05e-07     |\n",
      "|    reward               | 304.01172     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 4.94e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 232\n",
      "row: 5250, episode: 232\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4735018.18\n",
      "total_reward: 3735018.18\n",
      "total_cost: 121113.77\n",
      "total_trades: 329\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 254          |\n",
      "|    time_elapsed         | 1000         |\n",
      "|    total_timesteps      | 520192       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.829206e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000213     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.14e+06     |\n",
      "|    n_updates            | 5820         |\n",
      "|    policy_gradient_loss | -9.19e-06    |\n",
      "|    reward               | 52.67098     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 8.28e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 255           |\n",
      "|    time_elapsed         | 1004          |\n",
      "|    total_timesteps      | 522240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9196887e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000376      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.4e+07       |\n",
      "|    n_updates            | 5830          |\n",
      "|    policy_gradient_loss | 8.13e-06      |\n",
      "|    reward               | 276.85193     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.8e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 256           |\n",
      "|    time_elapsed         | 1009          |\n",
      "|    total_timesteps      | 524288        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7916623e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 3.95e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.54e+06      |\n",
      "|    n_updates            | 5840          |\n",
      "|    policy_gradient_loss | -1.76e-08     |\n",
      "|    reward               | 728.98        |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 9.07e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 233\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 257          |\n",
      "|    time_elapsed         | 1013         |\n",
      "|    total_timesteps      | 526336       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.363269e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.00021      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.24e+07     |\n",
      "|    n_updates            | 5850         |\n",
      "|    policy_gradient_loss | -3.7e-07     |\n",
      "|    reward               | 136.93741    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 6.48e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 258          |\n",
      "|    time_elapsed         | 1017         |\n",
      "|    total_timesteps      | 528384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.013985e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000129     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.32e+07     |\n",
      "|    n_updates            | 5860         |\n",
      "|    policy_gradient_loss | -5.78e-07    |\n",
      "|    reward               | 282.35498    |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 6.64e+07     |\n",
      "------------------------------------------\n",
      "Episode: 234\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 259           |\n",
      "|    time_elapsed         | 1021          |\n",
      "|    total_timesteps      | 530432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2302265e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000501     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.28e+06      |\n",
      "|    n_updates            | 5870          |\n",
      "|    policy_gradient_loss | -4.2e-06      |\n",
      "|    reward               | -0.09999657   |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 1.06e+07      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 518            |\n",
      "|    iterations           | 260            |\n",
      "|    time_elapsed         | 1026           |\n",
      "|    total_timesteps      | 532480         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.06025254e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.46          |\n",
      "|    explained_variance   | 0.000263       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 2.79e+07       |\n",
      "|    n_updates            | 5880           |\n",
      "|    policy_gradient_loss | 2.61e-06       |\n",
      "|    reward               | 85.27749       |\n",
      "|    std                  | 1.04           |\n",
      "|    value_loss           | 5.57e+07       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 261           |\n",
      "|    time_elapsed         | 1031          |\n",
      "|    total_timesteps      | 534528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8986287e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.00101      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.38e+05      |\n",
      "|    n_updates            | 5890          |\n",
      "|    policy_gradient_loss | -0.000119     |\n",
      "|    reward               | 213.48822     |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 2.76e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 235\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 262          |\n",
      "|    time_elapsed         | 1036         |\n",
      "|    total_timesteps      | 536576       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.307879e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -0.000653    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.06e+06     |\n",
      "|    n_updates            | 5900         |\n",
      "|    policy_gradient_loss | 8.01e-05     |\n",
      "|    reward               | 9.348838     |\n",
      "|    std                  | 1.04         |\n",
      "|    value_loss           | 6.12e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 263           |\n",
      "|    time_elapsed         | 1039          |\n",
      "|    total_timesteps      | 538624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5775163e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000395      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.54e+06      |\n",
      "|    n_updates            | 5910          |\n",
      "|    policy_gradient_loss | -2.11e-05     |\n",
      "|    reward               | 80.91037      |\n",
      "|    std                  | 1.04          |\n",
      "|    value_loss           | 7.09e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 264           |\n",
      "|    time_elapsed         | 1043          |\n",
      "|    total_timesteps      | 540672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3822126e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000425     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.46e+05      |\n",
      "|    n_updates            | 5920          |\n",
      "|    policy_gradient_loss | -0.000121     |\n",
      "|    reward               | 443.15436     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.93e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 236\n",
      "row: 5250, episode: 236\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5431543.50\n",
      "total_reward: 4431543.50\n",
      "total_cost: 176442.74\n",
      "total_trades: 95\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 265           |\n",
      "|    time_elapsed         | 1047          |\n",
      "|    total_timesteps      | 542720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9220111e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.00027       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.13e+07      |\n",
      "|    n_updates            | 5930          |\n",
      "|    policy_gradient_loss | 0.000113      |\n",
      "|    reward               | 153.73727     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 2.25e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 266           |\n",
      "|    time_elapsed         | 1051          |\n",
      "|    total_timesteps      | 544768        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1286846e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000135      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.27e+06      |\n",
      "|    n_updates            | 5940          |\n",
      "|    policy_gradient_loss | 4.95e-06      |\n",
      "|    reward               | 502.3668      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 6.54e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 237\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 267          |\n",
      "|    time_elapsed         | 1055         |\n",
      "|    total_timesteps      | 546816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.175833e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000278     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.77e+06     |\n",
      "|    n_updates            | 5950         |\n",
      "|    policy_gradient_loss | -3.09e-06    |\n",
      "|    reward               | 22.27938     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.95e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 268           |\n",
      "|    time_elapsed         | 1060          |\n",
      "|    total_timesteps      | 548864        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0986405e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000192      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.5e+07       |\n",
      "|    n_updates            | 5960          |\n",
      "|    policy_gradient_loss | -3.59e-05     |\n",
      "|    reward               | 220.22484     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 6.99e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 269          |\n",
      "|    time_elapsed         | 1065         |\n",
      "|    total_timesteps      | 550912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.736388e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | -9.63e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.19e+06     |\n",
      "|    n_updates            | 5970         |\n",
      "|    policy_gradient_loss | -1.28e-06    |\n",
      "|    reward               | 634.90045    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 6.39e+06     |\n",
      "------------------------------------------\n",
      "Episode: 238\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 270           |\n",
      "|    time_elapsed         | 1068          |\n",
      "|    total_timesteps      | 552960        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0713348e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.00033       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.69e+07      |\n",
      "|    n_updates            | 5980          |\n",
      "|    policy_gradient_loss | -1.13e-05     |\n",
      "|    reward               | 67.548134     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 5.38e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 271           |\n",
      "|    time_elapsed         | 1072          |\n",
      "|    total_timesteps      | 555008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.2346454e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000129      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.45e+07      |\n",
      "|    n_updates            | 5990          |\n",
      "|    policy_gradient_loss | -8.22e-06     |\n",
      "|    reward               | 186.2018      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 2.9e+07       |\n",
      "-------------------------------------------\n",
      "Episode: 239\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 272           |\n",
      "|    time_elapsed         | 1075          |\n",
      "|    total_timesteps      | 557056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0780793e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 2.84e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.29e+06      |\n",
      "|    n_updates            | 6000          |\n",
      "|    policy_gradient_loss | -2.83e-05     |\n",
      "|    reward               | 31.665812     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.58e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 273           |\n",
      "|    time_elapsed         | 1079          |\n",
      "|    total_timesteps      | 559104        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0117226e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000338      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.23e+07      |\n",
      "|    n_updates            | 6010          |\n",
      "|    policy_gradient_loss | -2.25e-05     |\n",
      "|    reward               | 216.90913     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 2.46e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 517         |\n",
      "|    iterations           | 274         |\n",
      "|    time_elapsed         | 1085        |\n",
      "|    total_timesteps      | 561152      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.83793e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.46       |\n",
      "|    explained_variance   | -4.98e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.04e+06    |\n",
      "|    n_updates            | 6020        |\n",
      "|    policy_gradient_loss | 9.44e-07    |\n",
      "|    reward               | 507.02197   |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 4.09e+06    |\n",
      "-----------------------------------------\n",
      "Episode: 240\n",
      "row: 5250, episode: 240\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8613235.66\n",
      "total_reward: 7613235.66\n",
      "total_cost: 94428.95\n",
      "total_trades: 557\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 275          |\n",
      "|    time_elapsed         | 1089         |\n",
      "|    total_timesteps      | 563200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.013985e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000225     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.18e+07     |\n",
      "|    n_updates            | 6030         |\n",
      "|    policy_gradient_loss | -6.33e-07    |\n",
      "|    reward               | 55.56924     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 4.35e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 276           |\n",
      "|    time_elapsed         | 1092          |\n",
      "|    total_timesteps      | 565248        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.1304385e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000139      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.12e+07      |\n",
      "|    n_updates            | 6040          |\n",
      "|    policy_gradient_loss | 7.86e-08      |\n",
      "|    reward               | 190.5803      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.23e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 241\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 277           |\n",
      "|    time_elapsed         | 1096          |\n",
      "|    total_timesteps      | 567296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8309524e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -9.45e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.42e+06      |\n",
      "|    n_updates            | 6050          |\n",
      "|    policy_gradient_loss | -7.7e-06      |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.83e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 278           |\n",
      "|    time_elapsed         | 1099          |\n",
      "|    total_timesteps      | 569344        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2599124e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000294      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.55e+06      |\n",
      "|    n_updates            | 6060          |\n",
      "|    policy_gradient_loss | 3.9e-06       |\n",
      "|    reward               | 210.60614     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.91e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 279           |\n",
      "|    time_elapsed         | 1103          |\n",
      "|    total_timesteps      | 571392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4709228e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.000162     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4e+06         |\n",
      "|    n_updates            | 6070          |\n",
      "|    policy_gradient_loss | -6.36e-06     |\n",
      "|    reward               | 623.47205     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 8.01e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 242\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 280           |\n",
      "|    time_elapsed         | 1106          |\n",
      "|    total_timesteps      | 573440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6982085e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 1.17e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.65e+07      |\n",
      "|    n_updates            | 6080          |\n",
      "|    policy_gradient_loss | 4.13e-06      |\n",
      "|    reward               | 32.35634      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 5.29e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 281           |\n",
      "|    time_elapsed         | 1111          |\n",
      "|    total_timesteps      | 575488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9435614e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000242      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.7e+07       |\n",
      "|    n_updates            | 6090          |\n",
      "|    policy_gradient_loss | -9.91e-06     |\n",
      "|    reward               | 206.63754     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 5.41e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 282           |\n",
      "|    time_elapsed         | 1116          |\n",
      "|    total_timesteps      | 577536        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4904072e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | -0.00129      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.85e+06      |\n",
      "|    n_updates            | 6100          |\n",
      "|    policy_gradient_loss | -7.39e-07     |\n",
      "|    reward               | 602.4226      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 3.69e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 243\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 283          |\n",
      "|    time_elapsed         | 1120         |\n",
      "|    total_timesteps      | 579584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.363231e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.46        |\n",
      "|    explained_variance   | 0.000122     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.07e+07     |\n",
      "|    n_updates            | 6110         |\n",
      "|    policy_gradient_loss | 1.59e-07     |\n",
      "|    reward               | 50.809967    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 4.14e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 284           |\n",
      "|    time_elapsed         | 1124          |\n",
      "|    total_timesteps      | 581632        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5623635e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.46         |\n",
      "|    explained_variance   | 0.000105      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.33e+06      |\n",
      "|    n_updates            | 6120          |\n",
      "|    policy_gradient_loss | -5.79e-05     |\n",
      "|    reward               | 248.12666     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 2.66e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 244\n",
      "row: 5250, episode: 244\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5541364.68\n",
      "total_reward: 4541364.68\n",
      "total_cost: 52785.82\n",
      "total_trades: 340\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 285          |\n",
      "|    time_elapsed         | 1128         |\n",
      "|    total_timesteps      | 583680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.053152e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.00024      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.39e+06     |\n",
      "|    n_updates            | 6130         |\n",
      "|    policy_gradient_loss | -0.00015     |\n",
      "|    reward               | 39.458794    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 6.77e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 286           |\n",
      "|    time_elapsed         | 1132          |\n",
      "|    total_timesteps      | 585728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6443525e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000277      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.04e+07      |\n",
      "|    n_updates            | 6140          |\n",
      "|    policy_gradient_loss | 6.03e-05      |\n",
      "|    reward               | 175.24507     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 2.09e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 287           |\n",
      "|    time_elapsed         | 1136          |\n",
      "|    total_timesteps      | 587776        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6699854e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000636     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.06e+06      |\n",
      "|    n_updates            | 6150          |\n",
      "|    policy_gradient_loss | -6.86e-06     |\n",
      "|    reward               | 518.56934     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.12e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 245\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 517           |\n",
      "|    iterations           | 288           |\n",
      "|    time_elapsed         | 1139          |\n",
      "|    total_timesteps      | 589824        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0180672e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000234      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.82e+07      |\n",
      "|    n_updates            | 6160          |\n",
      "|    policy_gradient_loss | 4.86e-06      |\n",
      "|    reward               | 354.0971      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 3.64e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 517          |\n",
      "|    iterations           | 289          |\n",
      "|    time_elapsed         | 1142         |\n",
      "|    total_timesteps      | 591872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 8.71e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.45e+06     |\n",
      "|    n_updates            | 6170         |\n",
      "|    policy_gradient_loss | -2.13e-06    |\n",
      "|    reward               | 574.2224     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.89e+07     |\n",
      "------------------------------------------\n",
      "Episode: 246\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 290           |\n",
      "|    time_elapsed         | 1146          |\n",
      "|    total_timesteps      | 593920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9557926e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000203      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.96e+07      |\n",
      "|    n_updates            | 6180          |\n",
      "|    policy_gradient_loss | -7.16e-06     |\n",
      "|    reward               | 9.293707      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 5.92e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 291           |\n",
      "|    time_elapsed         | 1150          |\n",
      "|    total_timesteps      | 595968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5972334e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000302      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.84e+07      |\n",
      "|    n_updates            | 6190          |\n",
      "|    policy_gradient_loss | -8.98e-06     |\n",
      "|    reward               | 54.19095      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 9.69e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 292           |\n",
      "|    time_elapsed         | 1153          |\n",
      "|    total_timesteps      | 598016        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3603207e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000442      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.66e+05      |\n",
      "|    n_updates            | 6200          |\n",
      "|    policy_gradient_loss | -2.14e-06     |\n",
      "|    reward               | 146.26741     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 7.31e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 247\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 293           |\n",
      "|    time_elapsed         | 1157          |\n",
      "|    total_timesteps      | 600064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9141298e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000484      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.14e+06      |\n",
      "|    n_updates            | 6210          |\n",
      "|    policy_gradient_loss | -1.76e-05     |\n",
      "|    reward               | 155.7248      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 2.29e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 294          |\n",
      "|    time_elapsed         | 1161         |\n",
      "|    total_timesteps      | 602112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.852811e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000952     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.98e+06     |\n",
      "|    n_updates            | 6220         |\n",
      "|    policy_gradient_loss | 1.19e-05     |\n",
      "|    reward               | 313.48965    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 3.96e+06     |\n",
      "------------------------------------------\n",
      "Episode: 248\n",
      "row: 5250, episode: 248\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6982163.33\n",
      "total_reward: 5982163.33\n",
      "total_cost: 103929.92\n",
      "total_trades: 306\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 295          |\n",
      "|    time_elapsed         | 1165         |\n",
      "|    total_timesteps      | 604160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.655778e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000429     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.74e+06     |\n",
      "|    n_updates            | 6230         |\n",
      "|    policy_gradient_loss | -1.48e-05    |\n",
      "|    reward               | 52.988426    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.15e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 518          |\n",
      "|    iterations           | 296          |\n",
      "|    time_elapsed         | 1168         |\n",
      "|    total_timesteps      | 606208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.015486e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000192     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.49e+07     |\n",
      "|    n_updates            | 6240         |\n",
      "|    policy_gradient_loss | -4.38e-06    |\n",
      "|    reward               | 194.3391     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 4.97e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 518           |\n",
      "|    iterations           | 297           |\n",
      "|    time_elapsed         | 1172          |\n",
      "|    total_timesteps      | 608256        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3189856e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -4.77e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.09e+06      |\n",
      "|    n_updates            | 6250          |\n",
      "|    policy_gradient_loss | -7.53e-07     |\n",
      "|    reward               | 740.2347      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 8.19e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 249\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 298           |\n",
      "|    time_elapsed         | 1175          |\n",
      "|    total_timesteps      | 610304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.7294105e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 8.58e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.58e+07      |\n",
      "|    n_updates            | 6260          |\n",
      "|    policy_gradient_loss | -2.68e-06     |\n",
      "|    reward               | 69.04359      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 5.17e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 299           |\n",
      "|    time_elapsed         | 1178          |\n",
      "|    total_timesteps      | 612352        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1862721e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000138      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.54e+07      |\n",
      "|    n_updates            | 6270          |\n",
      "|    policy_gradient_loss | -3.3e-05      |\n",
      "|    reward               | 202.02417     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 7.07e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 250\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 519           |\n",
      "|    iterations           | 300           |\n",
      "|    time_elapsed         | 1182          |\n",
      "|    total_timesteps      | 614400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5253318e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000699     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.8e+06       |\n",
      "|    n_updates            | 6280          |\n",
      "|    policy_gradient_loss | 2.69e-06      |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 3.6e+06       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 519          |\n",
      "|    iterations           | 301          |\n",
      "|    time_elapsed         | 1185         |\n",
      "|    total_timesteps      | 616448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.557637e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000365     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.16e+07     |\n",
      "|    n_updates            | 6290         |\n",
      "|    policy_gradient_loss | -5.59e-06    |\n",
      "|    reward               | 242.9523     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 4.31e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 302          |\n",
      "|    time_elapsed         | 1189         |\n",
      "|    total_timesteps      | 618496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.967189e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.000152    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.95e+06     |\n",
      "|    n_updates            | 6300         |\n",
      "|    policy_gradient_loss | -1.8e-05     |\n",
      "|    reward               | 512.23047    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 5.9e+06      |\n",
      "------------------------------------------\n",
      "Episode: 251\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 303          |\n",
      "|    time_elapsed         | 1192         |\n",
      "|    total_timesteps      | 620544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.817848e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 4.33e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.49e+07     |\n",
      "|    n_updates            | 6310         |\n",
      "|    policy_gradient_loss | 1.15e-05     |\n",
      "|    reward               | -1.7002747   |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 2.98e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 304          |\n",
      "|    time_elapsed         | 1196         |\n",
      "|    total_timesteps      | 622592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.519549e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000257     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.91e+07     |\n",
      "|    n_updates            | 6320         |\n",
      "|    policy_gradient_loss | -1.68e-05    |\n",
      "|    reward               | 157.18224    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 3.82e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 305          |\n",
      "|    time_elapsed         | 1200         |\n",
      "|    total_timesteps      | 624640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.285968e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.000471    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.24e+06     |\n",
      "|    n_updates            | 6330         |\n",
      "|    policy_gradient_loss | -2.82e-05    |\n",
      "|    reward               | 441.37915    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 2.47e+06     |\n",
      "------------------------------------------\n",
      "Episode: 252\n",
      "row: 5250, episode: 252\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5413791.37\n",
      "total_reward: 4413791.37\n",
      "total_cost: 120041.86\n",
      "total_trades: 221\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 306          |\n",
      "|    time_elapsed         | 1203         |\n",
      "|    total_timesteps      | 626688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.471543e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.00036      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.09e+07     |\n",
      "|    n_updates            | 6340         |\n",
      "|    policy_gradient_loss | 2.58e-05     |\n",
      "|    reward               | 88.213486    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 2.18e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 520           |\n",
      "|    iterations           | 307           |\n",
      "|    time_elapsed         | 1207          |\n",
      "|    total_timesteps      | 628736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2629683e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.00021       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.03e+06      |\n",
      "|    n_updates            | 6350          |\n",
      "|    policy_gradient_loss | -1.88e-05     |\n",
      "|    reward               | 235.98584     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 6.06e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 253\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 520          |\n",
      "|    iterations           | 308          |\n",
      "|    time_elapsed         | 1211         |\n",
      "|    total_timesteps      | 630784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.435482e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000131     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.71e+06     |\n",
      "|    n_updates            | 6360         |\n",
      "|    policy_gradient_loss | -2.67e-05    |\n",
      "|    reward               | 3.1450408    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 7.43e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 521          |\n",
      "|    iterations           | 309          |\n",
      "|    time_elapsed         | 1214         |\n",
      "|    total_timesteps      | 632832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.865825e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.00023      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.72e+07     |\n",
      "|    n_updates            | 6370         |\n",
      "|    policy_gradient_loss | -3.83e-05    |\n",
      "|    reward               | 25.009357    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 3.45e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 310           |\n",
      "|    time_elapsed         | 1218          |\n",
      "|    total_timesteps      | 634880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7751114e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000627     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.16e+05      |\n",
      "|    n_updates            | 6380          |\n",
      "|    policy_gradient_loss | -0.000104     |\n",
      "|    reward               | 122.69062     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.33e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 254\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 311           |\n",
      "|    time_elapsed         | 1221          |\n",
      "|    total_timesteps      | 636928        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4858753e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000704      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.77e+05      |\n",
      "|    n_updates            | 6390          |\n",
      "|    policy_gradient_loss | -3.99e-05     |\n",
      "|    reward               | 62.787533     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.95e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 521          |\n",
      "|    iterations           | 312          |\n",
      "|    time_elapsed         | 1225         |\n",
      "|    total_timesteps      | 638976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.910406e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000822     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.32e+05     |\n",
      "|    n_updates            | 6400         |\n",
      "|    policy_gradient_loss | -7.75e-06    |\n",
      "|    reward               | 129.07652    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.06e+06     |\n",
      "------------------------------------------\n",
      "Episode: 255\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 521          |\n",
      "|    iterations           | 313          |\n",
      "|    time_elapsed         | 1229         |\n",
      "|    total_timesteps      | 641024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.244386e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.000483    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.37e+06     |\n",
      "|    n_updates            | 6410         |\n",
      "|    policy_gradient_loss | 4.79e-06     |\n",
      "|    reward               | -3.7262213   |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 2.75e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 314           |\n",
      "|    time_elapsed         | 1232          |\n",
      "|    total_timesteps      | 643072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9859872e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.00065       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.44e+06      |\n",
      "|    n_updates            | 6420          |\n",
      "|    policy_gradient_loss | -2.43e-05     |\n",
      "|    reward               | 69.41084      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.29e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 315           |\n",
      "|    time_elapsed         | 1236          |\n",
      "|    total_timesteps      | 645120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4703779e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000377     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.32e+05      |\n",
      "|    n_updates            | 6430          |\n",
      "|    policy_gradient_loss | -3.76e-05     |\n",
      "|    reward               | 255.43524     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.63e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 256\n",
      "row: 5250, episode: 256\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4115909.81\n",
      "total_reward: 3115909.81\n",
      "total_cost: 96946.92\n",
      "total_trades: 167\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 316           |\n",
      "|    time_elapsed         | 1239          |\n",
      "|    total_timesteps      | 647168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0407588e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000498      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.7e+06       |\n",
      "|    n_updates            | 6440          |\n",
      "|    policy_gradient_loss | 1.13e-06      |\n",
      "|    reward               | 51.498356     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.14e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 317           |\n",
      "|    time_elapsed         | 1243          |\n",
      "|    total_timesteps      | 649216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3416575e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000475      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.14e+06      |\n",
      "|    n_updates            | 6450          |\n",
      "|    policy_gradient_loss | 7.81e-08      |\n",
      "|    reward               | 50.38566      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 8.27e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 257\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 318           |\n",
      "|    time_elapsed         | 1247          |\n",
      "|    total_timesteps      | 651264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7845305e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000901     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.02e+05      |\n",
      "|    n_updates            | 6460          |\n",
      "|    policy_gradient_loss | -2e-05        |\n",
      "|    reward               | -0.09999963   |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.4e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 319           |\n",
      "|    time_elapsed         | 1251          |\n",
      "|    total_timesteps      | 653312        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4412445e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.00118       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.34e+06      |\n",
      "|    n_updates            | 6470          |\n",
      "|    policy_gradient_loss | 1.66e-05      |\n",
      "|    reward               | 82.850555     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 2.69e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 320           |\n",
      "|    time_elapsed         | 1254          |\n",
      "|    total_timesteps      | 655360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1997927e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000835     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.82e+05      |\n",
      "|    n_updates            | 6480          |\n",
      "|    policy_gradient_loss | -6.31e-05     |\n",
      "|    reward               | 245.00626     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 7.65e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 258\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 522           |\n",
      "|    iterations           | 321           |\n",
      "|    time_elapsed         | 1257          |\n",
      "|    total_timesteps      | 657408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6606314e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -9.91e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.36e+06      |\n",
      "|    n_updates            | 6490          |\n",
      "|    policy_gradient_loss | -9.71e-05     |\n",
      "|    reward               | 5.4577866     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.73e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 322          |\n",
      "|    time_elapsed         | 1261         |\n",
      "|    total_timesteps      | 659456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.746021e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000639     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.61e+06     |\n",
      "|    n_updates            | 6500         |\n",
      "|    policy_gradient_loss | -1.3e-05     |\n",
      "|    reward               | 161.69739    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 5.21e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 323          |\n",
      "|    time_elapsed         | 1265         |\n",
      "|    total_timesteps      | 661504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.955494e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | -0.000395    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.12e+06     |\n",
      "|    n_updates            | 6510         |\n",
      "|    policy_gradient_loss | 3.39e-06     |\n",
      "|    reward               | 300.22842    |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 2.24e+06     |\n",
      "------------------------------------------\n",
      "Episode: 259\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 522          |\n",
      "|    iterations           | 324          |\n",
      "|    time_elapsed         | 1270         |\n",
      "|    total_timesteps      | 663552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.180221e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.47        |\n",
      "|    explained_variance   | 0.000102     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.01e+06     |\n",
      "|    n_updates            | 6520         |\n",
      "|    policy_gradient_loss | -2.01e-05    |\n",
      "|    reward               | 66.05795     |\n",
      "|    std                  | 1.05         |\n",
      "|    value_loss           | 1.8e+07      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 325           |\n",
      "|    time_elapsed         | 1276          |\n",
      "|    total_timesteps      | 665600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1721796e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000178      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.82e+05      |\n",
      "|    n_updates            | 6530          |\n",
      "|    policy_gradient_loss | -2.12e-05     |\n",
      "|    reward               | 221.55035     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.56e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 260\n",
      "row: 5250, episode: 260\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4448330.48\n",
      "total_reward: 3448330.48\n",
      "total_cost: 104327.30\n",
      "total_trades: 396\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 326           |\n",
      "|    time_elapsed         | 1280          |\n",
      "|    total_timesteps      | 667648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3597538e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000264      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.14e+06      |\n",
      "|    n_updates            | 6540          |\n",
      "|    policy_gradient_loss | 1.33e-05      |\n",
      "|    reward               | -11.645543    |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 4.29e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 327           |\n",
      "|    time_elapsed         | 1283          |\n",
      "|    total_timesteps      | 669696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3594475e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000353      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.36e+06      |\n",
      "|    n_updates            | 6550          |\n",
      "|    policy_gradient_loss | -1.35e-06     |\n",
      "|    reward               | 101.50529     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.47e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 328           |\n",
      "|    time_elapsed         | 1288          |\n",
      "|    total_timesteps      | 671744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0646472e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | -0.000101     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.34e+05      |\n",
      "|    n_updates            | 6560          |\n",
      "|    policy_gradient_loss | -7.27e-05     |\n",
      "|    reward               | 709.23486     |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 1.67e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 261\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 329           |\n",
      "|    time_elapsed         | 1293          |\n",
      "|    total_timesteps      | 673792        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6988604e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.47         |\n",
      "|    explained_variance   | 0.000111      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.68e+07      |\n",
      "|    n_updates            | 6570          |\n",
      "|    policy_gradient_loss | -0.000102     |\n",
      "|    reward               | 40.48175      |\n",
      "|    std                  | 1.05          |\n",
      "|    value_loss           | 3.36e+07      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent_ppo.train_model(model=trained_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trained_ppo = PPO(\"MlpPolicy\", env_train,n_steps=2048,ent_coef=0.01,learning_rate=0.00025,batch_size=2048,clip_range=0.1,\n",
    "#                   tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_ppo/\",verbose=10)\n",
    "# trained_ppo.learn(total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_train_gym = StockTradingEnv(df = trade_data, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = test_train_gym.reset()\n",
    "state = state[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action (array([-0.46568024], dtype=float32), None)\n",
      "Reward 0.0 at step 1.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 0.0 at step 2.\n",
      "Terminate False\n",
      "Action (array([0.48457778], dtype=float32), None)\n",
      "Reward -0.048456157409655864 at step 3.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.048456157409655864 at step 4.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.048456157409655864 at step 5.\n",
      "Terminate False\n",
      "Action (array([0.5664487], dtype=float32), None)\n",
      "Reward -0.07762497833251256 at step 6.\n",
      "Terminate False\n",
      "Action (array([-0.03085028], dtype=float32), None)\n",
      "Reward -0.07762497833251256 at step 7.\n",
      "Terminate False\n",
      "Action (array([0.91909885], dtype=float32), None)\n",
      "Reward -0.09810334779052064 at step 8.\n",
      "Terminate False\n",
      "Action (array([-0.80958605], dtype=float32), None)\n",
      "Reward -0.09810334779052064 at step 9.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09810334779052064 at step 10.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09810334779052064 at step 11.\n",
      "Terminate False\n",
      "Action (array([0.40440404], dtype=float32), None)\n",
      "Reward -0.09881064990386368 at step 12.\n",
      "Terminate False\n",
      "Action (array([-0.12025526], dtype=float32), None)\n",
      "Reward -0.09881064990386368 at step 13.\n",
      "Terminate False\n",
      "Action (array([0.3188955], dtype=float32), None)\n",
      "Reward -0.09915402172697942 at step 14.\n",
      "Terminate False\n",
      "Action (array([0.0934493], dtype=float32), None)\n",
      "Reward -0.09922026640166297 at step 15.\n",
      "Terminate False\n",
      "Action (array([0.04611516], dtype=float32), None)\n",
      "Reward -0.09924454440153205 at step 16.\n",
      "Terminate False\n",
      "Action (array([-0.3097736], dtype=float32), None)\n",
      "Reward -0.09924454440153205 at step 17.\n",
      "Terminate False\n",
      "Action (array([0.8807303], dtype=float32), None)\n",
      "Reward -0.09981972840575036 at step 18.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 19.\n",
      "Terminate False\n",
      "Action (array([-0.28423604], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 20.\n",
      "Terminate False\n",
      "Action (array([-0.9886899], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 21.\n",
      "Terminate False\n",
      "Action (array([-0.59597963], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 22.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 23.\n",
      "Terminate False\n",
      "Action (array([-0.95007646], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 24.\n",
      "Terminate False\n",
      "Action (array([-0.8162306], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 25.\n",
      "Terminate False\n",
      "Action (array([-0.01448816], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 26.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 27.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 28.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 29.\n",
      "Terminate False\n",
      "Action (array([0.8095131], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 30.\n",
      "Terminate False\n",
      "Action (array([-0.34687787], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 31.\n",
      "Terminate False\n",
      "Action (array([0.7767222], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 32.\n",
      "Terminate False\n",
      "Action (array([-0.34687915], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 33.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 34.\n",
      "Terminate False\n",
      "Action (array([0.34931406], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 35.\n",
      "Terminate False\n",
      "Action (array([0.40109736], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 36.\n",
      "Terminate False\n",
      "Action (array([-0.81968725], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 37.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 38.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 39.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 40.\n",
      "Terminate False\n",
      "Action (array([-0.66087997], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 41.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 42.\n",
      "Terminate False\n",
      "Action (array([-0.10495477], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 43.\n",
      "Terminate False\n",
      "Action (array([-0.85580933], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 44.\n",
      "Terminate False\n",
      "Action (array([0.292917], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 45.\n",
      "Terminate False\n",
      "Action (array([-0.20132905], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 46.\n",
      "Terminate False\n",
      "Action (array([-0.7756803], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 47.\n",
      "Terminate False\n",
      "Action (array([-0.9706264], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 48.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 49.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 50.\n",
      "Terminate False\n",
      "Action (array([-0.2555372], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 51.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 52.\n",
      "Terminate False\n",
      "Action (array([0.9892452], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 53.\n",
      "Terminate False\n",
      "Action (array([-0.37826136], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 54.\n",
      "Terminate False\n",
      "Action (array([0.17318384], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 55.\n",
      "Terminate False\n",
      "Action (array([-0.88318807], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 56.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 57.\n",
      "Terminate False\n",
      "Action (array([0.01265249], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 58.\n",
      "Terminate False\n",
      "Action (array([-0.5334322], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 59.\n",
      "Terminate False\n",
      "Action (array([-0.83712804], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 60.\n",
      "Terminate False\n",
      "Action (array([-0.6128561], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 61.\n",
      "Terminate False\n",
      "Action (array([0.36936402], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 62.\n",
      "Terminate False\n",
      "Action (array([0.38955447], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 63.\n",
      "Terminate False\n",
      "Action (array([-0.11806862], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 64.\n",
      "Terminate False\n",
      "Action (array([-0.6700999], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 65.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 66.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 67.\n",
      "Terminate False\n",
      "Action (array([-0.53446233], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 68.\n",
      "Terminate False\n",
      "Action (array([0.60169166], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 69.\n",
      "Terminate False\n",
      "Action (array([0.46876898], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 70.\n",
      "Terminate False\n",
      "Action (array([-0.07701955], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 71.\n",
      "Terminate False\n",
      "Action (array([-0.32896218], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 72.\n",
      "Terminate False\n",
      "Action (array([-0.4543618], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 73.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 74.\n",
      "Terminate False\n",
      "Action (array([-0.3125196], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 75.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 76.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 77.\n",
      "Terminate False\n",
      "Action (array([0.7092451], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 78.\n",
      "Terminate False\n",
      "Action (array([-0.00213059], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 79.\n",
      "Terminate False\n",
      "Action (array([-0.68018246], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 80.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 81.\n",
      "Terminate False\n",
      "Action (array([0.32263762], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 82.\n",
      "Terminate False\n",
      "Action (array([-0.11941677], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 83.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 84.\n",
      "Terminate False\n",
      "Action (array([0.16683783], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 85.\n",
      "Terminate False\n",
      "Action (array([-0.4768886], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 86.\n",
      "Terminate False\n",
      "Action (array([-0.20932314], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 87.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 88.\n",
      "Terminate False\n",
      "Action (array([-0.07678298], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 89.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 90.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 91.\n",
      "Terminate False\n",
      "Action (array([0.906092], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 92.\n",
      "Terminate False\n",
      "Action (array([0.32745853], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 93.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 94.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 95.\n",
      "Terminate False\n",
      "Action (array([0.8166766], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 96.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 97.\n",
      "Terminate False\n",
      "Action (array([0.71288973], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 98.\n",
      "Terminate False\n",
      "Action (array([0.05475138], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 99.\n",
      "Terminate False\n",
      "Action (array([0.9921733], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 100.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 101.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 102.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 103.\n",
      "Terminate False\n",
      "Action (array([0.04972207], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 104.\n",
      "Terminate False\n",
      "Action (array([-0.36711547], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 105.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 106.\n",
      "Terminate False\n",
      "Action (array([-0.93242246], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 107.\n",
      "Terminate False\n",
      "Action (array([0.1174235], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 108.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 109.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 110.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 111.\n",
      "Terminate False\n",
      "Action (array([0.89743346], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 112.\n",
      "Terminate False\n",
      "Action (array([0.34899628], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 113.\n",
      "Terminate False\n",
      "Action (array([0.57258415], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 114.\n",
      "Terminate False\n",
      "Action (array([-0.14967155], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 115.\n",
      "Terminate False\n",
      "Action (array([-0.35406953], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 116.\n",
      "Terminate False\n",
      "Action (array([-0.26133737], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 117.\n",
      "Terminate False\n",
      "Action (array([-0.76558137], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 118.\n",
      "Terminate False\n",
      "Action (array([0.03063826], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 119.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 120.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 121.\n",
      "Terminate False\n",
      "Action (array([-0.5662218], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 122.\n",
      "Terminate False\n",
      "Action (array([0.94911134], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 123.\n",
      "Terminate False\n",
      "Action (array([0.4281365], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 124.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 125.\n",
      "Terminate False\n",
      "Action (array([-0.85841185], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 126.\n",
      "Terminate False\n",
      "Action (array([-0.14870545], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 127.\n",
      "Terminate False\n",
      "Action (array([-0.14058529], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 128.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 129.\n",
      "Terminate False\n",
      "Action (array([-0.5499717], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 130.\n",
      "Terminate False\n",
      "Action (array([0.64241195], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 131.\n",
      "Terminate False\n",
      "Action (array([-0.9339034], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 132.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 133.\n",
      "Terminate False\n",
      "Action (array([0.3571961], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 134.\n",
      "Terminate False\n",
      "Action (array([0.872449], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 135.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 136.\n",
      "Terminate False\n",
      "Action (array([-0.38345847], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 137.\n",
      "Terminate False\n",
      "Action (array([0.22798179], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 138.\n",
      "Terminate False\n",
      "Action (array([-0.06716249], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 139.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 140.\n",
      "Terminate False\n",
      "Action (array([-0.29392552], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 141.\n",
      "Terminate False\n",
      "Action (array([-0.4148513], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 142.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 143.\n",
      "Terminate False\n",
      "Action (array([0.40745544], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 144.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 145.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 146.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 147.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 148.\n",
      "Terminate False\n",
      "Action (array([0.42366335], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 149.\n",
      "Terminate False\n",
      "Action (array([-0.5225048], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 150.\n",
      "Terminate False\n",
      "Action (array([-0.34195626], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 151.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 152.\n",
      "Terminate False\n",
      "Action (array([-0.41563505], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 153.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 154.\n",
      "Terminate False\n",
      "Action (array([0.69577557], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 155.\n",
      "Terminate False\n",
      "Action (array([0.8238423], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 156.\n",
      "Terminate False\n",
      "Action (array([-0.31414324], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 157.\n",
      "Terminate False\n",
      "Action (array([-0.08301489], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 158.\n",
      "Terminate False\n",
      "Action (array([0.7167316], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 159.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 160.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 161.\n",
      "Terminate False\n",
      "Action (array([0.29995722], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 162.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 163.\n",
      "Terminate False\n",
      "Action (array([0.6218867], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 164.\n",
      "Terminate False\n",
      "Action (array([0.58684987], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 165.\n",
      "Terminate False\n",
      "Action (array([0.1610662], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 166.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 167.\n",
      "Terminate False\n",
      "Action (array([-0.58507824], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 168.\n",
      "Terminate False\n",
      "Action (array([-0.54967153], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 169.\n",
      "Terminate False\n",
      "Action (array([-0.41631427], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 170.\n",
      "Terminate False\n",
      "Action (array([-0.02827231], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 171.\n",
      "Terminate False\n",
      "Action (array([-0.42875496], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 172.\n",
      "Terminate False\n",
      "Action (array([-0.27273926], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 173.\n",
      "Terminate False\n",
      "Action (array([0.29122928], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 174.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 175.\n",
      "Terminate False\n",
      "Action (array([0.07483004], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 176.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 177.\n",
      "Terminate False\n",
      "Action (array([-0.5107567], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 178.\n",
      "Terminate False\n",
      "Action (array([-0.18143506], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 179.\n",
      "Terminate False\n",
      "Action (array([-0.15654056], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 180.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 181.\n",
      "Terminate False\n",
      "Action (array([0.8436824], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 182.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 183.\n",
      "Terminate False\n",
      "Action (array([0.3819987], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 184.\n",
      "Terminate False\n",
      "Action (array([0.637905], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 185.\n",
      "Terminate False\n",
      "Action (array([-0.08734997], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 186.\n",
      "Terminate False\n",
      "Action (array([0.25751913], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 187.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 188.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 189.\n",
      "Terminate False\n",
      "Action (array([0.7573085], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 190.\n",
      "Terminate False\n",
      "Action (array([0.4608246], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 191.\n",
      "Terminate False\n",
      "Action (array([-0.32769847], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 192.\n",
      "Terminate False\n",
      "Action (array([-0.47646073], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 193.\n",
      "Terminate False\n",
      "Action (array([-0.11182993], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 194.\n",
      "Terminate False\n",
      "Action (array([0.09613483], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 195.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 196.\n",
      "Terminate False\n",
      "Action (array([-0.8713263], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 197.\n",
      "Terminate False\n",
      "Action (array([0.43504158], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 198.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 199.\n",
      "Terminate False\n",
      "Action (array([-0.6784297], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 200.\n",
      "Terminate False\n",
      "Action (array([-0.5451649], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 201.\n",
      "Terminate False\n",
      "Action (array([0.17824751], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 202.\n",
      "Terminate False\n",
      "Action (array([-0.38308707], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 203.\n",
      "Terminate False\n",
      "Action (array([0.7114067], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 204.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 205.\n",
      "Terminate False\n",
      "Action (array([-0.15800075], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 206.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 207.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 208.\n",
      "Terminate False\n",
      "Action (array([0.53973484], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 209.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 210.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 211.\n",
      "Terminate False\n",
      "Action (array([-0.6728328], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 212.\n",
      "Terminate False\n",
      "Action (array([0.72048193], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 213.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 214.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 215.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 216.\n",
      "Terminate False\n",
      "Action (array([-0.586209], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 217.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 218.\n",
      "Terminate False\n",
      "Action (array([0.9878579], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 219.\n",
      "Terminate False\n",
      "Action (array([0.8271916], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 220.\n",
      "Terminate False\n",
      "Action (array([0.07576597], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 221.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 222.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 223.\n",
      "Terminate False\n",
      "Action (array([-0.08433507], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 224.\n",
      "Terminate False\n",
      "Action (array([0.40145093], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 225.\n",
      "Terminate False\n",
      "Action (array([0.5652984], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 226.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 227.\n",
      "Terminate False\n",
      "Action (array([0.7169803], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 228.\n",
      "Terminate False\n",
      "Action (array([-0.25031987], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 229.\n",
      "Terminate False\n",
      "Action (array([0.9297963], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 230.\n",
      "Terminate False\n",
      "Action (array([-0.66321445], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 231.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 232.\n",
      "Terminate False\n",
      "Action (array([-0.49942243], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 233.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 234.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 235.\n",
      "Terminate False\n",
      "Action (array([0.6516408], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 236.\n",
      "Terminate False\n",
      "Action (array([0.6128307], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 237.\n",
      "Terminate False\n",
      "Action (array([-0.18823089], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 238.\n",
      "Terminate False\n",
      "Action (array([0.5813061], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 239.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 240.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 241.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 242.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 243.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 244.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 245.\n",
      "Terminate False\n",
      "Action (array([0.43258554], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 246.\n",
      "Terminate False\n",
      "Action (array([0.8755402], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 247.\n",
      "Terminate False\n",
      "Action (array([0.4309532], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 248.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 249.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 250.\n",
      "Terminate False\n",
      "Action (array([-0.05378817], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 251.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 252.\n",
      "Terminate False\n",
      "Action (array([-0.24444869], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 253.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 254.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 255.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 256.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 257.\n",
      "Terminate False\n",
      "Action (array([-0.38787764], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 258.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 259.\n",
      "Terminate False\n",
      "Action (array([-0.04284343], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 260.\n",
      "Terminate False\n",
      "Action (array([-0.3938013], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 261.\n",
      "Terminate False\n",
      "Action (array([0.2812898], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 262.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 263.\n",
      "Terminate False\n",
      "Action (array([-0.09455887], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 264.\n",
      "Terminate False\n",
      "Action (array([-0.6052686], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 265.\n",
      "Terminate False\n",
      "Action (array([-0.27253145], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 266.\n",
      "Terminate False\n",
      "Action (array([0.88418347], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 267.\n",
      "Terminate False\n",
      "Action (array([0.34520742], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 268.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 269.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 270.\n",
      "Terminate False\n",
      "Action (array([0.25059247], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 271.\n",
      "Terminate False\n",
      "Action (array([-0.5266037], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 272.\n",
      "Terminate False\n",
      "Action (array([0.48982644], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 273.\n",
      "Terminate False\n",
      "Action (array([-0.07472029], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 274.\n",
      "Terminate False\n",
      "Action (array([0.4332293], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 275.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 276.\n",
      "Terminate False\n",
      "Action (array([0.34816733], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 277.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 278.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 279.\n",
      "Terminate False\n",
      "Action (array([-0.8048689], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 280.\n",
      "Terminate False\n",
      "Action (array([0.79614323], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 281.\n",
      "Terminate False\n",
      "Action (array([0.05245371], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 282.\n",
      "Terminate False\n",
      "Action (array([-0.81564134], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 283.\n",
      "Terminate False\n",
      "Action (array([-0.82948107], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 284.\n",
      "Terminate False\n",
      "Action (array([0.07878382], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 285.\n",
      "Terminate False\n",
      "Action (array([0.09282103], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 286.\n",
      "Terminate False\n",
      "Action (array([0.5046178], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 287.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 288.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 289.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 290.\n",
      "Terminate False\n",
      "Action (array([-0.41017845], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 291.\n",
      "Terminate False\n",
      "Action (array([-0.55466944], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 292.\n",
      "Terminate False\n",
      "Action (array([-0.84562796], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 293.\n",
      "Terminate False\n",
      "Action (array([-0.7501959], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 294.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 295.\n",
      "Terminate False\n",
      "Action (array([0.8476707], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 296.\n",
      "Terminate False\n",
      "Action (array([0.37311333], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 297.\n",
      "Terminate False\n",
      "Action (array([-0.9451008], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 298.\n",
      "Terminate False\n",
      "Action (array([0.14611259], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 299.\n",
      "Terminate False\n",
      "Action (array([-0.6947488], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 300.\n",
      "Terminate False\n",
      "Action (array([0.48861772], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 301.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 302.\n",
      "Terminate False\n",
      "Action (array([-0.9434134], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 303.\n",
      "Terminate False\n",
      "Action (array([-0.8590855], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 304.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 305.\n",
      "Terminate False\n",
      "Action (array([0.73367363], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 306.\n",
      "Terminate False\n",
      "Action (array([0.7858306], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 307.\n",
      "Terminate False\n",
      "Action (array([0.44781658], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 308.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 309.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 310.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 311.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 312.\n",
      "Terminate False\n",
      "Action (array([-0.42913884], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 313.\n",
      "Terminate False\n",
      "Action (array([-0.25584608], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 314.\n",
      "Terminate False\n",
      "Action (array([-0.00783986], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 315.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 316.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 317.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 318.\n",
      "Terminate False\n",
      "Action (array([0.19931091], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 319.\n",
      "Terminate False\n",
      "Action (array([0.4575155], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 320.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 321.\n",
      "Terminate False\n",
      "Action (array([0.09888887], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 322.\n",
      "Terminate False\n",
      "Action (array([-0.36364275], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 323.\n",
      "Terminate False\n",
      "Action (array([-0.43775967], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 324.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 325.\n",
      "Terminate False\n",
      "Action (array([-0.7245619], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 326.\n",
      "Terminate False\n",
      "Action (array([-0.31756055], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 327.\n",
      "Terminate False\n",
      "Action (array([0.08949085], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 328.\n",
      "Terminate False\n",
      "Action (array([0.4499776], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 329.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 330.\n",
      "Terminate False\n",
      "Action (array([0.60782415], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 331.\n",
      "Terminate False\n",
      "Action (array([0.24407612], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 332.\n",
      "Terminate False\n",
      "Action (array([-0.5120895], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 333.\n",
      "Terminate False\n",
      "Action (array([-0.13584009], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 334.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 335.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 336.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 337.\n",
      "Terminate False\n",
      "Action (array([0.11471982], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 338.\n",
      "Terminate False\n",
      "Action (array([0.23090328], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 339.\n",
      "Terminate False\n",
      "Action (array([-0.7681551], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 340.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 341.\n",
      "Terminate False\n",
      "Action (array([-0.17146388], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 342.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 343.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 344.\n",
      "Terminate False\n",
      "Action (array([0.12772968], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 345.\n",
      "Terminate False\n",
      "Action (array([0.5555616], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 346.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 347.\n",
      "Terminate False\n",
      "Action (array([0.8481656], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 348.\n",
      "Terminate False\n",
      "Action (array([0.18710668], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 349.\n",
      "Terminate False\n",
      "Action (array([0.4987185], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 350.\n",
      "Terminate False\n",
      "Action (array([-0.95833373], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 351.\n",
      "Terminate False\n",
      "Action (array([0.53156304], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 352.\n",
      "Terminate False\n",
      "Action (array([-0.1949022], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 353.\n",
      "Terminate False\n",
      "Action (array([0.72316855], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 354.\n",
      "Terminate False\n",
      "Action (array([0.7421602], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 355.\n",
      "Terminate False\n",
      "Action (array([0.63285667], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 356.\n",
      "Terminate False\n",
      "Action (array([0.57783777], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 357.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 358.\n",
      "Terminate False\n",
      "Action (array([0.6421036], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 359.\n",
      "Terminate False\n",
      "Action (array([-0.31686026], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 360.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 361.\n",
      "Terminate False\n",
      "Action (array([0.14545734], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 362.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 363.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 364.\n",
      "Terminate False\n",
      "Action (array([-0.61934537], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 365.\n",
      "Terminate False\n",
      "Action (array([-0.35586768], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 366.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 367.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 368.\n",
      "Terminate False\n",
      "Action (array([0.48860985], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 369.\n",
      "Terminate False\n",
      "Action (array([0.01216239], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 370.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 371.\n",
      "Terminate False\n",
      "Action (array([-0.6132837], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 372.\n",
      "Terminate False\n",
      "Action (array([0.02302661], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 373.\n",
      "Terminate False\n",
      "Action (array([-0.47958115], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 374.\n",
      "Terminate False\n",
      "Action (array([-0.12474374], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 375.\n",
      "Terminate False\n",
      "Action (array([-0.61981744], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 376.\n",
      "Terminate False\n",
      "Action (array([-0.7607697], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 377.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 378.\n",
      "Terminate False\n",
      "Action (array([-0.12440427], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 379.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 380.\n",
      "Terminate False\n",
      "Action (array([-0.42174187], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 381.\n",
      "Terminate False\n",
      "Action (array([-0.02958742], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 382.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 383.\n",
      "Terminate False\n",
      "Action (array([0.19065608], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 384.\n",
      "Terminate False\n",
      "Action (array([-0.80618227], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 385.\n",
      "Terminate False\n",
      "Action (array([-0.12641464], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 386.\n",
      "Terminate False\n",
      "Action (array([0.17750283], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 387.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 388.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 389.\n",
      "Terminate False\n",
      "Action (array([-0.32894966], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 390.\n",
      "Terminate False\n",
      "Action (array([-0.19045644], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 391.\n",
      "Terminate False\n",
      "Action (array([0.24920867], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 392.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 393.\n",
      "Terminate False\n",
      "Action (array([-0.32781285], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 394.\n",
      "Terminate False\n",
      "Action (array([0.84145284], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 395.\n",
      "Terminate False\n",
      "Action (array([-0.9553062], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 396.\n",
      "Terminate False\n",
      "Action (array([0.4591522], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 397.\n",
      "Terminate False\n",
      "Action (array([-0.2551464], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 398.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 399.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 400.\n",
      "Terminate False\n",
      "Action (array([0.3690623], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 401.\n",
      "Terminate False\n",
      "Action (array([-0.87890553], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 402.\n",
      "Terminate False\n",
      "Action (array([0.5149715], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 403.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 404.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 405.\n",
      "Terminate False\n",
      "Action (array([-0.5456076], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 406.\n",
      "Terminate False\n",
      "Action (array([0.03493915], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 407.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 408.\n",
      "Terminate False\n",
      "Action (array([0.34639418], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 409.\n",
      "Terminate False\n",
      "Action (array([0.44088796], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 410.\n",
      "Terminate False\n",
      "Action (array([-0.10375658], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 411.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 412.\n",
      "Terminate False\n",
      "Action (array([-0.11211936], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 413.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 414.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 415.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 416.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 417.\n",
      "Terminate False\n",
      "Action (array([-0.7066381], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 418.\n",
      "Terminate False\n",
      "Action (array([-0.370602], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 419.\n",
      "Terminate False\n",
      "Action (array([0.6570998], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 420.\n",
      "Terminate False\n",
      "Action (array([-0.8671852], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 421.\n",
      "Terminate False\n",
      "Action (array([-0.39645544], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 422.\n",
      "Terminate False\n",
      "Action (array([-0.5462506], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 423.\n",
      "Terminate False\n",
      "Action (array([-0.30483145], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 424.\n",
      "Terminate False\n",
      "Action (array([0.06957122], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 425.\n",
      "Terminate False\n",
      "Action (array([-0.38047466], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 426.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 427.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 428.\n",
      "Terminate False\n",
      "Action (array([-0.3597575], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 429.\n",
      "Terminate False\n",
      "Action (array([-0.03089159], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 430.\n",
      "Terminate False\n",
      "Action (array([-0.5130427], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 431.\n",
      "Terminate False\n",
      "Action (array([-0.77422076], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 432.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 433.\n",
      "Terminate False\n",
      "Action (array([0.45137206], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 434.\n",
      "Terminate False\n",
      "Action (array([0.8015157], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 435.\n",
      "Terminate False\n",
      "Action (array([0.848926], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 436.\n",
      "Terminate False\n",
      "Action (array([0.17000896], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 437.\n",
      "Terminate False\n",
      "Action (array([-0.30794865], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 438.\n",
      "Terminate False\n",
      "Action (array([0.10139193], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 439.\n",
      "Terminate False\n",
      "Action (array([-0.50044125], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 440.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 441.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 442.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 443.\n",
      "Terminate False\n",
      "Action (array([-0.1549772], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 444.\n",
      "Terminate False\n",
      "Action (array([-0.2862037], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 445.\n",
      "Terminate False\n",
      "Action (array([-0.7945847], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 446.\n",
      "Terminate False\n",
      "Action (array([-0.6049081], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 447.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 448.\n",
      "Terminate False\n",
      "Action (array([0.07020578], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 449.\n",
      "Terminate False\n",
      "Action (array([-0.4563409], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 450.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 451.\n",
      "Terminate False\n",
      "Action (array([-0.2676514], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 452.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 453.\n",
      "Terminate False\n",
      "Action (array([0.1819663], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 454.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 455.\n",
      "Terminate False\n",
      "Action (array([0.7963099], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 456.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 457.\n",
      "Terminate False\n",
      "Action (array([-0.4093108], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 458.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 459.\n",
      "Terminate False\n",
      "Action (array([0.39966896], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 460.\n",
      "Terminate False\n",
      "Action (array([0.63767403], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 461.\n",
      "Terminate False\n",
      "Action (array([0.807481], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 462.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 463.\n",
      "Terminate False\n",
      "Action (array([-0.33313632], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 464.\n",
      "Terminate False\n",
      "Action (array([-0.7871275], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 465.\n",
      "Terminate False\n",
      "Action (array([-0.41568643], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 466.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 467.\n",
      "Terminate False\n",
      "Action (array([0.76977956], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 468.\n",
      "Terminate False\n",
      "Action (array([0.12934126], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 469.\n",
      "Terminate False\n",
      "Action (array([0.03084935], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 470.\n",
      "Terminate False\n",
      "Action (array([0.07007059], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 471.\n",
      "Terminate False\n",
      "Action (array([-0.36213934], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 472.\n",
      "Terminate False\n",
      "Action (array([-0.14527859], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 473.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 474.\n",
      "Terminate False\n",
      "Action (array([-0.29170927], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 475.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 476.\n",
      "Terminate False\n",
      "Action (array([-0.6228377], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 477.\n",
      "Terminate False\n",
      "Action (array([-0.7271454], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 478.\n",
      "Terminate False\n",
      "Action (array([-0.54475963], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 479.\n",
      "Terminate False\n",
      "Action (array([-0.3263685], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 480.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 481.\n",
      "Terminate False\n",
      "Action (array([-0.75710505], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 482.\n",
      "Terminate False\n",
      "Action (array([-0.5634168], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 483.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 484.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 485.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 486.\n",
      "Terminate False\n",
      "Action (array([-0.344429], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 487.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 488.\n",
      "Terminate False\n",
      "Action (array([-0.03643058], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 489.\n",
      "Terminate False\n",
      "Action (array([0.29199922], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 490.\n",
      "Terminate False\n",
      "Action (array([0.78110737], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 491.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 492.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 493.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward -0.09989959058836102 at step 494.\n",
      "Terminate False\n",
      "Action (array([-0.11370125], dtype=float32), None)\n",
      "Reward 32.274961865710466 at step 495.\n",
      "Terminate False\n",
      "Action (array([-0.13855127], dtype=float32), None)\n",
      "Reward 32.26376716042177 at step 496.\n",
      "Terminate False\n",
      "Action (array([0.75360763], dtype=float32), None)\n",
      "Reward 32.25534728782275 at step 497.\n",
      "Terminate False\n",
      "Action (array([0.3955884], dtype=float32), None)\n",
      "Reward 41.58328770282823 at step 498.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 41.54478985236436 at step 499.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 45.954286451088755 at step 500.\n",
      "Terminate False\n",
      "Action (array([-0.41226158], dtype=float32), None)\n",
      "Reward 45.94401221685871 at step 501.\n",
      "Terminate False\n",
      "Action (array([0.8737402], dtype=float32), None)\n",
      "Reward 45.935083387281075 at step 502.\n",
      "Terminate False\n",
      "Action (array([-0.1244609], dtype=float32), None)\n",
      "Reward 45.935083387281075 at step 503.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.22744084272541 at step 504.\n",
      "Terminate False\n",
      "Action (array([0.5835588], dtype=float32), None)\n",
      "Reward 46.22670730894245 at step 505.\n",
      "Terminate False\n",
      "Action (array([-0.31916246], dtype=float32), None)\n",
      "Reward 46.23961741636433 at step 506.\n",
      "Terminate False\n",
      "Action (array([0.96022743], dtype=float32), None)\n",
      "Reward 46.24042070345539 at step 507.\n",
      "Terminate False\n",
      "Action (array([0.22061399], dtype=float32), None)\n",
      "Reward 46.238004200005 at step 508.\n",
      "Terminate False\n",
      "Action (array([0.52277356], dtype=float32), None)\n",
      "Reward 46.237993267004796 at step 509.\n",
      "Terminate False\n",
      "Action (array([0.02530368], dtype=float32), None)\n",
      "Reward 46.263201282446694 at step 510.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.257753360824985 at step 511.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571407411587 at step 512.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 513.\n",
      "Terminate False\n",
      "Action (array([-0.4311069], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 514.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 515.\n",
      "Terminate False\n",
      "Action (array([0.62097543], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 516.\n",
      "Terminate False\n",
      "Action (array([-0.64389765], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 517.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 518.\n",
      "Terminate False\n",
      "Action (array([-0.3512129], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 519.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 520.\n",
      "Terminate False\n",
      "Action (array([-0.08744462], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 521.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 522.\n",
      "Terminate False\n",
      "Action (array([0.80764097], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 523.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 524.\n",
      "Terminate False\n",
      "Action (array([0.2790907], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 525.\n",
      "Terminate False\n",
      "Action (array([-0.459501], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 526.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 527.\n",
      "Terminate False\n",
      "Action (array([-0.9056835], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 528.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 529.\n",
      "Terminate False\n",
      "Action (array([-0.23937838], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 530.\n",
      "Terminate False\n",
      "Action (array([0.6852394], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 531.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 532.\n",
      "Terminate False\n",
      "Action (array([0.51306003], dtype=float32), None)\n",
      "Reward 46.257130628228424 at step 533.\n",
      "Terminate False\n",
      "Action (array([0.93874496], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 534.\n",
      "Terminate False\n",
      "Action (array([0.49844503], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 535.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 536.\n",
      "Terminate False\n",
      "Action (array([0.05400296], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 537.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 538.\n",
      "Terminate False\n",
      "Action (array([-0.63832027], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 539.\n",
      "Terminate False\n",
      "Action (array([0.45737308], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 540.\n",
      "Terminate False\n",
      "Action (array([-0.8715035], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 541.\n",
      "Terminate False\n",
      "Action (array([-0.83997613], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 542.\n",
      "Terminate False\n",
      "Action (array([0.31853995], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 543.\n",
      "Terminate False\n",
      "Action (array([0.13334027], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 544.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 545.\n",
      "Terminate False\n",
      "Action (array([0.49085838], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 546.\n",
      "Terminate False\n",
      "Action (array([0.5573612], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 547.\n",
      "Terminate False\n",
      "Action (array([-0.5009062], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 548.\n",
      "Terminate False\n",
      "Action (array([-0.892467], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 549.\n",
      "Terminate False\n",
      "Action (array([0.62136686], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 550.\n",
      "Terminate False\n",
      "Action (array([0.53204226], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 551.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 552.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 553.\n",
      "Terminate False\n",
      "Action (array([0.34656724], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 554.\n",
      "Terminate False\n",
      "Action (array([0.5681392], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 555.\n",
      "Terminate False\n",
      "Action (array([-0.4718515], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 556.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 557.\n",
      "Terminate False\n",
      "Action (array([0.9476464], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 558.\n",
      "Terminate False\n",
      "Action (array([0.32431266], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 559.\n",
      "Terminate False\n",
      "Action (array([-0.34838006], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 560.\n",
      "Terminate False\n",
      "Action (array([-0.64957464], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 561.\n",
      "Terminate False\n",
      "Action (array([0.09843719], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 562.\n",
      "Terminate False\n",
      "Action (array([-0.452515], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 563.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 564.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 565.\n",
      "Terminate False\n",
      "Action (array([0.04274562], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 566.\n",
      "Terminate False\n",
      "Action (array([-0.27450615], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 567.\n",
      "Terminate False\n",
      "Action (array([0.2464257], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 568.\n",
      "Terminate False\n",
      "Action (array([0.3628615], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 569.\n",
      "Terminate False\n",
      "Action (array([-0.646717], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 570.\n",
      "Terminate False\n",
      "Action (array([-0.14159805], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 571.\n",
      "Terminate False\n",
      "Action (array([0.07807909], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 572.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 573.\n",
      "Terminate False\n",
      "Action (array([0.21648729], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 574.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 575.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 576.\n",
      "Terminate False\n",
      "Action (array([-0.9972252], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 577.\n",
      "Terminate False\n",
      "Action (array([0.17863506], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 578.\n",
      "Terminate False\n",
      "Action (array([0.31423637], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 579.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 580.\n",
      "Terminate False\n",
      "Action (array([-0.9096707], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 581.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 582.\n",
      "Terminate False\n",
      "Action (array([0.3563373], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 583.\n",
      "Terminate False\n",
      "Action (array([-0.17829888], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 584.\n",
      "Terminate False\n",
      "Action (array([0.73575264], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 585.\n",
      "Terminate False\n",
      "Action (array([-0.01807813], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 586.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 587.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 588.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 589.\n",
      "Terminate False\n",
      "Action (array([-0.0614105], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 590.\n",
      "Terminate False\n",
      "Action (array([-0.49811062], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 591.\n",
      "Terminate False\n",
      "Action (array([-0.5022388], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 592.\n",
      "Terminate False\n",
      "Action (array([-0.69606906], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 593.\n",
      "Terminate False\n",
      "Action (array([0.6161358], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 594.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 595.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 596.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 597.\n",
      "Terminate False\n",
      "Action (array([0.8011266], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 598.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 599.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 600.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 601.\n",
      "Terminate False\n",
      "Action (array([0.35153195], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 602.\n",
      "Terminate False\n",
      "Action (array([0.12144478], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 603.\n",
      "Terminate False\n",
      "Action (array([-0.5790762], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 604.\n",
      "Terminate False\n",
      "Action (array([-0.7341136], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 605.\n",
      "Terminate False\n",
      "Action (array([0.6992507], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 606.\n",
      "Terminate False\n",
      "Action (array([-0.10910201], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 607.\n",
      "Terminate False\n",
      "Action (array([-0.22888571], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 608.\n",
      "Terminate False\n",
      "Action (array([0.2401901], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 609.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 610.\n",
      "Terminate False\n",
      "Action (array([-0.98427224], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 611.\n",
      "Terminate False\n",
      "Action (array([-0.3419929], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 612.\n",
      "Terminate False\n",
      "Action (array([0.02315491], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 613.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 614.\n",
      "Terminate False\n",
      "Action (array([-0.29673713], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 615.\n",
      "Terminate False\n",
      "Action (array([-0.5661995], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 616.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 617.\n",
      "Terminate False\n",
      "Action (array([0.5009696], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 618.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 619.\n",
      "Terminate False\n",
      "Action (array([-0.10606871], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 620.\n",
      "Terminate False\n",
      "Action (array([0.26025516], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 621.\n",
      "Terminate False\n",
      "Action (array([-0.16925168], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 622.\n",
      "Terminate False\n",
      "Action (array([0.5427514], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 623.\n",
      "Terminate False\n",
      "Action (array([-0.8557804], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 624.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 625.\n",
      "Terminate False\n",
      "Action (array([-0.00228713], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 626.\n",
      "Terminate False\n",
      "Action (array([0.00845122], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 627.\n",
      "Terminate False\n",
      "Action (array([0.9505793], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 628.\n",
      "Terminate False\n",
      "Action (array([-0.2627459], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 629.\n",
      "Terminate False\n",
      "Action (array([0.98341525], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 630.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 631.\n",
      "Terminate False\n",
      "Action (array([-0.45252633], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 632.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 633.\n",
      "Terminate False\n",
      "Action (array([-0.24410653], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 634.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 635.\n",
      "Terminate False\n",
      "Action (array([0.04275531], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 636.\n",
      "Terminate False\n",
      "Action (array([-0.26026064], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 637.\n",
      "Terminate False\n",
      "Action (array([0.8925957], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 638.\n",
      "Terminate False\n",
      "Action (array([0.7432028], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 639.\n",
      "Terminate False\n",
      "Action (array([-0.27535266], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 640.\n",
      "Terminate False\n",
      "Action (array([0.14554961], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 641.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 642.\n",
      "Terminate False\n",
      "Action (array([0.80390954], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 643.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 644.\n",
      "Terminate False\n",
      "Action (array([0.7465967], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 645.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 646.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 647.\n",
      "Terminate False\n",
      "Action (array([0.75867146], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 648.\n",
      "Terminate False\n",
      "Action (array([0.179848], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 649.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 650.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 651.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 652.\n",
      "Terminate False\n",
      "Action (array([0.8607839], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 653.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 654.\n",
      "Terminate False\n",
      "Action (array([0.19813769], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 655.\n",
      "Terminate False\n",
      "Action (array([0.9276272], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 656.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 657.\n",
      "Terminate False\n",
      "Action (array([0.86278284], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 658.\n",
      "Terminate False\n",
      "Action (array([0.71065664], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 659.\n",
      "Terminate False\n",
      "Action (array([0.34935656], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 660.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 661.\n",
      "Terminate False\n",
      "Action (array([-0.7348564], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 662.\n",
      "Terminate False\n",
      "Action (array([0.577651], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 663.\n",
      "Terminate False\n",
      "Action (array([0.43947813], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 664.\n",
      "Terminate False\n",
      "Action (array([-0.7340853], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 665.\n",
      "Terminate False\n",
      "Action (array([-0.19875209], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 666.\n",
      "Terminate False\n",
      "Action (array([-0.2677358], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 667.\n",
      "Terminate False\n",
      "Action (array([-0.5778083], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 668.\n",
      "Terminate False\n",
      "Action (array([-0.30028948], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 669.\n",
      "Terminate False\n",
      "Action (array([-0.9879826], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 670.\n",
      "Terminate False\n",
      "Action (array([0.8011523], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 671.\n",
      "Terminate False\n",
      "Action (array([0.27735132], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 672.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 673.\n",
      "Terminate False\n",
      "Action (array([0.16715688], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 674.\n",
      "Terminate False\n",
      "Action (array([0.57694775], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 675.\n",
      "Terminate False\n",
      "Action (array([0.2091779], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 676.\n",
      "Terminate False\n",
      "Action (array([0.88369226], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 677.\n",
      "Terminate False\n",
      "Action (array([0.68335307], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 678.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 679.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 680.\n",
      "Terminate False\n",
      "Action (array([0.02132033], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 681.\n",
      "Terminate False\n",
      "Action (array([0.39858466], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 682.\n",
      "Terminate False\n",
      "Action (array([-0.01596758], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 683.\n",
      "Terminate False\n",
      "Action (array([-0.18878545], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 684.\n",
      "Terminate False\n",
      "Action (array([-0.5540174], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 685.\n",
      "Terminate False\n",
      "Action (array([0.58068407], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 686.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 687.\n",
      "Terminate False\n",
      "Action (array([0.62152064], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 688.\n",
      "Terminate False\n",
      "Action (array([-0.23057476], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 689.\n",
      "Terminate False\n",
      "Action (array([0.39504585], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 690.\n",
      "Terminate False\n",
      "Action (array([0.54897803], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 691.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 692.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 693.\n",
      "Terminate False\n",
      "Action (array([0.12299289], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 694.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 695.\n",
      "Terminate False\n",
      "Action (array([-0.7841047], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 696.\n",
      "Terminate False\n",
      "Action (array([0.49760014], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 697.\n",
      "Terminate False\n",
      "Action (array([-0.22282691], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 698.\n",
      "Terminate False\n",
      "Action (array([0.8649705], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 699.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 700.\n",
      "Terminate False\n",
      "Action (array([0.29244083], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 701.\n",
      "Terminate False\n",
      "Action (array([0.6965598], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 702.\n",
      "Terminate False\n",
      "Action (array([0.7900203], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 703.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 704.\n",
      "Terminate False\n",
      "Action (array([0.80200547], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 705.\n",
      "Terminate False\n",
      "Action (array([-0.02393606], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 706.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 707.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 708.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 709.\n",
      "Terminate False\n",
      "Action (array([-0.54178816], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 710.\n",
      "Terminate False\n",
      "Action (array([-0.60076284], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 711.\n",
      "Terminate False\n",
      "Action (array([-0.06340016], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 712.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 713.\n",
      "Terminate False\n",
      "Action (array([0.36307895], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 714.\n",
      "Terminate False\n",
      "Action (array([0.6764624], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 715.\n",
      "Terminate False\n",
      "Action (array([0.33930278], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 716.\n",
      "Terminate False\n",
      "Action (array([-0.11891775], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 717.\n",
      "Terminate False\n",
      "Action (array([-0.35428306], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 718.\n",
      "Terminate False\n",
      "Action (array([-0.98122054], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 719.\n",
      "Terminate False\n",
      "Action (array([-0.14915109], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 720.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 721.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 722.\n",
      "Terminate False\n",
      "Action (array([0.18964285], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 723.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 724.\n",
      "Terminate False\n",
      "Action (array([-0.83527595], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 725.\n",
      "Terminate False\n",
      "Action (array([0.38137433], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 726.\n",
      "Terminate False\n",
      "Action (array([-0.6227388], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 727.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 728.\n",
      "Terminate False\n",
      "Action (array([-0.40097028], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 729.\n",
      "Terminate False\n",
      "Action (array([-0.7035994], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 730.\n",
      "Terminate False\n",
      "Action (array([0.22083874], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 731.\n",
      "Terminate False\n",
      "Action (array([0.8067211], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 732.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 733.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 734.\n",
      "Terminate False\n",
      "Action (array([-0.16294171], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 735.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 736.\n",
      "Terminate False\n",
      "Action (array([-0.08014483], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 737.\n",
      "Terminate False\n",
      "Action (array([0.48314363], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 738.\n",
      "Terminate False\n",
      "Action (array([-0.6348209], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 739.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 740.\n",
      "Terminate False\n",
      "Action (array([-0.31254727], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 741.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 742.\n",
      "Terminate False\n",
      "Action (array([0.18204668], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 743.\n",
      "Terminate False\n",
      "Action (array([0.45286152], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 744.\n",
      "Terminate False\n",
      "Action (array([-0.4581214], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 745.\n",
      "Terminate False\n",
      "Action (array([-0.87376845], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 746.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 747.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 748.\n",
      "Terminate False\n",
      "Action (array([-0.15490943], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 749.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 750.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 751.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 752.\n",
      "Terminate False\n",
      "Action (array([0.61109006], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 753.\n",
      "Terminate False\n",
      "Action (array([-0.64566576], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 754.\n",
      "Terminate False\n",
      "Action (array([-0.94024277], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 755.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 756.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 757.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 758.\n",
      "Terminate False\n",
      "Action (array([-0.22238918], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 759.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 760.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 761.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 762.\n",
      "Terminate False\n",
      "Action (array([0.8245482], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 763.\n",
      "Terminate False\n",
      "Action (array([-0.2319286], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 764.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 765.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 766.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 767.\n",
      "Terminate False\n",
      "Action (array([0.29530746], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 768.\n",
      "Terminate False\n",
      "Action (array([0.28931242], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 769.\n",
      "Terminate False\n",
      "Action (array([0.08851703], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 770.\n",
      "Terminate False\n",
      "Action (array([0.6275264], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 771.\n",
      "Terminate False\n",
      "Action (array([0.2291993], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 772.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 773.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 774.\n",
      "Terminate False\n",
      "Action (array([-0.10905884], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 775.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 776.\n",
      "Terminate False\n",
      "Action (array([-0.59484774], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 777.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 778.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 779.\n",
      "Terminate False\n",
      "Action (array([-0.7800972], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 780.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 781.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 782.\n",
      "Terminate False\n",
      "Action (array([-0.39443225], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 783.\n",
      "Terminate False\n",
      "Action (array([-0.98549616], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 784.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 785.\n",
      "Terminate False\n",
      "Action (array([-0.94572794], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 786.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 787.\n",
      "Terminate False\n",
      "Action (array([0.09322221], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 788.\n",
      "Terminate False\n",
      "Action (array([0.80110973], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 789.\n",
      "Terminate False\n",
      "Action (array([0.39336872], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 790.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 791.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 792.\n",
      "Terminate False\n",
      "Action (array([0.2258872], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 793.\n",
      "Terminate False\n",
      "Action (array([-0.3534854], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 794.\n",
      "Terminate False\n",
      "Action (array([0.18830626], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 795.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 796.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 797.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 798.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 799.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 800.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 801.\n",
      "Terminate False\n",
      "Action (array([0.16719745], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 802.\n",
      "Terminate False\n",
      "Action (array([-0.4789871], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 803.\n",
      "Terminate False\n",
      "Action (array([0.84234947], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 804.\n",
      "Terminate False\n",
      "Action (array([0.6095013], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 805.\n",
      "Terminate False\n",
      "Action (array([-0.36534795], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 806.\n",
      "Terminate False\n",
      "Action (array([-0.8740467], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 807.\n",
      "Terminate False\n",
      "Action (array([-0.31020558], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 808.\n",
      "Terminate False\n",
      "Action (array([-0.8973206], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 809.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 810.\n",
      "Terminate False\n",
      "Action (array([-0.5004579], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 811.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 812.\n",
      "Terminate False\n",
      "Action (array([-0.5310718], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 813.\n",
      "Terminate False\n",
      "Action (array([-0.6566265], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 814.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 815.\n",
      "Terminate False\n",
      "Action (array([0.01848782], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 816.\n",
      "Terminate False\n",
      "Action (array([-0.5418345], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 817.\n",
      "Terminate False\n",
      "Action (array([0.5857066], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 818.\n",
      "Terminate False\n",
      "Action (array([-0.42369425], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 819.\n",
      "Terminate False\n",
      "Action (array([0.29735923], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 820.\n",
      "Terminate False\n",
      "Action (array([0.81418484], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 821.\n",
      "Terminate False\n",
      "Action (array([0.5435412], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 822.\n",
      "Terminate False\n",
      "Action (array([0.4356057], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 823.\n",
      "Terminate False\n",
      "Action (array([-0.09996915], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 824.\n",
      "Terminate False\n",
      "Action (array([-0.66300696], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 825.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 826.\n",
      "Terminate False\n",
      "Action (array([0.42669174], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 827.\n",
      "Terminate False\n",
      "Action (array([-0.77242744], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 828.\n",
      "Terminate False\n",
      "Action (array([-0.36782765], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 829.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 830.\n",
      "Terminate False\n",
      "Action (array([0.44873023], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 831.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 832.\n",
      "Terminate False\n",
      "Action (array([0.7653165], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 833.\n",
      "Terminate False\n",
      "Action (array([0.79135907], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 834.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 835.\n",
      "Terminate False\n",
      "Action (array([-0.29706264], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 836.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 837.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 838.\n",
      "Terminate False\n",
      "Action (array([-0.6439814], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 839.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 840.\n",
      "Terminate False\n",
      "Action (array([0.3087589], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 841.\n",
      "Terminate False\n",
      "Action (array([0.1795732], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 842.\n",
      "Terminate False\n",
      "Action (array([-0.02347467], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 843.\n",
      "Terminate False\n",
      "Action (array([0.29246286], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 844.\n",
      "Terminate False\n",
      "Action (array([0.10515374], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 845.\n",
      "Terminate False\n",
      "Action (array([0.1147868], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 846.\n",
      "Terminate False\n",
      "Action (array([0.93342453], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 847.\n",
      "Terminate False\n",
      "Action (array([-0.7377541], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 848.\n",
      "Terminate False\n",
      "Action (array([0.17924567], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 849.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 850.\n",
      "Terminate False\n",
      "Action (array([0.10499895], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 851.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 852.\n",
      "Terminate False\n",
      "Action (array([0.30565596], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 853.\n",
      "Terminate False\n",
      "Action (array([0.7861172], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 854.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 855.\n",
      "Terminate False\n",
      "Action (array([0.46335736], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 856.\n",
      "Terminate False\n",
      "Action (array([0.8290646], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 857.\n",
      "Terminate False\n",
      "Action (array([0.89513284], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 858.\n",
      "Terminate False\n",
      "Action (array([-0.37816572], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 859.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 860.\n",
      "Terminate False\n",
      "Action (array([-0.10439597], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 861.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 862.\n",
      "Terminate False\n",
      "Action (array([-0.28307873], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 863.\n",
      "Terminate False\n",
      "Action (array([-0.37475], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 864.\n",
      "Terminate False\n",
      "Action (array([-0.8385625], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 865.\n",
      "Terminate False\n",
      "Action (array([-0.07308056], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 866.\n",
      "Terminate False\n",
      "Action (array([0.8770761], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 867.\n",
      "Terminate False\n",
      "Action (array([-0.11065402], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 868.\n",
      "Terminate False\n",
      "Action (array([-0.6173404], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 869.\n",
      "Terminate False\n",
      "Action (array([-0.9199576], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 870.\n",
      "Terminate False\n",
      "Action (array([-0.16592468], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 871.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 872.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 873.\n",
      "Terminate False\n",
      "Action (array([-0.8613188], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 874.\n",
      "Terminate False\n",
      "Action (array([-0.35138884], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 875.\n",
      "Terminate False\n",
      "Action (array([-0.0922576], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 876.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 877.\n",
      "Terminate False\n",
      "Action (array([-0.02967731], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 878.\n",
      "Terminate False\n",
      "Action (array([0.6701682], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 879.\n",
      "Terminate False\n",
      "Action (array([-0.32728693], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 880.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 881.\n",
      "Terminate False\n",
      "Action (array([0.21226306], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 882.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 883.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 884.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 885.\n",
      "Terminate False\n",
      "Action (array([0.22766785], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 886.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 887.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 888.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 889.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 890.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 891.\n",
      "Terminate False\n",
      "Action (array([0.58020955], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 892.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 893.\n",
      "Terminate False\n",
      "Action (array([-0.01645851], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 894.\n",
      "Terminate False\n",
      "Action (array([-0.2721704], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 895.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 896.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 897.\n",
      "Terminate False\n",
      "Action (array([0.04243463], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 898.\n",
      "Terminate False\n",
      "Action (array([-0.47812414], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 899.\n",
      "Terminate False\n",
      "Action (array([-0.4355953], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 900.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 901.\n",
      "Terminate False\n",
      "Action (array([0.8712777], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 902.\n",
      "Terminate False\n",
      "Action (array([-0.61085594], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 903.\n",
      "Terminate False\n",
      "Action (array([-0.21733674], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 904.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 905.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 906.\n",
      "Terminate False\n",
      "Action (array([0.64589185], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 907.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 908.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 909.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 910.\n",
      "Terminate False\n",
      "Action (array([0.25020626], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 911.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 912.\n",
      "Terminate False\n",
      "Action (array([0.8740578], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 913.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 914.\n",
      "Terminate False\n",
      "Action (array([0.5422201], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 915.\n",
      "Terminate False\n",
      "Action (array([-0.42340156], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 916.\n",
      "Terminate False\n",
      "Action (array([0.5477021], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 917.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 918.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 919.\n",
      "Terminate False\n",
      "Action (array([0.04889968], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 920.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 921.\n",
      "Terminate False\n",
      "Action (array([-0.17414679], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 922.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 923.\n",
      "Terminate False\n",
      "Action (array([0.43817484], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 924.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 925.\n",
      "Terminate False\n",
      "Action (array([0.508289], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 926.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 927.\n",
      "Terminate False\n",
      "Action (array([-0.6741862], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 928.\n",
      "Terminate False\n",
      "Action (array([-0.2849575], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 929.\n",
      "Terminate False\n",
      "Action (array([0.6321014], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 930.\n",
      "Terminate False\n",
      "Action (array([-0.784741], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 931.\n",
      "Terminate False\n",
      "Action (array([-0.16319549], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 932.\n",
      "Terminate False\n",
      "Action (array([0.00812996], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 933.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 934.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 935.\n",
      "Terminate False\n",
      "Action (array([-0.04021813], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 936.\n",
      "Terminate False\n",
      "Action (array([0.13568717], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 937.\n",
      "Terminate False\n",
      "Action (array([-0.08798547], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 938.\n",
      "Terminate False\n",
      "Action (array([0.18429297], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 939.\n",
      "Terminate False\n",
      "Action (array([0.4950841], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 940.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 941.\n",
      "Terminate False\n",
      "Action (array([0.14109401], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 942.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 943.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 944.\n",
      "Terminate False\n",
      "Action (array([-0.4519151], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 945.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 946.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 947.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 948.\n",
      "Terminate False\n",
      "Action (array([-0.7849552], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 949.\n",
      "Terminate False\n",
      "Action (array([0.84413314], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 950.\n",
      "Terminate False\n",
      "Action (array([0.20416355], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 951.\n",
      "Terminate False\n",
      "Action (array([-0.20210958], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 952.\n",
      "Terminate False\n",
      "Action (array([0.1322153], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 953.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 954.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 955.\n",
      "Terminate False\n",
      "Action (array([-0.3412072], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 956.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 957.\n",
      "Terminate False\n",
      "Action (array([0.5044273], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 958.\n",
      "Terminate False\n",
      "Action (array([-0.23782659], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 959.\n",
      "Terminate False\n",
      "Action (array([-0.7810208], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 960.\n",
      "Terminate False\n",
      "Action (array([-0.23151492], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 961.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 962.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 963.\n",
      "Terminate False\n",
      "Action (array([0.6529022], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 964.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 965.\n",
      "Terminate False\n",
      "Action (array([0.8055687], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 966.\n",
      "Terminate False\n",
      "Action (array([-0.71322846], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 967.\n",
      "Terminate False\n",
      "Action (array([0.66962284], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 968.\n",
      "Terminate False\n",
      "Action (array([-0.68812376], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 969.\n",
      "Terminate False\n",
      "Action (array([-0.6773708], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 970.\n",
      "Terminate False\n",
      "Action (array([0.04601852], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 971.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 972.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 973.\n",
      "Terminate False\n",
      "Action (array([0.4000692], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 974.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 975.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 976.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 977.\n",
      "Terminate False\n",
      "Action (array([-0.6244202], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 978.\n",
      "Terminate False\n",
      "Action (array([0.5654884], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 979.\n",
      "Terminate False\n",
      "Action (array([0.7116935], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 980.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 981.\n",
      "Terminate False\n",
      "Action (array([-0.09296883], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 982.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 983.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 984.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 985.\n",
      "Terminate False\n",
      "Action (array([0.3701397], dtype=float32), None)\n",
      "Reward 46.2571294850491 at step 986.\n",
      "Terminate False\n",
      "Action (array([-0.5279685], dtype=float32), None)\n",
      "Reward 32.44941780292017 at step 987.\n",
      "Terminate False\n",
      "Action (array([0.04303753], dtype=float32), None)\n",
      "Reward 31.68568981830103 at step 988.\n",
      "Terminate False\n",
      "Action (array([-0.17948498], dtype=float32), None)\n",
      "Reward 31.83064552246896 at step 989.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 31.829410067418312 at step 990.\n",
      "Terminate False\n",
      "Action (array([0.3075982], dtype=float32), None)\n",
      "Reward 41.21078747991526 at step 991.\n",
      "Terminate False\n",
      "Action (array([0.2751325], dtype=float32), None)\n",
      "Reward 38.00732930870706 at step 992.\n",
      "Terminate False\n",
      "Action (array([0.00088914], dtype=float32), None)\n",
      "Reward 39.372627241904084 at step 993.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 39.362333114393245 at step 994.\n",
      "Terminate False\n",
      "Action (array([-0.68365705], dtype=float32), None)\n",
      "Reward 39.362333114393245 at step 995.\n",
      "Terminate False\n",
      "Action (array([0.44917428], dtype=float32), None)\n",
      "Reward 39.06041888728754 at step 996.\n",
      "Terminate False\n",
      "Action (array([0.04666454], dtype=float32), None)\n",
      "Reward 39.060131598240304 at step 997.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.71179160883294 at step 998.\n",
      "Terminate False\n",
      "Action (array([-0.89534765], dtype=float32), None)\n",
      "Reward 38.71426152325098 at step 999.\n",
      "Terminate False\n",
      "Action (array([-0.2574469], dtype=float32), None)\n",
      "Reward 38.714242692910396 at step 1000.\n",
      "Terminate False\n",
      "Action (array([-0.33400023], dtype=float32), None)\n",
      "Reward 38.714242692910396 at step 1001.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.701351717214614 at step 1002.\n",
      "Terminate False\n",
      "Action (array([0.826872], dtype=float32), None)\n",
      "Reward 38.7013448132145 at step 1003.\n",
      "Terminate False\n",
      "Action (array([-0.00053671], dtype=float32), None)\n",
      "Reward 38.7013448132145 at step 1004.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.672356013165675 at step 1005.\n",
      "Terminate False\n",
      "Action (array([0.3543576], dtype=float32), None)\n",
      "Reward 38.66893071256753 at step 1006.\n",
      "Terminate False\n",
      "Action (array([-0.62983716], dtype=float32), None)\n",
      "Reward 38.66892736875268 at step 1007.\n",
      "Terminate False\n",
      "Action (array([-0.8512547], dtype=float32), None)\n",
      "Reward 38.66892736875268 at step 1008.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66892736875268 at step 1009.\n",
      "Terminate False\n",
      "Action (array([0.35880938], dtype=float32), None)\n",
      "Reward 38.66892736875268 at step 1010.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66892736875268 at step 1011.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66892736875268 at step 1012.\n",
      "Terminate False\n",
      "Action (array([-0.98076403], dtype=float32), None)\n",
      "Reward 38.66892736875268 at step 1013.\n",
      "Terminate False\n",
      "Action (array([-0.92320806], dtype=float32), None)\n",
      "Reward 38.66892736875268 at step 1014.\n",
      "Terminate False\n",
      "Action (array([0.33935237], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1015.\n",
      "Terminate False\n",
      "Action (array([0.5749365], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1016.\n",
      "Terminate False\n",
      "Action (array([0.92408764], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1017.\n",
      "Terminate False\n",
      "Action (array([-0.08181708], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1018.\n",
      "Terminate False\n",
      "Action (array([0.7886419], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1019.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1020.\n",
      "Terminate False\n",
      "Action (array([0.7428714], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1021.\n",
      "Terminate False\n",
      "Action (array([-0.8880699], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1022.\n",
      "Terminate False\n",
      "Action (array([0.8361771], dtype=float32), None)\n",
      "Reward 38.668924058978284 at step 1023.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873949782106 at step 1024.\n",
      "Terminate False\n",
      "Action (array([-0.08958011], dtype=float32), None)\n",
      "Reward 38.66873949782106 at step 1025.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873949782106 at step 1026.\n",
      "Terminate False\n",
      "Action (array([0.1722024], dtype=float32), None)\n",
      "Reward 38.66873949782106 at step 1027.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873949782106 at step 1028.\n",
      "Terminate False\n",
      "Action (array([0.56489134], dtype=float32), None)\n",
      "Reward 38.66873516182101 at step 1029.\n",
      "Terminate False\n",
      "Action (array([-0.1103652], dtype=float32), None)\n",
      "Reward 38.66873516182101 at step 1030.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873516182101 at step 1031.\n",
      "Terminate False\n",
      "Action (array([-0.29807994], dtype=float32), None)\n",
      "Reward 38.66873516182101 at step 1032.\n",
      "Terminate False\n",
      "Action (array([0.1713057], dtype=float32), None)\n",
      "Reward 38.66873516182101 at step 1033.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873516182101 at step 1034.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873516182101 at step 1035.\n",
      "Terminate False\n",
      "Action (array([0.8739179], dtype=float32), None)\n",
      "Reward 38.66873516182101 at step 1036.\n",
      "Terminate False\n",
      "Action (array([0.69215137], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1037.\n",
      "Terminate False\n",
      "Action (array([-0.10966688], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1038.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1039.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1040.\n",
      "Terminate False\n",
      "Action (array([0.77415717], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1041.\n",
      "Terminate False\n",
      "Action (array([-0.83465064], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1042.\n",
      "Terminate False\n",
      "Action (array([0.4330138], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1043.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1044.\n",
      "Terminate False\n",
      "Action (array([0.03136939], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1045.\n",
      "Terminate False\n",
      "Action (array([-0.30067232], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1046.\n",
      "Terminate False\n",
      "Action (array([0.12541097], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1047.\n",
      "Terminate False\n",
      "Action (array([-0.6604137], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1048.\n",
      "Terminate False\n",
      "Action (array([0.59351087], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1049.\n",
      "Terminate False\n",
      "Action (array([-0.89270294], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1050.\n",
      "Terminate False\n",
      "Action (array([-0.3007849], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1051.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1052.\n",
      "Terminate False\n",
      "Action (array([0.5143592], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1053.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1054.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1055.\n",
      "Terminate False\n",
      "Action (array([0.64477056], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1056.\n",
      "Terminate False\n",
      "Action (array([-0.44005483], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1057.\n",
      "Terminate False\n",
      "Action (array([-0.95543057], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1058.\n",
      "Terminate False\n",
      "Action (array([-0.4723776], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1059.\n",
      "Terminate False\n",
      "Action (array([0.04206611], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1060.\n",
      "Terminate False\n",
      "Action (array([0.43354383], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1061.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1062.\n",
      "Terminate False\n",
      "Action (array([0.16309561], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1063.\n",
      "Terminate False\n",
      "Action (array([-0.64866716], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1064.\n",
      "Terminate False\n",
      "Action (array([-0.87670934], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1065.\n",
      "Terminate False\n",
      "Action (array([0.15489046], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1066.\n",
      "Terminate False\n",
      "Action (array([0.499938], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1067.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1068.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1069.\n",
      "Terminate False\n",
      "Action (array([-0.08336002], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1070.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1071.\n",
      "Terminate False\n",
      "Action (array([0.8378384], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1072.\n",
      "Terminate False\n",
      "Action (array([-0.67011946], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1073.\n",
      "Terminate False\n",
      "Action (array([0.84436184], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1074.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1075.\n",
      "Terminate False\n",
      "Action (array([-0.44413358], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1076.\n",
      "Terminate False\n",
      "Action (array([0.82782364], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1077.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1078.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1079.\n",
      "Terminate False\n",
      "Action (array([-0.4923958], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1080.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1081.\n",
      "Terminate False\n",
      "Action (array([0.51827073], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1082.\n",
      "Terminate False\n",
      "Action (array([0.1163335], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1083.\n",
      "Terminate False\n",
      "Action (array([-0.564236], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1084.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1085.\n",
      "Terminate False\n",
      "Action (array([0.7224279], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1086.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1087.\n",
      "Terminate False\n",
      "Action (array([0.4758377], dtype=float32), None)\n",
      "Reward 38.668733206086564 at step 1088.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1089.\n",
      "Terminate False\n",
      "Action (array([0.362448], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1090.\n",
      "Terminate False\n",
      "Action (array([0.4356371], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1091.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1092.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1093.\n",
      "Terminate False\n",
      "Action (array([-0.6918062], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1094.\n",
      "Terminate False\n",
      "Action (array([0.46594957], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1095.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1096.\n",
      "Terminate False\n",
      "Action (array([-0.754505], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1097.\n",
      "Terminate False\n",
      "Action (array([0.11757542], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1098.\n",
      "Terminate False\n",
      "Action (array([-0.08533699], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1099.\n",
      "Terminate False\n",
      "Action (array([0.9901564], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1100.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1101.\n",
      "Terminate False\n",
      "Action (array([0.75119275], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1102.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1103.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1104.\n",
      "Terminate False\n",
      "Action (array([-0.438064], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1105.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1106.\n",
      "Terminate False\n",
      "Action (array([-0.49987602], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1107.\n",
      "Terminate False\n",
      "Action (array([-0.02475949], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1108.\n",
      "Terminate False\n",
      "Action (array([0.7249866], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1109.\n",
      "Terminate False\n",
      "Action (array([0.81996405], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1110.\n",
      "Terminate False\n",
      "Action (array([-0.21196002], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1111.\n",
      "Terminate False\n",
      "Action (array([-0.75442535], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1112.\n",
      "Terminate False\n",
      "Action (array([0.7678562], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1113.\n",
      "Terminate False\n",
      "Action (array([0.7387591], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1114.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1115.\n",
      "Terminate False\n",
      "Action (array([0.72171307], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1116.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1117.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1118.\n",
      "Terminate False\n",
      "Action (array([-0.29913202], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1119.\n",
      "Terminate False\n",
      "Action (array([0.9499371], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1120.\n",
      "Terminate False\n",
      "Action (array([-0.1837931], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1121.\n",
      "Terminate False\n",
      "Action (array([0.22791149], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1122.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1123.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1124.\n",
      "Terminate False\n",
      "Action (array([0.32233384], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1125.\n",
      "Terminate False\n",
      "Action (array([-0.4405777], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1126.\n",
      "Terminate False\n",
      "Action (array([-0.95346564], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1127.\n",
      "Terminate False\n",
      "Action (array([0.21583563], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1128.\n",
      "Terminate False\n",
      "Action (array([-0.5631132], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1129.\n",
      "Terminate False\n",
      "Action (array([0.7257624], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1130.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1131.\n",
      "Terminate False\n",
      "Action (array([0.04007467], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1132.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1133.\n",
      "Terminate False\n",
      "Action (array([0.02098172], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1134.\n",
      "Terminate False\n",
      "Action (array([-0.45212045], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1135.\n",
      "Terminate False\n",
      "Action (array([0.9385408], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1136.\n",
      "Terminate False\n",
      "Action (array([0.38349724], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1137.\n",
      "Terminate False\n",
      "Action (array([-0.781199], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1138.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1139.\n",
      "Terminate False\n",
      "Action (array([0.25358602], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1140.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1141.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1142.\n",
      "Terminate False\n",
      "Action (array([-0.411887], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1143.\n",
      "Terminate False\n",
      "Action (array([0.09090978], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1144.\n",
      "Terminate False\n",
      "Action (array([-0.9804545], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1145.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1146.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1147.\n",
      "Terminate False\n",
      "Action (array([0.9712893], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1148.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1149.\n",
      "Terminate False\n",
      "Action (array([0.33401152], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1150.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1151.\n",
      "Terminate False\n",
      "Action (array([0.66225725], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1152.\n",
      "Terminate False\n",
      "Action (array([0.01157416], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1153.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1154.\n",
      "Terminate False\n",
      "Action (array([-0.13565214], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1155.\n",
      "Terminate False\n",
      "Action (array([-0.5213648], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1156.\n",
      "Terminate False\n",
      "Action (array([0.557097], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1157.\n",
      "Terminate False\n",
      "Action (array([0.15122722], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1158.\n",
      "Terminate False\n",
      "Action (array([-0.45961794], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1159.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1160.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1161.\n",
      "Terminate False\n",
      "Action (array([-0.26031122], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1162.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1163.\n",
      "Terminate False\n",
      "Action (array([-0.54943824], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1164.\n",
      "Terminate False\n",
      "Action (array([0.22393875], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1165.\n",
      "Terminate False\n",
      "Action (array([-0.07571987], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1166.\n",
      "Terminate False\n",
      "Action (array([-0.4363644], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1167.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1168.\n",
      "Terminate False\n",
      "Action (array([0.83391076], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1169.\n",
      "Terminate False\n",
      "Action (array([0.12448205], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1170.\n",
      "Terminate False\n",
      "Action (array([-0.60619295], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1171.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1172.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1173.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1174.\n",
      "Terminate False\n",
      "Action (array([-0.2662832], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1175.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1176.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1177.\n",
      "Terminate False\n",
      "Action (array([0.6822145], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1178.\n",
      "Terminate False\n",
      "Action (array([0.7516922], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1179.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1180.\n",
      "Terminate False\n",
      "Action (array([0.840435], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1181.\n",
      "Terminate False\n",
      "Action (array([0.92965704], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1182.\n",
      "Terminate False\n",
      "Action (array([0.5582341], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1183.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1184.\n",
      "Terminate False\n",
      "Action (array([0.43818936], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1185.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1186.\n",
      "Terminate False\n",
      "Action (array([0.5257108], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1187.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1188.\n",
      "Terminate False\n",
      "Action (array([-0.3512991], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1189.\n",
      "Terminate False\n",
      "Action (array([0.92115], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1190.\n",
      "Terminate False\n",
      "Action (array([-0.24689713], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1191.\n",
      "Terminate False\n",
      "Action (array([0.00500515], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1192.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1193.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1194.\n",
      "Terminate False\n",
      "Action (array([-0.15222049], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1195.\n",
      "Terminate False\n",
      "Action (array([-0.25880787], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1196.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1197.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1198.\n",
      "Terminate False\n",
      "Action (array([0.11058354], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1199.\n",
      "Terminate False\n",
      "Action (array([-0.26318467], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1200.\n",
      "Terminate False\n",
      "Action (array([-0.35788968], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1201.\n",
      "Terminate False\n",
      "Action (array([-0.5048819], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1202.\n",
      "Terminate False\n",
      "Action (array([-0.86046], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1203.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1204.\n",
      "Terminate False\n",
      "Action (array([0.18587027], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1205.\n",
      "Terminate False\n",
      "Action (array([-0.16695666], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1206.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1207.\n",
      "Terminate False\n",
      "Action (array([-0.17762733], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1208.\n",
      "Terminate False\n",
      "Action (array([-0.66421336], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1209.\n",
      "Terminate False\n",
      "Action (array([-0.2226769], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1210.\n",
      "Terminate False\n",
      "Action (array([-0.9005827], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1211.\n",
      "Terminate False\n",
      "Action (array([-0.8487052], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1212.\n",
      "Terminate False\n",
      "Action (array([-0.39229715], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1213.\n",
      "Terminate False\n",
      "Action (array([0.88531345], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1214.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1215.\n",
      "Terminate False\n",
      "Action (array([0.7996793], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1216.\n",
      "Terminate False\n",
      "Action (array([0.9901109], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1217.\n",
      "Terminate False\n",
      "Action (array([-0.16659911], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1218.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1219.\n",
      "Terminate False\n",
      "Action (array([-0.11206125], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1220.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1221.\n",
      "Terminate False\n",
      "Action (array([-0.22979209], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1222.\n",
      "Terminate False\n",
      "Action (array([-0.06530112], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1223.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1224.\n",
      "Terminate False\n",
      "Action (array([-0.59074116], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1225.\n",
      "Terminate False\n",
      "Action (array([0.2643024], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1226.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1227.\n",
      "Terminate False\n",
      "Action (array([-0.45845997], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1228.\n",
      "Terminate False\n",
      "Action (array([0.7983298], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1229.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1230.\n",
      "Terminate False\n",
      "Action (array([-0.78109306], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1231.\n",
      "Terminate False\n",
      "Action (array([-0.49483448], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1232.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1233.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1234.\n",
      "Terminate False\n",
      "Action (array([0.76699525], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1235.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1236.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1237.\n",
      "Terminate False\n",
      "Action (array([-0.629222], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1238.\n",
      "Terminate False\n",
      "Action (array([-0.6953927], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1239.\n",
      "Terminate False\n",
      "Action (array([-0.41233146], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1240.\n",
      "Terminate False\n",
      "Action (array([0.24501257], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1241.\n",
      "Terminate False\n",
      "Action (array([-0.3894947], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1242.\n",
      "Terminate False\n",
      "Action (array([-0.67419845], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1243.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1244.\n",
      "Terminate False\n",
      "Action (array([0.77763504], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1245.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1246.\n",
      "Terminate False\n",
      "Action (array([-0.59832865], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1247.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1248.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1249.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1250.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1251.\n",
      "Terminate False\n",
      "Action (array([0.5964613], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1252.\n",
      "Terminate False\n",
      "Action (array([0.7161883], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1253.\n",
      "Terminate False\n",
      "Action (array([-0.961034], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1254.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1255.\n",
      "Terminate False\n",
      "Action (array([0.0620617], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1256.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1257.\n",
      "Terminate False\n",
      "Action (array([-0.03319164], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1258.\n",
      "Terminate False\n",
      "Action (array([-0.55778766], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1259.\n",
      "Terminate False\n",
      "Action (array([-0.64034325], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1260.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1261.\n",
      "Terminate False\n",
      "Action (array([-0.06972846], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1262.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1263.\n",
      "Terminate False\n",
      "Action (array([-0.0275806], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1264.\n",
      "Terminate False\n",
      "Action (array([-0.18070506], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1265.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1266.\n",
      "Terminate False\n",
      "Action (array([-0.39480263], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1267.\n",
      "Terminate False\n",
      "Action (array([-0.38422737], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1268.\n",
      "Terminate False\n",
      "Action (array([0.4148833], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1269.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1270.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1271.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1272.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1273.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1274.\n",
      "Terminate False\n",
      "Action (array([0.77192277], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1275.\n",
      "Terminate False\n",
      "Action (array([0.12118089], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1276.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1277.\n",
      "Terminate False\n",
      "Action (array([-0.4166169], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1278.\n",
      "Terminate False\n",
      "Action (array([-0.0546416], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1279.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1280.\n",
      "Terminate False\n",
      "Action (array([-0.1956706], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1281.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1282.\n",
      "Terminate False\n",
      "Action (array([-0.69973266], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1283.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1284.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1285.\n",
      "Terminate False\n",
      "Action (array([-0.597669], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1286.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1287.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1288.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1289.\n",
      "Terminate False\n",
      "Action (array([0.05668243], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1290.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1291.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1292.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1293.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1294.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1295.\n",
      "Terminate False\n",
      "Action (array([0.3667403], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1296.\n",
      "Terminate False\n",
      "Action (array([-0.8908635], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1297.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1298.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1299.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1300.\n",
      "Terminate False\n",
      "Action (array([-0.06869058], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1301.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1302.\n",
      "Terminate False\n",
      "Action (array([-0.34129456], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1303.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1304.\n",
      "Terminate False\n",
      "Action (array([0.2624303], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1305.\n",
      "Terminate False\n",
      "Action (array([0.1083858], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1306.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1307.\n",
      "Terminate False\n",
      "Action (array([0.6311166], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1308.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1309.\n",
      "Terminate False\n",
      "Action (array([0.36525646], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1310.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1311.\n",
      "Terminate False\n",
      "Action (array([0.957095], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1312.\n",
      "Terminate False\n",
      "Action (array([-0.49762225], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1313.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1314.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1315.\n",
      "Terminate False\n",
      "Action (array([0.3128351], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1316.\n",
      "Terminate False\n",
      "Action (array([-0.32649773], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1317.\n",
      "Terminate False\n",
      "Action (array([0.2401234], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1318.\n",
      "Terminate False\n",
      "Action (array([-0.26570368], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1319.\n",
      "Terminate False\n",
      "Action (array([-0.8781907], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1320.\n",
      "Terminate False\n",
      "Action (array([0.70748824], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1321.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1322.\n",
      "Terminate False\n",
      "Action (array([-0.4534805], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1323.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1324.\n",
      "Terminate False\n",
      "Action (array([-0.04953951], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1325.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1326.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1327.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1328.\n",
      "Terminate False\n",
      "Action (array([-0.2329819], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1329.\n",
      "Terminate False\n",
      "Action (array([0.18146376], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1330.\n",
      "Terminate False\n",
      "Action (array([-0.7560048], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1331.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1332.\n",
      "Terminate False\n",
      "Action (array([0.55908585], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1333.\n",
      "Terminate False\n",
      "Action (array([-0.15204504], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1334.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1335.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1336.\n",
      "Terminate False\n",
      "Action (array([0.15110894], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1337.\n",
      "Terminate False\n",
      "Action (array([0.10395393], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1338.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1339.\n",
      "Terminate False\n",
      "Action (array([-0.35167262], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1340.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1341.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1342.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1343.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1344.\n",
      "Terminate False\n",
      "Action (array([-0.2827581], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1345.\n",
      "Terminate False\n",
      "Action (array([-0.8514128], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1346.\n",
      "Terminate False\n",
      "Action (array([0.38624728], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1347.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1348.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1349.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1350.\n",
      "Terminate False\n",
      "Action (array([-0.8127412], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1351.\n",
      "Terminate False\n",
      "Action (array([0.3008831], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1352.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1353.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1354.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1355.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1356.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1357.\n",
      "Terminate False\n",
      "Action (array([-0.7404213], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1358.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1359.\n",
      "Terminate False\n",
      "Action (array([-0.16348138], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1360.\n",
      "Terminate False\n",
      "Action (array([0.28486145], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1361.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1362.\n",
      "Terminate False\n",
      "Action (array([0.56111896], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1363.\n",
      "Terminate False\n",
      "Action (array([0.96405804], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1364.\n",
      "Terminate False\n",
      "Action (array([-0.36755055], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1365.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1366.\n",
      "Terminate False\n",
      "Action (array([-0.4276403], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1367.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1368.\n",
      "Terminate False\n",
      "Action (array([0.4181623], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1369.\n",
      "Terminate False\n",
      "Action (array([-0.77428615], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1370.\n",
      "Terminate False\n",
      "Action (array([0.37891835], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1371.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1372.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1373.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1374.\n",
      "Terminate False\n",
      "Action (array([0.7610313], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1375.\n",
      "Terminate False\n",
      "Action (array([-0.49826983], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1376.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1377.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1378.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1379.\n",
      "Terminate False\n",
      "Action (array([-0.26526976], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1380.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1381.\n",
      "Terminate False\n",
      "Action (array([0.6058191], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1382.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1383.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1384.\n",
      "Terminate False\n",
      "Action (array([-0.5619981], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1385.\n",
      "Terminate False\n",
      "Action (array([-0.13821033], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1386.\n",
      "Terminate False\n",
      "Action (array([0.50364465], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1387.\n",
      "Terminate False\n",
      "Action (array([0.5954317], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1388.\n",
      "Terminate False\n",
      "Action (array([0.61712027], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1389.\n",
      "Terminate False\n",
      "Action (array([-0.7780605], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1390.\n",
      "Terminate False\n",
      "Action (array([-0.5076904], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1391.\n",
      "Terminate False\n",
      "Action (array([-0.11363605], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1392.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1393.\n",
      "Terminate False\n",
      "Action (array([-0.00475109], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1394.\n",
      "Terminate False\n",
      "Action (array([0.16675152], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1395.\n",
      "Terminate False\n",
      "Action (array([-0.20117211], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1396.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1397.\n",
      "Terminate False\n",
      "Action (array([0.69003034], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1398.\n",
      "Terminate False\n",
      "Action (array([0.14589956], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1399.\n",
      "Terminate False\n",
      "Action (array([0.30947092], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1400.\n",
      "Terminate False\n",
      "Action (array([0.84235275], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1401.\n",
      "Terminate False\n",
      "Action (array([0.5938471], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1402.\n",
      "Terminate False\n",
      "Action (array([0.3314681], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1403.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1404.\n",
      "Terminate False\n",
      "Action (array([0.2728836], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1405.\n",
      "Terminate False\n",
      "Action (array([-0.880112], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1406.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1407.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1408.\n",
      "Terminate False\n",
      "Action (array([-0.35699105], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1409.\n",
      "Terminate False\n",
      "Action (array([0.71168554], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1410.\n",
      "Terminate False\n",
      "Action (array([-0.44291756], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1411.\n",
      "Terminate False\n",
      "Action (array([-0.14022686], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1412.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1413.\n",
      "Terminate False\n",
      "Action (array([-0.7148133], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1414.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1415.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1416.\n",
      "Terminate False\n",
      "Action (array([-0.31747627], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1417.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1418.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1419.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1420.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1421.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1422.\n",
      "Terminate False\n",
      "Action (array([-0.10913954], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1423.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1424.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1425.\n",
      "Terminate False\n",
      "Action (array([-0.7026803], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1426.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1427.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1428.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1429.\n",
      "Terminate False\n",
      "Action (array([-0.60826766], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1430.\n",
      "Terminate False\n",
      "Action (array([-0.7656233], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1431.\n",
      "Terminate False\n",
      "Action (array([-0.5542921], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1432.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1433.\n",
      "Terminate False\n",
      "Action (array([-0.43689933], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1434.\n",
      "Terminate False\n",
      "Action (array([-0.6293903], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1435.\n",
      "Terminate False\n",
      "Action (array([-0.5779586], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1436.\n",
      "Terminate False\n",
      "Action (array([0.7372037], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1437.\n",
      "Terminate False\n",
      "Action (array([-0.86227465], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1438.\n",
      "Terminate False\n",
      "Action (array([0.08852077], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1439.\n",
      "Terminate False\n",
      "Action (array([0.0549486], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1440.\n",
      "Terminate False\n",
      "Action (array([0.2973432], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1441.\n",
      "Terminate False\n",
      "Action (array([0.24681927], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1442.\n",
      "Terminate False\n",
      "Action (array([0.5203677], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1443.\n",
      "Terminate False\n",
      "Action (array([-0.22148588], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1444.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1445.\n",
      "Terminate False\n",
      "Action (array([-0.9467613], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1446.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1447.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1448.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1449.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1450.\n",
      "Terminate False\n",
      "Action (array([0.36950022], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1451.\n",
      "Terminate False\n",
      "Action (array([0.39523813], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1452.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1453.\n",
      "Terminate False\n",
      "Action (array([-0.48555395], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1454.\n",
      "Terminate False\n",
      "Action (array([-0.18752366], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1455.\n",
      "Terminate False\n",
      "Action (array([-0.75871426], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1456.\n",
      "Terminate False\n",
      "Action (array([0.7429499], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1457.\n",
      "Terminate False\n",
      "Action (array([0.6038502], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1458.\n",
      "Terminate False\n",
      "Action (array([0.56975603], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1459.\n",
      "Terminate False\n",
      "Action (array([-0.08363391], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1460.\n",
      "Terminate False\n",
      "Action (array([0.20227222], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1461.\n",
      "Terminate False\n",
      "Action (array([-0.10812068], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1462.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1463.\n",
      "Terminate False\n",
      "Action (array([-0.88652915], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1464.\n",
      "Terminate False\n",
      "Action (array([0.28830898], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1465.\n",
      "Terminate False\n",
      "Action (array([0.27503166], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1466.\n",
      "Terminate False\n",
      "Action (array([-0.3322881], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1467.\n",
      "Terminate False\n",
      "Action (array([0.7959367], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1468.\n",
      "Terminate False\n",
      "Action (array([-0.9310739], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1469.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1470.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1471.\n",
      "Terminate False\n",
      "Action (array([1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1472.\n",
      "Terminate False\n",
      "Action (array([-0.8078837], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1473.\n",
      "Terminate False\n",
      "Action (array([0.50390154], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1474.\n",
      "Terminate False\n",
      "Action (array([-1.], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1475.\n",
      "Terminate False\n",
      "Action (array([-0.47711128], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1476.\n",
      "Terminate False\n",
      "Action (array([-0.19404612], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1477.\n",
      "Terminate False\n",
      "Action (array([-0.28668508], dtype=float32), None)\n",
      "Reward 38.66873169117051 at step 1478.\n",
      "Terminate False\n",
      "Action (array([-0.39692333], dtype=float32), None)\n",
      "Reward 39.51809779224473 at step 1479.\n",
      "Terminate False\n",
      "Action (array([0.11506335], dtype=float32), None)\n",
      "Reward 42.593070134163675 at step 1480.\n",
      "Terminate False\n",
      "Action (array([0.2726117], dtype=float32), None)\n",
      "Reward 42.66085359058457 at step 1481.\n",
      "Terminate False\n",
      "Action (array([-0.9220104], dtype=float32), None)\n",
      "Reward 42.670097846550775 at step 1482.\n",
      "Terminate False\n",
      "Action (array([-0.3534316], dtype=float32), None)\n",
      "Episode: 1\n",
      "Reward 42.670097846550775 at step 1482.\n",
      "Terminate True\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(test_train_gym.df)):\n",
    "    action = trained_ppo.predict(np.asarray(state))\n",
    "    print(f'Action {action}')\n",
    "    state,reward,terminal,truncated,info = test_train_gym.step(action)\n",
    "    print(f'Reward {reward} at step {test_train_gym.row}.\\nTerminate {terminal}') # at state:\\n{state}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_ddpg = DRLAgent(env = env_train)\n",
    "# DDPG_PARAMS = {\"batch_size\": 128, \"buffer_size\": episode_length, \"learning_rate\": 0.001}\n",
    "# model_ddpg = agent_ddpg.get_model(\"ddpg\",model_kwargs = DDPG_PARAMS,tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_ddpg_tensorboard/\")\n",
    "\n",
    "# # set up logger\n",
    "# tmp_path = TENSORBOARD_LOG_DIR + '/ddpg'\n",
    "# new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# # Set new logger\n",
    "# model_sac.set_logger(new_logger_ddpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the training log just show up when total_timesteps higher than 15.000\n",
    "# trained_ddpg = agent_ddpg.train_model(model=model_ddpg,\n",
    "#                              tb_log_name='ddpg',\n",
    "#                              total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.noise import NormalActionNoise\n",
    "\n",
    "# # The noise objects for DDPG\n",
    "# n_actions = env_train.action_space.shape[-1]\n",
    "# action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# # Train the agent\n",
    "# trained_ddpg = DDPG(\"MlpPolicy\", env_train, action_noise=action_noise, verbose=1)\n",
    "# trained_ddpg.learn(total_timesteps=total_timesteps, log_interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_length = len(e_train_gym.df) + 1\n",
    "# episode_amount = 4\n",
    "# total_training_step = episode_length*episode_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 2048, 'buffer_size': 58204, 'learning_rate': 0.0003, 'learning_starts': 58204, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# agent_sac = DRLAgent(env = env_train)\n",
    "# SAC_PARAMS = {\n",
    "#     \"batch_size\": 2048,\n",
    "#     \"buffer_size\": episode_length,\n",
    "#     \"learning_rate\": 0.0003,\n",
    "#     \"learning_starts\": episode_length,\n",
    "#     \"ent_coef\": \"auto_0.1\",\n",
    "# }\n",
    "\n",
    "# model_sac = agent_sac.get_model(\"sac\",model_kwargs = SAC_PARAMS,tensorboard_log=TENSORBOARD_LOG_DIR + \"/test_sac/\")\n",
    "\n",
    "# # set up logger\n",
    "# tmp_path = TENSORBOARD_LOG_DIR + '/test_sac/sac_12'\n",
    "# new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# # Set new logger\n",
    "# model_sac.set_logger(new_logger_sac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trained_sac = agent_sac.train_model(model=model_sac, \n",
    "#                              tb_log_name='sac',\n",
    "#                              total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: TD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_td3 = DRLAgent(env = env_train)\n",
    "# TD3_PARAMS = {\"batch_size\": 100, \n",
    "#               \"buffer_size\": 1000000, \n",
    "#               \"learning_rate\": 0.001}\n",
    "\n",
    "# model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_td3 = agent.train_model(model=model_td3, \n",
    "#                              tb_log_name='td3',\n",
    "#                              total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: A2C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent_a2c = DRLAgent(env = env_train)\n",
    "\n",
    "# A2C_PARAMS = {\"n_steps\": 5, \"ent_coef\": 0.005, \"learning_rate\": 0.0002}\n",
    "# model_a2c = agent.get_model(model_name=\"a2c\",model_kwargs = A2C_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_a2c = agent.train_model(model=model_a2c, \n",
    "#                                 tb_log_name='a2c',\n",
    "#                                 total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trading\n",
    "Assume that we have $1,000,000 initial capital at the begining. We use the PPO model to trade Dow jones 30 stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade\n",
    "\n",
    "DRL model needs to update periodically in order to take full advantage of the data, ideally we need to retrain our model yearly, quarterly, or monthly. We also need to tune the parameters along the way, in this notebook I only use the in-sample data from 2009-01 to 2018-12 to tune the parameters once, so there is some alpha decay here as the length of trade date extends. \n",
    "\n",
    "Numerous hyperparameters – e.g. the learning rate, the total number of samples to train on – influence the learning process and are usually determined by testing some variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trade_data = trade_data.reset_index(drop=True)\n",
    "# trade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trade_data = data_split(processed_full, TEST_START_DATE, TEST_END_DATE)\n",
    "e_trade_gym = StockTradingEnv(df = trade_data, **env_kwargs)\n",
    "# env_trade, obs_trade = e_trade_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DRL_prediction function allow testing the model using trading_data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DRL_prediction(model, environment, deterministic=False):\n",
    "        \"\"\"make a prediction and get results\"\"\"\n",
    "        test_env, test_obs = environment.get_sb_env()\n",
    "        account_memory = None  # This help avoid unnecessary list creation\n",
    "        actions_memory = None  # optimize memory consumption\n",
    "\n",
    "        test_env.reset()\n",
    "        max_steps = len(environment.df.index.unique()) - 1\n",
    "\n",
    "        for i in range(len(environment.df.index.unique())):\n",
    "            action, _states = model.predict(test_obs, deterministic=deterministic)\n",
    "            test_obs, rewards, dones, info = test_env.step(action)\n",
    "\n",
    "            if (i == max_steps - 1):  # more descriptive condition for early termination to clarify the logic\n",
    "                account_memory = test_env.env_method(method_name=\"save_asset_memory\")\n",
    "                actions_memory = test_env.env_method(method_name=\"save_action_memory\")\n",
    "\n",
    "            if dones[0]:\n",
    "                print(\"hit end!\")\n",
    "                break\n",
    "        return account_memory[0], actions_memory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 2\n",
      "hit end!\n"
     ]
    }
   ],
   "source": [
    "df_account_value_ppo, df_actions_ppo = DRL_prediction(model=trained_ppo, environment = e_trade_gym)\n",
    "\n",
    "# df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "#     model=trained_ddpg, \n",
    "#     environment = e_trade_gym)\n",
    "\n",
    "# df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "#     model=trained_sac, \n",
    "#     environment = e_trade_gym)\n",
    "\n",
    "# df_account_value_a2c, df_actions_a2c = DRLAgent.DRL_prediction(\n",
    "#     model=trained_a2c, \n",
    "#     environment = e_trade_gym) if if_using_a2c else [None, None]\n",
    "\n",
    "# df_account_value_td3, df_actions_td3 = DRLAgent.DRL_prediction(\n",
    "#     model=trained_td3, \n",
    "#     environment = e_trade_gym) if if_using_td3 else [None, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1483, 2)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_ppo.shape\n",
    "# df_account_value_ddpg.shape\n",
    "# df_account_value_sac.shape\n",
    "# df_account_value_td3.shape\n",
    "# df_account_value_a2c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_account_value_ppo['date'] = trade_data.date\n",
    "# df_account_value_ddpg['date'] = trade_data.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1478</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>1.130607e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479</th>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>1.130607e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1480</th>\n",
       "      <td>2024-01-28</td>\n",
       "      <td>1.131040e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1481</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>1.131040e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>1.131040e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           date  account_value\n",
       "1478 2023-12-31   1.130607e+06\n",
       "1479 2024-01-28   1.130607e+06\n",
       "1480 2024-01-28   1.131040e+06\n",
       "1481 2024-01-31   1.131040e+06\n",
       "1482 2024-01-31   1.131040e+06"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value_ppo.tail()\n",
    "# df_account_value_ddpg.tail()\n",
    "# df_account_value_sac.tail()\n",
    "# df_account_value_td3.tail()\n",
    "# df_account_value_a2c.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_actions_ppo.actions.unique()\n",
    "# df_actions_ddpg.head()\n",
    "# df_actions_sac.head()\n",
    "# df_actions_td3.head()\n",
    "# df_actions_a2c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = df_actions_ppo.actions.iloc[:]\n",
    "actions = actions.to_list()\n",
    "# actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>999430.893865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-30</td>\n",
       "      <td>999077.564139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>999054.291617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>999054.291617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>999001.104151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  account_value\n",
       "0 2021-01-29  999430.893865\n",
       "1 2021-01-30  999077.564139\n",
       "2 2021-01-31  999054.291617\n",
       "3 2021-02-28  999054.291617\n",
       "4 2021-03-31  999001.104151"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value = pd.DataFrame(df_account_value_ppo.date.unique())\n",
    "df_account_value.columns = ['date']\n",
    "df_account_value['account_value'] = df_account_value.apply(lambda x: \\\n",
    "                                    df_account_value_ppo[df_account_value_ppo.date == x.date].iloc[-1].account_value, axis=1)\n",
    "df_account_value.head()\n",
    "\n",
    "# df_account_value = pd.DataFrame(df_account_value_ddpg.date.unique())\n",
    "# df_account_value.columns = ['date']\n",
    "# df_account_value['account_value'] = df_account_value.apply(lambda x: \\\n",
    "#                                     df_account_value_ddpg[df_account_value_ddpg.date == x.date].iloc[-1].account_value, axis=1)\n",
    "# df_account_value.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a missmatch between the quarterly computation of df_account_value and the daily frequency of the comparison datasets. We need to transform the df_account_value to a daily basis.\n",
    "To fill up the missing data, th **ffill** function effectively imputes values, providing the continous picture of account value trends until the next recorded change. However, for the entries preceding the first recorded change, we will use the **initial_amount**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>account_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-31</td>\n",
       "      <td>9.990543e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>9.990543e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>9.990011e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>9.990011e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-31</td>\n",
       "      <td>9.990010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>9.990010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-07-31</td>\n",
       "      <td>9.990010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-08-31</td>\n",
       "      <td>9.990010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>9.990010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>9.990010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-11-30</td>\n",
       "      <td>9.990010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>9.990010e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>1.133382e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>1.133382e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>1.163971e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>1.163971e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-05-31</td>\n",
       "      <td>1.163964e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>1.163964e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-07-31</td>\n",
       "      <td>1.163964e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-08-31</td>\n",
       "      <td>1.163964e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-09-30</td>\n",
       "      <td>1.163957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>1.163957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2022-11-30</td>\n",
       "      <td>1.163957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2022-12-31</td>\n",
       "      <td>1.163957e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>1.049938e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2023-02-28</td>\n",
       "      <td>1.125925e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>1.130610e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2023-04-30</td>\n",
       "      <td>1.130610e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2023-05-31</td>\n",
       "      <td>1.130594e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2023-06-30</td>\n",
       "      <td>1.130611e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2023-07-31</td>\n",
       "      <td>1.130611e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2023-08-31</td>\n",
       "      <td>1.130611e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2023-09-30</td>\n",
       "      <td>1.130611e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2023-10-31</td>\n",
       "      <td>1.130611e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2023-11-30</td>\n",
       "      <td>1.130611e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2023-12-31</td>\n",
       "      <td>1.130607e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>1.131040e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2024-02-29</td>\n",
       "      <td>1.131040e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2024-03-31</td>\n",
       "      <td>1.131040e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  account_value\n",
       "0  2021-01-31   9.990543e+05\n",
       "1  2021-02-28   9.990543e+05\n",
       "2  2021-03-31   9.990011e+05\n",
       "3  2021-04-30   9.990011e+05\n",
       "4  2021-05-31   9.990010e+05\n",
       "5  2021-06-30   9.990010e+05\n",
       "6  2021-07-31   9.990010e+05\n",
       "7  2021-08-31   9.990010e+05\n",
       "8  2021-09-30   9.990010e+05\n",
       "9  2021-10-31   9.990010e+05\n",
       "10 2021-11-30   9.990010e+05\n",
       "11 2021-12-31   9.990010e+05\n",
       "12 2022-01-31   1.133382e+06\n",
       "13 2022-02-28   1.133382e+06\n",
       "14 2022-03-31   1.163971e+06\n",
       "15 2022-04-30   1.163971e+06\n",
       "16 2022-05-31   1.163964e+06\n",
       "17 2022-06-30   1.163964e+06\n",
       "18 2022-07-31   1.163964e+06\n",
       "19 2022-08-31   1.163964e+06\n",
       "20 2022-09-30   1.163957e+06\n",
       "21 2022-10-31   1.163957e+06\n",
       "22 2022-11-30   1.163957e+06\n",
       "23 2022-12-31   1.163957e+06\n",
       "24 2023-01-31   1.049938e+06\n",
       "25 2023-02-28   1.125925e+06\n",
       "26 2023-03-31   1.130610e+06\n",
       "27 2023-04-30   1.130610e+06\n",
       "28 2023-05-31   1.130594e+06\n",
       "29 2023-06-30   1.130611e+06\n",
       "30 2023-07-31   1.130611e+06\n",
       "31 2023-08-31   1.130611e+06\n",
       "32 2023-09-30   1.130611e+06\n",
       "33 2023-10-31   1.130611e+06\n",
       "34 2023-11-30   1.130611e+06\n",
       "35 2023-12-31   1.130607e+06\n",
       "36 2024-01-31   1.131040e+06\n",
       "37 2024-02-29   1.131040e+06\n",
       "38 2024-03-31   1.131040e+06"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_account_value = pd.DataFrame(df.date.unique())\n",
    "daily_account_value.columns = ['date']\n",
    "daily_account_value['date'] = pd.to_datetime(daily_account_value.date)\n",
    "daily_account_value = daily_account_value.merge(df_account_value,how='left')\n",
    "daily_account_value.ffill(inplace=True)\n",
    "daily_account_value.fillna(env_kwargs[\"initial_amount\"],inplace=True)\n",
    "daily_account_value = daily_account_value[daily_account_value.date >= TEST_START_DATE]\n",
    "daily_account_value.reset_index(inplace=True,drop=True)\n",
    "daily_account_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "\n",
      " ppo:\n",
      "Annual return          0.021145\n",
      "Cumulative returns     0.131040\n",
      "Annual volatility      0.067588\n",
      "Sharpe ratio           0.343224\n",
      "Calmar ratio           0.213974\n",
      "Stability              0.524782\n",
      "Max drawdown          -0.098819\n",
      "Omega ratio            2.204772\n",
      "Sortino ratio          0.582348\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio                  NaN\n",
      "Daily value at risk   -0.008423\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "# now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "print(\"\\n ppo:\")\n",
    "perf_stats_all_ppo = backtest_stats(account_value=df_account_value_ppo)\n",
    "perf_stats_all_ppo = pd.DataFrame(perf_stats_all_ppo)\n",
    "# perf_stats_all_ppo.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_ppo_\"+now+'.csv')\n",
    "\n",
    "# print(\"\\n ddpg:\")\n",
    "# perf_stats_all_ddpg = backtest_stats(account_value=df_account_value_ddpg)\n",
    "# perf_stats_all_ddpg = pd.DataFrame(perf_stats_all_ddpg)\n",
    "# perf_stats_all_ddpg.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_ddpg_\"+now+'.csv')\n",
    "\n",
    "# print(\"\\n sac:\")\n",
    "# perf_stats_all_sac = backtest_stats(account_value=df_account_value_sac)\n",
    "# perf_stats_all_sac = pd.DataFrame(perf_stats_all_sac)\n",
    "#   perf_stats_all_sac.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_sac_\"+now+'.csv')\n",
    "\n",
    "#   print(\"\\n atd3:\")\n",
    "#   perf_stats_all_td3 = backtest_stats(account_value=df_account_value_td3)\n",
    "#   perf_stats_all_td3 = pd.DataFrame(perf_stats_all_td3)\n",
    "#   perf_stats_all_td3.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_td3_\"+now+'.csv')\n",
    "\n",
    "#   print(\"\\n a2c:\")\n",
    "#   perf_stats_all_a2c = backtest_stats(account_value=df_account_value_a2c)\n",
    "#   perf_stats_all_a2c = pd.DataFrame(perf_stats_all_a2c)\n",
    "#   perf_stats_all_a2c.to_csv(\"./\"+config.RESULTS_DIR+\"/perf_stats_all_a2c_\"+now+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (776, 8)\n",
      "Annual return          0.083174\n",
      "Cumulative returns     0.278936\n",
      "Annual volatility      0.148583\n",
      "Sharpe ratio           0.612766\n",
      "Calmar ratio           0.379085\n",
      "Stability              0.040036\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            1.109764\n",
      "Sortino ratio          0.874493\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.007442\n",
      "Daily value at risk   -0.018358\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "baseline_df = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = TEST_START_DATE,\n",
    "        end = TEST_END_DATE)\n",
    "\n",
    "stats = backtest_stats(baseline_df, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTest with DJIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Compare to DJIA===========\n",
      "Shape of DataFrame:  (776, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\"><th>Start date</th><td colspan=2>2021-01-31</td></tr>\n",
       "    <tr style=\"text-align: right;\"><th>End date</th><td colspan=2>2024-03-31</td></tr>\n",
       "    <tr style=\"text-align: right;\"><th>Total months</th><td colspan=2>1</td></tr>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Backtest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Annual return</th>\n",
       "      <td>122.949%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cumulative returns</th>\n",
       "      <td>13.211%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Annual volatility</th>\n",
       "      <td>47.52%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sharpe ratio</th>\n",
       "      <td>1.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Calmar ratio</th>\n",
       "      <td>12.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stability</th>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max drawdown</th>\n",
       "      <td>-9.797%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Omega ratio</th>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sortino ratio</th>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Skew</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kurtosis</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tail ratio</th>\n",
       "      <td>1661.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Daily value at risk</th>\n",
       "      <td>-5.617%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alpha</th>\n",
       "      <td>1.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Beta</th>\n",
       "      <td>-0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Worst drawdown periods</th>\n",
       "      <th>Net drawdown in %</th>\n",
       "      <th>Peak date</th>\n",
       "      <th>Valley date</th>\n",
       "      <th>Recovery date</th>\n",
       "      <th>Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.80</td>\n",
       "      <td>2022-04-30</td>\n",
       "      <td>2023-01-31</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>2021-02-28</td>\n",
       "      <td>2021-09-30</td>\n",
       "      <td>2022-01-31</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ConversionError",
     "evalue": "Failed to convert value(s) to axis units: (NaT, Timestamp('2024-03-31 00:00:00+0000', tz='UTC'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axis.py:1769\u001b[0m, in \u001b[0;36mAxis.convert_units\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1769\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pandas/plotting/_matplotlib/converter.py:237\u001b[0m, in \u001b[0;36mPeriodConverter.convert\u001b[0;34m(values, units, axis)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mPeriodConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pandas/plotting/_matplotlib/converter.py:267\u001b[0m, in \u001b[0;36mPeriodConverter._convert_1d\u001b[0;34m(values, units, axis)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, Index)):\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [get_datevalue(x, axis\u001b[38;5;241m.\u001b[39mfreq) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pandas/plotting/_matplotlib/converter.py:267\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, Index)):\n\u001b[0;32m--> 267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mget_datevalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m values]\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pandas/plotting/_matplotlib/converter.py:275\u001b[0m, in \u001b[0;36mget_datevalue\u001b[0;34m(date, freq)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(date, (\u001b[38;5;28mstr\u001b[39m, datetime, pydt\u001b[38;5;241m.\u001b[39mdate, pydt\u001b[38;5;241m.\u001b[39mtime, np\u001b[38;5;241m.\u001b[39mdatetime64)):\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPeriod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mordinal\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    277\u001b[0m     is_integer(date)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m is_float(date)\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(date, (np\u001b[38;5;241m.\u001b[39mndarray, Index)) \u001b[38;5;129;01mand\u001b[39;00m (date\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    280\u001b[0m ):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NaTType' object has no attribute 'ordinal'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConversionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# S&P 500: ^GSPC\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Dow Jones Index: ^DJI\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# NASDAQ 100: ^NDX\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mbacktest_plot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdaily_account_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_ticker\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m^DJI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTEST_START_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTEST_END_DATE\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/finrl/plot.py:71\u001b[0m, in \u001b[0;36mbacktest_plot\u001b[0;34m(account_value, baseline_start, baseline_end, baseline_ticker, value_col_name)\u001b[0m\n\u001b[1;32m     68\u001b[0m baseline_returns \u001b[38;5;241m=\u001b[39m get_daily_return(baseline_df, value_col_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pyfolio\u001b[38;5;241m.\u001b[39mplotting\u001b[38;5;241m.\u001b[39mplotting_context(font_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.1\u001b[39m):\n\u001b[0;32m---> 71\u001b[0m     \u001b[43mpyfolio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_full_tear_sheet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_returns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbenchmark_rets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_returns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pyfolio/tears.py:180\u001b[0m, in \u001b[0;36mcreate_full_tear_sheet\u001b[0;34m(returns, positions, transactions, market_data, benchmark_rets, slippage, live_start_date, sector_mappings, round_trips, estimate_intraday, hide_positions, cone_std, bootstrap, unadjusted_returns, turnover_denom, set_context, factor_returns, factor_loadings, pos_in_dollars, header_rows, factor_partitions)\u001b[0m\n\u001b[1;32m    174\u001b[0m     returns \u001b[38;5;241m=\u001b[39m txn\u001b[38;5;241m.\u001b[39madjust_returns_for_slippage(returns, positions,\n\u001b[1;32m    175\u001b[0m                                               transactions, slippage)\n\u001b[1;32m    177\u001b[0m positions \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mcheck_intraday(estimate_intraday, returns,\n\u001b[1;32m    178\u001b[0m                                  positions, transactions)\n\u001b[0;32m--> 180\u001b[0m \u001b[43mcreate_returns_tear_sheet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_start_date\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_start_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcone_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcone_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbenchmark_rets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbenchmark_rets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mturnover_denom\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mturnover_denom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mset_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mset_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m create_interesting_times_tear_sheet(returns,\n\u001b[1;32m    193\u001b[0m                                     benchmark_rets\u001b[38;5;241m=\u001b[39mbenchmark_rets,\n\u001b[1;32m    194\u001b[0m                                     set_context\u001b[38;5;241m=\u001b[39mset_context)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m positions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pyfolio/plotting.py:54\u001b[0m, in \u001b[0;36mcustomize.<locals>.call_w_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pyfolio/tears.py:569\u001b[0m, in \u001b[0;36mcreate_returns_tear_sheet\u001b[0;34m(returns, positions, transactions, live_start_date, cone_std, benchmark_rets, bootstrap, turnover_denom, header_rows, return_fig)\u001b[0m\n\u001b[1;32m    565\u001b[0m plotting\u001b[38;5;241m.\u001b[39mplot_rolling_sharpe(\n\u001b[1;32m    566\u001b[0m     returns, ax\u001b[38;5;241m=\u001b[39max_rolling_sharpe)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# Drawdowns\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[43mplotting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_drawdown_periods\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max_drawdown\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m plotting\u001b[38;5;241m.\u001b[39mplot_drawdown_underwater(\n\u001b[1;32m    573\u001b[0m     returns\u001b[38;5;241m=\u001b[39mreturns, ax\u001b[38;5;241m=\u001b[39max_underwater)\n\u001b[1;32m    575\u001b[0m plotting\u001b[38;5;241m.\u001b[39mplot_monthly_returns_heatmap(returns, ax\u001b[38;5;241m=\u001b[39max_monthly_heatmap)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/pyfolio/plotting.py:442\u001b[0m, in \u001b[0;36mplot_drawdown_periods\u001b[0;34m(returns, top, ax, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misnull(recovery):\n\u001b[1;32m    441\u001b[0m         recovery \u001b[38;5;241m=\u001b[39m returns\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 442\u001b[0m     \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_between\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeak\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecovery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlim\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlim\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m                    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolors\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(lim)\n\u001b[1;32m    448\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m drawdown periods\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m top)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5512\u001b[0m, in \u001b[0;36mAxes.fill_between\u001b[0;34m(self, x, y1, y2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfill_between\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y1, y2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   5511\u001b[0m                  step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 5512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fill_between_x_or_y\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5513\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5413\u001b[0m, in \u001b[0;36mAxes._fill_between_x_or_y\u001b[0;34m(self, ind_dir, ind, dep1, dep2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5408\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m   5409\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_patches_for_fill\u001b[38;5;241m.\u001b[39mget_next_color()\n\u001b[1;32m   5411\u001b[0m \u001b[38;5;66;03m# Handle united data, such as dates\u001b[39;00m\n\u001b[1;32m   5412\u001b[0m ind, dep1, dep2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[0;32m-> 5413\u001b[0m     ma\u001b[38;5;241m.\u001b[39mmasked_invalid, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_unit_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5414\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mind_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdep_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdep1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdep_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdep2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   5416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, array \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m   5417\u001b[0m         (ind_dir, ind), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, dep1), (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdep_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m, dep2)]:\n\u001b[1;32m   5418\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axes/_base.py:2573\u001b[0m, in \u001b[0;36m_AxesBase._process_unit_info\u001b[0;34m(self, datasets, kwargs, convert)\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m dataset_axis_name \u001b[38;5;241m==\u001b[39m axis_name \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2572\u001b[0m                 axis\u001b[38;5;241m.\u001b[39mupdate_units(data)\n\u001b[0;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [axis_map[axis_name]\u001b[38;5;241m.\u001b[39mconvert_units(data)\n\u001b[1;32m   2574\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m convert \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data\n\u001b[1;32m   2575\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m axis_name, data \u001b[38;5;129;01min\u001b[39;00m datasets]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axes/_base.py:2573\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2571\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m dataset_axis_name \u001b[38;5;241m==\u001b[39m axis_name \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2572\u001b[0m                 axis\u001b[38;5;241m.\u001b[39mupdate_units(data)\n\u001b[0;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43maxis_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m convert \u001b[38;5;129;01mand\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data\n\u001b[1;32m   2575\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m axis_name, data \u001b[38;5;129;01min\u001b[39;00m datasets]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fin_rl_env/lib/python3.10/site-packages/matplotlib/axis.py:1771\u001b[0m, in \u001b[0;36mAxis.convert_units\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1769\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconverter\u001b[38;5;241m.\u001b[39mconvert(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1770\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m munits\u001b[38;5;241m.\u001b[39mConversionError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to convert value(s) to axis \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1772\u001b[0m                                  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1773\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mConversionError\u001b[0m: Failed to convert value(s) to axis units: (NaT, Timestamp('2024-03-31 00:00:00+0000', tz='UTC'))"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x7200 with 13 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"==============Compare to DJIA===========\")\n",
    "%matplotlib inline\n",
    "# S&P 500: ^GSPC\n",
    "# Dow Jones Index: ^DJI\n",
    "# NASDAQ 100: ^NDX\n",
    "\n",
    "backtest_plot(daily_account_value, \n",
    "            baseline_ticker = '^DJI', \n",
    "            baseline_start = TEST_START_DATE,\n",
    "            baseline_end = TEST_END_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Buy&Hold Strategy\n",
    "pass in df_account_value, this information is stored in env class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTest with Buy&Hold Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(DOW_30_TICKER)\n",
    "test_portfolio = DOW_30_TICKER[:2]\n",
    "len(test_portfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "modify_fields = ['open','high','low','close']\n",
    "used_columns = ['date','close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stock_test_ = get_baseline(\n",
    "#         ticker='AXP', \n",
    "#         start = TEST_START_DATE,\n",
    "#         end = TEST_END_DATE)\n",
    "# df_stock_test_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for field in modify_fields:\n",
    "#     df_stock_test_[field] = df_stock_test_[field]/df_stock_test_.iloc[0][field]/len(test_portfolio)\n",
    "# df_stock_test_ = df_stock_test_[used_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1058, 2)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_account_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "oBQx4bVQFi-a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n",
      "Shape of DataFrame:  (1057, 8)\n",
      "Shape of DataFrame:  (1057, 8)\n",
      "Annual return          0.113655\n",
      "Cumulative returns     0.571369\n",
      "Annual volatility      0.272331\n",
      "Sharpe ratio           0.530754\n",
      "Calmar ratio           0.322387\n",
      "Stability              0.732961\n",
      "Max drawdown          -0.352541\n",
      "Omega ratio            1.105579\n",
      "Sortino ratio          0.796600\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.051671\n",
      "Daily value at risk   -0.033737\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "shape = daily_account_value.shape\n",
    "df_hold_ = pd.DataFrame(0,index=range(shape[0]), columns=range(shape[1]))\n",
    "df_hold_.columns = used_columns\n",
    "df_hold_['date'] = daily_account_value.date\n",
    "\n",
    "for stock in test_portfolio:\n",
    "    df_stock_ = get_baseline(\n",
    "        ticker=stock, \n",
    "        start = TEST_START_DATE,\n",
    "        end = TEST_END_DATE)\n",
    "    # for field in modify_fields:\n",
    "    if len(df_stock_) < len(df_hold_):\n",
    "        final_row = df_stock_.iloc[-1]\n",
    "        new_rows = pd.DataFrame([final_row] * (len(df_hold_) - len(df_stock_)))\n",
    "        df_stock_ = pd.concat([df_stock_, new_rows], ignore_index=True)\n",
    "        \n",
    "        for i in range(0,len(df_stock_) < len(df_hold_)):\n",
    "            df_stock_.iloc[en(df_stock_)] = df_stock_.iloc[len(df_stock_-1)]\n",
    "    df_stock_['close'] = df_stock_['close']/df_stock_.iloc[0]['close']/len(test_portfolio)\n",
    "    df_hold_['close'] = df_hold_.close + df_stock_.close\n",
    "\n",
    "stats = backtest_stats(df_hold_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>0.991639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>0.993305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>0.986016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>0.994853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>2024-03-11</td>\n",
       "      <td>1.579257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>2024-03-12</td>\n",
       "      <td>1.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>2024-03-13</td>\n",
       "      <td>1.591768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056</th>\n",
       "      <td>2024-03-14</td>\n",
       "      <td>1.571369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1057</th>\n",
       "      <td>2024-03-15</td>\n",
       "      <td>1.571369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1058 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           date     close\n",
       "0    2020-01-02  1.000000\n",
       "1    2020-01-03  0.991639\n",
       "2    2020-01-06  0.993305\n",
       "3    2020-01-07  0.986016\n",
       "4    2020-01-08  0.994853\n",
       "...         ...       ...\n",
       "1053 2024-03-11  1.579257\n",
       "1054 2024-03-12  1.592400\n",
       "1055 2024-03-13  1.591768\n",
       "1056 2024-03-14  1.571369\n",
       "1057 2024-03-15  1.571369\n",
       "\n",
       "[1058 rows x 2 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hold_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model' calculation encompass the entire timeframe, including non-working days. To address this, we need to determine the most appropriate method to fill up the weekend plots, for example backfilling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hold_.date = pd.to_datetime(df_hold_['date'])\n",
    "# df_hold_ = pd.merge(df_account_value_ppo['date'],df_hold_,how='left')\n",
    "# df_hold_.bfill(inplace=True)\n",
    "# df_hold_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_hold:                      hold\n",
      "date                    \n",
      "2020-01-02  1.000000e+06\n",
      "2020-01-03  9.916392e+05\n",
      "2020-01-06  9.933047e+05\n",
      "2020-01-07  9.860160e+05\n",
      "2020-01-08  9.948534e+05\n",
      "...                  ...\n",
      "2024-03-11  1.579257e+06\n",
      "2024-03-12  1.592400e+06\n",
      "2024-03-13  1.591768e+06\n",
      "2024-03-14  1.571369e+06\n",
      "2024-03-15  1.571369e+06\n",
      "\n",
      "[1058 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_hold = pd.DataFrame()\n",
    "df_hold['date'] = daily_account_value['date']\n",
    "df_hold['hold'] = df_hold_['close'] / df_hold_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "# print(\"df_hold: \", df_hold)\n",
    "# df_dji.to_csv(\"df_dji.csv\")\n",
    "df_hold = df_hold.set_index(df_hold.columns[0])\n",
    "print(\"df_hold: \", df_hold)\n",
    "# df_hold.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "# df_account_value.to_csv('df_account_value.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Compare to Buy&Hold===========\n",
      "result:                     stock          hold\n",
      "date                                  \n",
      "2020-01-02  1.000000e+06  1.000000e+06\n",
      "2020-01-02  9.990001e+05  1.000000e+06\n",
      "2020-01-03  9.922118e+05  9.916392e+05\n",
      "2020-01-03  9.922118e+05  9.916392e+05\n",
      "2020-01-06  9.998330e+05  9.933047e+05\n",
      "...                  ...           ...\n",
      "2024-03-12  1.311546e+06  1.592400e+06\n",
      "2024-03-13  1.308936e+06  1.591768e+06\n",
      "2024-03-13  1.308936e+06  1.591768e+06\n",
      "2024-03-14  1.287816e+06  1.571369e+06\n",
      "2024-03-14  1.287816e+06  1.571369e+06\n",
      "\n",
      "[2114 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"==============Compare to Buy&Hold===========\")\n",
    "df_result = pd.DataFrame({'date': df_account_value_ppo['date'], 'stock': df_account_value_ppo['account_value']})\n",
    "df_result = df_result.set_index('date')\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result = pd.merge(df_result, df_hold, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "# result.to_csv(\"result.csv\")\n",
    "result.columns = ['model', 'buy_and_hold']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7'></a>\n",
    "# 8. Save and load model #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.1'></a>\n",
    "## 8.1 Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.1 Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Desktop/GameProjects/DataScience/DeepReinforcementLearning/FinRL-Tutorials-master/6-Binhlai_Testing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./trained_models/sp500_ppo_yearly_1024episode'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"./\"+TRAINED_MODEL_DIR+\"/sp500_ppo_yearly_1024episode\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_ppo.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1.2 Save model as ONNX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.onnx\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3 import DDPG\n",
    "from stable_baselines3.common.policies import BasePolicy\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnnxableSB3Policy(th.nn.Module):\n",
    "    def __init__(self, policy: BasePolicy):\n",
    "        super().__init__()\n",
    "        self.policy = policy\n",
    "\n",
    "    def forward(self, observation: th.Tensor) -> Tuple[th.Tensor, th.Tensor, th.Tensor]:\n",
    "        # NOTE: Preprocessing is included, but postprocessing\n",
    "        # (clipping/inscaling actions) is not,\n",
    "        # If needed, you also need to transpose the images so that they are channel first\n",
    "        # use deterministic=False if you want to export the stochastic policy\n",
    "        # policy() returns `actions, values, log_prob` for PPO\n",
    "        return self.policy(observation, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_policy = OnnxableSB3Policy(trained_ppo.policy)\n",
    "observation_size = trained_ppo.observation_space.shape\n",
    "dummy_input = th.randn(1, *observation_size)\n",
    "\n",
    "th.onnx.export(\n",
    "    onnx_policy,\n",
    "    dummy_input,\n",
    "    \"sac_model_1200k_sac9.onnx\",\n",
    "    opset_version=17,\n",
    "    input_names=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='7.2'></a>\n",
    "## 8.2 Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.1 Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/Desktop/GameProjects/DataScience/DeepReinforcementLearning/FinRL-Tutorials-master/6-Binhlai_Testing\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./trained_models/sp500_ppo_yearly_1024episode'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"./\"+TRAINED_MODEL_DIR+\"/sp500_ppo_yearly_1024episode\"\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = PPO.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continous training based on the previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Dimension: 1, State Space: 22\n"
     ]
    }
   ],
   "source": [
    "ratio_list = train_data.columns.drop(['date','tic','close'])\n",
    "action_dimension = 1 # k float in range (-1,1) to decide sell (k<0) or buy (k>0) decisions\n",
    "state_space = 1 + 4*action_dimension + len(ratio_list)\n",
    "print(f\"Action Dimension: {action_dimension}, State Space: {state_space}\")\n",
    "\n",
    "episode_length = len(train_data)\n",
    "episode_amount = 256\n",
    "total_training_step = episode_length*episode_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the environment\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001,\n",
    "    \"sell_cost_pct\": 0.01,\n",
    "    \"state_space\": state_space, \n",
    "    \"tech_indicator_list\": ratio_list, \n",
    "    \"action_space\": action_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"stop_loss\": 0.3,\n",
    "    \"print_verbosity\":4,\n",
    "    \"punishment_rate\": 0.01\n",
    "}\n",
    "\n",
    "#Establish the training environment using StockTradingEnv() class\n",
    "e_train_gym = StockTradingEnv(df = train_data, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "agent = DRLAgent(env = env_train)\n",
    "trained_model.env = env_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to tensorboard_log/test_ppo/ppo_29\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    fps             | 625      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 2048     |\n",
      "| train/             |          |\n",
      "|    reward          | 157.1141 |\n",
      "---------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 543           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 7             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6309787e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -7.62e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.36e+05      |\n",
      "|    n_updates            | 23020         |\n",
      "|    policy_gradient_loss | -7.86e-06     |\n",
      "|    reward               | 479.43527     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 8.72e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 649\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 507           |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 6144          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7351932e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 7.99e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.03e+07      |\n",
      "|    n_updates            | 23030         |\n",
      "|    policy_gradient_loss | 4.72e-07      |\n",
      "|    reward               | -10.074907    |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 2.06e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 532           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 15            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 5.36e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.39e+07      |\n",
      "|    n_updates            | 23040         |\n",
      "|    policy_gradient_loss | 1e-06         |\n",
      "|    reward               | 220.41664     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 1.08e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 515          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.050008e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 9.42e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.45e+06     |\n",
      "|    n_updates            | 23050        |\n",
      "|    policy_gradient_loss | -1.72e-05    |\n",
      "|    reward               | 1060.2478    |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 4.9e+06      |\n",
      "------------------------------------------\n",
      "Episode: 650\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 536           |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 22            |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4342977e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 9.36e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.55e+07      |\n",
      "|    n_updates            | 23060         |\n",
      "|    policy_gradient_loss | 1.06e-05      |\n",
      "|    reward               | 224.19414     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 9.1e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 521           |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 27            |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4193938e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.79e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.14e+07      |\n",
      "|    n_updates            | 23070         |\n",
      "|    policy_gradient_loss | -7.37e-06     |\n",
      "|    reward               | 704.5832      |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 4.28e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 651\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 501          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.054738e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | -4.17e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.65e+07     |\n",
      "|    n_updates            | 23080        |\n",
      "|    policy_gradient_loss | -4.34e-07    |\n",
      "|    reward               | 7.1002493    |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 5.29e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 509           |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 36            |\n",
      "|    total_timesteps      | 18432         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7066562e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.19e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.08e+08      |\n",
      "|    n_updates            | 23090         |\n",
      "|    policy_gradient_loss | 1.2e-06       |\n",
      "|    reward               | 117.59964     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 2.15e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 506          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.993542e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | -2.26e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.25e+05     |\n",
      "|    n_updates            | 23100        |\n",
      "|    policy_gradient_loss | -4.29e-05    |\n",
      "|    reward               | 351.92838    |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 8.51e+05     |\n",
      "------------------------------------------\n",
      "Episode: 652\n",
      "row: 5250, episode: 652\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5021555.80\n",
      "total_reward: 4021555.80\n",
      "total_cost: 163083.21\n",
      "total_trades: 317\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 507           |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 44            |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3696466e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 2.83e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.4e+06       |\n",
      "|    n_updates            | 23110         |\n",
      "|    policy_gradient_loss | -0.000102     |\n",
      "|    reward               | 20.055435     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 1.48e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 510           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 48            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0407543e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.37e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.14e+06      |\n",
      "|    n_updates            | 23120         |\n",
      "|    policy_gradient_loss | -1.51e-05     |\n",
      "|    reward               | 234.07193     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 1.03e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 653\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 508           |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 52            |\n",
      "|    total_timesteps      | 26624         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3562385e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.28e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.71e+06      |\n",
      "|    n_updates            | 23130         |\n",
      "|    policy_gradient_loss | 3.67e-06      |\n",
      "|    reward               | 33.129154     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 3.42e+06      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 503         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.09859e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 1.13e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.5e+07     |\n",
      "|    n_updates            | 23140       |\n",
      "|    policy_gradient_loss | -4.77e-05   |\n",
      "|    reward               | 210.66446   |\n",
      "|    std                  | 1.21        |\n",
      "|    value_loss           | 3e+07       |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 487           |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 63            |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3493194e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -2.24e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.45e+06      |\n",
      "|    n_updates            | 23150         |\n",
      "|    policy_gradient_loss | -1.8e-05      |\n",
      "|    reward               | 605.68896     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 4.89e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 654\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 486           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 67            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2703898e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.31e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2e+07         |\n",
      "|    n_updates            | 23160         |\n",
      "|    policy_gradient_loss | 1.94e-05      |\n",
      "|    reward               | 7.4805727     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 3.99e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 486          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 71           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.123446e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 8.94e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.42e+07     |\n",
      "|    n_updates            | 23170        |\n",
      "|    policy_gradient_loss | -2.87e-06    |\n",
      "|    reward               | 149.55031    |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 4.84e+07     |\n",
      "------------------------------------------\n",
      "Episode: 655\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 475           |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 77            |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2805685e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -3.04e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.29e+06      |\n",
      "|    n_updates            | 23180         |\n",
      "|    policy_gradient_loss | -9.27e-07     |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 2.58e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 479          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 81           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.993084e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 5.36e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.52e+07     |\n",
      "|    n_updates            | 23190        |\n",
      "|    policy_gradient_loss | -3.26e-06    |\n",
      "|    reward               | 357.8349     |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 5.05e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 478          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.400537e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | -1.05e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.98e+06     |\n",
      "|    n_updates            | 23200        |\n",
      "|    policy_gradient_loss | -1.39e-06    |\n",
      "|    reward               | 947.58606    |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 1.2e+07      |\n",
      "------------------------------------------\n",
      "Episode: 656\n",
      "row: 5250, episode: 656\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14818822.06\n",
      "total_reward: 13818822.06\n",
      "total_cost: 557250.40\n",
      "total_trades: 276\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 478           |\n",
      "|    iterations           | 21            |\n",
      "|    time_elapsed         | 89            |\n",
      "|    total_timesteps      | 43008         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1292286e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.66e+07      |\n",
      "|    n_updates            | 23210         |\n",
      "|    policy_gradient_loss | -3.88e-06     |\n",
      "|    reward               | -2.6810308    |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.13e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 472           |\n",
      "|    iterations           | 22            |\n",
      "|    time_elapsed         | 95            |\n",
      "|    total_timesteps      | 45056         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8317951e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.13e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.06e+08      |\n",
      "|    n_updates            | 23220         |\n",
      "|    policy_gradient_loss | -7.93e-06     |\n",
      "|    reward               | 222.55753     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.13e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 456           |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 103           |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1414522e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.73e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.33e+06      |\n",
      "|    n_updates            | 23230         |\n",
      "|    policy_gradient_loss | -3.17e-06     |\n",
      "|    reward               | 1600.2417     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.67e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 657\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 106          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.835004e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 5.48e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.22e+07     |\n",
      "|    n_updates            | 23240        |\n",
      "|    policy_gradient_loss | 9.92e-07     |\n",
      "|    reward               | 35.576927    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.44e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 457           |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 111           |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8626451e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.31e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.3e+07       |\n",
      "|    n_updates            | 23250         |\n",
      "|    policy_gradient_loss | -5e-07        |\n",
      "|    reward               | 331.10233     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 4.61e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 658\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 452           |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 117           |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8335413e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -4.53e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.21e+07      |\n",
      "|    n_updates            | 23260         |\n",
      "|    policy_gradient_loss | -5.4e-07      |\n",
      "|    reward               | -10.84276     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.43e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 455           |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 121           |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3591489e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.07e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.29e+07      |\n",
      "|    n_updates            | 23270         |\n",
      "|    policy_gradient_loss | -6.52e-07     |\n",
      "|    reward               | 195.37154     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 4.57e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 457           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 125           |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4444536e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -1.43e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.72e+06      |\n",
      "|    n_updates            | 23280         |\n",
      "|    policy_gradient_loss | -9.03e-06     |\n",
      "|    reward               | 485.5414      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 5.44e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 659\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 460           |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 129           |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9656803e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 2.25e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.55e+07      |\n",
      "|    n_updates            | 23290         |\n",
      "|    policy_gradient_loss | 9.16e-06      |\n",
      "|    reward               | 221.20966     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 5.09e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 460         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.48199e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 8.94e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.71e+06    |\n",
      "|    n_updates            | 23300       |\n",
      "|    policy_gradient_loss | -7.85e-08   |\n",
      "|    reward               | 802.3023    |\n",
      "|    std                  | 1.22        |\n",
      "|    value_loss           | 1.94e+07    |\n",
      "-----------------------------------------\n",
      "Episode: 660\n",
      "row: 5250, episode: 660\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 13278380.22\n",
      "total_reward: 12278380.22\n",
      "total_cost: 502241.15\n",
      "total_trades: 99\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 463           |\n",
      "|    iterations           | 31            |\n",
      "|    time_elapsed         | 137           |\n",
      "|    total_timesteps      | 63488         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1745433e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -2.03e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.73e+07      |\n",
      "|    n_updates            | 23310         |\n",
      "|    policy_gradient_loss | -6.44e-06     |\n",
      "|    reward               | 1.8599567     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 7.47e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 464           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 141           |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.0221874e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 3.99e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.2e+07       |\n",
      "|    n_updates            | 23320         |\n",
      "|    policy_gradient_loss | -3.43e-07     |\n",
      "|    reward               | 82.386185     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.84e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 459           |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 147           |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7310175e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -3.81e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.41e+05      |\n",
      "|    n_updates            | 23330         |\n",
      "|    policy_gradient_loss | -3.61e-05     |\n",
      "|    reward               | 407.6982      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 6.83e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 661\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 460           |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 151           |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9123344e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.34e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.65e+06      |\n",
      "|    n_updates            | 23340         |\n",
      "|    policy_gradient_loss | 1.25e-05      |\n",
      "|    reward               | 81.658646     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 1.33e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 460           |\n",
      "|    iterations           | 35            |\n",
      "|    time_elapsed         | 155           |\n",
      "|    total_timesteps      | 71680         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9127114e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 2.15e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.12e+07      |\n",
      "|    n_updates            | 23350         |\n",
      "|    policy_gradient_loss | -5.65e-06     |\n",
      "|    reward               | 380.34915     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 2.24e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 662\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 456           |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 161           |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0088413e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -4.41e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.84e+06      |\n",
      "|    n_updates            | 23360         |\n",
      "|    policy_gradient_loss | -3.18e-05     |\n",
      "|    reward               | -0.099912144  |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.17e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 458           |\n",
      "|    iterations           | 37            |\n",
      "|    time_elapsed         | 165           |\n",
      "|    total_timesteps      | 75776         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0638614e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -9.54e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.38e+07      |\n",
      "|    n_updates            | 23370         |\n",
      "|    policy_gradient_loss | -8.54e-06     |\n",
      "|    reward               | 173.97726     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.08e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 456          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 170          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.542832e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | -1.82e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.68e+05     |\n",
      "|    n_updates            | 23380        |\n",
      "|    policy_gradient_loss | -7.56e-06    |\n",
      "|    reward               | 395.52475    |\n",
      "|    std                  | 1.21         |\n",
      "|    value_loss           | 1.34e+06     |\n",
      "------------------------------------------\n",
      "Episode: 663\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 175           |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6874705e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 3.64e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.17e+07      |\n",
      "|    n_updates            | 23390         |\n",
      "|    policy_gradient_loss | -8.62e-06     |\n",
      "|    reward               | 13.293634     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 2.35e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 447           |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 182           |\n",
      "|    total_timesteps      | 81920         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3969839e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 1.26e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.13e+07      |\n",
      "|    n_updates            | 23400         |\n",
      "|    policy_gradient_loss | 1.49e-06      |\n",
      "|    reward               | 174.14433     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 6.25e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 445           |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 188           |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2532414e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -0.000171     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 23410         |\n",
      "|    policy_gradient_loss | -2.62e-06     |\n",
      "|    reward               | 436.1711      |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 2.34e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 664\n",
      "row: 5250, episode: 664\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5361711.25\n",
      "total_reward: 4361711.25\n",
      "total_cost: 235080.94\n",
      "total_trades: 542\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 446         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 192         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 3.74479e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.61       |\n",
      "|    explained_variance   | 1.61e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.53e+07    |\n",
      "|    n_updates            | 23420       |\n",
      "|    policy_gradient_loss | -8.42e-06   |\n",
      "|    reward               | 159.60559   |\n",
      "|    std                  | 1.21        |\n",
      "|    value_loss           | 3.06e+07    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 445           |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 197           |\n",
      "|    total_timesteps      | 88064         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2497185e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -2.69e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.23e+06      |\n",
      "|    n_updates            | 23430         |\n",
      "|    policy_gradient_loss | -5.78e-06     |\n",
      "|    reward               | 574.43994     |\n",
      "|    std                  | 1.21          |\n",
      "|    value_loss           | 2.46e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 665\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 445           |\n",
      "|    iterations           | 44            |\n",
      "|    time_elapsed         | 202           |\n",
      "|    total_timesteps      | 90112         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8675928e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -4.77e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.03e+07      |\n",
      "|    n_updates            | 23440         |\n",
      "|    policy_gradient_loss | -3.21e-06     |\n",
      "|    reward               | 28.678446     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 4.06e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 206           |\n",
      "|    total_timesteps      | 92160         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6585109e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.65e+07      |\n",
      "|    n_updates            | 23450         |\n",
      "|    policy_gradient_loss | -2.45e-05     |\n",
      "|    reward               | 215.62988     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 9.3e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 444           |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 211           |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4764373e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 8.94e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.73e+06      |\n",
      "|    n_updates            | 23460         |\n",
      "|    policy_gradient_loss | -2.22e-05     |\n",
      "|    reward               | 1040.7439     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 5.46e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 666\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 446          |\n",
      "|    iterations           | 47           |\n",
      "|    time_elapsed         | 215          |\n",
      "|    total_timesteps      | 96256        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.128409e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 9.66e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.8e+07      |\n",
      "|    n_updates            | 23470        |\n",
      "|    policy_gradient_loss | 1.54e-05     |\n",
      "|    reward               | 115.27651    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 7.61e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 443           |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 221           |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.7928416e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 2.09e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.14e+07      |\n",
      "|    n_updates            | 23480         |\n",
      "|    policy_gradient_loss | -1.52e-05     |\n",
      "|    reward               | 365.42953     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 4.28e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 667\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 442          |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 226          |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.749783e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | -8.34e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.87e+06     |\n",
      "|    n_updates            | 23490        |\n",
      "|    policy_gradient_loss | 5.6e-06      |\n",
      "|    reward               | 8.1009       |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.17e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 443          |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 230          |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.355105e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.62e+07     |\n",
      "|    n_updates            | 23500        |\n",
      "|    policy_gradient_loss | 3.99e-07     |\n",
      "|    reward               | 55.607914    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 5.24e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 444           |\n",
      "|    iterations           | 51            |\n",
      "|    time_elapsed         | 235           |\n",
      "|    total_timesteps      | 104448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7002283e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 2.98e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.86e+05      |\n",
      "|    n_updates            | 23510         |\n",
      "|    policy_gradient_loss | -6.07e-05     |\n",
      "|    reward               | 285.93622     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.72e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 668\n",
      "row: 5250, episode: 668\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4911910.13\n",
      "total_reward: 3911910.13\n",
      "total_cost: 163973.71\n",
      "total_trades: 343\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 446          |\n",
      "|    iterations           | 52           |\n",
      "|    time_elapsed         | 238          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.133742e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 2.24e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.09e+06     |\n",
      "|    n_updates            | 23520        |\n",
      "|    policy_gradient_loss | 9.93e-05     |\n",
      "|    reward               | 66.90447     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 8.17e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 447          |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 242          |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.629845e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 2.68e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.82e+06     |\n",
      "|    n_updates            | 23530        |\n",
      "|    policy_gradient_loss | -8.03e-06    |\n",
      "|    reward               | 206.1517     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 9.63e+06     |\n",
      "------------------------------------------\n",
      "Episode: 669\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 54            |\n",
      "|    time_elapsed         | 247           |\n",
      "|    total_timesteps      | 110592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9087241e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -5.01e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.61e+06      |\n",
      "|    n_updates            | 23540         |\n",
      "|    policy_gradient_loss | -2.18e-05     |\n",
      "|    reward               | 52.988426     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.22e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 448           |\n",
      "|    iterations           | 55            |\n",
      "|    time_elapsed         | 251           |\n",
      "|    total_timesteps      | 112640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6930065e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 3.81e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.34e+06      |\n",
      "|    n_updates            | 23550         |\n",
      "|    policy_gradient_loss | -2.52e-05     |\n",
      "|    reward               | 303.39642     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.07e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 447           |\n",
      "|    iterations           | 56            |\n",
      "|    time_elapsed         | 256           |\n",
      "|    total_timesteps      | 114688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7066562e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 7.93e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.32e+06      |\n",
      "|    n_updates            | 23560         |\n",
      "|    policy_gradient_loss | 1.49e-06      |\n",
      "|    reward               | 768.02496     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.06e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 670\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 447           |\n",
      "|    iterations           | 57            |\n",
      "|    time_elapsed         | 260           |\n",
      "|    total_timesteps      | 116736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5203917e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -3.34e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.24e+07      |\n",
      "|    n_updates            | 23570         |\n",
      "|    policy_gradient_loss | -5.81e-06     |\n",
      "|    reward               | 201.23007     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.05e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 264          |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.193257e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.61        |\n",
      "|    explained_variance   | 3.58e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.46e+07     |\n",
      "|    n_updates            | 23580        |\n",
      "|    policy_gradient_loss | 1.61e-06     |\n",
      "|    reward               | 488.43396    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 8.91e+07     |\n",
      "------------------------------------------\n",
      "Episode: 671\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 448           |\n",
      "|    iterations           | 59            |\n",
      "|    time_elapsed         | 269           |\n",
      "|    total_timesteps      | 120832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2805685e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | -2.46e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.42e+07      |\n",
      "|    n_updates            | 23590         |\n",
      "|    policy_gradient_loss | -5.64e-06     |\n",
      "|    reward               | -0.09999953   |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.85e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 448           |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 273           |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5430851e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.61         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.86e+07      |\n",
      "|    n_updates            | 23600         |\n",
      "|    policy_gradient_loss | -1.62e-06     |\n",
      "|    reward               | 244.84662     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.97e+08      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 449            |\n",
      "|    iterations           | 61             |\n",
      "|    time_elapsed         | 278            |\n",
      "|    total_timesteps      | 124928         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.03667844e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.61          |\n",
      "|    explained_variance   | -2.92e-05      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 2.43e+06       |\n",
      "|    n_updates            | 23610          |\n",
      "|    policy_gradient_loss | -4.86e-06      |\n",
      "|    reward               | 1013.3148      |\n",
      "|    std                  | 1.22           |\n",
      "|    value_loss           | 4.86e+06       |\n",
      "--------------------------------------------\n",
      "Episode: 672\n",
      "row: 5250, episode: 672\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 31563026.30\n",
      "total_reward: 30563026.30\n",
      "total_cost: 571798.17\n",
      "total_trades: 407\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 449          |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 282          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.169445e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | -2.5e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.22e+07     |\n",
      "|    n_updates            | 23620        |\n",
      "|    policy_gradient_loss | -1.74e-06    |\n",
      "|    reward               | 15.33082     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.04e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 63            |\n",
      "|    time_elapsed         | 286           |\n",
      "|    total_timesteps      | 129024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4633558e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.62e+08      |\n",
      "|    n_updates            | 23630         |\n",
      "|    policy_gradient_loss | 1.95e-06      |\n",
      "|    reward               | 334.53366     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 7.24e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 290          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.060283e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 4.05e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.34e+06     |\n",
      "|    n_updates            | 23640        |\n",
      "|    policy_gradient_loss | -1.3e-05     |\n",
      "|    reward               | 764.1587     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.27e+07     |\n",
      "------------------------------------------\n",
      "Episode: 673\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 65            |\n",
      "|    time_elapsed         | 294           |\n",
      "|    total_timesteps      | 133120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7896098e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.08e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.74e+07      |\n",
      "|    n_updates            | 23650         |\n",
      "|    policy_gradient_loss | 5.87e-06      |\n",
      "|    reward               | 266.19745     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 9.47e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 299           |\n",
      "|    total_timesteps      | 135168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6269041e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -2.62e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.03e+07      |\n",
      "|    n_updates            | 23660         |\n",
      "|    policy_gradient_loss | -1.38e-06     |\n",
      "|    reward               | 690.86005     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.07e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 674\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 67           |\n",
      "|    time_elapsed         | 304          |\n",
      "|    total_timesteps      | 137216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.867899e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | -3.46e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.76e+07     |\n",
      "|    n_updates            | 23670        |\n",
      "|    policy_gradient_loss | 6.51e-07     |\n",
      "|    reward               | 105.62909    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 5.52e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 452           |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 307           |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2305252e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.46e+07      |\n",
      "|    n_updates            | 23680         |\n",
      "|    policy_gradient_loss | -1.26e-06     |\n",
      "|    reward               | 340.25302     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.89e+08      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 453            |\n",
      "|    iterations           | 69             |\n",
      "|    time_elapsed         | 311            |\n",
      "|    total_timesteps      | 141312         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.36496965e-08 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.61          |\n",
      "|    explained_variance   | 1.91e-05       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 7.96e+06       |\n",
      "|    n_updates            | 23690          |\n",
      "|    policy_gradient_loss | -1.64e-06      |\n",
      "|    reward               | 728.47864      |\n",
      "|    std                  | 1.22           |\n",
      "|    value_loss           | 1.59e+07       |\n",
      "--------------------------------------------\n",
      "Episode: 675\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 454          |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 315          |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.849265e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 1.25e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.01e+07     |\n",
      "|    n_updates            | 23700        |\n",
      "|    policy_gradient_loss | -2.31e-06    |\n",
      "|    reward               | 32.223454    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 8.02e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 454          |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 320          |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.810557e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 4.77e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.92e+07     |\n",
      "|    n_updates            | 23710        |\n",
      "|    policy_gradient_loss | -4.68e-06    |\n",
      "|    reward               | 178.56396    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 7.85e+07     |\n",
      "------------------------------------------\n",
      "Episode: 676\n",
      "row: 5250, episode: 676\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4215872.65\n",
      "total_reward: 3215872.65\n",
      "total_cost: 131065.64\n",
      "total_trades: 451\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 454           |\n",
      "|    iterations           | 72            |\n",
      "|    time_elapsed         | 324           |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3733807e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 2.04e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.91e+06      |\n",
      "|    n_updates            | 23720         |\n",
      "|    policy_gradient_loss | -1.61e-05     |\n",
      "|    reward               | 51.30613      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.82e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 456          |\n",
      "|    iterations           | 73           |\n",
      "|    time_elapsed         | 327          |\n",
      "|    total_timesteps      | 149504       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.220878e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 2.32e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.52e+06     |\n",
      "|    n_updates            | 23730        |\n",
      "|    policy_gradient_loss | -7.41e-05    |\n",
      "|    reward               | 260.2064     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.1e+07      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 456          |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 332          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.607121e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | -6.08e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.51e+06     |\n",
      "|    n_updates            | 23740        |\n",
      "|    policy_gradient_loss | 1.82e-05     |\n",
      "|    reward               | 534.97235    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 5.02e+06     |\n",
      "------------------------------------------\n",
      "Episode: 677\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 456           |\n",
      "|    iterations           | 75            |\n",
      "|    time_elapsed         | 336           |\n",
      "|    total_timesteps      | 153600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7948485e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.06e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.69e+07      |\n",
      "|    n_updates            | 23750         |\n",
      "|    policy_gradient_loss | -1.33e-06     |\n",
      "|    reward               | 50.91043      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.38e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 456          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 341          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.169974e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 3.58e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.72e+07     |\n",
      "|    n_updates            | 23760        |\n",
      "|    policy_gradient_loss | -6.23e-07    |\n",
      "|    reward               | 333.04184    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 5.45e+07     |\n",
      "------------------------------------------\n",
      "Episode: 678\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 453          |\n",
      "|    iterations           | 77           |\n",
      "|    time_elapsed         | 347          |\n",
      "|    total_timesteps      | 157696       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.773028e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | -1.75e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.47e+06     |\n",
      "|    n_updates            | 23770        |\n",
      "|    policy_gradient_loss | 4.17e-07     |\n",
      "|    reward               | -0.09992797  |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.09e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 454          |\n",
      "|    iterations           | 78           |\n",
      "|    time_elapsed         | 351          |\n",
      "|    total_timesteps      | 159744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.04e+07     |\n",
      "|    n_updates            | 23780        |\n",
      "|    policy_gradient_loss | 3.62e-07     |\n",
      "|    reward               | 149.14201    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.01e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 79            |\n",
      "|    time_elapsed         | 356           |\n",
      "|    total_timesteps      | 161792        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4197077e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -3.18e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.27e+06      |\n",
      "|    n_updates            | 23790         |\n",
      "|    policy_gradient_loss | -8.08e-06     |\n",
      "|    reward               | 360.10654     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.55e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 679\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 453          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 361          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.934838e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 6.32e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.14e+07     |\n",
      "|    n_updates            | 23800        |\n",
      "|    policy_gradient_loss | -6.93e-07    |\n",
      "|    reward               | 26.455622    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 2.28e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 453          |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 365          |\n",
      "|    total_timesteps      | 165888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.970643e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 1.01e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.96e+07     |\n",
      "|    n_updates            | 23810        |\n",
      "|    policy_gradient_loss | -1.54e-05    |\n",
      "|    reward               | 290.48688    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 3.91e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 82            |\n",
      "|    time_elapsed         | 371           |\n",
      "|    total_timesteps      | 167936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2698292e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.09e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.12e+06      |\n",
      "|    n_updates            | 23820         |\n",
      "|    policy_gradient_loss | -1.68e-05     |\n",
      "|    reward               | 588.7255      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 8.25e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 680\n",
      "row: 5250, episode: 680\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6887255.00\n",
      "total_reward: 5887255.00\n",
      "total_cost: 255253.33\n",
      "total_trades: 521\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 452           |\n",
      "|    iterations           | 83            |\n",
      "|    time_elapsed         | 375           |\n",
      "|    total_timesteps      | 169984        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2805914e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.31e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.86e+07      |\n",
      "|    n_updates            | 23830         |\n",
      "|    policy_gradient_loss | 5.92e-06      |\n",
      "|    reward               | 173.90086     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.71e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 84            |\n",
      "|    time_elapsed         | 381           |\n",
      "|    total_timesteps      | 172032        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4351175e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -1.08e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.73e+06      |\n",
      "|    n_updates            | 23840         |\n",
      "|    policy_gradient_loss | -3.57e-06     |\n",
      "|    reward               | 562.802       |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 5.46e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 681\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 386           |\n",
      "|    total_timesteps      | 174080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8419814e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -1.19e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.84e+07      |\n",
      "|    n_updates            | 23850         |\n",
      "|    policy_gradient_loss | 6.77e-06      |\n",
      "|    reward               | 45.933987     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.69e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 86           |\n",
      "|    time_elapsed         | 390          |\n",
      "|    total_timesteps      | 176128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.768946e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.2e+07      |\n",
      "|    n_updates            | 23860        |\n",
      "|    policy_gradient_loss | -3.91e-06    |\n",
      "|    reward               | 324.97372    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.24e+08     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 449          |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 396          |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.988098e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | -2.1e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.22e+06     |\n",
      "|    n_updates            | 23870        |\n",
      "|    policy_gradient_loss | -1.19e-05    |\n",
      "|    reward               | 1317.808     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.24e+07     |\n",
      "------------------------------------------\n",
      "Episode: 682\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 400          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.553325e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 1.57e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.83e+07     |\n",
      "|    n_updates            | 23880        |\n",
      "|    policy_gradient_loss | -1.18e-05    |\n",
      "|    reward               | 201.18427    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 1.77e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 89            |\n",
      "|    time_elapsed         | 405           |\n",
      "|    total_timesteps      | 182272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8402352e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.06e+08      |\n",
      "|    n_updates            | 23890         |\n",
      "|    policy_gradient_loss | -1.62e-06     |\n",
      "|    reward               | 402.5188      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.13e+08      |\n",
      "-------------------------------------------\n",
      "Episode: 683\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 90            |\n",
      "|    time_elapsed         | 410           |\n",
      "|    total_timesteps      | 184320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0227162e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 3.24e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.96e+06      |\n",
      "|    n_updates            | 23900         |\n",
      "|    policy_gradient_loss | -9.58e-07     |\n",
      "|    reward               | -2.0605445    |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.99e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 91            |\n",
      "|    time_elapsed         | 414           |\n",
      "|    total_timesteps      | 186368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7636921e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -4.77e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.04e+07      |\n",
      "|    n_updates            | 23910         |\n",
      "|    policy_gradient_loss | 5.28e-07      |\n",
      "|    reward               | 22.363089     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 6.08e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 447          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 421          |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014098989 |\n",
      "|    clip_fraction        | 0.012        |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | -7.9e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.29e+04     |\n",
      "|    n_updates            | 23920        |\n",
      "|    policy_gradient_loss | -0.00059     |\n",
      "|    reward               | 185.47243    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 8.58e+04     |\n",
      "------------------------------------------\n",
      "Episode: 684\n",
      "row: 5250, episode: 684\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3390401.53\n",
      "total_reward: 2390401.53\n",
      "total_cost: 141280.98\n",
      "total_trades: 930\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 93            |\n",
      "|    time_elapsed         | 426           |\n",
      "|    total_timesteps      | 190464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00028056678 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -1.01e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.19e+06      |\n",
      "|    n_updates            | 23930         |\n",
      "|    policy_gradient_loss | -0.000322     |\n",
      "|    reward               | 110.54878     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.37e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 445           |\n",
      "|    iterations           | 94            |\n",
      "|    time_elapsed         | 432           |\n",
      "|    total_timesteps      | 192512        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7787668e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 0.000118      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.17e+06      |\n",
      "|    n_updates            | 23940         |\n",
      "|    policy_gradient_loss | 0.000141      |\n",
      "|    reward               | 677.7833      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 4.33e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 685\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 443         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 438         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.98727e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | 3.3e-05     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.91e+07    |\n",
      "|    n_updates            | 23950       |\n",
      "|    policy_gradient_loss | 1.15e-06    |\n",
      "|    reward               | 52.533043   |\n",
      "|    std                  | 1.22        |\n",
      "|    value_loss           | 3.83e+07    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 445           |\n",
      "|    iterations           | 96            |\n",
      "|    time_elapsed         | 441           |\n",
      "|    total_timesteps      | 196608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4368713e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -4.77e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.94e+07      |\n",
      "|    n_updates            | 23960         |\n",
      "|    policy_gradient_loss | -1.53e-05     |\n",
      "|    reward               | 293.08127     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.39e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 97            |\n",
      "|    time_elapsed         | 445           |\n",
      "|    total_timesteps      | 198656        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1810487e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 3.3e-05       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.88e+06      |\n",
      "|    n_updates            | 23970         |\n",
      "|    policy_gradient_loss | 1.21e-07      |\n",
      "|    reward               | 629.2094      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 7.76e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 686\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 98            |\n",
      "|    time_elapsed         | 449           |\n",
      "|    total_timesteps      | 200704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -2.5e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.69e+07      |\n",
      "|    n_updates            | 23980         |\n",
      "|    policy_gradient_loss | -5.03e-08     |\n",
      "|    reward               | 32.04442      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 7.38e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 99            |\n",
      "|    time_elapsed         | 453           |\n",
      "|    total_timesteps      | 202752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1059456e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.07e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.22e+07      |\n",
      "|    n_updates            | 23990         |\n",
      "|    policy_gradient_loss | -6.52e-07     |\n",
      "|    reward               | 299.99222     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 1.04e+08      |\n",
      "-------------------------------------------\n",
      "Episode: 687\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 445           |\n",
      "|    iterations           | 100           |\n",
      "|    time_elapsed         | 459           |\n",
      "|    total_timesteps      | 204800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6400237e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -5.21e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.19e+06      |\n",
      "|    n_updates            | 24000         |\n",
      "|    policy_gradient_loss | -1.23e-05     |\n",
      "|    reward               | -0.099995315  |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 4.38e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 101           |\n",
      "|    time_elapsed         | 463           |\n",
      "|    total_timesteps      | 206848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0210004e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 8.94e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.13e+07      |\n",
      "|    n_updates            | 24010         |\n",
      "|    policy_gradient_loss | -4.25e-06     |\n",
      "|    reward               | 128.89822     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 8.26e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 102           |\n",
      "|    time_elapsed         | 468           |\n",
      "|    total_timesteps      | 208896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4176301e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -3.68e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.32e+05      |\n",
      "|    n_updates            | 24020         |\n",
      "|    policy_gradient_loss | -7.59e-05     |\n",
      "|    reward               | 301.45886     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 8.64e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 688\n",
      "row: 5250, episode: 688\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7661225.09\n",
      "total_reward: 6661225.09\n",
      "total_cost: 245711.06\n",
      "total_trades: 602\n",
      "=================================\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 446            |\n",
      "|    iterations           | 103            |\n",
      "|    time_elapsed         | 472            |\n",
      "|    total_timesteps      | 210944         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.45172235e-05 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.62          |\n",
      "|    explained_variance   | 3.1e-06        |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 6.12e+06       |\n",
      "|    n_updates            | 24030          |\n",
      "|    policy_gradient_loss | -1.37e-05      |\n",
      "|    reward               | -13.603203     |\n",
      "|    std                  | 1.22           |\n",
      "|    value_loss           | 1.22e+07       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 447           |\n",
      "|    iterations           | 104           |\n",
      "|    time_elapsed         | 475           |\n",
      "|    total_timesteps      | 212992        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8437568e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.25e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.91e+07      |\n",
      "|    n_updates            | 24040         |\n",
      "|    policy_gradient_loss | 1.32e-06      |\n",
      "|    reward               | 60.890488     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.82e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 447           |\n",
      "|    iterations           | 105           |\n",
      "|    time_elapsed         | 480           |\n",
      "|    total_timesteps      | 215040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1958334e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -6.32e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.12e+05      |\n",
      "|    n_updates            | 24050         |\n",
      "|    policy_gradient_loss | -2.14e-05     |\n",
      "|    reward               | 272.0564      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 4.25e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 689\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 447           |\n",
      "|    iterations           | 106           |\n",
      "|    time_elapsed         | 484           |\n",
      "|    total_timesteps      | 217088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7415884e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 2.7e-05       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.55e+06      |\n",
      "|    n_updates            | 24060         |\n",
      "|    policy_gradient_loss | -2.26e-05     |\n",
      "|    reward               | 50.248173     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 7.1e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 448           |\n",
      "|    iterations           | 107           |\n",
      "|    time_elapsed         | 488           |\n",
      "|    total_timesteps      | 219136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0726875e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 3.7e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.22e+06      |\n",
      "|    n_updates            | 24070         |\n",
      "|    policy_gradient_loss | -1.32e-05     |\n",
      "|    reward               | 123.587845    |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.43e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 690\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 447          |\n",
      "|    iterations           | 108          |\n",
      "|    time_elapsed         | 493          |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.519897e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | -4.86e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.21e+06     |\n",
      "|    n_updates            | 24080        |\n",
      "|    policy_gradient_loss | -1.51e-05    |\n",
      "|    reward               | 33.63106     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 2.43e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 109          |\n",
      "|    time_elapsed         | 498          |\n",
      "|    total_timesteps      | 223232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.838746e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 1.01e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.74e+06     |\n",
      "|    n_updates            | 24090        |\n",
      "|    policy_gradient_loss | -0.000151    |\n",
      "|    reward               | 175.68712    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 9.49e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 447           |\n",
      "|    iterations           | 110           |\n",
      "|    time_elapsed         | 503           |\n",
      "|    total_timesteps      | 225280        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.8975787e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -2.13e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.32e+06      |\n",
      "|    n_updates            | 24100         |\n",
      "|    policy_gradient_loss | -1.11e-05     |\n",
      "|    reward               | 548.4413      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.64e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 691\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 448           |\n",
      "|    iterations           | 111           |\n",
      "|    time_elapsed         | 507           |\n",
      "|    total_timesteps      | 227328        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0432559e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.03e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.92e+07      |\n",
      "|    n_updates            | 24110         |\n",
      "|    policy_gradient_loss | 2.53e-06      |\n",
      "|    reward               | 159.98886     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.84e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 112          |\n",
      "|    time_elapsed         | 511          |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.362649e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 4.29e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.36e+07     |\n",
      "|    n_updates            | 24120        |\n",
      "|    policy_gradient_loss | -4.2e-06     |\n",
      "|    reward               | 411.46533    |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 2.73e+07     |\n",
      "------------------------------------------\n",
      "Episode: 692\n",
      "row: 5250, episode: 692\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7658304.92\n",
      "total_reward: 6658304.92\n",
      "total_cost: 294610.68\n",
      "total_trades: 308\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 113           |\n",
      "|    time_elapsed         | 514           |\n",
      "|    total_timesteps      | 231424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.1013346e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 6.68e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.5e+07       |\n",
      "|    n_updates            | 24130         |\n",
      "|    policy_gradient_loss | 1.45e-07      |\n",
      "|    reward               | 32.89008      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 2.99e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 114           |\n",
      "|    time_elapsed         | 518           |\n",
      "|    total_timesteps      | 233472        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2957101e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.18e+07      |\n",
      "|    n_updates            | 24140         |\n",
      "|    policy_gradient_loss | -8.18e-06     |\n",
      "|    reward               | 261.96722     |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 6.35e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 523          |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.932831e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | -1.57e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.23e+06     |\n",
      "|    n_updates            | 24150        |\n",
      "|    policy_gradient_loss | -1.39e-05    |\n",
      "|    reward               | 542.9116     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 8.47e+06     |\n",
      "------------------------------------------\n",
      "Episode: 693\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 116          |\n",
      "|    time_elapsed         | 527          |\n",
      "|    total_timesteps      | 237568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.546049e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 1.5e-05      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.73e+07     |\n",
      "|    n_updates            | 24160        |\n",
      "|    policy_gradient_loss | -1.73e-05    |\n",
      "|    reward               | 19.31076     |\n",
      "|    std                  | 1.22         |\n",
      "|    value_loss           | 3.45e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 117           |\n",
      "|    time_elapsed         | 530           |\n",
      "|    total_timesteps      | 239616        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7494465e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 3.1e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.63e+07      |\n",
      "|    n_updates            | 24170         |\n",
      "|    policy_gradient_loss | -5.27e-06     |\n",
      "|    reward               | 85.81514      |\n",
      "|    std                  | 1.22          |\n",
      "|    value_loss           | 3.27e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 694\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 452           |\n",
      "|    iterations           | 118           |\n",
      "|    time_elapsed         | 534           |\n",
      "|    total_timesteps      | 241664        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0569929e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 7.51e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.32e+05      |\n",
      "|    n_updates            | 24180         |\n",
      "|    policy_gradient_loss | -8.63e-06     |\n",
      "|    reward               | -0.09999917   |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.65e+05      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 453         |\n",
      "|    iterations           | 119         |\n",
      "|    time_elapsed         | 537         |\n",
      "|    total_timesteps      | 243712      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.72359e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.62       |\n",
      "|    explained_variance   | 1.67e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.16e+06    |\n",
      "|    n_updates            | 24190       |\n",
      "|    policy_gradient_loss | 8.58e-06    |\n",
      "|    reward               | 87.49525    |\n",
      "|    std                  | 1.23        |\n",
      "|    value_loss           | 4.32e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 120           |\n",
      "|    time_elapsed         | 542           |\n",
      "|    total_timesteps      | 245760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7549762e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -4.68e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.59e+05      |\n",
      "|    n_updates            | 24200         |\n",
      "|    policy_gradient_loss | -5.55e-06     |\n",
      "|    reward               | 242.68233     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.18e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 695\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 121           |\n",
      "|    time_elapsed         | 546           |\n",
      "|    total_timesteps      | 247808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3638928e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 4.47e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.23e+06      |\n",
      "|    n_updates            | 24210         |\n",
      "|    policy_gradient_loss | -1.26e-05     |\n",
      "|    reward               | -7.5895486    |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.45e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 453          |\n",
      "|    iterations           | 122          |\n",
      "|    time_elapsed         | 551          |\n",
      "|    total_timesteps      | 249856       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.519511e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 1.67e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.51e+06     |\n",
      "|    n_updates            | 24220        |\n",
      "|    policy_gradient_loss | 2.44e-06     |\n",
      "|    reward               | 183.41847    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.9e+07      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 123           |\n",
      "|    time_elapsed         | 558           |\n",
      "|    total_timesteps      | 251904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7066562e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -9.54e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.04e+05      |\n",
      "|    n_updates            | 24230         |\n",
      "|    policy_gradient_loss | -7.02e-06     |\n",
      "|    reward               | 699.7183      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.81e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 696\n",
      "row: 5250, episode: 696\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7997184.48\n",
      "total_reward: 6997184.48\n",
      "total_cost: 266750.95\n",
      "total_trades: 802\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 124           |\n",
      "|    time_elapsed         | 562           |\n",
      "|    total_timesteps      | 253952        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4389543e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 8.52e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.45e+07      |\n",
      "|    n_updates            | 24240         |\n",
      "|    policy_gradient_loss | -3.18e-05     |\n",
      "|    reward               | 51.307163     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.89e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 125           |\n",
      "|    time_elapsed         | 567           |\n",
      "|    total_timesteps      | 256000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8672436e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.67e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.07e+06      |\n",
      "|    n_updates            | 24250         |\n",
      "|    policy_gradient_loss | -1.86e-05     |\n",
      "|    reward               | 219.58345     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.15e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 697\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 126           |\n",
      "|    time_elapsed         | 573           |\n",
      "|    total_timesteps      | 258048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3800025e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -2.28e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.51e+06      |\n",
      "|    n_updates            | 24260         |\n",
      "|    policy_gradient_loss | -4.11e-07     |\n",
      "|    reward               | 56.543465     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.01e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 127          |\n",
      "|    time_elapsed         | 577          |\n",
      "|    total_timesteps      | 260096       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.648494e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 1.25e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.76e+06     |\n",
      "|    n_updates            | 24270        |\n",
      "|    policy_gradient_loss | -9.75e-06    |\n",
      "|    reward               | 220.86679    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.35e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 582           |\n",
      "|    total_timesteps      | 262144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.0873724e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -8.94e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.5e+06       |\n",
      "|    n_updates            | 24280         |\n",
      "|    policy_gradient_loss | -1.42e-05     |\n",
      "|    reward               | 682.9775      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.99e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 698\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 129           |\n",
      "|    time_elapsed         | 586           |\n",
      "|    total_timesteps      | 264192        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8743095e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.19e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.85e+07      |\n",
      "|    n_updates            | 24290         |\n",
      "|    policy_gradient_loss | -5.74e-06     |\n",
      "|    reward               | 67.69415      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.7e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 590           |\n",
      "|    total_timesteps      | 266240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8326682e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 8.34e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.53e+07      |\n",
      "|    n_updates            | 24300         |\n",
      "|    policy_gradient_loss | -4.44e-06     |\n",
      "|    reward               | 249.14914     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.06e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 699\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 131          |\n",
      "|    time_elapsed         | 594          |\n",
      "|    total_timesteps      | 268288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.111506e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.62        |\n",
      "|    explained_variance   | 3.14e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.01e+06     |\n",
      "|    n_updates            | 24310        |\n",
      "|    policy_gradient_loss | -9.84e-06    |\n",
      "|    reward               | 10.398309    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 6.03e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 452           |\n",
      "|    iterations           | 132           |\n",
      "|    time_elapsed         | 597           |\n",
      "|    total_timesteps      | 270336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0622025e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 1.55e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.51e+07      |\n",
      "|    n_updates            | 24320         |\n",
      "|    policy_gradient_loss | -1.16e-05     |\n",
      "|    reward               | 165.65077     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.01e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 133           |\n",
      "|    time_elapsed         | 601           |\n",
      "|    total_timesteps      | 272384        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8902356e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -2.38e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.16e+05      |\n",
      "|    n_updates            | 24330         |\n",
      "|    policy_gradient_loss | -2.09e-05     |\n",
      "|    reward               | 298.61505     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.33e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 700\n",
      "row: 5250, episode: 700\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6487526.08\n",
      "total_reward: 5487526.08\n",
      "total_cost: 238748.75\n",
      "total_trades: 173\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 134           |\n",
      "|    time_elapsed         | 604           |\n",
      "|    total_timesteps      | 274432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4315883e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 3.76e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1e+07         |\n",
      "|    n_updates            | 24340         |\n",
      "|    policy_gradient_loss | -1.06e-05     |\n",
      "|    reward               | 160.27528     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.01e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 454           |\n",
      "|    iterations           | 135           |\n",
      "|    time_elapsed         | 608           |\n",
      "|    total_timesteps      | 276480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7584534e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.45e+06      |\n",
      "|    n_updates            | 24350         |\n",
      "|    policy_gradient_loss | 1.34e-06      |\n",
      "|    reward               | 691.6686      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.89e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 701\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 136           |\n",
      "|    time_elapsed         | 614           |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6973582e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -1.29e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.45e+07      |\n",
      "|    n_updates            | 24360         |\n",
      "|    policy_gradient_loss | -8.56e-07     |\n",
      "|    reward               | -0.09990017   |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.89e+07      |\n",
      "-------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 453      |\n",
      "|    iterations           | 137      |\n",
      "|    time_elapsed         | 618      |\n",
      "|    total_timesteps      | 280576   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.1      |\n",
      "|    entropy_loss         | -1.62    |\n",
      "|    explained_variance   | 2.98e-07 |\n",
      "|    learning_rate        | 0.00025  |\n",
      "|    loss                 | 9.5e+07  |\n",
      "|    n_updates            | 24370    |\n",
      "|    policy_gradient_loss | 1.11e-06 |\n",
      "|    reward               | 91.24621 |\n",
      "|    std                  | 1.23     |\n",
      "|    value_loss           | 1.9e+08  |\n",
      "--------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 138           |\n",
      "|    time_elapsed         | 623           |\n",
      "|    total_timesteps      | 282624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3857802e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.62         |\n",
      "|    explained_variance   | -2.67e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2e+05         |\n",
      "|    n_updates            | 24380         |\n",
      "|    policy_gradient_loss | -3.32e-05     |\n",
      "|    reward               | 244.3707      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.01e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 702\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 453           |\n",
      "|    iterations           | 139           |\n",
      "|    time_elapsed         | 628           |\n",
      "|    total_timesteps      | 284672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0503586e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.74e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.17e+06      |\n",
      "|    n_updates            | 24390         |\n",
      "|    policy_gradient_loss | 1.46e-05      |\n",
      "|    reward               | 34.191814     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.03e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 453          |\n",
      "|    iterations           | 140          |\n",
      "|    time_elapsed         | 632          |\n",
      "|    total_timesteps      | 286720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.746873e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 9.54e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.85e+06     |\n",
      "|    n_updates            | 24400        |\n",
      "|    policy_gradient_loss | -6.66e-06    |\n",
      "|    reward               | 200.19225    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.97e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 452           |\n",
      "|    iterations           | 141           |\n",
      "|    time_elapsed         | 638           |\n",
      "|    total_timesteps      | 288768        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.5958163e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -4.94e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.58e+06      |\n",
      "|    n_updates            | 24410         |\n",
      "|    policy_gradient_loss | -5.99e-06     |\n",
      "|    reward               | 509.59338     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.16e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 703\n",
      "Episode: 704\n",
      "row: 655, episode: 704\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 697846.88\n",
      "total_reward: -302153.12\n",
      "total_cost: 4884.79\n",
      "total_trades: 2\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 452          |\n",
      "|    iterations           | 142          |\n",
      "|    time_elapsed         | 642          |\n",
      "|    total_timesteps      | 290816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.452036e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 8.94e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.79e+07     |\n",
      "|    n_updates            | 24420        |\n",
      "|    policy_gradient_loss | -1.81e-05    |\n",
      "|    reward               | 145.89775    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 3.58e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 451          |\n",
      "|    iterations           | 143          |\n",
      "|    time_elapsed         | 648          |\n",
      "|    total_timesteps      | 292864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.837691e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 2.92e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.37e+05     |\n",
      "|    n_updates            | 24430        |\n",
      "|    policy_gradient_loss | -4.44e-05    |\n",
      "|    reward               | 461.34775    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.47e+06     |\n",
      "------------------------------------------\n",
      "Episode: 705\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 144          |\n",
      "|    time_elapsed         | 656          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.457959e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -9.78e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.34e+07     |\n",
      "|    n_updates            | 24440        |\n",
      "|    policy_gradient_loss | 3.14e-05     |\n",
      "|    reward               | -0.09999944  |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 2.68e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 145           |\n",
      "|    time_elapsed         | 660           |\n",
      "|    total_timesteps      | 296960        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0858791e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.56e+07      |\n",
      "|    n_updates            | 24450         |\n",
      "|    policy_gradient_loss | 2.6e-05       |\n",
      "|    reward               | 259.40814     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.11e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 146           |\n",
      "|    time_elapsed         | 665           |\n",
      "|    total_timesteps      | 299008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3149187e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -6.32e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.79e+06      |\n",
      "|    n_updates            | 24460         |\n",
      "|    policy_gradient_loss | -5.89e-06     |\n",
      "|    reward               | 510.28476     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.59e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 706\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 449          |\n",
      "|    iterations           | 147          |\n",
      "|    time_elapsed         | 669          |\n",
      "|    total_timesteps      | 301056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.169407e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.3e+07      |\n",
      "|    n_updates            | 24470        |\n",
      "|    policy_gradient_loss | 5.64e-06     |\n",
      "|    reward               | 16.212425    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 4.6e+07      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 148          |\n",
      "|    time_elapsed         | 673          |\n",
      "|    total_timesteps      | 303104       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.085006e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.99e+07     |\n",
      "|    n_updates            | 24480        |\n",
      "|    policy_gradient_loss | 1.07e-07     |\n",
      "|    reward               | 286.51865    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 3.97e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 149           |\n",
      "|    time_elapsed         | 678           |\n",
      "|    total_timesteps      | 305152        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7386704e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.13e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.33e+06      |\n",
      "|    n_updates            | 24490         |\n",
      "|    policy_gradient_loss | -7.22e-06     |\n",
      "|    reward               | 767.8688      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.66e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 707\n",
      "Episode: 708\n",
      "row: 719, episode: 708\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 540941.41\n",
      "total_reward: -459058.59\n",
      "total_cost: 5053.80\n",
      "total_trades: 24\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 150           |\n",
      "|    time_elapsed         | 682           |\n",
      "|    total_timesteps      | 307200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8862193e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.64e+07      |\n",
      "|    n_updates            | 24500         |\n",
      "|    policy_gradient_loss | 5.26e-06      |\n",
      "|    reward               | 22.044676     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.28e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 151          |\n",
      "|    time_elapsed         | 685          |\n",
      "|    total_timesteps      | 309248       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.003523e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 7.75e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.56e+06     |\n",
      "|    n_updates            | 24510        |\n",
      "|    policy_gradient_loss | -0.000272    |\n",
      "|    reward               | 220.20253    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 3.11e+06     |\n",
      "------------------------------------------\n",
      "Episode: 709\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 690          |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.974715e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -3.58e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.64e+06     |\n",
      "|    n_updates            | 24520        |\n",
      "|    policy_gradient_loss | -5.66e-05    |\n",
      "|    reward               | -0.09999944  |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 7.28e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 153           |\n",
      "|    time_elapsed         | 694           |\n",
      "|    total_timesteps      | 313344        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2804288e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.36e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.61e+07      |\n",
      "|    n_updates            | 24530         |\n",
      "|    policy_gradient_loss | -2.92e-05     |\n",
      "|    reward               | 319.52386     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.21e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 154           |\n",
      "|    time_elapsed         | 698           |\n",
      "|    total_timesteps      | 315392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0832598e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -9.89e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.67e+06      |\n",
      "|    n_updates            | 24540         |\n",
      "|    policy_gradient_loss | -1.96e-05     |\n",
      "|    reward               | 574.57043     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.33e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 710\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 155           |\n",
      "|    time_elapsed         | 703           |\n",
      "|    total_timesteps      | 317440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.8420195e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 3.46e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.73e+07      |\n",
      "|    n_updates            | 24550         |\n",
      "|    policy_gradient_loss | 7.6e-06       |\n",
      "|    reward               | 105.62909     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.46e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 451           |\n",
      "|    iterations           | 156           |\n",
      "|    time_elapsed         | 708           |\n",
      "|    total_timesteps      | 319488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1263182e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.43e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.27e+07      |\n",
      "|    n_updates            | 24560         |\n",
      "|    policy_gradient_loss | 8.97e-07      |\n",
      "|    reward               | 612.7734      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.25e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 157           |\n",
      "|    time_elapsed         | 714           |\n",
      "|    total_timesteps      | 321536        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1723175e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.04e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.57e+07      |\n",
      "|    n_updates            | 24570         |\n",
      "|    policy_gradient_loss | -2.08e-06     |\n",
      "|    reward               | 1327.8463     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.15e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 711\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 450           |\n",
      "|    iterations           | 158           |\n",
      "|    time_elapsed         | 718           |\n",
      "|    total_timesteps      | 323584        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4214387e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 9.78e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.29e+08      |\n",
      "|    n_updates            | 24580         |\n",
      "|    policy_gradient_loss | 5.53e-07      |\n",
      "|    reward               | 343.36356     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.59e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 450          |\n",
      "|    iterations           | 159          |\n",
      "|    time_elapsed         | 723          |\n",
      "|    total_timesteps      | 325632       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.783498e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -4.77e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.04e+07     |\n",
      "|    n_updates            | 24590        |\n",
      "|    policy_gradient_loss | -4.74e-07    |\n",
      "|    reward               | 1481.3757    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 4.09e+07     |\n",
      "------------------------------------------\n",
      "Episode: 712\n",
      "row: 5250, episode: 712\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 33430315.76\n",
      "total_reward: 32430315.76\n",
      "total_cost: 935235.18\n",
      "total_trades: 619\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 729           |\n",
      "|    total_timesteps      | 327680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1059456e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.86e+07      |\n",
      "|    n_updates            | 24600         |\n",
      "|    policy_gradient_loss | -9.38e-07     |\n",
      "|    reward               | -6.152599     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.57e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 161           |\n",
      "|    time_elapsed         | 733           |\n",
      "|    total_timesteps      | 329728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3940735e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.4e+08       |\n",
      "|    n_updates            | 24610         |\n",
      "|    policy_gradient_loss | -2.69e-06     |\n",
      "|    reward               | 96.542885     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.28e+09      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 162           |\n",
      "|    time_elapsed         | 738           |\n",
      "|    total_timesteps      | 331776        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9786315e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.1e-05       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.24e+05      |\n",
      "|    n_updates            | 24620         |\n",
      "|    policy_gradient_loss | -3.44e-05     |\n",
      "|    reward               | 270.61453     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.47e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 713\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 163           |\n",
      "|    time_elapsed         | 742           |\n",
      "|    total_timesteps      | 333824        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1759188e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.01e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.9e+06       |\n",
      "|    n_updates            | 24630         |\n",
      "|    policy_gradient_loss | 2.15e-05      |\n",
      "|    reward               | 213.85944     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.81e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 164           |\n",
      "|    time_elapsed         | 747           |\n",
      "|    total_timesteps      | 335872        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0035001e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.13e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.56e+06      |\n",
      "|    n_updates            | 24640         |\n",
      "|    policy_gradient_loss | 6.52e-07      |\n",
      "|    reward               | 581.77454     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.11e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 714\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 448           |\n",
      "|    iterations           | 165           |\n",
      "|    time_elapsed         | 753           |\n",
      "|    total_timesteps      | 337920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5652192e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.29e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.91e+07      |\n",
      "|    n_updates            | 24650         |\n",
      "|    policy_gradient_loss | -2.98e-06     |\n",
      "|    reward               | 50.92579      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.83e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 166           |\n",
      "|    time_elapsed         | 756           |\n",
      "|    total_timesteps      | 339968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1862721e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.79e+07      |\n",
      "|    n_updates            | 24660         |\n",
      "|    policy_gradient_loss | -1.98e-06     |\n",
      "|    reward               | 233.90672     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.57e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 449          |\n",
      "|    iterations           | 167          |\n",
      "|    time_elapsed         | 761          |\n",
      "|    total_timesteps      | 342016       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.094633e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -9.78e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.69e+06     |\n",
      "|    n_updates            | 24670        |\n",
      "|    policy_gradient_loss | -1.1e-05     |\n",
      "|    reward               | 613.1478     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 5.39e+06     |\n",
      "------------------------------------------\n",
      "Episode: 715\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 449           |\n",
      "|    iterations           | 168           |\n",
      "|    time_elapsed         | 766           |\n",
      "|    total_timesteps      | 344064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7517596e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.03e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.17e+07      |\n",
      "|    n_updates            | 24680         |\n",
      "|    policy_gradient_loss | 9.82e-06      |\n",
      "|    reward               | 49.324154     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.35e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 448          |\n",
      "|    iterations           | 169          |\n",
      "|    time_elapsed         | 770          |\n",
      "|    total_timesteps      | 346112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.646143e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.83e+07     |\n",
      "|    n_updates            | 24690        |\n",
      "|    policy_gradient_loss | -2.1e-06     |\n",
      "|    reward               | 394.6386     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 5.66e+07     |\n",
      "------------------------------------------\n",
      "Episode: 716\n",
      "row: 5250, episode: 716\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6084918.07\n",
      "total_reward: 5084918.07\n",
      "total_cost: 299346.88\n",
      "total_trades: 681\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 447           |\n",
      "|    iterations           | 170           |\n",
      "|    time_elapsed         | 777           |\n",
      "|    total_timesteps      | 348160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4971087e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -4.41e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.45e+06      |\n",
      "|    n_updates            | 24700         |\n",
      "|    policy_gradient_loss | -1.18e-06     |\n",
      "|    reward               | -0.09990202   |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.09e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 447          |\n",
      "|    iterations           | 171          |\n",
      "|    time_elapsed         | 782          |\n",
      "|    total_timesteps      | 350208       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.78e+07     |\n",
      "|    n_updates            | 24710        |\n",
      "|    policy_gradient_loss | -2.7e-07     |\n",
      "|    reward               | 118.70079    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 5.55e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 445           |\n",
      "|    iterations           | 172           |\n",
      "|    time_elapsed         | 789           |\n",
      "|    total_timesteps      | 352256        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5513215e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -3.83e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.67e+05      |\n",
      "|    n_updates            | 24720         |\n",
      "|    policy_gradient_loss | -1.05e-05     |\n",
      "|    reward               | 326.4371      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.35e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 717\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 444           |\n",
      "|    iterations           | 173           |\n",
      "|    time_elapsed         | 796           |\n",
      "|    total_timesteps      | 354304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4389543e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.66e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.96e+06      |\n",
      "|    n_updates            | 24730         |\n",
      "|    policy_gradient_loss | 5.49e-06      |\n",
      "|    reward               | 36.612637     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.59e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 444          |\n",
      "|    iterations           | 174          |\n",
      "|    time_elapsed         | 801          |\n",
      "|    total_timesteps      | 356352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.265488e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.49e+07     |\n",
      "|    n_updates            | 24740        |\n",
      "|    policy_gradient_loss | 2.31e-06     |\n",
      "|    reward               | 365.10022    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 4.98e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 443          |\n",
      "|    iterations           | 175          |\n",
      "|    time_elapsed         | 808          |\n",
      "|    total_timesteps      | 358400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.651941e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -2.04e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.35e+06     |\n",
      "|    n_updates            | 24750        |\n",
      "|    policy_gradient_loss | -9.58e-06    |\n",
      "|    reward               | 629.5093     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.27e+07     |\n",
      "------------------------------------------\n",
      "Episode: 718\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 443           |\n",
      "|    iterations           | 176           |\n",
      "|    time_elapsed         | 812           |\n",
      "|    total_timesteps      | 360448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1525117e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.27e+07      |\n",
      "|    n_updates            | 24760         |\n",
      "|    policy_gradient_loss | 6.03e-06      |\n",
      "|    reward               | 329.7715      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 6.53e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 444           |\n",
      "|    iterations           | 177           |\n",
      "|    time_elapsed         | 815           |\n",
      "|    total_timesteps      | 362496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2037344e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -9.3e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.13e+06      |\n",
      "|    n_updates            | 24770         |\n",
      "|    policy_gradient_loss | -1.45e-06     |\n",
      "|    reward               | 917.5953      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.23e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 719\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 444          |\n",
      "|    iterations           | 178          |\n",
      "|    time_elapsed         | 819          |\n",
      "|    total_timesteps      | 364544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.018105e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -2.15e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.71e+07     |\n",
      "|    n_updates            | 24780        |\n",
      "|    policy_gradient_loss | -3.34e-06    |\n",
      "|    reward               | 41.14437     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 9.42e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 444           |\n",
      "|    iterations           | 179           |\n",
      "|    time_elapsed         | 824           |\n",
      "|    total_timesteps      | 366592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5660771e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.35e+08      |\n",
      "|    n_updates            | 24790         |\n",
      "|    policy_gradient_loss | -1.13e-05     |\n",
      "|    reward               | 244.11462     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.69e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 443           |\n",
      "|    iterations           | 180           |\n",
      "|    time_elapsed         | 830           |\n",
      "|    total_timesteps      | 368640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.4983185e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.67e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.97e+06      |\n",
      "|    n_updates            | 24800         |\n",
      "|    policy_gradient_loss | -2.45e-06     |\n",
      "|    reward               | 821.3429      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.95e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 720\n",
      "row: 5250, episode: 720\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9210133.70\n",
      "total_reward: 8210133.70\n",
      "total_cost: 393577.79\n",
      "total_trades: 778\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 443           |\n",
      "|    iterations           | 181           |\n",
      "|    time_elapsed         | 835           |\n",
      "|    total_timesteps      | 370688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4709304e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 6.91e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.58e+07      |\n",
      "|    n_updates            | 24810         |\n",
      "|    policy_gradient_loss | -2.96e-05     |\n",
      "|    reward               | 180.61823     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.15e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 443           |\n",
      "|    iterations           | 182           |\n",
      "|    time_elapsed         | 839           |\n",
      "|    total_timesteps      | 372736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6210834e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 8.34e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.38e+07      |\n",
      "|    n_updates            | 24820         |\n",
      "|    policy_gradient_loss | 3.16e-06      |\n",
      "|    reward               | 348.7625      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.77e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 721\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 443           |\n",
      "|    iterations           | 183           |\n",
      "|    time_elapsed         | 845           |\n",
      "|    total_timesteps      | 374784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5035039e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -2.16e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.28e+07      |\n",
      "|    n_updates            | 24830         |\n",
      "|    policy_gradient_loss | -5.41e-06     |\n",
      "|    reward               | 51.306213     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.57e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 442           |\n",
      "|    iterations           | 184           |\n",
      "|    time_elapsed         | 850           |\n",
      "|    total_timesteps      | 376832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3096724e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.13e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.66e+07      |\n",
      "|    n_updates            | 24840         |\n",
      "|    policy_gradient_loss | 5.24e-07      |\n",
      "|    reward               | 230.26883     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.32e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 442           |\n",
      "|    iterations           | 185           |\n",
      "|    time_elapsed         | 857           |\n",
      "|    total_timesteps      | 378880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9030704e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.17e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.4e+06       |\n",
      "|    n_updates            | 24850         |\n",
      "|    policy_gradient_loss | -3.23e-05     |\n",
      "|    reward               | 706.50214     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.8e+06       |\n",
      "-------------------------------------------\n",
      "Episode: 722\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 441          |\n",
      "|    iterations           | 186          |\n",
      "|    time_elapsed         | 862          |\n",
      "|    total_timesteps      | 380928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.943874e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 8.58e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.86e+07     |\n",
      "|    n_updates            | 24860        |\n",
      "|    policy_gradient_loss | 7.1e-06      |\n",
      "|    reward               | 80.29963     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 5.73e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 441           |\n",
      "|    iterations           | 187           |\n",
      "|    time_elapsed         | 868           |\n",
      "|    total_timesteps      | 382976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2660319e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.74e+07      |\n",
      "|    n_updates            | 24870         |\n",
      "|    policy_gradient_loss | -4.52e-06     |\n",
      "|    reward               | 341.628       |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.49e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 723\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 440          |\n",
      "|    iterations           | 188          |\n",
      "|    time_elapsed         | 873          |\n",
      "|    total_timesteps      | 385024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.406917e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 8.94e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.05e+06     |\n",
      "|    n_updates            | 24880        |\n",
      "|    policy_gradient_loss | -1.43e-05    |\n",
      "|    reward               | 20.496622    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.41e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 440           |\n",
      "|    iterations           | 189           |\n",
      "|    time_elapsed         | 877           |\n",
      "|    total_timesteps      | 387072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0576332e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.56e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.32e+07      |\n",
      "|    n_updates            | 24890         |\n",
      "|    policy_gradient_loss | -2.67e-05     |\n",
      "|    reward               | 163.71992     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.64e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 439           |\n",
      "|    iterations           | 190           |\n",
      "|    time_elapsed         | 884           |\n",
      "|    total_timesteps      | 389120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4331675e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.57e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.13e+06      |\n",
      "|    n_updates            | 24900         |\n",
      "|    policy_gradient_loss | -3.48e-05     |\n",
      "|    reward               | 479.96255     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.25e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 724\n",
      "row: 5250, episode: 724\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8803976.14\n",
      "total_reward: 7803976.14\n",
      "total_cost: 305740.97\n",
      "total_trades: 815\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 439          |\n",
      "|    iterations           | 191          |\n",
      "|    time_elapsed         | 889          |\n",
      "|    total_timesteps      | 391168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.341622e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 5.96e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.39e+07     |\n",
      "|    n_updates            | 24910        |\n",
      "|    policy_gradient_loss | -5.11e-06    |\n",
      "|    reward               | 144.82156    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 2.79e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 440           |\n",
      "|    iterations           | 192           |\n",
      "|    time_elapsed         | 893           |\n",
      "|    total_timesteps      | 393216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4934631e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.37e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.41e+07      |\n",
      "|    n_updates            | 24920         |\n",
      "|    policy_gradient_loss | -3.06e-05     |\n",
      "|    reward               | 398.23596     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.82e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 725\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 439           |\n",
      "|    iterations           | 193           |\n",
      "|    time_elapsed         | 898           |\n",
      "|    total_timesteps      | 395264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6487396e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -6.91e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.93e+06      |\n",
      "|    n_updates            | 24930         |\n",
      "|    policy_gradient_loss | 6.12e-06      |\n",
      "|    reward               | -0.09999657   |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.99e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 440           |\n",
      "|    iterations           | 194           |\n",
      "|    time_elapsed         | 901           |\n",
      "|    total_timesteps      | 397312        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8615115e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.71e+07      |\n",
      "|    n_updates            | 24940         |\n",
      "|    policy_gradient_loss | -5.7e-06      |\n",
      "|    reward               | 49.272564     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.42e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 440          |\n",
      "|    iterations           | 195          |\n",
      "|    time_elapsed         | 907          |\n",
      "|    total_timesteps      | 399360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.782196e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -4.98e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.02e+05     |\n",
      "|    n_updates            | 24950        |\n",
      "|    policy_gradient_loss | -1.82e-05    |\n",
      "|    reward               | 160.36206    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 4.05e+05     |\n",
      "------------------------------------------\n",
      "Episode: 726\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 439           |\n",
      "|    iterations           | 196           |\n",
      "|    time_elapsed         | 912           |\n",
      "|    total_timesteps      | 401408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2506006e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 7.39e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.38e+05      |\n",
      "|    n_updates            | 24960         |\n",
      "|    policy_gradient_loss | -1.51e-05     |\n",
      "|    reward               | 105.62909     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.68e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 440          |\n",
      "|    iterations           | 197          |\n",
      "|    time_elapsed         | 916          |\n",
      "|    total_timesteps      | 403456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.218922e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 3.4e-06      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.97e+06     |\n",
      "|    n_updates            | 24970        |\n",
      "|    policy_gradient_loss | 7.43e-06     |\n",
      "|    reward               | 610.2648     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 7.95e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 440           |\n",
      "|    iterations           | 198           |\n",
      "|    time_elapsed         | 921           |\n",
      "|    total_timesteps      | 405504        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6184945e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.19e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.3e+07       |\n",
      "|    n_updates            | 24980         |\n",
      "|    policy_gradient_loss | -7.67e-06     |\n",
      "|    reward               | 1695.9288     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.61e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 727\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 440           |\n",
      "|    iterations           | 199           |\n",
      "|    time_elapsed         | 924           |\n",
      "|    total_timesteps      | 407552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1338852e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 4.41e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.52e+08      |\n",
      "|    n_updates            | 24990         |\n",
      "|    policy_gradient_loss | 5.32e-06      |\n",
      "|    reward               | 78.99871      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.04e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 440           |\n",
      "|    iterations           | 200           |\n",
      "|    time_elapsed         | 929           |\n",
      "|    total_timesteps      | 409600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7701178e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.46e+07      |\n",
      "|    n_updates            | 25000         |\n",
      "|    policy_gradient_loss | -1.12e-06     |\n",
      "|    reward               | 142.53409     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 6.92e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 728\n",
      "row: 5250, episode: 728\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4238731.02\n",
      "total_reward: 3238731.02\n",
      "total_cost: 207200.26\n",
      "total_trades: 707\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 440          |\n",
      "|    iterations           | 201          |\n",
      "|    time_elapsed         | 935          |\n",
      "|    total_timesteps      | 411648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.468043e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -6.96e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.43e+06     |\n",
      "|    n_updates            | 25010        |\n",
      "|    policy_gradient_loss | -1.47e-06    |\n",
      "|    reward               | 17.786123    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 4.86e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 440           |\n",
      "|    iterations           | 202           |\n",
      "|    time_elapsed         | 938           |\n",
      "|    total_timesteps      | 413696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0409567e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.26e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.39e+06      |\n",
      "|    n_updates            | 25020         |\n",
      "|    policy_gradient_loss | -1.96e-05     |\n",
      "|    reward               | 123.68361     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.78e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 440           |\n",
      "|    iterations           | 203           |\n",
      "|    time_elapsed         | 943           |\n",
      "|    total_timesteps      | 415744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9163784e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.78e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.74e+05      |\n",
      "|    n_updates            | 25030         |\n",
      "|    policy_gradient_loss | -2.02e-05     |\n",
      "|    reward               | 615.67737     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.75e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 729\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 440         |\n",
      "|    iterations           | 204         |\n",
      "|    time_elapsed         | 949         |\n",
      "|    total_timesteps      | 417792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.01724e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | 8.29e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.45e+07    |\n",
      "|    n_updates            | 25040       |\n",
      "|    policy_gradient_loss | 1.03e-05    |\n",
      "|    reward               | 37.0759     |\n",
      "|    std                  | 1.23        |\n",
      "|    value_loss           | 4.9e+07     |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 438          |\n",
      "|    iterations           | 205          |\n",
      "|    time_elapsed         | 956          |\n",
      "|    total_timesteps      | 419840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.156632e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 1.07e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.77e+07     |\n",
      "|    n_updates            | 25050        |\n",
      "|    policy_gradient_loss | -8.34e-07    |\n",
      "|    reward               | 431.23593    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 3.54e+07     |\n",
      "------------------------------------------\n",
      "Episode: 730\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 206           |\n",
      "|    time_elapsed         | 963           |\n",
      "|    total_timesteps      | 421888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.42e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.92e+06      |\n",
      "|    n_updates            | 25060         |\n",
      "|    policy_gradient_loss | -1.73e-06     |\n",
      "|    reward               | -3.1962595    |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 9.85e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 207           |\n",
      "|    time_elapsed         | 968           |\n",
      "|    total_timesteps      | 423936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2194505e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.97e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.79e+07      |\n",
      "|    n_updates            | 25070         |\n",
      "|    policy_gradient_loss | -2.48e-06     |\n",
      "|    reward               | 59.460434     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.59e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 973          |\n",
      "|    total_timesteps      | 425984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.136324e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -1.32e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.23e+05     |\n",
      "|    n_updates            | 25080        |\n",
      "|    policy_gradient_loss | -0.000147    |\n",
      "|    reward               | 210.26778    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 2.47e+05     |\n",
      "------------------------------------------\n",
      "Episode: 731\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 437            |\n",
      "|    iterations           | 209            |\n",
      "|    time_elapsed         | 977            |\n",
      "|    total_timesteps      | 428032         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.000114620314 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.63          |\n",
      "|    explained_variance   | 2.78e-05       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 2.51e+06       |\n",
      "|    n_updates            | 25090          |\n",
      "|    policy_gradient_loss | -0.00025       |\n",
      "|    reward               | 131.18301      |\n",
      "|    std                  | 1.23           |\n",
      "|    value_loss           | 5.02e+06       |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 210           |\n",
      "|    time_elapsed         | 982           |\n",
      "|    total_timesteps      | 430080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6401802e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.62e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.71e+06      |\n",
      "|    n_updates            | 25100         |\n",
      "|    policy_gradient_loss | -9.93e-05     |\n",
      "|    reward               | 404.0082      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.43e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 732\n",
      "row: 5250, episode: 732\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9473532.65\n",
      "total_reward: 8473532.65\n",
      "total_cost: 396260.42\n",
      "total_trades: 485\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 211           |\n",
      "|    time_elapsed         | 987           |\n",
      "|    total_timesteps      | 432128        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7131907e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -8.46e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.13e+07      |\n",
      "|    n_updates            | 25110         |\n",
      "|    policy_gradient_loss | 1.83e-05      |\n",
      "|    reward               | -0.09991826   |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.25e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 212           |\n",
      "|    time_elapsed         | 992           |\n",
      "|    total_timesteps      | 434176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8536534e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 7.15e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.68e+07      |\n",
      "|    n_updates            | 25120         |\n",
      "|    policy_gradient_loss | -8.72e-06     |\n",
      "|    reward               | 280.73297     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.14e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 213           |\n",
      "|    time_elapsed         | 997           |\n",
      "|    total_timesteps      | 436224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1161395e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -5.72e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.23e+06      |\n",
      "|    n_updates            | 25130         |\n",
      "|    policy_gradient_loss | -3.33e-06     |\n",
      "|    reward               | 474.42654     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.46e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 733\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 214           |\n",
      "|    time_elapsed         | 1004          |\n",
      "|    total_timesteps      | 438272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.5460564e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.31e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.13e+07      |\n",
      "|    n_updates            | 25140         |\n",
      "|    policy_gradient_loss | 1.39e-06      |\n",
      "|    reward               | 103.54198     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 6.27e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 215           |\n",
      "|    time_elapsed         | 1009          |\n",
      "|    total_timesteps      | 440320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.0221874e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.25e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.13e+07      |\n",
      "|    n_updates            | 25150         |\n",
      "|    policy_gradient_loss | 9.8e-09       |\n",
      "|    reward               | 553.8037      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 6.25e+07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 435       |\n",
      "|    iterations           | 216       |\n",
      "|    time_elapsed         | 1016      |\n",
      "|    total_timesteps      | 442368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -1.63     |\n",
      "|    explained_variance   | 8.64e-06  |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 1.94e+07  |\n",
      "|    n_updates            | 25160     |\n",
      "|    policy_gradient_loss | 3.74e-07  |\n",
      "|    reward               | 1578.1321 |\n",
      "|    std                  | 1.23      |\n",
      "|    value_loss           | 3.89e+07  |\n",
      "---------------------------------------\n",
      "Episode: 734\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 217           |\n",
      "|    time_elapsed         | 1019          |\n",
      "|    total_timesteps      | 444416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.15e+08      |\n",
      "|    n_updates            | 25170         |\n",
      "|    policy_gradient_loss | -5.28e-07     |\n",
      "|    reward               | 164.37419     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.31e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 218           |\n",
      "|    time_elapsed         | 1023          |\n",
      "|    total_timesteps      | 446464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2834789e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 8.94e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.47e+07      |\n",
      "|    n_updates            | 25180         |\n",
      "|    policy_gradient_loss | -8.55e-07     |\n",
      "|    reward               | 633.1695      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.93e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 735\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 219          |\n",
      "|    time_elapsed         | 1028         |\n",
      "|    total_timesteps      | 448512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.183997e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -9.06e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.58e+07     |\n",
      "|    n_updates            | 25190        |\n",
      "|    policy_gradient_loss | -3.57e-06    |\n",
      "|    reward               | 6.3263288    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 5.16e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 220          |\n",
      "|    time_elapsed         | 1032         |\n",
      "|    total_timesteps      | 450560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.131572e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 3.58e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.28e+07     |\n",
      "|    n_updates            | 25200        |\n",
      "|    policy_gradient_loss | -7.54e-07    |\n",
      "|    reward               | 134.54494    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.26e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 221           |\n",
      "|    time_elapsed         | 1037          |\n",
      "|    total_timesteps      | 452608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6344863e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.79e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.02e+06      |\n",
      "|    n_updates            | 25210         |\n",
      "|    policy_gradient_loss | -4.82e-06     |\n",
      "|    reward               | 447.4734      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.04e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 736\n",
      "row: 5250, episode: 736\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5603946.35\n",
      "total_reward: 4603946.35\n",
      "total_cost: 192675.89\n",
      "total_trades: 464\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 222          |\n",
      "|    time_elapsed         | 1041         |\n",
      "|    total_timesteps      | 454656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.876377e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 1.66e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.01e+07     |\n",
      "|    n_updates            | 25220        |\n",
      "|    policy_gradient_loss | -1.86e-05    |\n",
      "|    reward               | 55.94881     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 2.02e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 223          |\n",
      "|    time_elapsed         | 1045         |\n",
      "|    total_timesteps      | 456704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.011672e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 4.77e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.73e+06     |\n",
      "|    n_updates            | 25230        |\n",
      "|    policy_gradient_loss | -7.9e-06     |\n",
      "|    reward               | 184.99138    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 9.45e+06     |\n",
      "------------------------------------------\n",
      "Episode: 737\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 224           |\n",
      "|    time_elapsed         | 1049          |\n",
      "|    total_timesteps      | 458752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2188684e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -8.34e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.77e+06      |\n",
      "|    n_updates            | 25240         |\n",
      "|    policy_gradient_loss | 1.2e-06       |\n",
      "|    reward               | 8.787649      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.54e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 225           |\n",
      "|    time_elapsed         | 1053          |\n",
      "|    total_timesteps      | 460800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.17e+06      |\n",
      "|    n_updates            | 25250         |\n",
      "|    policy_gradient_loss | 5.51e-08      |\n",
      "|    reward               | 161.00201     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.83e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 226           |\n",
      "|    time_elapsed         | 1058          |\n",
      "|    total_timesteps      | 462848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2397068e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 3.81e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.12e+05      |\n",
      "|    n_updates            | 25260         |\n",
      "|    policy_gradient_loss | -9.1e-06      |\n",
      "|    reward               | 406.62784     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.02e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 738\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 227           |\n",
      "|    time_elapsed         | 1062          |\n",
      "|    total_timesteps      | 464896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7090074e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.66e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.69e+06      |\n",
      "|    n_updates            | 25270         |\n",
      "|    policy_gradient_loss | 1.82e-05      |\n",
      "|    reward               | 246.52663     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.74e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 228          |\n",
      "|    time_elapsed         | 1067         |\n",
      "|    total_timesteps      | 466944       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.668859e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 5.36e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.08e+07     |\n",
      "|    n_updates            | 25280        |\n",
      "|    policy_gradient_loss | -1.55e-06    |\n",
      "|    reward               | 573.8696     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 2.15e+07     |\n",
      "------------------------------------------\n",
      "Episode: 739\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 229          |\n",
      "|    time_elapsed         | 1073         |\n",
      "|    total_timesteps      | 468992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.410402e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -2.15e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.52e+07     |\n",
      "|    n_updates            | 25290        |\n",
      "|    policy_gradient_loss | 8.02e-07     |\n",
      "|    reward               | 51.93686     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 3.04e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 230           |\n",
      "|    time_elapsed         | 1077          |\n",
      "|    total_timesteps      | 471040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2386895e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.96e+07      |\n",
      "|    n_updates            | 25300         |\n",
      "|    policy_gradient_loss | -1.93e-06     |\n",
      "|    reward               | 250.4303      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.99e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 231           |\n",
      "|    time_elapsed         | 1082          |\n",
      "|    total_timesteps      | 473088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9538136e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.05e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.97e+06      |\n",
      "|    n_updates            | 25310         |\n",
      "|    policy_gradient_loss | -7.05e-05     |\n",
      "|    reward               | 662.96155     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.94e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 740\n",
      "row: 5250, episode: 740\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10735065.50\n",
      "total_reward: 9735065.50\n",
      "total_cost: 389063.71\n",
      "total_trades: 662\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 232           |\n",
      "|    time_elapsed         | 1088          |\n",
      "|    total_timesteps      | 475136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3652468e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 6.02e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.57e+07      |\n",
      "|    n_updates            | 25320         |\n",
      "|    policy_gradient_loss | 2.48e-05      |\n",
      "|    reward               | 106.419556    |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 5.14e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 233           |\n",
      "|    time_elapsed         | 1091          |\n",
      "|    total_timesteps      | 477184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6333222e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.07e+07      |\n",
      "|    n_updates            | 25330         |\n",
      "|    policy_gradient_loss | -2.62e-06     |\n",
      "|    reward               | 261.2417      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.01e+08      |\n",
      "-------------------------------------------\n",
      "Episode: 741\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 234           |\n",
      "|    time_elapsed         | 1097          |\n",
      "|    total_timesteps      | 479232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0301668e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -7.22e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.56e+06      |\n",
      "|    n_updates            | 25340         |\n",
      "|    policy_gradient_loss | -4.02e-05     |\n",
      "|    reward               | -0.042111132  |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.13e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 235           |\n",
      "|    time_elapsed         | 1103          |\n",
      "|    total_timesteps      | 481280        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8634386e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.19e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.04e+07      |\n",
      "|    n_updates            | 25350         |\n",
      "|    policy_gradient_loss | -4.06e-05     |\n",
      "|    reward               | 224.8022      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.09e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 434          |\n",
      "|    iterations           | 236          |\n",
      "|    time_elapsed         | 1112         |\n",
      "|    total_timesteps      | 483328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.788185e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -2.06e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.51e+06     |\n",
      "|    n_updates            | 25360        |\n",
      "|    policy_gradient_loss | 4.84e-07     |\n",
      "|    reward               | 479.8545     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 5.01e+06     |\n",
      "------------------------------------------\n",
      "Episode: 742\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 433           |\n",
      "|    iterations           | 237           |\n",
      "|    time_elapsed         | 1118          |\n",
      "|    total_timesteps      | 485376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.5795106e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.01e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.6e+07       |\n",
      "|    n_updates            | 25370         |\n",
      "|    policy_gradient_loss | -2.03e-06     |\n",
      "|    reward               | 105.62909     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.19e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 434          |\n",
      "|    iterations           | 238          |\n",
      "|    time_elapsed         | 1122         |\n",
      "|    total_timesteps      | 487424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.750312e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 7.75e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.04e+07     |\n",
      "|    n_updates            | 25380        |\n",
      "|    policy_gradient_loss | 2.16e-06     |\n",
      "|    reward               | 506.4971     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 6.09e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 434           |\n",
      "|    iterations           | 239           |\n",
      "|    time_elapsed         | 1126          |\n",
      "|    total_timesteps      | 489472        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7640136e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 6.02e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.19e+07      |\n",
      "|    n_updates            | 25390         |\n",
      "|    policy_gradient_loss | -2.3e-05      |\n",
      "|    reward               | 684.4859      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.39e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 743\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 240           |\n",
      "|    time_elapsed         | 1129          |\n",
      "|    total_timesteps      | 491520        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1388329e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.42e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.05e+07      |\n",
      "|    n_updates            | 25400         |\n",
      "|    policy_gradient_loss | -3.35e-05     |\n",
      "|    reward               | 92.24062      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.01e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 241           |\n",
      "|    time_elapsed         | 1134          |\n",
      "|    total_timesteps      | 493568        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5905534e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.26e+06      |\n",
      "|    n_updates            | 25410         |\n",
      "|    policy_gradient_loss | -1.46e-05     |\n",
      "|    reward               | 217.19373     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.45e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 744\n",
      "row: 5250, episode: 744\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7133415.02\n",
      "total_reward: 6133415.02\n",
      "total_cost: 201297.68\n",
      "total_trades: 642\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 434          |\n",
      "|    iterations           | 242          |\n",
      "|    time_elapsed         | 1140         |\n",
      "|    total_timesteps      | 495616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.118215e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -9.54e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.33e+06     |\n",
      "|    n_updates            | 25420        |\n",
      "|    policy_gradient_loss | -4.54e-07    |\n",
      "|    reward               | 2.1916406    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 6.66e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 434          |\n",
      "|    iterations           | 243          |\n",
      "|    time_elapsed         | 1144         |\n",
      "|    total_timesteps      | 497664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.089655e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 8.94e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.52e+07     |\n",
      "|    n_updates            | 25430        |\n",
      "|    policy_gradient_loss | -2.07e-06    |\n",
      "|    reward               | 143.08218    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 3.04e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 434           |\n",
      "|    iterations           | 244           |\n",
      "|    time_elapsed         | 1149          |\n",
      "|    total_timesteps      | 499712        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3833127e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 3.7e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7e+05         |\n",
      "|    n_updates            | 25440         |\n",
      "|    policy_gradient_loss | -3.79e-06     |\n",
      "|    reward               | 426.1373      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.4e+06       |\n",
      "-------------------------------------------\n",
      "Episode: 745\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 434           |\n",
      "|    iterations           | 245           |\n",
      "|    time_elapsed         | 1153          |\n",
      "|    total_timesteps      | 501760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7933856e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.32e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.06e+07      |\n",
      "|    n_updates            | 25450         |\n",
      "|    policy_gradient_loss | -2.82e-06     |\n",
      "|    reward               | 253.25357     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.12e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 435          |\n",
      "|    iterations           | 246          |\n",
      "|    time_elapsed         | 1157         |\n",
      "|    total_timesteps      | 503808       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.956658e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 6.56e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.74e+06     |\n",
      "|    n_updates            | 25460        |\n",
      "|    policy_gradient_loss | -1.03e-05    |\n",
      "|    reward               | 1459.2727    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.95e+07     |\n",
      "------------------------------------------\n",
      "Episode: 746\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 435         |\n",
      "|    iterations           | 247         |\n",
      "|    time_elapsed         | 1161        |\n",
      "|    total_timesteps      | 505856      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 6.44417e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | -6.91e-06   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.91e+07    |\n",
      "|    n_updates            | 25470       |\n",
      "|    policy_gradient_loss | -1.58e-05   |\n",
      "|    reward               | 13.054293   |\n",
      "|    std                  | 1.23        |\n",
      "|    value_loss           | 1.98e+08    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 248           |\n",
      "|    time_elapsed         | 1166          |\n",
      "|    total_timesteps      | 507904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2741657e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.84e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.04e+08      |\n",
      "|    n_updates            | 25480         |\n",
      "|    policy_gradient_loss | 1.76e-06      |\n",
      "|    reward               | 143.49205     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.08e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 434          |\n",
      "|    iterations           | 249          |\n",
      "|    time_elapsed         | 1172         |\n",
      "|    total_timesteps      | 509952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.487374e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 2.27e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.81e+05     |\n",
      "|    n_updates            | 25490        |\n",
      "|    policy_gradient_loss | -3.17e-05    |\n",
      "|    reward               | 455.7724     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.36e+06     |\n",
      "------------------------------------------\n",
      "Episode: 747\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 435          |\n",
      "|    iterations           | 250          |\n",
      "|    time_elapsed         | 1176         |\n",
      "|    total_timesteps      | 512000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.391229e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 1.36e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 9.3e+06      |\n",
      "|    n_updates            | 25500        |\n",
      "|    policy_gradient_loss | 4.98e-05     |\n",
      "|    reward               | 60.37018     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.86e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 251           |\n",
      "|    time_elapsed         | 1180          |\n",
      "|    total_timesteps      | 514048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.37e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.24e+07      |\n",
      "|    n_updates            | 25510         |\n",
      "|    policy_gradient_loss | 2.31e-07      |\n",
      "|    reward               | 271.70303     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.49e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 748\n",
      "row: 5250, episode: 748\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6167516.27\n",
      "total_reward: 5167516.27\n",
      "total_cost: 249871.30\n",
      "total_trades: 431\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 252           |\n",
      "|    time_elapsed         | 1185          |\n",
      "|    total_timesteps      | 516096        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1391468e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.37e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.19e+06      |\n",
      "|    n_updates            | 25520         |\n",
      "|    policy_gradient_loss | -9.94e-06     |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.39e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 435          |\n",
      "|    iterations           | 253          |\n",
      "|    time_elapsed         | 1188         |\n",
      "|    total_timesteps      | 518144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.061504e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 2.26e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.96e+07     |\n",
      "|    n_updates            | 25530        |\n",
      "|    policy_gradient_loss | -2.18e-05    |\n",
      "|    reward               | 313.9343     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 3.91e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 254           |\n",
      "|    time_elapsed         | 1192          |\n",
      "|    total_timesteps      | 520192        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7805876e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -9.89e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.3e+06       |\n",
      "|    n_updates            | 25540         |\n",
      "|    policy_gradient_loss | -4.45e-07     |\n",
      "|    reward               | 635.85266     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.06e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 749\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 255           |\n",
      "|    time_elapsed         | 1196          |\n",
      "|    total_timesteps      | 522240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5920176e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -2.26e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.43e+07      |\n",
      "|    n_updates            | 25550         |\n",
      "|    policy_gradient_loss | 1.05e-05      |\n",
      "|    reward               | 8.146144      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.86e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 256           |\n",
      "|    time_elapsed         | 1200          |\n",
      "|    total_timesteps      | 524288        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1935557e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.19e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.79e+07      |\n",
      "|    n_updates            | 25560         |\n",
      "|    policy_gradient_loss | -1.04e-05     |\n",
      "|    reward               | 277.82858     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 7.58e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 257           |\n",
      "|    time_elapsed         | 1204          |\n",
      "|    total_timesteps      | 526336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.7241566e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.8e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.38e+06      |\n",
      "|    n_updates            | 25570         |\n",
      "|    policy_gradient_loss | 6.17e-08      |\n",
      "|    reward               | 839.4926      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 8.76e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 750\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 258          |\n",
      "|    time_elapsed         | 1208         |\n",
      "|    total_timesteps      | 528384       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 5.19e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.13e+07     |\n",
      "|    n_updates            | 25580        |\n",
      "|    policy_gradient_loss | -8.32e-07    |\n",
      "|    reward               | 31.33093     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 8.26e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 259           |\n",
      "|    time_elapsed         | 1214          |\n",
      "|    total_timesteps      | 530432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.3108746e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 3.46e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.58e+06      |\n",
      "|    n_updates            | 25590         |\n",
      "|    policy_gradient_loss | -5.71e-06     |\n",
      "|    reward               | 157.91046     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.12e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 751\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 260           |\n",
      "|    time_elapsed         | 1219          |\n",
      "|    total_timesteps      | 532480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7319766e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -2.34e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.92e+06      |\n",
      "|    n_updates            | 25600         |\n",
      "|    policy_gradient_loss | -1.32e-05     |\n",
      "|    reward               | 78.63111      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 3.84e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 261           |\n",
      "|    time_elapsed         | 1223          |\n",
      "|    total_timesteps      | 534528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8978393e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.49e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.49e+06      |\n",
      "|    n_updates            | 25610         |\n",
      "|    policy_gradient_loss | -3.92e-05     |\n",
      "|    reward               | 404.5495      |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.9e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 262           |\n",
      "|    time_elapsed         | 1229          |\n",
      "|    total_timesteps      | 536576        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3358658e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -4.17e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.02e+07      |\n",
      "|    n_updates            | 25620         |\n",
      "|    policy_gradient_loss | 1.93e-05      |\n",
      "|    reward               | 1452.5178     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.03e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 752\n",
      "row: 5250, episode: 752\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 19399321.20\n",
      "total_reward: 18399321.20\n",
      "total_cost: 547916.23\n",
      "total_trades: 559\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 263           |\n",
      "|    time_elapsed         | 1233          |\n",
      "|    total_timesteps      | 538624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1618249e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 3.04e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.16e+08      |\n",
      "|    n_updates            | 25630         |\n",
      "|    policy_gradient_loss | 8.01e-07      |\n",
      "|    reward               | 262.59805     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 2.32e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 264           |\n",
      "|    time_elapsed         | 1237          |\n",
      "|    total_timesteps      | 540672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0553354e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.77e+07      |\n",
      "|    n_updates            | 25640         |\n",
      "|    policy_gradient_loss | -2.31e-06     |\n",
      "|    reward               | 583.61487     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 1.75e+08      |\n",
      "-------------------------------------------\n",
      "Episode: 753\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 265          |\n",
      "|    time_elapsed         | 1241         |\n",
      "|    total_timesteps      | 542720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 5.96e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.25e+07     |\n",
      "|    n_updates            | 25650        |\n",
      "|    policy_gradient_loss | 4.14e-07     |\n",
      "|    reward               | -2.1195657   |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 4.5e+07      |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                   |          |\n",
      "|    fps                  | 437      |\n",
      "|    iterations           | 266      |\n",
      "|    time_elapsed         | 1245     |\n",
      "|    total_timesteps      | 544768   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.1      |\n",
      "|    entropy_loss         | -1.63    |\n",
      "|    explained_variance   | 1.55e-06 |\n",
      "|    learning_rate        | 0.00025  |\n",
      "|    loss                 | 8.79e+07 |\n",
      "|    n_updates            | 25660    |\n",
      "|    policy_gradient_loss | 8.14e-08 |\n",
      "|    reward               | 59.40226 |\n",
      "|    std                  | 1.23     |\n",
      "|    value_loss           | 1.76e+08 |\n",
      "--------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 267          |\n",
      "|    time_elapsed         | 1250         |\n",
      "|    total_timesteps      | 546816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.525733e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 1.02e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.8e+05      |\n",
      "|    n_updates            | 25670        |\n",
      "|    policy_gradient_loss | -2.74e-05    |\n",
      "|    reward               | 383.04843    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 3.61e+05     |\n",
      "------------------------------------------\n",
      "Episode: 754\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 268          |\n",
      "|    time_elapsed         | 1254         |\n",
      "|    total_timesteps      | 548864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.552821e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 9.18e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.46e+06     |\n",
      "|    n_updates            | 25680        |\n",
      "|    policy_gradient_loss | 1.3e-05      |\n",
      "|    reward               | 90.772675    |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 6.91e+06     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 437            |\n",
      "|    iterations           | 269            |\n",
      "|    time_elapsed         | 1259           |\n",
      "|    total_timesteps      | 550912         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.15833245e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.63          |\n",
      "|    explained_variance   | 3.1e-06        |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 7.38e+06       |\n",
      "|    n_updates            | 25690          |\n",
      "|    policy_gradient_loss | 3.29e-06       |\n",
      "|    reward               | 300.31082      |\n",
      "|    std                  | 1.23           |\n",
      "|    value_loss           | 1.48e+07       |\n",
      "--------------------------------------------\n",
      "Episode: 755\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 270          |\n",
      "|    time_elapsed         | 1265         |\n",
      "|    total_timesteps      | 552960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.122274e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -1e-05       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.23e+06     |\n",
      "|    n_updates            | 25700        |\n",
      "|    policy_gradient_loss | 2.11e-06     |\n",
      "|    reward               | -0.099921905 |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 1.05e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 271          |\n",
      "|    time_elapsed         | 1268         |\n",
      "|    total_timesteps      | 555008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.707261e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 2.03e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.26e+07     |\n",
      "|    n_updates            | 25710        |\n",
      "|    policy_gradient_loss | 4.22e-06     |\n",
      "|    reward               | 90.21248     |\n",
      "|    std                  | 1.23         |\n",
      "|    value_loss           | 4.52e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 272           |\n",
      "|    time_elapsed         | 1273          |\n",
      "|    total_timesteps      | 557056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9918662e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -2.28e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.21e+05      |\n",
      "|    n_updates            | 25720         |\n",
      "|    policy_gradient_loss | -7.22e-06     |\n",
      "|    reward               | 218.90529     |\n",
      "|    std                  | 1.23          |\n",
      "|    value_loss           | 4.42e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 756\n",
      "row: 5250, episode: 756\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4065751.63\n",
      "total_reward: 3065751.63\n",
      "total_cost: 175668.66\n",
      "total_trades: 424\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 437           |\n",
      "|    iterations           | 273           |\n",
      "|    time_elapsed         | 1278          |\n",
      "|    total_timesteps      | 559104        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2407039e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -8.34e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.98e+06      |\n",
      "|    n_updates            | 25730         |\n",
      "|    policy_gradient_loss | -3.17e-06     |\n",
      "|    reward               | 20.710289     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 7.96e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 437          |\n",
      "|    iterations           | 274          |\n",
      "|    time_elapsed         | 1282         |\n",
      "|    total_timesteps      | 561152       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.002665e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 6.56e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.12e+06     |\n",
      "|    n_updates            | 25740        |\n",
      "|    policy_gradient_loss | -1.59e-05    |\n",
      "|    reward               | 141.5617     |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 1.02e+07     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 437         |\n",
      "|    iterations           | 275         |\n",
      "|    time_elapsed         | 1288        |\n",
      "|    total_timesteps      | 563200      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.56119e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | -3.85e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.13e+05    |\n",
      "|    n_updates            | 25750       |\n",
      "|    policy_gradient_loss | -1.29e-05   |\n",
      "|    reward               | 517.54144   |\n",
      "|    std                  | 1.24        |\n",
      "|    value_loss           | 1.63e+06    |\n",
      "-----------------------------------------\n",
      "Episode: 757\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 276           |\n",
      "|    time_elapsed         | 1294          |\n",
      "|    total_timesteps      | 565248        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2260356e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.37e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.72e+07      |\n",
      "|    n_updates            | 25760         |\n",
      "|    policy_gradient_loss | -1.38e-05     |\n",
      "|    reward               | 205.55885     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 3.44e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 277           |\n",
      "|    time_elapsed         | 1301          |\n",
      "|    total_timesteps      | 567296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5163096e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.72e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.28e+06      |\n",
      "|    n_updates            | 25770         |\n",
      "|    policy_gradient_loss | -1.48e-06     |\n",
      "|    reward               | 495.4517      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 2.57e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 758\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 278           |\n",
      "|    time_elapsed         | 1306          |\n",
      "|    total_timesteps      | 569344        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2805762e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.55e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.53e+07      |\n",
      "|    n_updates            | 25780         |\n",
      "|    policy_gradient_loss | -5.87e-06     |\n",
      "|    reward               | 105.62909     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 3.06e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 279          |\n",
      "|    time_elapsed         | 1309         |\n",
      "|    total_timesteps      | 571392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.982009e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 2.26e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.53e+07     |\n",
      "|    n_updates            | 25790        |\n",
      "|    policy_gradient_loss | 1.01e-06     |\n",
      "|    reward               | 267.58408    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 7.06e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 280           |\n",
      "|    time_elapsed         | 1314          |\n",
      "|    total_timesteps      | 573440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.7619916e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.43e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.33e+06      |\n",
      "|    n_updates            | 25800         |\n",
      "|    policy_gradient_loss | -4.3e-06      |\n",
      "|    reward               | 874.7823      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.27e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 759\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 281          |\n",
      "|    time_elapsed         | 1318         |\n",
      "|    total_timesteps      | 575488       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.004725e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 4.23e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.05e+07     |\n",
      "|    n_updates            | 25810        |\n",
      "|    policy_gradient_loss | 9.69e-07     |\n",
      "|    reward               | 51.30123     |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 8.09e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 282           |\n",
      "|    time_elapsed         | 1324          |\n",
      "|    total_timesteps      | 577536        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7398422e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.42e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.42e+07      |\n",
      "|    n_updates            | 25820         |\n",
      "|    policy_gradient_loss | -3.43e-06     |\n",
      "|    reward               | 304.3774      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 2.83e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 760\n",
      "row: 5250, episode: 760\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5808133.97\n",
      "total_reward: 4808133.97\n",
      "total_cost: 197797.84\n",
      "total_trades: 423\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 283           |\n",
      "|    time_elapsed         | 1329          |\n",
      "|    total_timesteps      | 579584        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0017615e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -1.43e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.33e+06      |\n",
      "|    n_updates            | 25830         |\n",
      "|    policy_gradient_loss | -4.19e-06     |\n",
      "|    reward               | -0.7058021    |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.07e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 284           |\n",
      "|    time_elapsed         | 1333          |\n",
      "|    total_timesteps      | 581632        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4191332e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 3.93e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.51e+07      |\n",
      "|    n_updates            | 25840         |\n",
      "|    policy_gradient_loss | 2.06e-06      |\n",
      "|    reward               | 123.86317     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 3.03e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 285           |\n",
      "|    time_elapsed         | 1338          |\n",
      "|    total_timesteps      | 583680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1136493e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -8.46e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.34e+05      |\n",
      "|    n_updates            | 25850         |\n",
      "|    policy_gradient_loss | -5.4e-05      |\n",
      "|    reward               | 369.3636      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 6.67e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 761\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 436           |\n",
      "|    iterations           | 286           |\n",
      "|    time_elapsed         | 1342          |\n",
      "|    total_timesteps      | 585728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2552133e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 1.18e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.43e+06      |\n",
      "|    n_updates            | 25860         |\n",
      "|    policy_gradient_loss | 5.74e-07      |\n",
      "|    reward               | 67.70675      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.29e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 287          |\n",
      "|    time_elapsed         | 1347         |\n",
      "|    total_timesteps      | 587776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.335423e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 1.97e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 8.16e+06     |\n",
      "|    n_updates            | 25870        |\n",
      "|    policy_gradient_loss | -4.25e-05    |\n",
      "|    reward               | 345.65054    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 1.63e+07     |\n",
      "------------------------------------------\n",
      "Episode: 762\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 435          |\n",
      "|    iterations           | 288          |\n",
      "|    time_elapsed         | 1353         |\n",
      "|    total_timesteps      | 589824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.042994e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 8.05e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.63e+06     |\n",
      "|    n_updates            | 25880        |\n",
      "|    policy_gradient_loss | 9.65e-06     |\n",
      "|    reward               | 52.988426    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 9.27e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 289           |\n",
      "|    time_elapsed         | 1357          |\n",
      "|    total_timesteps      | 591872        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 9.36e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.75e+07      |\n",
      "|    n_updates            | 25890         |\n",
      "|    policy_gradient_loss | 3.6e-07       |\n",
      "|    reward               | 366.78015     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 3.49e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 290           |\n",
      "|    time_elapsed         | 1363          |\n",
      "|    total_timesteps      | 593920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1311204e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -5.6e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.55e+06      |\n",
      "|    n_updates            | 25900         |\n",
      "|    policy_gradient_loss | -2.73e-05     |\n",
      "|    reward               | 737.9478      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.31e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 763\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 435          |\n",
      "|    iterations           | 291          |\n",
      "|    time_elapsed         | 1368         |\n",
      "|    total_timesteps      | 595968       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.310591e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 2.56e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.7e+07      |\n",
      "|    n_updates            | 25910        |\n",
      "|    policy_gradient_loss | 1.86e-05     |\n",
      "|    reward               | 30.982624    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 9.4e+07      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 435           |\n",
      "|    iterations           | 292           |\n",
      "|    time_elapsed         | 1373          |\n",
      "|    total_timesteps      | 598016        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3256795e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.38e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.73e+07      |\n",
      "|    n_updates            | 25920         |\n",
      "|    policy_gradient_loss | -4.27e-06     |\n",
      "|    reward               | 196.6509      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 7.46e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 764\n",
      "row: 5250, episode: 764\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7822515.92\n",
      "total_reward: 6822515.92\n",
      "total_cost: 252096.87\n",
      "total_trades: 661\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 434           |\n",
      "|    iterations           | 293           |\n",
      "|    time_elapsed         | 1381          |\n",
      "|    total_timesteps      | 600064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9892468e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -8.94e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.12e+06      |\n",
      "|    n_updates            | 25930         |\n",
      "|    policy_gradient_loss | -7.85e-06     |\n",
      "|    reward               | -0.09996148   |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 4.24e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 433          |\n",
      "|    iterations           | 294          |\n",
      "|    time_elapsed         | 1388         |\n",
      "|    total_timesteps      | 602112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.693276e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 1.07e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.75e+07     |\n",
      "|    n_updates            | 25940        |\n",
      "|    policy_gradient_loss | 1.42e-06     |\n",
      "|    reward               | 90.87829     |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 5.49e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 433           |\n",
      "|    iterations           | 295           |\n",
      "|    time_elapsed         | 1394          |\n",
      "|    total_timesteps      | 604160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4240885e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -5.58e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.74e+05      |\n",
      "|    n_updates            | 25950         |\n",
      "|    policy_gradient_loss | -6.89e-06     |\n",
      "|    reward               | 182.13734     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 5.48e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 765\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 432           |\n",
      "|    iterations           | 296           |\n",
      "|    time_elapsed         | 1400          |\n",
      "|    total_timesteps      | 606208        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1330703e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 2.44e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.61e+06      |\n",
      "|    n_updates            | 25960         |\n",
      "|    policy_gradient_loss | -6.57e-06     |\n",
      "|    reward               | 9.419136      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 5.23e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 432           |\n",
      "|    iterations           | 297           |\n",
      "|    time_elapsed         | 1404          |\n",
      "|    total_timesteps      | 608256        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6825305e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 5.72e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.77e+06      |\n",
      "|    n_updates            | 25970         |\n",
      "|    policy_gradient_loss | -6.17e-06     |\n",
      "|    reward               | 153.66586     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.15e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 432           |\n",
      "|    iterations           | 298           |\n",
      "|    time_elapsed         | 1411          |\n",
      "|    total_timesteps      | 610304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7805876e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 9.78e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.09e+06      |\n",
      "|    n_updates            | 25980         |\n",
      "|    policy_gradient_loss | -3.53e-06     |\n",
      "|    reward               | 253.88823     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 2.17e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 766\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 432           |\n",
      "|    iterations           | 299           |\n",
      "|    time_elapsed         | 1415          |\n",
      "|    total_timesteps      | 612352        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.4214768e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 3.87e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.2e+06       |\n",
      "|    n_updates            | 25990         |\n",
      "|    policy_gradient_loss | 3.47e-06      |\n",
      "|    reward               | 42.15652      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.64e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 432          |\n",
      "|    iterations           | 300          |\n",
      "|    time_elapsed         | 1420         |\n",
      "|    total_timesteps      | 614400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.907917e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 1.07e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.48e+05     |\n",
      "|    n_updates            | 26000        |\n",
      "|    policy_gradient_loss | -1.85e-05    |\n",
      "|    reward               | 242.53479    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 1.5e+06      |\n",
      "------------------------------------------\n",
      "Episode: 767\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 432           |\n",
      "|    iterations           | 301           |\n",
      "|    time_elapsed         | 1424          |\n",
      "|    total_timesteps      | 616448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1873228e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | -8.94e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.59e+06      |\n",
      "|    n_updates            | 26010         |\n",
      "|    policy_gradient_loss | -0.000102     |\n",
      "|    reward               | 18.879576     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 5.18e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 432           |\n",
      "|    iterations           | 302           |\n",
      "|    time_elapsed         | 1429          |\n",
      "|    total_timesteps      | 618496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7006685e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.06e+06      |\n",
      "|    n_updates            | 26020         |\n",
      "|    policy_gradient_loss | 2.05e-05      |\n",
      "|    reward               | 96.68862      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.81e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 432          |\n",
      "|    iterations           | 303          |\n",
      "|    time_elapsed         | 1435         |\n",
      "|    total_timesteps      | 620544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.331255e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 2.75e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.52e+05     |\n",
      "|    n_updates            | 26030        |\n",
      "|    policy_gradient_loss | -5.28e-06    |\n",
      "|    reward               | 441.19473    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 9.04e+05     |\n",
      "------------------------------------------\n",
      "Episode: 768\n",
      "row: 5250, episode: 768\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6446660.37\n",
      "total_reward: 5446660.37\n",
      "total_cost: 154670.62\n",
      "total_trades: 659\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 432           |\n",
      "|    iterations           | 304           |\n",
      "|    time_elapsed         | 1440          |\n",
      "|    total_timesteps      | 622592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0692747e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 7.33e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.46e+06      |\n",
      "|    n_updates            | 26040         |\n",
      "|    policy_gradient_loss | -2.93e-06     |\n",
      "|    reward               | 103.21964     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.69e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 431           |\n",
      "|    iterations           | 305           |\n",
      "|    time_elapsed         | 1447          |\n",
      "|    total_timesteps      | 624640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2255332e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 7.75e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.16e+06      |\n",
      "|    n_updates            | 26050         |\n",
      "|    policy_gradient_loss | -1.5e-05      |\n",
      "|    reward               | 393.86255     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.63e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 769\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 430          |\n",
      "|    iterations           | 306          |\n",
      "|    time_elapsed         | 1454         |\n",
      "|    total_timesteps      | 626688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.423643e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | 1.67e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.36e+06     |\n",
      "|    n_updates            | 26060        |\n",
      "|    policy_gradient_loss | -1.67e-05    |\n",
      "|    reward               | -3.143236    |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 1.27e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 307           |\n",
      "|    time_elapsed         | 1459          |\n",
      "|    total_timesteps      | 628736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4243643e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.63         |\n",
      "|    explained_variance   | 8.34e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.7e+07       |\n",
      "|    n_updates            | 26070         |\n",
      "|    policy_gradient_loss | 2.17e-07      |\n",
      "|    reward               | 52.82126      |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 5.4e+07       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 430          |\n",
      "|    iterations           | 308          |\n",
      "|    time_elapsed         | 1466         |\n",
      "|    total_timesteps      | 630784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.023976e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.63        |\n",
      "|    explained_variance   | -3.15e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.24e+05     |\n",
      "|    n_updates            | 26080        |\n",
      "|    policy_gradient_loss | -0.000101    |\n",
      "|    reward               | 262.8215     |\n",
      "|    std                  | 1.24         |\n",
      "|    value_loss           | 2.49e+05     |\n",
      "------------------------------------------\n",
      "Episode: 770\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 309           |\n",
      "|    time_elapsed         | 1470          |\n",
      "|    total_timesteps      | 632832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8446964e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.41e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.46e+06      |\n",
      "|    n_updates            | 26090         |\n",
      "|    policy_gradient_loss | 0.000103      |\n",
      "|    reward               | 44.281742     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 4.92e+06      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 430         |\n",
      "|    iterations           | 310         |\n",
      "|    time_elapsed         | 1474        |\n",
      "|    total_timesteps      | 634880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.44478e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 1.25e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.03e+06    |\n",
      "|    n_updates            | 26100       |\n",
      "|    policy_gradient_loss | 2.57e-05    |\n",
      "|    reward               | 173.5764    |\n",
      "|    std                  | 1.24        |\n",
      "|    value_loss           | 1.01e+07    |\n",
      "-----------------------------------------\n",
      "Episode: 771\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 311           |\n",
      "|    time_elapsed         | 1478          |\n",
      "|    total_timesteps      | 636928        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1944517e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -1.16e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.05e+06      |\n",
      "|    n_updates            | 26110         |\n",
      "|    policy_gradient_loss | -9.34e-06     |\n",
      "|    reward               | -0.09991134   |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 4.11e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 312           |\n",
      "|    time_elapsed         | 1482          |\n",
      "|    total_timesteps      | 638976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1935785e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -3.22e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.98e+07      |\n",
      "|    n_updates            | 26120         |\n",
      "|    policy_gradient_loss | 7.72e-06      |\n",
      "|    reward               | 52.745132     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 7.97e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 313           |\n",
      "|    time_elapsed         | 1487          |\n",
      "|    total_timesteps      | 641024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0143495e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -3.81e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.92e+04      |\n",
      "|    n_updates            | 26130         |\n",
      "|    policy_gradient_loss | -2.99e-05     |\n",
      "|    reward               | 186.35776     |\n",
      "|    std                  | 1.24          |\n",
      "|    value_loss           | 1.58e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 772\n",
      "row: 5250, episode: 772\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4258465.42\n",
      "total_reward: 3258465.42\n",
      "total_cost: 190806.87\n",
      "total_trades: 666\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 314           |\n",
      "|    time_elapsed         | 1492          |\n",
      "|    total_timesteps      | 643072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1422242e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -5.96e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.52e+06      |\n",
      "|    n_updates            | 26140         |\n",
      "|    policy_gradient_loss | -1.5e-05      |\n",
      "|    reward               | 34.5463       |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 5.03e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 315           |\n",
      "|    time_elapsed         | 1497          |\n",
      "|    total_timesteps      | 645120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1414231e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.49e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.42e+06      |\n",
      "|    n_updates            | 26150         |\n",
      "|    policy_gradient_loss | -1.55e-05     |\n",
      "|    reward               | 179.58028     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.08e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 316           |\n",
      "|    time_elapsed         | 1502          |\n",
      "|    total_timesteps      | 647168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7104612e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -8.94e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.58e+06      |\n",
      "|    n_updates            | 26160         |\n",
      "|    policy_gradient_loss | -7.65e-06     |\n",
      "|    reward               | 740.0968      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 5.16e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 773\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 431           |\n",
      "|    iterations           | 317           |\n",
      "|    time_elapsed         | 1505          |\n",
      "|    total_timesteps      | 649216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5259367e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 5.36e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.35e+07      |\n",
      "|    n_updates            | 26170         |\n",
      "|    policy_gradient_loss | -4.91e-06     |\n",
      "|    reward               | 13.1272545    |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.71e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 431          |\n",
      "|    iterations           | 318          |\n",
      "|    time_elapsed         | 1510         |\n",
      "|    total_timesteps      | 651264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.229957e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 1.31e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.15e+06     |\n",
      "|    n_updates            | 26180        |\n",
      "|    policy_gradient_loss | -2.97e-05    |\n",
      "|    reward               | 98.10191     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 4.29e+06     |\n",
      "------------------------------------------\n",
      "Episode: 774\n",
      "Episode: 775\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 431           |\n",
      "|    iterations           | 319           |\n",
      "|    time_elapsed         | 1515          |\n",
      "|    total_timesteps      | 653312        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1224929e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 7.15e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.71e+05      |\n",
      "|    n_updates            | 26190         |\n",
      "|    policy_gradient_loss | -6.21e-05     |\n",
      "|    reward               | -0.099902764  |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.74e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 431           |\n",
      "|    iterations           | 320           |\n",
      "|    time_elapsed         | 1519          |\n",
      "|    total_timesteps      | 655360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4774268e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 2.5e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.8e+06       |\n",
      "|    n_updates            | 26200         |\n",
      "|    policy_gradient_loss | 2.93e-05      |\n",
      "|    reward               | 108.42561     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 7.6e+06       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 431           |\n",
      "|    iterations           | 321           |\n",
      "|    time_elapsed         | 1524          |\n",
      "|    total_timesteps      | 657408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5850022e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -3.6e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.19e+05      |\n",
      "|    n_updates            | 26210         |\n",
      "|    policy_gradient_loss | -3.98e-06     |\n",
      "|    reward               | 253.61076     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 6.38e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 776\n",
      "row: 5250, episode: 776\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5515913.52\n",
      "total_reward: 4515913.52\n",
      "total_cost: 187741.21\n",
      "total_trades: 648\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 431          |\n",
      "|    iterations           | 322          |\n",
      "|    time_elapsed         | 1529         |\n",
      "|    total_timesteps      | 659456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.924739e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -3.34e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.55e+06     |\n",
      "|    n_updates            | 26220        |\n",
      "|    policy_gradient_loss | -7.4e-05     |\n",
      "|    reward               | 5.253831     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 9.1e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 323           |\n",
      "|    time_elapsed         | 1535          |\n",
      "|    total_timesteps      | 661504        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7957583e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 2.15e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.01e+07      |\n",
      "|    n_updates            | 26230         |\n",
      "|    policy_gradient_loss | -4.28e-05     |\n",
      "|    reward               | 266.1274      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.02e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 430          |\n",
      "|    iterations           | 324          |\n",
      "|    time_elapsed         | 1542         |\n",
      "|    total_timesteps      | 663552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.838512e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 4.59e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2e+06        |\n",
      "|    n_updates            | 26240        |\n",
      "|    policy_gradient_loss | 2.78e-06     |\n",
      "|    reward               | 972.0907     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 4.01e+06     |\n",
      "------------------------------------------\n",
      "Episode: 777\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 325           |\n",
      "|    time_elapsed         | 1546          |\n",
      "|    total_timesteps      | 665600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2282197e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 7.45e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.75e+07      |\n",
      "|    n_updates            | 26250         |\n",
      "|    policy_gradient_loss | -1.39e-06     |\n",
      "|    reward               | 141.81352     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.15e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 430           |\n",
      "|    iterations           | 326           |\n",
      "|    time_elapsed         | 1551          |\n",
      "|    total_timesteps      | 667648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6880222e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.13e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.3e+06       |\n",
      "|    n_updates            | 26260         |\n",
      "|    policy_gradient_loss | -5.9e-06      |\n",
      "|    reward               | 560.7535      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.86e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 778\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 429           |\n",
      "|    iterations           | 327           |\n",
      "|    time_elapsed         | 1557          |\n",
      "|    total_timesteps      | 669696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0340591e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -7.75e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.8e+07       |\n",
      "|    n_updates            | 26270         |\n",
      "|    policy_gradient_loss | -3.42e-06     |\n",
      "|    reward               | 18.903831     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 3.61e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 429           |\n",
      "|    iterations           | 328           |\n",
      "|    time_elapsed         | 1563          |\n",
      "|    total_timesteps      | 671744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7142156e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 7.75e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.36e+07      |\n",
      "|    n_updates            | 26280         |\n",
      "|    policy_gradient_loss | -4.29e-06     |\n",
      "|    reward               | 81.51111      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 8.73e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 429           |\n",
      "|    iterations           | 329           |\n",
      "|    time_elapsed         | 1569          |\n",
      "|    total_timesteps      | 673792        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7412898e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -9.06e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.31e+05      |\n",
      "|    n_updates            | 26290         |\n",
      "|    policy_gradient_loss | -2.98e-06     |\n",
      "|    reward               | 464.51248     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.06e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 779\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 429           |\n",
      "|    iterations           | 330           |\n",
      "|    time_elapsed         | 1575          |\n",
      "|    total_timesteps      | 675840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2893073e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 8.29e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.18e+07      |\n",
      "|    n_updates            | 26300         |\n",
      "|    policy_gradient_loss | -3.04e-06     |\n",
      "|    reward               | 81.75554      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.37e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 428           |\n",
      "|    iterations           | 331           |\n",
      "|    time_elapsed         | 1580          |\n",
      "|    total_timesteps      | 677888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8594513e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.37e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.26e+07      |\n",
      "|    n_updates            | 26310         |\n",
      "|    policy_gradient_loss | -1.25e-05     |\n",
      "|    reward               | 227.39134     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.53e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 780\n",
      "row: 5250, episode: 780\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8933840.25\n",
      "total_reward: 7933840.25\n",
      "total_cost: 220461.74\n",
      "total_trades: 747\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 428           |\n",
      "|    iterations           | 332           |\n",
      "|    time_elapsed         | 1587          |\n",
      "|    total_timesteps      | 679936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5104888e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.84e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.27e+06      |\n",
      "|    n_updates            | 26320         |\n",
      "|    policy_gradient_loss | 1.53e-06      |\n",
      "|    reward               | 7.747708      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.55e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 428           |\n",
      "|    iterations           | 333           |\n",
      "|    time_elapsed         | 1591          |\n",
      "|    total_timesteps      | 681984        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1236989e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.13e+07      |\n",
      "|    n_updates            | 26330         |\n",
      "|    policy_gradient_loss | 1.58e-06      |\n",
      "|    reward               | 187.7619      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.25e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 428          |\n",
      "|    iterations           | 334          |\n",
      "|    time_elapsed         | 1596         |\n",
      "|    total_timesteps      | 684032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.287699e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -8.7e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.34e+06     |\n",
      "|    n_updates            | 26340        |\n",
      "|    policy_gradient_loss | -1.7e-05     |\n",
      "|    reward               | 308.9373     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 2.68e+06     |\n",
      "------------------------------------------\n",
      "Episode: 781\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 428           |\n",
      "|    iterations           | 335           |\n",
      "|    time_elapsed         | 1601          |\n",
      "|    total_timesteps      | 686080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4319313e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 9.12e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.37e+06      |\n",
      "|    n_updates            | 26350         |\n",
      "|    policy_gradient_loss | 1.86e-05      |\n",
      "|    reward               | 30.840628     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.47e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 428         |\n",
      "|    iterations           | 336         |\n",
      "|    time_elapsed         | 1607        |\n",
      "|    total_timesteps      | 688128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.00545e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 1.25e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.34e+06    |\n",
      "|    n_updates            | 26360       |\n",
      "|    policy_gradient_loss | -1.78e-05   |\n",
      "|    reward               | 159.88661   |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 1.27e+07    |\n",
      "-----------------------------------------\n",
      "Episode: 782\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 427         |\n",
      "|    iterations           | 337         |\n",
      "|    time_elapsed         | 1613        |\n",
      "|    total_timesteps      | 690176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 3.70224e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | -3.47e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.75e+05    |\n",
      "|    n_updates            | 26370       |\n",
      "|    policy_gradient_loss | -2.36e-05   |\n",
      "|    reward               | -0.09999944 |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 1.55e+06    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 427           |\n",
      "|    iterations           | 338           |\n",
      "|    time_elapsed         | 1618          |\n",
      "|    total_timesteps      | 692224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3792305e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 2.62e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.61e+06      |\n",
      "|    n_updates            | 26380         |\n",
      "|    policy_gradient_loss | 6.93e-06      |\n",
      "|    reward               | 270.65268     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 9.22e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 427           |\n",
      "|    iterations           | 339           |\n",
      "|    time_elapsed         | 1624          |\n",
      "|    total_timesteps      | 694272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0197982e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -1.45e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.61e+06      |\n",
      "|    n_updates            | 26390         |\n",
      "|    policy_gradient_loss | -5.45e-07     |\n",
      "|    reward               | 433.48816     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 7.21e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 783\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 340           |\n",
      "|    time_elapsed         | 1631          |\n",
      "|    total_timesteps      | 696320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9208528e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -4.53e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.17e+07      |\n",
      "|    n_updates            | 26400         |\n",
      "|    policy_gradient_loss | -6.17e-07     |\n",
      "|    reward               | 19.811499     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.35e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 427           |\n",
      "|    iterations           | 341           |\n",
      "|    time_elapsed         | 1635          |\n",
      "|    total_timesteps      | 698368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.85e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.8e+07       |\n",
      "|    n_updates            | 26410         |\n",
      "|    policy_gradient_loss | -1.05e-07     |\n",
      "|    reward               | 136.1389      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 7.6e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 342           |\n",
      "|    time_elapsed         | 1640          |\n",
      "|    total_timesteps      | 700416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5861588e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -1.11e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.03e+06      |\n",
      "|    n_updates            | 26420         |\n",
      "|    policy_gradient_loss | -2.01e-05     |\n",
      "|    reward               | 345.22498     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.05e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 784\n",
      "row: 5250, episode: 784\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4452249.74\n",
      "total_reward: 3452249.74\n",
      "total_cost: 163560.63\n",
      "total_trades: 362\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 427           |\n",
      "|    iterations           | 343           |\n",
      "|    time_elapsed         | 1644          |\n",
      "|    total_timesteps      | 702464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.4418494e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1e+07         |\n",
      "|    n_updates            | 26430         |\n",
      "|    policy_gradient_loss | 2.93e-05      |\n",
      "|    reward               | 201.12288     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2e+07         |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 344           |\n",
      "|    time_elapsed         | 1650          |\n",
      "|    total_timesteps      | 704512        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4465844e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -1.53e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.09e+06      |\n",
      "|    n_updates            | 26440         |\n",
      "|    policy_gradient_loss | -2.51e-05     |\n",
      "|    reward               | 350.66376     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.18e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 785\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 345           |\n",
      "|    time_elapsed         | 1657          |\n",
      "|    total_timesteps      | 706560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7393322e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.85e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.99e+06      |\n",
      "|    n_updates            | 26450         |\n",
      "|    policy_gradient_loss | -9.32e-06     |\n",
      "|    reward               | -9.396354     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2e+07         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 426          |\n",
      "|    iterations           | 346          |\n",
      "|    time_elapsed         | 1661         |\n",
      "|    total_timesteps      | 708608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.697634e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 1.79e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.32e+07     |\n",
      "|    n_updates            | 26460        |\n",
      "|    policy_gradient_loss | 1.75e-08     |\n",
      "|    reward               | 199.51213    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 4.65e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 426          |\n",
      "|    iterations           | 347          |\n",
      "|    time_elapsed         | 1666         |\n",
      "|    total_timesteps      | 710656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.434252e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 3.28e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.36e+06     |\n",
      "|    n_updates            | 26470        |\n",
      "|    policy_gradient_loss | -7.58e-07    |\n",
      "|    reward               | 563.7445     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 4.71e+06     |\n",
      "------------------------------------------\n",
      "Episode: 786\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 348           |\n",
      "|    time_elapsed         | 1670          |\n",
      "|    total_timesteps      | 712704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9549043e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 7.75e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.21e+07      |\n",
      "|    n_updates            | 26480         |\n",
      "|    policy_gradient_loss | -9.14e-06     |\n",
      "|    reward               | 44.112934     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.41e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 349           |\n",
      "|    time_elapsed         | 1674          |\n",
      "|    total_timesteps      | 714752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9192673e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.37e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.91e+06      |\n",
      "|    n_updates            | 26490         |\n",
      "|    policy_gradient_loss | -4.58e-05     |\n",
      "|    reward               | 169.56535     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.18e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 787\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 350           |\n",
      "|    time_elapsed         | 1678          |\n",
      "|    total_timesteps      | 716800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2572273e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -7.03e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.27e+06      |\n",
      "|    n_updates            | 26500         |\n",
      "|    policy_gradient_loss | -7.54e-05     |\n",
      "|    reward               | 4.5246        |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.55e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 427          |\n",
      "|    iterations           | 351          |\n",
      "|    time_elapsed         | 1683         |\n",
      "|    total_timesteps      | 718848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.010871e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 5.36e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.7e+06      |\n",
      "|    n_updates            | 26510        |\n",
      "|    policy_gradient_loss | 3.95e-05     |\n",
      "|    reward               | 179.64178    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 9.41e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 352           |\n",
      "|    time_elapsed         | 1688          |\n",
      "|    total_timesteps      | 720896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.3853804e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -7.75e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.46e+06      |\n",
      "|    n_updates            | 26520         |\n",
      "|    policy_gradient_loss | -3.44e-06     |\n",
      "|    reward               | 427.20285     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.92e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 788\n",
      "row: 5250, episode: 788\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5998079.19\n",
      "total_reward: 4998079.19\n",
      "total_cost: 210524.83\n",
      "total_trades: 571\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 353           |\n",
      "|    time_elapsed         | 1693          |\n",
      "|    total_timesteps      | 722944        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4176628e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.32e+07      |\n",
      "|    n_updates            | 26530         |\n",
      "|    policy_gradient_loss | -4.28e-06     |\n",
      "|    reward               | 185.76236     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.64e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 426         |\n",
      "|    iterations           | 354         |\n",
      "|    time_elapsed         | 1698        |\n",
      "|    total_timesteps      | 724992      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 3.38041e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.64       |\n",
      "|    explained_variance   | 6.56e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.37e+06    |\n",
      "|    n_updates            | 26540       |\n",
      "|    policy_gradient_loss | -3.89e-06   |\n",
      "|    reward               | 266.52902   |\n",
      "|    std                  | 1.25        |\n",
      "|    value_loss           | 1.87e+07    |\n",
      "-----------------------------------------\n",
      "Episode: 789\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 355           |\n",
      "|    time_elapsed         | 1705          |\n",
      "|    total_timesteps      | 727040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.9168087e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.48e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.44e+06      |\n",
      "|    n_updates            | 26550         |\n",
      "|    policy_gradient_loss | -3.25e-06     |\n",
      "|    reward               | 37.129913     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.29e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 356           |\n",
      "|    time_elapsed         | 1709          |\n",
      "|    total_timesteps      | 729088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2148015e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.35e+07      |\n",
      "|    n_updates            | 26560         |\n",
      "|    policy_gradient_loss | -2.68e-06     |\n",
      "|    reward               | 198.5931      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.71e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 357           |\n",
      "|    time_elapsed         | 1714          |\n",
      "|    total_timesteps      | 731136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0603108e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -1.29e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.26e+06      |\n",
      "|    n_updates            | 26570         |\n",
      "|    policy_gradient_loss | -1.71e-05     |\n",
      "|    reward               | 657.9695      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.53e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 790\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 358           |\n",
      "|    time_elapsed         | 1720          |\n",
      "|    total_timesteps      | 733184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3728277e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 4.17e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.43e+07      |\n",
      "|    n_updates            | 26580         |\n",
      "|    policy_gradient_loss | -1.29e-05     |\n",
      "|    reward               | 69.00434      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.85e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 426           |\n",
      "|    iterations           | 359           |\n",
      "|    time_elapsed         | 1724          |\n",
      "|    total_timesteps      | 735232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4956458e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.05e+07      |\n",
      "|    n_updates            | 26590         |\n",
      "|    policy_gradient_loss | 2.05e-06      |\n",
      "|    reward               | 191.90744     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 6.1e+07       |\n",
      "-------------------------------------------\n",
      "Episode: 791\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 426          |\n",
      "|    iterations           | 360          |\n",
      "|    time_elapsed         | 1730         |\n",
      "|    total_timesteps      | 737280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.635643e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -1.1e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.79e+06     |\n",
      "|    n_updates            | 26600        |\n",
      "|    policy_gradient_loss | -5.95e-06    |\n",
      "|    reward               | -0.09993849  |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 5.59e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 426          |\n",
      "|    iterations           | 361          |\n",
      "|    time_elapsed         | 1734         |\n",
      "|    total_timesteps      | 739328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.524489e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.19e+07     |\n",
      "|    n_updates            | 26610        |\n",
      "|    policy_gradient_loss | -7.67e-06    |\n",
      "|    reward               | 179.03091    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 2.38e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 425           |\n",
      "|    iterations           | 362           |\n",
      "|    time_elapsed         | 1740          |\n",
      "|    total_timesteps      | 741376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1842577e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | -1.22e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.01e+06      |\n",
      "|    n_updates            | 26620         |\n",
      "|    policy_gradient_loss | -8.59e-06     |\n",
      "|    reward               | 344.2293      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.02e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 792\n",
      "row: 5250, episode: 792\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5249258.15\n",
      "total_reward: 4249258.15\n",
      "total_cost: 234597.95\n",
      "total_trades: 493\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 425           |\n",
      "|    iterations           | 363           |\n",
      "|    time_elapsed         | 1747          |\n",
      "|    total_timesteps      | 743424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8210267e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 1.49e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.58e+06      |\n",
      "|    n_updates            | 26630         |\n",
      "|    policy_gradient_loss | -7.57e-06     |\n",
      "|    reward               | 40.204605     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.72e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 425          |\n",
      "|    iterations           | 364          |\n",
      "|    time_elapsed         | 1752         |\n",
      "|    total_timesteps      | 745472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.930189e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 9.54e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.63e+07     |\n",
      "|    n_updates            | 26640        |\n",
      "|    policy_gradient_loss | -8.61e-07    |\n",
      "|    reward               | 194.6104     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 3.25e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 425          |\n",
      "|    iterations           | 365          |\n",
      "|    time_elapsed         | 1758         |\n",
      "|    total_timesteps      | 747520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.432835e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -5.96e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.08e+06     |\n",
      "|    n_updates            | 26650        |\n",
      "|    policy_gradient_loss | -1.15e-05    |\n",
      "|    reward               | 1938.3472    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 4.17e+06     |\n",
      "------------------------------------------\n",
      "Episode: 793\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 425           |\n",
      "|    iterations           | 366           |\n",
      "|    time_elapsed         | 1762          |\n",
      "|    total_timesteps      | 749568        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4289981e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 2.09e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.42e+07      |\n",
      "|    n_updates            | 26660         |\n",
      "|    policy_gradient_loss | 1.71e-05      |\n",
      "|    reward               | 18.884008     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 6.83e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 425          |\n",
      "|    iterations           | 367          |\n",
      "|    time_elapsed         | 1767         |\n",
      "|    total_timesteps      | 751616       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.005859e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 5.96e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.28e+07     |\n",
      "|    n_updates            | 26670        |\n",
      "|    policy_gradient_loss | -6.41e-07    |\n",
      "|    reward               | 123.926956   |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 8.56e+07     |\n",
      "------------------------------------------\n",
      "Episode: 794\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 424          |\n",
      "|    iterations           | 368          |\n",
      "|    time_elapsed         | 1774         |\n",
      "|    total_timesteps      | 753664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.641456e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -7.27e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.13e+06     |\n",
      "|    n_updates            | 26680        |\n",
      "|    policy_gradient_loss | -2.08e-06    |\n",
      "|    reward               | 25.017561    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 2.25e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 369           |\n",
      "|    time_elapsed         | 1779          |\n",
      "|    total_timesteps      | 755712        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3182594e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 7.75e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.76e+06      |\n",
      "|    n_updates            | 26690         |\n",
      "|    policy_gradient_loss | -5.91e-05     |\n",
      "|    reward               | 207.3645      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 3.52e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 424          |\n",
      "|    iterations           | 370          |\n",
      "|    time_elapsed         | 1785         |\n",
      "|    total_timesteps      | 757760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.080729e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -5.96e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.4e+06      |\n",
      "|    n_updates            | 26700        |\n",
      "|    policy_gradient_loss | -2.74e-05    |\n",
      "|    reward               | 671.54974    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 4.81e+06     |\n",
      "------------------------------------------\n",
      "Episode: 795\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 371           |\n",
      "|    time_elapsed         | 1789          |\n",
      "|    total_timesteps      | 759808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6744889e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 6.32e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.6e+07       |\n",
      "|    n_updates            | 26710         |\n",
      "|    policy_gradient_loss | -1.63e-05     |\n",
      "|    reward               | 156.324       |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 5.19e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 372           |\n",
      "|    time_elapsed         | 1794          |\n",
      "|    total_timesteps      | 761856        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6059722e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.9e+07       |\n",
      "|    n_updates            | 26720         |\n",
      "|    policy_gradient_loss | -2.72e-05     |\n",
      "|    reward               | 384.51895     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 5.8e+07       |\n",
      "-------------------------------------------\n",
      "Episode: 796\n",
      "row: 5250, episode: 796\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5975499.21\n",
      "total_reward: 4975499.21\n",
      "total_cost: 234508.62\n",
      "total_trades: 455\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 373           |\n",
      "|    time_elapsed         | 1799          |\n",
      "|    total_timesteps      | 763904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3550972e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 5.3e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.54e+06      |\n",
      "|    n_updates            | 26730         |\n",
      "|    policy_gradient_loss | -9.35e-06     |\n",
      "|    reward               | 51.99726      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.31e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 424          |\n",
      "|    iterations           | 374          |\n",
      "|    time_elapsed         | 1804         |\n",
      "|    total_timesteps      | 765952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.454058e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.96e+07     |\n",
      "|    n_updates            | 26740        |\n",
      "|    policy_gradient_loss | -1.07e-05    |\n",
      "|    reward               | 198.73616    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 3.91e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 424          |\n",
      "|    iterations           | 375          |\n",
      "|    time_elapsed         | 1810         |\n",
      "|    total_timesteps      | 768000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.656651e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | -8.58e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.11e+06     |\n",
      "|    n_updates            | 26750        |\n",
      "|    policy_gradient_loss | -3.02e-06    |\n",
      "|    reward               | 561.4286     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 6.22e+06     |\n",
      "------------------------------------------\n",
      "Episode: 797\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 424          |\n",
      "|    iterations           | 376          |\n",
      "|    time_elapsed         | 1814         |\n",
      "|    total_timesteps      | 770048       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.792229e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.64        |\n",
      "|    explained_variance   | 6.5e-06      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.56e+07     |\n",
      "|    n_updates            | 26760        |\n",
      "|    policy_gradient_loss | -5.04e-06    |\n",
      "|    reward               | 33.486336    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 3.11e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 377           |\n",
      "|    time_elapsed         | 1819          |\n",
      "|    total_timesteps      | 772096        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8999057e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.64         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.77e+07      |\n",
      "|    n_updates            | 26770         |\n",
      "|    policy_gradient_loss | -1.08e-05     |\n",
      "|    reward               | 207.22546     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 3.55e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 798\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 378           |\n",
      "|    time_elapsed         | 1824          |\n",
      "|    total_timesteps      | 774144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2849646e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -7.87e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.3e+06       |\n",
      "|    n_updates            | 26780         |\n",
      "|    policy_gradient_loss | -1.31e-05     |\n",
      "|    reward               | -0.09989936   |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.59e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 379           |\n",
      "|    time_elapsed         | 1829          |\n",
      "|    total_timesteps      | 776192        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3344397e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.45e+07      |\n",
      "|    n_updates            | 26790         |\n",
      "|    policy_gradient_loss | -1.76e-06     |\n",
      "|    reward               | 100.821594    |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.9e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 380           |\n",
      "|    time_elapsed         | 1834          |\n",
      "|    total_timesteps      | 778240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7648285e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -2.29e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.22e+05      |\n",
      "|    n_updates            | 26800         |\n",
      "|    policy_gradient_loss | -4.72e-05     |\n",
      "|    reward               | 237.35216     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.44e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 799\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 381           |\n",
      "|    time_elapsed         | 1840          |\n",
      "|    total_timesteps      | 780288        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6902126e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.13e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.34e+06      |\n",
      "|    n_updates            | 26810         |\n",
      "|    policy_gradient_loss | 4.69e-05      |\n",
      "|    reward               | 12.978628     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.87e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 382           |\n",
      "|    time_elapsed         | 1844          |\n",
      "|    total_timesteps      | 782336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.6630386e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.01e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.17e+07      |\n",
      "|    n_updates            | 26820         |\n",
      "|    policy_gradient_loss | -3.29e-06     |\n",
      "|    reward               | 119.933846    |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.34e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 383           |\n",
      "|    time_elapsed         | 1848          |\n",
      "|    total_timesteps      | 784384        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8882794e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.08e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.32e+06      |\n",
      "|    n_updates            | 26830         |\n",
      "|    policy_gradient_loss | -5.38e-05     |\n",
      "|    reward               | 764.339       |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.63e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 800\n",
      "row: 5250, episode: 800\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8643390.05\n",
      "total_reward: 7643390.05\n",
      "total_cost: 221357.22\n",
      "total_trades: 363\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 384           |\n",
      "|    time_elapsed         | 1853          |\n",
      "|    total_timesteps      | 786432        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5425473e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.03e+07      |\n",
      "|    n_updates            | 26840         |\n",
      "|    policy_gradient_loss | 1.22e-05      |\n",
      "|    reward               | 182.74866     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 4.06e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 424           |\n",
      "|    iterations           | 385           |\n",
      "|    time_elapsed         | 1859          |\n",
      "|    total_timesteps      | 788480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1900709e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -3.7e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.58e+06      |\n",
      "|    n_updates            | 26850         |\n",
      "|    policy_gradient_loss | -2.88e-06     |\n",
      "|    reward               | 558.48035     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 7.16e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 801\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 386          |\n",
      "|    time_elapsed         | 1864         |\n",
      "|    total_timesteps      | 790528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.802132e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 9.54e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.5e+07      |\n",
      "|    n_updates            | 26860        |\n",
      "|    policy_gradient_loss | 8.93e-07     |\n",
      "|    reward               | 85.512215    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 3e+07        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 387          |\n",
      "|    time_elapsed         | 1870         |\n",
      "|    total_timesteps      | 792576       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.41e+07     |\n",
      "|    n_updates            | 26870        |\n",
      "|    policy_gradient_loss | -4.27e-07    |\n",
      "|    reward               | 496.83688    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 8.82e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 388           |\n",
      "|    time_elapsed         | 1878          |\n",
      "|    total_timesteps      | 794624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1286618e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.79e+07      |\n",
      "|    n_updates            | 26880         |\n",
      "|    policy_gradient_loss | -2.12e-06     |\n",
      "|    reward               | 1559.705      |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 3.58e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 802\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 389          |\n",
      "|    time_elapsed         | 1884         |\n",
      "|    total_timesteps      | 796672       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.498885e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 3.76e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.52e+08     |\n",
      "|    n_updates            | 26890        |\n",
      "|    policy_gradient_loss | -2.29e-06    |\n",
      "|    reward               | 54.26594     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 3.04e+08     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 390          |\n",
      "|    time_elapsed         | 1891         |\n",
      "|    total_timesteps      | 798720       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.48e+07     |\n",
      "|    n_updates            | 26900        |\n",
      "|    policy_gradient_loss | -5.61e-08    |\n",
      "|    reward               | 155.58371    |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 1.1e+08      |\n",
      "------------------------------------------\n",
      "Episode: 803\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 391           |\n",
      "|    time_elapsed         | 1896          |\n",
      "|    total_timesteps      | 800768        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4993711e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 2.24e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.49e+06      |\n",
      "|    n_updates            | 26910         |\n",
      "|    policy_gradient_loss | -2.28e-05     |\n",
      "|    reward               | 7.8387694     |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 2.98e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 392           |\n",
      "|    time_elapsed         | 1904          |\n",
      "|    total_timesteps      | 802816        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.0315007e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.73e+06      |\n",
      "|    n_updates            | 26920         |\n",
      "|    policy_gradient_loss | 1.17e-05      |\n",
      "|    reward               | 224.618       |\n",
      "|    std                  | 1.25          |\n",
      "|    value_loss           | 1.55e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 393          |\n",
      "|    time_elapsed         | 1912         |\n",
      "|    total_timesteps      | 804864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.329754e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -5.96e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.09e+06     |\n",
      "|    n_updates            | 26930        |\n",
      "|    policy_gradient_loss | -2.13e-05    |\n",
      "|    reward               | 607.5248     |\n",
      "|    std                  | 1.25         |\n",
      "|    value_loss           | 4.18e+06     |\n",
      "------------------------------------------\n",
      "Episode: 804\n",
      "row: 5250, episode: 804\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7236064.27\n",
      "total_reward: 6236064.27\n",
      "total_cost: 261537.03\n",
      "total_trades: 826\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 394           |\n",
      "|    time_elapsed         | 1917          |\n",
      "|    total_timesteps      | 806912        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5120313e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 6.02e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.71e+07      |\n",
      "|    n_updates            | 26940         |\n",
      "|    policy_gradient_loss | -5.56e-06     |\n",
      "|    reward               | 23.664612     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 5.42e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 395          |\n",
      "|    time_elapsed         | 1922         |\n",
      "|    total_timesteps      | 808960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.095506e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 1.43e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.49e+07     |\n",
      "|    n_updates            | 26950        |\n",
      "|    policy_gradient_loss | -8.45e-06    |\n",
      "|    reward               | 203.45819    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 2.99e+07     |\n",
      "------------------------------------------\n",
      "Episode: 805\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 396           |\n",
      "|    time_elapsed         | 1927          |\n",
      "|    total_timesteps      | 811008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3359531e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -7.15e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.87e+06      |\n",
      "|    n_updates            | 26960         |\n",
      "|    policy_gradient_loss | -1.66e-05     |\n",
      "|    reward               | 9.180766      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.74e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 397           |\n",
      "|    time_elapsed         | 1931          |\n",
      "|    total_timesteps      | 813056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7083657e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 7.15e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.19e+06      |\n",
      "|    n_updates            | 26970         |\n",
      "|    policy_gradient_loss | -2.01e-05     |\n",
      "|    reward               | 151.63063     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.64e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 421         |\n",
      "|    iterations           | 398         |\n",
      "|    time_elapsed         | 1936        |\n",
      "|    total_timesteps      | 815104      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 5.60045e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 6.85e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.27e+05    |\n",
      "|    n_updates            | 26980       |\n",
      "|    policy_gradient_loss | -8.35e-06   |\n",
      "|    reward               | 306.79285   |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 1.05e+06    |\n",
      "-----------------------------------------\n",
      "Episode: 806\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 421         |\n",
      "|    iterations           | 399         |\n",
      "|    time_elapsed         | 1940        |\n",
      "|    total_timesteps      | 817152      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 3.38885e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 9.48e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.03e+06    |\n",
      "|    n_updates            | 26990       |\n",
      "|    policy_gradient_loss | -1.07e-05   |\n",
      "|    reward               | 257.17996   |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 1.01e+07    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 400           |\n",
      "|    time_elapsed         | 1943          |\n",
      "|    total_timesteps      | 819200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3416284e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.46e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.76e+06      |\n",
      "|    n_updates            | 27000         |\n",
      "|    policy_gradient_loss | -2.78e-05     |\n",
      "|    reward               | 226.14992     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.95e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 807\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 401           |\n",
      "|    time_elapsed         | 1947          |\n",
      "|    total_timesteps      | 821248        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7928542e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -5.01e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.04e+06      |\n",
      "|    n_updates            | 27010         |\n",
      "|    policy_gradient_loss | -2.16e-05     |\n",
      "|    reward               | -0.09990611   |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.01e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 421          |\n",
      "|    iterations           | 402          |\n",
      "|    time_elapsed         | 1951         |\n",
      "|    total_timesteps      | 823296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.442125e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 1.28e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.04e+07     |\n",
      "|    n_updates            | 27020        |\n",
      "|    policy_gradient_loss | 6.6e-07      |\n",
      "|    reward               | 317.71573    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 4.08e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 403           |\n",
      "|    time_elapsed         | 1956          |\n",
      "|    total_timesteps      | 825344        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2194886e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.06e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.84e+06      |\n",
      "|    n_updates            | 27030         |\n",
      "|    policy_gradient_loss | -5.96e-06     |\n",
      "|    reward               | 651.74023     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 7.69e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 808\n",
      "row: 5250, episode: 808\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 10054348.57\n",
      "total_reward: 9054348.57\n",
      "total_cost: 383308.26\n",
      "total_trades: 395\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 404           |\n",
      "|    time_elapsed         | 1960          |\n",
      "|    total_timesteps      | 827392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2092794e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -2.5e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.99e+07      |\n",
      "|    n_updates            | 27040         |\n",
      "|    policy_gradient_loss | -9.15e-06     |\n",
      "|    reward               | -14.752289    |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 5.98e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 405           |\n",
      "|    time_elapsed         | 1964          |\n",
      "|    total_timesteps      | 829440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3626566e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.31e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.28e+07      |\n",
      "|    n_updates            | 27050         |\n",
      "|    policy_gradient_loss | -1.49e-05     |\n",
      "|    reward               | 42.48917      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.06e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 406           |\n",
      "|    time_elapsed         | 1971          |\n",
      "|    total_timesteps      | 831488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.8330864e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.43e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.34e+05      |\n",
      "|    n_updates            | 27060         |\n",
      "|    policy_gradient_loss | -0.000218     |\n",
      "|    reward               | 207.09416     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.67e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 809\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 407          |\n",
      "|    time_elapsed         | 1974         |\n",
      "|    total_timesteps      | 833536       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.321671e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 2.41e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.38e+06     |\n",
      "|    n_updates            | 27070        |\n",
      "|    policy_gradient_loss | 3.68e-05     |\n",
      "|    reward               | 151.76468    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 2.75e+06     |\n",
      "------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 422            |\n",
      "|    iterations           | 408            |\n",
      "|    time_elapsed         | 1978           |\n",
      "|    total_timesteps      | 835584         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.10972906e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.65          |\n",
      "|    explained_variance   | -5.36e-06      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 1.16e+06       |\n",
      "|    n_updates            | 27080          |\n",
      "|    policy_gradient_loss | 2.32e-05       |\n",
      "|    reward               | 305.82385      |\n",
      "|    std                  | 1.26           |\n",
      "|    value_loss           | 2.32e+06       |\n",
      "--------------------------------------------\n",
      "Episode: 810\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 409           |\n",
      "|    time_elapsed         | 1982          |\n",
      "|    total_timesteps      | 837632        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9457747e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -7.63e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.8e+06       |\n",
      "|    n_updates            | 27090         |\n",
      "|    policy_gradient_loss | -4.96e-05     |\n",
      "|    reward               | 7.6119776     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.36e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 410           |\n",
      "|    time_elapsed         | 1986          |\n",
      "|    total_timesteps      | 839680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0637235e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.24e+07      |\n",
      "|    n_updates            | 27100         |\n",
      "|    policy_gradient_loss | -2.11e-05     |\n",
      "|    reward               | 53.738415     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 4.49e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 411           |\n",
      "|    time_elapsed         | 1990          |\n",
      "|    total_timesteps      | 841728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8518593e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 9.89e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.99e+05      |\n",
      "|    n_updates            | 27110         |\n",
      "|    policy_gradient_loss | -7.98e-05     |\n",
      "|    reward               | 176.12285     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.98e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 811\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 412           |\n",
      "|    time_elapsed         | 1994          |\n",
      "|    total_timesteps      | 843776        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1636674e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.42e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.92e+06      |\n",
      "|    n_updates            | 27120         |\n",
      "|    policy_gradient_loss | 8.06e-05      |\n",
      "|    reward               | 132.99078     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.85e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 413           |\n",
      "|    time_elapsed         | 1998          |\n",
      "|    total_timesteps      | 845824        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1705852e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 7.63e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.61e+06      |\n",
      "|    n_updates            | 27130         |\n",
      "|    policy_gradient_loss | -1.95e-05     |\n",
      "|    reward               | 618.23236     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.22e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 812\n",
      "row: 5250, episode: 812\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14442503.39\n",
      "total_reward: 13442503.39\n",
      "total_cost: 413335.79\n",
      "total_trades: 344\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 414          |\n",
      "|    time_elapsed         | 2002         |\n",
      "|    total_timesteps      | 847872       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.693567e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -7.99e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.43e+07     |\n",
      "|    n_updates            | 27140        |\n",
      "|    policy_gradient_loss | 1.17e-05     |\n",
      "|    reward               | -1.7894554   |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 2.86e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 415           |\n",
      "|    time_elapsed         | 2006          |\n",
      "|    total_timesteps      | 849920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4680048e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.67e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.12e+08      |\n",
      "|    n_updates            | 27150         |\n",
      "|    policy_gradient_loss | -2.17e-06     |\n",
      "|    reward               | 108.497       |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.23e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 416           |\n",
      "|    time_elapsed         | 2011          |\n",
      "|    total_timesteps      | 851968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1309166e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.07e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.62e+05      |\n",
      "|    n_updates            | 27160         |\n",
      "|    policy_gradient_loss | -1.39e-05     |\n",
      "|    reward               | 325.8844      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.12e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 813\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 417           |\n",
      "|    time_elapsed         | 2016          |\n",
      "|    total_timesteps      | 854016        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2225216e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 3.34e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.35e+06      |\n",
      "|    n_updates            | 27170         |\n",
      "|    policy_gradient_loss | -3.37e-05     |\n",
      "|    reward               | 43.553665     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 8.71e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 418           |\n",
      "|    time_elapsed         | 2021          |\n",
      "|    total_timesteps      | 856064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1774474e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.43e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.87e+06      |\n",
      "|    n_updates            | 27180         |\n",
      "|    policy_gradient_loss | -4.75e-05     |\n",
      "|    reward               | 295.67377     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.37e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 814\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 419           |\n",
      "|    time_elapsed         | 2027          |\n",
      "|    total_timesteps      | 858112        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1165407e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -9.06e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.06e+06      |\n",
      "|    n_updates            | 27190         |\n",
      "|    policy_gradient_loss | -5.45e-05     |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 8.13e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 420          |\n",
      "|    time_elapsed         | 2031         |\n",
      "|    total_timesteps      | 860160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.788896e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.81e+07     |\n",
      "|    n_updates            | 27200        |\n",
      "|    policy_gradient_loss | 2.06e-05     |\n",
      "|    reward               | 287.97632    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 1.16e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 421           |\n",
      "|    time_elapsed         | 2035          |\n",
      "|    total_timesteps      | 862208        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4381152e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -6.56e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.26e+06      |\n",
      "|    n_updates            | 27210         |\n",
      "|    policy_gradient_loss | -4.11e-05     |\n",
      "|    reward               | 725.75146     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 6.52e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 815\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 422          |\n",
      "|    time_elapsed         | 2040         |\n",
      "|    total_timesteps      | 864256       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.901996e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 5.36e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.93e+07     |\n",
      "|    n_updates            | 27220        |\n",
      "|    policy_gradient_loss | 4.28e-05     |\n",
      "|    reward               | 52.993355    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 5.85e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 423           |\n",
      "|    time_elapsed         | 2045          |\n",
      "|    total_timesteps      | 866304        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0972144e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.97e+07      |\n",
      "|    n_updates            | 27230         |\n",
      "|    policy_gradient_loss | -1.88e-06     |\n",
      "|    reward               | 884.52075     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.99e+08      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 423         |\n",
      "|    iterations           | 424         |\n",
      "|    time_elapsed         | 2051        |\n",
      "|    total_timesteps      | 868352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.95351e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 3.7e-06     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.59e+07    |\n",
      "|    n_updates            | 27240       |\n",
      "|    policy_gradient_loss | -4.29e-07   |\n",
      "|    reward               | 1826.4708   |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 7.18e+07    |\n",
      "-----------------------------------------\n",
      "Episode: 816\n",
      "row: 5250, episode: 816\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 19264724.58\n",
      "total_reward: 18264724.58\n",
      "total_cost: 859531.70\n",
      "total_trades: 704\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 425           |\n",
      "|    time_elapsed         | 2055          |\n",
      "|    total_timesteps      | 870400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 3.16e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.93e+08      |\n",
      "|    n_updates            | 27250         |\n",
      "|    policy_gradient_loss | -3.78e-07     |\n",
      "|    reward               | 249.35085     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 5.86e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 426          |\n",
      "|    time_elapsed         | 2060         |\n",
      "|    total_timesteps      | 872448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.431334e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 7.75e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.76e+07     |\n",
      "|    n_updates            | 27260        |\n",
      "|    policy_gradient_loss | -7.19e-06    |\n",
      "|    reward               | 622.8738     |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 5.52e+07     |\n",
      "------------------------------------------\n",
      "Episode: 817\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 427          |\n",
      "|    time_elapsed         | 2066         |\n",
      "|    total_timesteps      | 874496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.566682e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -1.91e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.37e+07     |\n",
      "|    n_updates            | 27270        |\n",
      "|    policy_gradient_loss | -1.04e-05    |\n",
      "|    reward               | 103.756996   |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 4.74e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 423           |\n",
      "|    iterations           | 428           |\n",
      "|    time_elapsed         | 2072          |\n",
      "|    total_timesteps      | 876544        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6298145e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 2.5e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.53e+07      |\n",
      "|    n_updates            | 27280         |\n",
      "|    policy_gradient_loss | 4.16e-06      |\n",
      "|    reward               | 669.53564     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 9.05e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 429          |\n",
      "|    time_elapsed         | 2077         |\n",
      "|    total_timesteps      | 878592       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.840005e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 2.44e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.12e+07     |\n",
      "|    n_updates            | 27290        |\n",
      "|    policy_gradient_loss | -3.27e-05    |\n",
      "|    reward               | 1369.821     |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 4.24e+07     |\n",
      "------------------------------------------\n",
      "Episode: 818\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 430          |\n",
      "|    time_elapsed         | 2081         |\n",
      "|    total_timesteps      | 880640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.024179e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 3.99e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.48e+08     |\n",
      "|    n_updates            | 27300        |\n",
      "|    policy_gradient_loss | 4.45e-06     |\n",
      "|    reward               | 44.17612     |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 2.96e+08     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 431          |\n",
      "|    time_elapsed         | 2086         |\n",
      "|    total_timesteps      | 882688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.892369e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 5.36e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.18e+07     |\n",
      "|    n_updates            | 27310        |\n",
      "|    policy_gradient_loss | -2.73e-05    |\n",
      "|    reward               | 343.02005    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 1.44e+08     |\n",
      "------------------------------------------\n",
      "Episode: 819\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 432           |\n",
      "|    time_elapsed         | 2093          |\n",
      "|    total_timesteps      | 884736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5669578e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 6.91e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.71e+06      |\n",
      "|    n_updates            | 27320         |\n",
      "|    policy_gradient_loss | 1.46e-06      |\n",
      "|    reward               | 7.8439145     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 5.43e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 433          |\n",
      "|    time_elapsed         | 2097         |\n",
      "|    total_timesteps      | 886784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.863817e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 1.67e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.42e+07     |\n",
      "|    n_updates            | 27330        |\n",
      "|    policy_gradient_loss | 4.92e-06     |\n",
      "|    reward               | 275.86536    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 2.85e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 434          |\n",
      "|    time_elapsed         | 2103         |\n",
      "|    total_timesteps      | 888832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.189504e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -3.81e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.1e+06      |\n",
      "|    n_updates            | 27340        |\n",
      "|    policy_gradient_loss | -1.13e-05    |\n",
      "|    reward               | 1016.97186   |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 6.2e+06      |\n",
      "------------------------------------------\n",
      "Episode: 820\n",
      "row: 5250, episode: 820\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11430679.49\n",
      "total_reward: 10430679.49\n",
      "total_cost: 457595.31\n",
      "total_trades: 308\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 435           |\n",
      "|    time_elapsed         | 2108          |\n",
      "|    total_timesteps      | 890880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4774268e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 3.4e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.3e+07       |\n",
      "|    n_updates            | 27350         |\n",
      "|    policy_gradient_loss | -7.33e-06     |\n",
      "|    reward               | 71.51434      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 8.6e+07       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 436          |\n",
      "|    time_elapsed         | 2113         |\n",
      "|    total_timesteps      | 892928       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.679329e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 5.36e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.6e+07      |\n",
      "|    n_updates            | 27360        |\n",
      "|    policy_gradient_loss | 4.91e-06     |\n",
      "|    reward               | 280.19574    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 9.2e+07      |\n",
      "------------------------------------------\n",
      "Episode: 821\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 422         |\n",
      "|    iterations           | 437         |\n",
      "|    time_elapsed         | 2120        |\n",
      "|    total_timesteps      | 894976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.88831e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | -1.01e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.97e+06    |\n",
      "|    n_updates            | 27370       |\n",
      "|    policy_gradient_loss | -2.77e-06   |\n",
      "|    reward               | -0.09992003 |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 7.94e+06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 438          |\n",
      "|    time_elapsed         | 2124         |\n",
      "|    total_timesteps      | 897024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.231697e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.47e+07     |\n",
      "|    n_updates            | 27380        |\n",
      "|    policy_gradient_loss | 2.69e-06     |\n",
      "|    reward               | 179.62973    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 1.09e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 439           |\n",
      "|    time_elapsed         | 2130          |\n",
      "|    total_timesteps      | 899072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0485237e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -9.66e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.1e+06       |\n",
      "|    n_updates            | 27390         |\n",
      "|    policy_gradient_loss | -1.56e-05     |\n",
      "|    reward               | 336.65994     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.21e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 822\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 440           |\n",
      "|    time_elapsed         | 2136          |\n",
      "|    total_timesteps      | 901120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.6202407e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 3.87e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.02e+07      |\n",
      "|    n_updates            | 27400         |\n",
      "|    policy_gradient_loss | 1.15e-05      |\n",
      "|    reward               | 61.617165     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.04e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 421          |\n",
      "|    iterations           | 441          |\n",
      "|    time_elapsed         | 2141         |\n",
      "|    total_timesteps      | 903168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.093775e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 8.34e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.84e+07     |\n",
      "|    n_updates            | 27410        |\n",
      "|    policy_gradient_loss | -2.74e-06    |\n",
      "|    reward               | 237.13803    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 5.69e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 442           |\n",
      "|    time_elapsed         | 2145          |\n",
      "|    total_timesteps      | 905216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4133125e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -2.09e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.56e+06      |\n",
      "|    n_updates            | 27420         |\n",
      "|    policy_gradient_loss | -2.68e-07     |\n",
      "|    reward               | 843.0624      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 7.13e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 823\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 443           |\n",
      "|    time_elapsed         | 2148          |\n",
      "|    total_timesteps      | 907264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.6915755e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 7.75e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.98e+07      |\n",
      "|    n_updates            | 27430         |\n",
      "|    policy_gradient_loss | -4.94e-06     |\n",
      "|    reward               | 97.28878      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 5.97e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 444           |\n",
      "|    time_elapsed         | 2153          |\n",
      "|    total_timesteps      | 909312        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6172434e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -5.13e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.03e+05      |\n",
      "|    n_updates            | 27440         |\n",
      "|    policy_gradient_loss | -7.98e-05     |\n",
      "|    reward               | 152.15628     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 8.07e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 824\n",
      "row: 5250, episode: 824\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5131671.48\n",
      "total_reward: 4131671.48\n",
      "total_cost: 169088.30\n",
      "total_trades: 364\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 445           |\n",
      "|    time_elapsed         | 2157          |\n",
      "|    total_timesteps      | 911360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2592314e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.79e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.92e+06      |\n",
      "|    n_updates            | 27450         |\n",
      "|    policy_gradient_loss | 4.26e-05      |\n",
      "|    reward               | 25.741177     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 5.85e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 446           |\n",
      "|    time_elapsed         | 2161          |\n",
      "|    total_timesteps      | 913408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.8257366e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.79e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.73e+06      |\n",
      "|    n_updates            | 27460         |\n",
      "|    policy_gradient_loss | 1.71e-05      |\n",
      "|    reward               | 148.53104     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.15e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 447           |\n",
      "|    time_elapsed         | 2167          |\n",
      "|    total_timesteps      | 915456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4700954e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -2.62e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.75e+06      |\n",
      "|    n_updates            | 27470         |\n",
      "|    policy_gradient_loss | -1.93e-05     |\n",
      "|    reward               | 329.77573     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.5e+06       |\n",
      "-------------------------------------------\n",
      "Episode: 825\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 448           |\n",
      "|    time_elapsed         | 2171          |\n",
      "|    total_timesteps      | 917504        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.9529127e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.31e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.35e+07      |\n",
      "|    n_updates            | 27480         |\n",
      "|    policy_gradient_loss | 1.08e-05      |\n",
      "|    reward               | 86.86437      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.7e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 449           |\n",
      "|    time_elapsed         | 2177          |\n",
      "|    total_timesteps      | 919552        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5348778e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 7.75e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.06e+06      |\n",
      "|    n_updates            | 27490         |\n",
      "|    policy_gradient_loss | -1.39e-05     |\n",
      "|    reward               | 184.40117     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 4.12e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 826\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 450           |\n",
      "|    time_elapsed         | 2184          |\n",
      "|    total_timesteps      | 921600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1647429e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -4.77e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.56e+06      |\n",
      "|    n_updates            | 27500         |\n",
      "|    policy_gradient_loss | 6.97e-06      |\n",
      "|    reward               | 7.100347      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.12e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 451          |\n",
      "|    time_elapsed         | 2188         |\n",
      "|    total_timesteps      | 923648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.143811e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 8.94e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.98e+06     |\n",
      "|    n_updates            | 27510        |\n",
      "|    policy_gradient_loss | -8.92e-06    |\n",
      "|    reward               | 61.786953    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 1.2e+07      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 452           |\n",
      "|    time_elapsed         | 2193          |\n",
      "|    total_timesteps      | 925696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4319465e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -2.86e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.52e+05      |\n",
      "|    n_updates            | 27520         |\n",
      "|    policy_gradient_loss | -7.5e-07      |\n",
      "|    reward               | 335.5885      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 5.05e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 827\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 453           |\n",
      "|    time_elapsed         | 2197          |\n",
      "|    total_timesteps      | 927744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3243989e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 7.27e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.09e+06      |\n",
      "|    n_updates            | 27530         |\n",
      "|    policy_gradient_loss | -2.72e-05     |\n",
      "|    reward               | 87.07468      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.02e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 454          |\n",
      "|    time_elapsed         | 2202         |\n",
      "|    total_timesteps      | 929792       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.922913e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 9.54e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.59e+06     |\n",
      "|    n_updates            | 27540        |\n",
      "|    policy_gradient_loss | 4.37e-06     |\n",
      "|    reward               | 472.17142    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 1.12e+07     |\n",
      "------------------------------------------\n",
      "Episode: 828\n",
      "row: 5250, episode: 828\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7874546.66\n",
      "total_reward: 6874546.66\n",
      "total_cost: 212658.61\n",
      "total_trades: 449\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 422           |\n",
      "|    iterations           | 455           |\n",
      "|    time_elapsed         | 2207          |\n",
      "|    total_timesteps      | 931840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3201802e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.67e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.37e+07      |\n",
      "|    n_updates            | 27550         |\n",
      "|    policy_gradient_loss | -4.51e-07     |\n",
      "|    reward               | 45.38851      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.75e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 422         |\n",
      "|    iterations           | 456         |\n",
      "|    time_elapsed         | 2211        |\n",
      "|    total_timesteps      | 933888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.76311e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.65       |\n",
      "|    explained_variance   | 4.77e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.23e+07    |\n",
      "|    n_updates            | 27560       |\n",
      "|    policy_gradient_loss | -8.08e-06   |\n",
      "|    reward               | 118.03905   |\n",
      "|    std                  | 1.26        |\n",
      "|    value_loss           | 6.45e+07    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 457           |\n",
      "|    time_elapsed         | 2220          |\n",
      "|    total_timesteps      | 935936        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7741847e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.53e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.94e+05      |\n",
      "|    n_updates            | 27570         |\n",
      "|    policy_gradient_loss | -6.29e-06     |\n",
      "|    reward               | 441.34955     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.79e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 829\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 458           |\n",
      "|    time_elapsed         | 2224          |\n",
      "|    total_timesteps      | 937984        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6437996e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 5.01e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.84e+06      |\n",
      "|    n_updates            | 27580         |\n",
      "|    policy_gradient_loss | -6.55e-06     |\n",
      "|    reward               | 43.68077      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.57e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 459           |\n",
      "|    time_elapsed         | 2229          |\n",
      "|    total_timesteps      | 940032        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6434933e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.79e+07      |\n",
      "|    n_updates            | 27590         |\n",
      "|    policy_gradient_loss | -2.61e-06     |\n",
      "|    reward               | 430.1735      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.58e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 830\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 460           |\n",
      "|    time_elapsed         | 2235          |\n",
      "|    total_timesteps      | 942080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8725557e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -4.53e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.54e+06      |\n",
      "|    n_updates            | 27600         |\n",
      "|    policy_gradient_loss | -9.05e-06     |\n",
      "|    reward               | -0.09994529   |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.51e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 461           |\n",
      "|    time_elapsed         | 2240          |\n",
      "|    total_timesteps      | 944128        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7127604e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.47e+07      |\n",
      "|    n_updates            | 27610         |\n",
      "|    policy_gradient_loss | 6.88e-06      |\n",
      "|    reward               | 90.1845       |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.69e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 462           |\n",
      "|    time_elapsed         | 2245          |\n",
      "|    total_timesteps      | 946176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9851723e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -2.75e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.75e+05      |\n",
      "|    n_updates            | 27620         |\n",
      "|    policy_gradient_loss | -7.38e-06     |\n",
      "|    reward               | 300.90402     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.5e+05       |\n",
      "-------------------------------------------\n",
      "Episode: 831\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 463           |\n",
      "|    time_elapsed         | 2250          |\n",
      "|    total_timesteps      | 948224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7331331e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.28e+06      |\n",
      "|    n_updates            | 27630         |\n",
      "|    policy_gradient_loss | 3.04e-08      |\n",
      "|    reward               | 73.73994      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 8.55e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 464           |\n",
      "|    time_elapsed         | 2255          |\n",
      "|    total_timesteps      | 950272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7706847e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.37e+07      |\n",
      "|    n_updates            | 27640         |\n",
      "|    policy_gradient_loss | 7.67e-07      |\n",
      "|    reward               | 462.75467     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 6.73e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 465           |\n",
      "|    time_elapsed         | 2261          |\n",
      "|    total_timesteps      | 952320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3416866e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 2.98e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.21e+07      |\n",
      "|    n_updates            | 27650         |\n",
      "|    policy_gradient_loss | -6.6e-06      |\n",
      "|    reward               | 1028.6536     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.43e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 832\n",
      "row: 5250, episode: 832\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11286526.10\n",
      "total_reward: 10286526.10\n",
      "total_cost: 437441.85\n",
      "total_trades: 592\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 466           |\n",
      "|    time_elapsed         | 2265          |\n",
      "|    total_timesteps      | 954368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9316365e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.83e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.37e+07      |\n",
      "|    n_updates            | 27660         |\n",
      "|    policy_gradient_loss | -1.86e-05     |\n",
      "|    reward               | 161.78249     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.27e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 421           |\n",
      "|    iterations           | 467           |\n",
      "|    time_elapsed         | 2271          |\n",
      "|    total_timesteps      | 956416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0361344e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.19e+07      |\n",
      "|    n_updates            | 27670         |\n",
      "|    policy_gradient_loss | -7.56e-07     |\n",
      "|    reward               | 413.24588     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.38e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 833\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 468           |\n",
      "|    time_elapsed         | 2277          |\n",
      "|    total_timesteps      | 958464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2194886e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.19e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.05e+07      |\n",
      "|    n_updates            | 27680         |\n",
      "|    policy_gradient_loss | -8.39e-07     |\n",
      "|    reward               | -5.829748     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.09e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 469           |\n",
      "|    time_elapsed         | 2281          |\n",
      "|    total_timesteps      | 960512        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.2771294e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.59e+07      |\n",
      "|    n_updates            | 27690         |\n",
      "|    policy_gradient_loss | -2.44e-06     |\n",
      "|    reward               | 213.35747     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 9.17e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 470          |\n",
      "|    time_elapsed         | 2287         |\n",
      "|    total_timesteps      | 962560       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.252644e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -2.03e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.73e+06     |\n",
      "|    n_updates            | 27700        |\n",
      "|    policy_gradient_loss | -6.46e-06    |\n",
      "|    reward               | 520.4229     |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 7.45e+06     |\n",
      "------------------------------------------\n",
      "Episode: 834\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 471           |\n",
      "|    time_elapsed         | 2292          |\n",
      "|    total_timesteps      | 964608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4258576e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 7.33e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2e+07         |\n",
      "|    n_updates            | 27710         |\n",
      "|    policy_gradient_loss | -2.43e-05     |\n",
      "|    reward               | 130.06346     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.99e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 472          |\n",
      "|    time_elapsed         | 2297         |\n",
      "|    total_timesteps      | 966656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.989568e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.37e+07     |\n",
      "|    n_updates            | 27720        |\n",
      "|    policy_gradient_loss | 2.23e-06     |\n",
      "|    reward               | 261.3629     |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 2.73e+07     |\n",
      "------------------------------------------\n",
      "Episode: 835\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 473           |\n",
      "|    time_elapsed         | 2303          |\n",
      "|    total_timesteps      | 968704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2663153e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -3.93e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.9e+06       |\n",
      "|    n_updates            | 27730         |\n",
      "|    policy_gradient_loss | -4.04e-06     |\n",
      "|    reward               | 52.800205     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 7.79e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 474           |\n",
      "|    time_elapsed         | 2306          |\n",
      "|    total_timesteps      | 970752        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1542579e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.83e+07      |\n",
      "|    n_updates            | 27740         |\n",
      "|    policy_gradient_loss | -6e-06        |\n",
      "|    reward               | 290.7935      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.66e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 475           |\n",
      "|    time_elapsed         | 2312          |\n",
      "|    total_timesteps      | 972800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.0833055e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.37e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.62e+06      |\n",
      "|    n_updates            | 27750         |\n",
      "|    policy_gradient_loss | -1.21e-05     |\n",
      "|    reward               | 626.8652      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 9.24e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 836\n",
      "row: 5250, episode: 836\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7707735.25\n",
      "total_reward: 6707735.25\n",
      "total_cost: 296293.75\n",
      "total_trades: 394\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 476          |\n",
      "|    time_elapsed         | 2316         |\n",
      "|    total_timesteps      | 974848       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.047186e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 1.79e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.45e+07     |\n",
      "|    n_updates            | 27760        |\n",
      "|    policy_gradient_loss | -1.02e-05    |\n",
      "|    reward               | 62.034225    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 4.89e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 477           |\n",
      "|    time_elapsed         | 2321          |\n",
      "|    total_timesteps      | 976896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.5655483e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.97e+07      |\n",
      "|    n_updates            | 27770         |\n",
      "|    policy_gradient_loss | -2.18e-05     |\n",
      "|    reward               | 390.78384     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.95e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 837\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 478           |\n",
      "|    time_elapsed         | 2327          |\n",
      "|    total_timesteps      | 978944        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7395588e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -4.89e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.47e+06      |\n",
      "|    n_updates            | 27780         |\n",
      "|    policy_gradient_loss | -9.02e-07     |\n",
      "|    reward               | -0.09990411   |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 1.69e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 479           |\n",
      "|    time_elapsed         | 2332          |\n",
      "|    total_timesteps      | 980992        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0297088e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.67e+08      |\n",
      "|    n_updates            | 27790         |\n",
      "|    policy_gradient_loss | 1.45e-06      |\n",
      "|    reward               | 164.5154      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 3.34e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 480           |\n",
      "|    time_elapsed         | 2339          |\n",
      "|    total_timesteps      | 983040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7031638e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -5.72e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.1e+06       |\n",
      "|    n_updates            | 27800         |\n",
      "|    policy_gradient_loss | -7.82e-06     |\n",
      "|    reward               | 421.32367     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 2.19e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 838\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 481          |\n",
      "|    time_elapsed         | 2344         |\n",
      "|    total_timesteps      | 985088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.615293e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 2.15e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.21e+07     |\n",
      "|    n_updates            | 27810        |\n",
      "|    policy_gradient_loss | -4.8e-06     |\n",
      "|    reward               | 28.40289     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.41e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 482          |\n",
      "|    time_elapsed         | 2348         |\n",
      "|    total_timesteps      | 987136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.082134e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 8.34e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.03e+07     |\n",
      "|    n_updates            | 27820        |\n",
      "|    policy_gradient_loss | 2.14e-06     |\n",
      "|    reward               | 294.18607    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 4.05e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 483          |\n",
      "|    time_elapsed         | 2355         |\n",
      "|    total_timesteps      | 989184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.221235e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -8.82e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.32e+06     |\n",
      "|    n_updates            | 27830        |\n",
      "|    policy_gradient_loss | -1.56e-05    |\n",
      "|    reward               | 772.0446     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 8.64e+06     |\n",
      "------------------------------------------\n",
      "Episode: 839\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 484          |\n",
      "|    time_elapsed         | 2358         |\n",
      "|    total_timesteps      | 991232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.152542e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.32e+07     |\n",
      "|    n_updates            | 27840        |\n",
      "|    policy_gradient_loss | 5.97e-06     |\n",
      "|    reward               | 193.66826    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 8.64e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 485          |\n",
      "|    time_elapsed         | 2364         |\n",
      "|    total_timesteps      | 993280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.398186e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -2.74e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.15e+06     |\n",
      "|    n_updates            | 27850        |\n",
      "|    policy_gradient_loss | -1.37e-05    |\n",
      "|    reward               | 599.0394     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 8.31e+06     |\n",
      "------------------------------------------\n",
      "Episode: 840\n",
      "row: 5250, episode: 840\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9080528.66\n",
      "total_reward: 8080528.66\n",
      "total_cost: 363011.69\n",
      "total_trades: 341\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 486           |\n",
      "|    time_elapsed         | 2369          |\n",
      "|    total_timesteps      | 995328        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6920676e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.13e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.73e+07      |\n",
      "|    n_updates            | 27860         |\n",
      "|    policy_gradient_loss | -3.04e-05     |\n",
      "|    reward               | 62.81478      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.46e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 420          |\n",
      "|    iterations           | 487          |\n",
      "|    time_elapsed         | 2374         |\n",
      "|    total_timesteps      | 997376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.980837e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.02e+07     |\n",
      "|    n_updates            | 27870        |\n",
      "|    policy_gradient_loss | -2.71e-06    |\n",
      "|    reward               | 249.68431    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 1e+08        |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 488           |\n",
      "|    time_elapsed         | 2382          |\n",
      "|    total_timesteps      | 999424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8230716e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.67e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.53e+06      |\n",
      "|    n_updates            | 27880         |\n",
      "|    policy_gradient_loss | -1.53e-06     |\n",
      "|    reward               | 1577.8745     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.11e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 841\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 489           |\n",
      "|    time_elapsed         | 2387          |\n",
      "|    total_timesteps      | 1001472       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.3056283e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.55e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.22e+08      |\n",
      "|    n_updates            | 27890         |\n",
      "|    policy_gradient_loss | -6.14e-07     |\n",
      "|    reward               | 97.29874      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.44e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 490           |\n",
      "|    time_elapsed         | 2392          |\n",
      "|    total_timesteps      | 1003520       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5861588e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6e+07         |\n",
      "|    n_updates            | 27900         |\n",
      "|    policy_gradient_loss | -9.64e-07     |\n",
      "|    reward               | 544.7989      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.2e+08       |\n",
      "-------------------------------------------\n",
      "Episode: 842\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 491           |\n",
      "|    time_elapsed         | 2397          |\n",
      "|    total_timesteps      | 1005568       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7811925e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.23e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.26e+06      |\n",
      "|    n_updates            | 27910         |\n",
      "|    policy_gradient_loss | -1.04e-06     |\n",
      "|    reward               | 2.900613      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.85e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 492           |\n",
      "|    time_elapsed         | 2401          |\n",
      "|    total_timesteps      | 1007616       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1350494e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.84e+07      |\n",
      "|    n_updates            | 27920         |\n",
      "|    policy_gradient_loss | 2.95e-07      |\n",
      "|    reward               | 82.687904     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 9.67e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 493           |\n",
      "|    time_elapsed         | 2406          |\n",
      "|    total_timesteps      | 1009664       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1555246e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -5.6e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.01e+05      |\n",
      "|    n_updates            | 27930         |\n",
      "|    policy_gradient_loss | -1.75e-05     |\n",
      "|    reward               | 252.79906     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 6.03e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 843\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 494          |\n",
      "|    time_elapsed         | 2411         |\n",
      "|    total_timesteps      | 1011712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.545015e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 9.72e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.08e+06     |\n",
      "|    n_updates            | 27940        |\n",
      "|    policy_gradient_loss | -5.54e-05    |\n",
      "|    reward               | 19.362354    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 8.16e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 495           |\n",
      "|    time_elapsed         | 2417          |\n",
      "|    total_timesteps      | 1013760       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.6729415e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 8.94e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.09e+06      |\n",
      "|    n_updates            | 27950         |\n",
      "|    policy_gradient_loss | 8.89e-06      |\n",
      "|    reward               | 123.47343     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 6.19e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 844\n",
      "row: 5250, episode: 844\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3008972.81\n",
      "total_reward: 2008972.81\n",
      "total_cost: 142324.31\n",
      "total_trades: 794\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 496          |\n",
      "|    time_elapsed         | 2423         |\n",
      "|    total_timesteps      | 1015808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.009111e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -2.86e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.62e+05     |\n",
      "|    n_updates            | 27960        |\n",
      "|    policy_gradient_loss | -3.07e-05    |\n",
      "|    reward               | 50.63322     |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 1.32e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 497          |\n",
      "|    time_elapsed         | 2427         |\n",
      "|    total_timesteps      | 1017856      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.611764e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 9.54e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.91e+06     |\n",
      "|    n_updates            | 27970        |\n",
      "|    policy_gradient_loss | -3.27e-05    |\n",
      "|    reward               | 289.18542    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 5.81e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 498          |\n",
      "|    time_elapsed         | 2432         |\n",
      "|    total_timesteps      | 1019904      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.715433e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -3.22e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.64e+06     |\n",
      "|    n_updates            | 27980        |\n",
      "|    policy_gradient_loss | 7.83e-06     |\n",
      "|    reward               | 687.5707     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 9.27e+06     |\n",
      "------------------------------------------\n",
      "Episode: 845\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 499           |\n",
      "|    time_elapsed         | 2438          |\n",
      "|    total_timesteps      | 1021952       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5925844e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.71e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.07e+07      |\n",
      "|    n_updates            | 27990         |\n",
      "|    policy_gradient_loss | -2.02e-06     |\n",
      "|    reward               | 215.58904     |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 6.14e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 500          |\n",
      "|    time_elapsed         | 2444         |\n",
      "|    total_timesteps      | 1024000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.788147e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.08e+08     |\n",
      "|    n_updates            | 28000        |\n",
      "|    policy_gradient_loss | -2.63e-06    |\n",
      "|    reward               | 649.13184    |\n",
      "|    std                  | 1.26         |\n",
      "|    value_loss           | 2.16e+08     |\n",
      "------------------------------------------\n",
      "Episode: 846\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 501           |\n",
      "|    time_elapsed         | 2453          |\n",
      "|    total_timesteps      | 1026048       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6088674e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -8.34e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.19e+07      |\n",
      "|    n_updates            | 28010         |\n",
      "|    policy_gradient_loss | -1.16e-05     |\n",
      "|    reward               | -0.099899754  |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 4.39e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 502           |\n",
      "|    time_elapsed         | 2458          |\n",
      "|    total_timesteps      | 1028096       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4706242e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.29e+08      |\n",
      "|    n_updates            | 28020         |\n",
      "|    policy_gradient_loss | -6.62e-06     |\n",
      "|    reward               | 82.60703      |\n",
      "|    std                  | 1.26          |\n",
      "|    value_loss           | 4.58e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 503           |\n",
      "|    time_elapsed         | 2465          |\n",
      "|    total_timesteps      | 1030144       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2444735e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -3.79e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.97e+05      |\n",
      "|    n_updates            | 28030         |\n",
      "|    policy_gradient_loss | -2.35e-05     |\n",
      "|    reward               | 330.22366     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.95e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 847\n",
      "Episode: 848\n",
      "row: 655, episode: 848\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 697572.33\n",
      "total_reward: -302427.67\n",
      "total_cost: 8083.91\n",
      "total_trades: 14\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 504           |\n",
      "|    time_elapsed         | 2470          |\n",
      "|    total_timesteps      | 1032192       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.1804236e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -1.43e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.35e+06      |\n",
      "|    n_updates            | 28040         |\n",
      "|    policy_gradient_loss | -1.32e-05     |\n",
      "|    reward               | 25.474892     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.27e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 505           |\n",
      "|    time_elapsed         | 2475          |\n",
      "|    total_timesteps      | 1034240       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0914013e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.58e+07      |\n",
      "|    n_updates            | 28050         |\n",
      "|    policy_gradient_loss | 1.37e-05      |\n",
      "|    reward               | 119.540115    |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.16e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 506          |\n",
      "|    time_elapsed         | 2480         |\n",
      "|    total_timesteps      | 1036288      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.814394e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.65        |\n",
      "|    explained_variance   | -8.7e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.48e+05     |\n",
      "|    n_updates            | 28060        |\n",
      "|    policy_gradient_loss | -1.01e-05    |\n",
      "|    reward               | 359.08005    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 1.3e+06      |\n",
      "------------------------------------------\n",
      "Episode: 849\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 507           |\n",
      "|    time_elapsed         | 2485          |\n",
      "|    total_timesteps      | 1038336       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3933459e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 4.65e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.12e+07      |\n",
      "|    n_updates            | 28070         |\n",
      "|    policy_gradient_loss | -1.45e-05     |\n",
      "|    reward               | 48.358006     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.24e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 508           |\n",
      "|    time_elapsed         | 2489          |\n",
      "|    total_timesteps      | 1040384       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.1153045e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.58e+07      |\n",
      "|    n_updates            | 28080         |\n",
      "|    policy_gradient_loss | -9.26e-06     |\n",
      "|    reward               | 129.6007      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.16e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 850\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 509           |\n",
      "|    time_elapsed         | 2495          |\n",
      "|    total_timesteps      | 1042432       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7136412e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.65         |\n",
      "|    explained_variance   | -5.08e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.09e+06      |\n",
      "|    n_updates            | 28090         |\n",
      "|    policy_gradient_loss | -4.39e-06     |\n",
      "|    reward               | -0.09225461   |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.18e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 510           |\n",
      "|    time_elapsed         | 2500          |\n",
      "|    total_timesteps      | 1044480       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.3938434e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.22e+06      |\n",
      "|    n_updates            | 28100         |\n",
      "|    policy_gradient_loss | -2.51e-05     |\n",
      "|    reward               | 138.4967      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.24e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 417         |\n",
      "|    iterations           | 511         |\n",
      "|    time_elapsed         | 2507        |\n",
      "|    total_timesteps      | 1046528     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.81987e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | -2.49e-05   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.08e+05    |\n",
      "|    n_updates            | 28110       |\n",
      "|    policy_gradient_loss | 4.83e-07    |\n",
      "|    reward               | 331.9511    |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 8.17e+05    |\n",
      "-----------------------------------------\n",
      "Episode: 851\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 512           |\n",
      "|    time_elapsed         | 2513          |\n",
      "|    total_timesteps      | 1048576       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9569492e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.74e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.98e+06      |\n",
      "|    n_updates            | 28120         |\n",
      "|    policy_gradient_loss | -7.3e-07      |\n",
      "|    reward               | 2.3078504     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.6e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 513           |\n",
      "|    time_elapsed         | 2517          |\n",
      "|    total_timesteps      | 1050624       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9959407e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.09e+07      |\n",
      "|    n_updates            | 28130         |\n",
      "|    policy_gradient_loss | -5.37e-06     |\n",
      "|    reward               | 302.8957      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.18e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 514           |\n",
      "|    time_elapsed         | 2523          |\n",
      "|    total_timesteps      | 1052672       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3981556e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.86e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.11e+06      |\n",
      "|    n_updates            | 28140         |\n",
      "|    policy_gradient_loss | 2.77e-07      |\n",
      "|    reward               | 1285.9761     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 8.21e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 852\n",
      "row: 5250, episode: 852\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 13844643.01\n",
      "total_reward: 12844643.01\n",
      "total_cost: 312257.89\n",
      "total_trades: 478\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 515           |\n",
      "|    time_elapsed         | 2527          |\n",
      "|    total_timesteps      | 1054720       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.44e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.31e+07      |\n",
      "|    n_updates            | 28150         |\n",
      "|    policy_gradient_loss | -1.79e-08     |\n",
      "|    reward               | 166.34177     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 8.62e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 516           |\n",
      "|    time_elapsed         | 2531          |\n",
      "|    total_timesteps      | 1056768       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3582758e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.67e+07      |\n",
      "|    n_updates            | 28160         |\n",
      "|    policy_gradient_loss | -4.64e-06     |\n",
      "|    reward               | 233.69331     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 5.34e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 853\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 517          |\n",
      "|    time_elapsed         | 2537         |\n",
      "|    total_timesteps      | 1058816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.026806e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 1.65e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.6e+06      |\n",
      "|    n_updates            | 28170        |\n",
      "|    policy_gradient_loss | -6.03e-06    |\n",
      "|    reward               | 25.170877    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 9.2e+06      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 518           |\n",
      "|    time_elapsed         | 2541          |\n",
      "|    total_timesteps      | 1060864       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2779568e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.96e+07      |\n",
      "|    n_updates            | 28180         |\n",
      "|    policy_gradient_loss | -1.98e-06     |\n",
      "|    reward               | 75.02548      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.92e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 519          |\n",
      "|    time_elapsed         | 2547         |\n",
      "|    total_timesteps      | 1062912      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.726813e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -8.7e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.42e+05     |\n",
      "|    n_updates            | 28190        |\n",
      "|    policy_gradient_loss | -6.86e-05    |\n",
      "|    reward               | 297.8148     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 1.08e+06     |\n",
      "------------------------------------------\n",
      "Episode: 854\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 520          |\n",
      "|    time_elapsed         | 2551         |\n",
      "|    total_timesteps      | 1064960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.517985e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 4.59e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.96e+06     |\n",
      "|    n_updates            | 28200        |\n",
      "|    policy_gradient_loss | 4.58e-05     |\n",
      "|    reward               | 59.255257    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 9.92e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 521           |\n",
      "|    time_elapsed         | 2558          |\n",
      "|    total_timesteps      | 1067008       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0827498e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.87e+06      |\n",
      "|    n_updates            | 28210         |\n",
      "|    policy_gradient_loss | -1.94e-05     |\n",
      "|    reward               | 332.61865     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 9.74e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 855\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 416           |\n",
      "|    iterations           | 522           |\n",
      "|    time_elapsed         | 2565          |\n",
      "|    total_timesteps      | 1069056       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6593625e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 5.96e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.13e+06      |\n",
      "|    n_updates            | 28220         |\n",
      "|    policy_gradient_loss | -7.66e-06     |\n",
      "|    reward               | 52.988426     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.03e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 416           |\n",
      "|    iterations           | 523           |\n",
      "|    time_elapsed         | 2569          |\n",
      "|    total_timesteps      | 1071104       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4913183e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.96e+07      |\n",
      "|    n_updates            | 28230         |\n",
      "|    policy_gradient_loss | -7.88e-06     |\n",
      "|    reward               | 292.6487      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.91e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 416           |\n",
      "|    iterations           | 524           |\n",
      "|    time_elapsed         | 2573          |\n",
      "|    total_timesteps      | 1073152       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0267984e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.67e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.66e+06      |\n",
      "|    n_updates            | 28240         |\n",
      "|    policy_gradient_loss | 1.8e-06       |\n",
      "|    reward               | 759.7822      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.13e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 856\n",
      "row: 5250, episode: 856\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 11637630.97\n",
      "total_reward: 10637630.97\n",
      "total_cost: 343663.42\n",
      "total_trades: 169\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 525           |\n",
      "|    time_elapsed         | 2577          |\n",
      "|    total_timesteps      | 1075200       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.4703864e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.53e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.69e+07      |\n",
      "|    n_updates            | 28250         |\n",
      "|    policy_gradient_loss | -4.07e-06     |\n",
      "|    reward               | 134.38261     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 7.37e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 526          |\n",
      "|    time_elapsed         | 2582         |\n",
      "|    total_timesteps      | 1077248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.479684e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.33e+07     |\n",
      "|    n_updates            | 28260        |\n",
      "|    policy_gradient_loss | -5.6e-08     |\n",
      "|    reward               | 447.91373    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 8.66e+07     |\n",
      "------------------------------------------\n",
      "Episode: 857\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 416           |\n",
      "|    iterations           | 527           |\n",
      "|    time_elapsed         | 2588          |\n",
      "|    total_timesteps      | 1079296       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9467552e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -2.86e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.12e+07      |\n",
      "|    n_updates            | 28270         |\n",
      "|    policy_gradient_loss | -6.52e-06     |\n",
      "|    reward               | -0.09990225   |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.24e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 528           |\n",
      "|    time_elapsed         | 2592          |\n",
      "|    total_timesteps      | 1081344       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6437843e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6e+07         |\n",
      "|    n_updates            | 28280         |\n",
      "|    policy_gradient_loss | 6.68e-07      |\n",
      "|    reward               | 53.6008       |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.2e+08       |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 529          |\n",
      "|    time_elapsed         | 2597         |\n",
      "|    total_timesteps      | 1083392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.528894e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -3.04e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.54e+05     |\n",
      "|    n_updates            | 28290        |\n",
      "|    policy_gradient_loss | -3.73e-05    |\n",
      "|    reward               | 147.07967    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 3.07e+05     |\n",
      "------------------------------------------\n",
      "Episode: 858\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 530          |\n",
      "|    time_elapsed         | 2602         |\n",
      "|    total_timesteps      | 1085440      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.578624e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -1.67e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.67e+06     |\n",
      "|    n_updates            | 28300        |\n",
      "|    policy_gradient_loss | 3.16e-05     |\n",
      "|    reward               | -4.4888234   |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 3.34e+06     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 531           |\n",
      "|    time_elapsed         | 2607          |\n",
      "|    total_timesteps      | 1087488       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5759876e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.77e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.81e+06      |\n",
      "|    n_updates            | 28310         |\n",
      "|    policy_gradient_loss | 4.37e-06      |\n",
      "|    reward               | 153.98308     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.56e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 532           |\n",
      "|    time_elapsed         | 2612          |\n",
      "|    total_timesteps      | 1089536       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6507693e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 3.52e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.68e+05      |\n",
      "|    n_updates            | 28320         |\n",
      "|    policy_gradient_loss | -1.11e-05     |\n",
      "|    reward               | 473.7803      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.34e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 859\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 533          |\n",
      "|    time_elapsed         | 2615         |\n",
      "|    total_timesteps      | 1091584      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.995389e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 5.36e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.04e+07     |\n",
      "|    n_updates            | 28330        |\n",
      "|    policy_gradient_loss | -1.85e-07    |\n",
      "|    reward               | 287.5743     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.08e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 534          |\n",
      "|    time_elapsed         | 2619         |\n",
      "|    total_timesteps      | 1093632      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.028575e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -4.53e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.04e+06     |\n",
      "|    n_updates            | 28340        |\n",
      "|    policy_gradient_loss | -2.91e-06    |\n",
      "|    reward               | 1153.8654    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 1.21e+07     |\n",
      "------------------------------------------\n",
      "Episode: 860\n",
      "row: 5250, episode: 860\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8034852.40\n",
      "total_reward: 7034852.40\n",
      "total_cost: 362805.16\n",
      "total_trades: 245\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 535           |\n",
      "|    time_elapsed         | 2623          |\n",
      "|    total_timesteps      | 1095680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7648639e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.64e+07      |\n",
      "|    n_updates            | 28350         |\n",
      "|    policy_gradient_loss | 2.43e-06      |\n",
      "|    reward               | -3.275456     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 7.29e+07      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 417       |\n",
      "|    iterations           | 536       |\n",
      "|    time_elapsed         | 2627      |\n",
      "|    total_timesteps      | 1097728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.1       |\n",
      "|    entropy_loss         | -1.66     |\n",
      "|    explained_variance   | 2.98e-07  |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 6.6e+07   |\n",
      "|    n_updates            | 28360     |\n",
      "|    policy_gradient_loss | -2.47e-07 |\n",
      "|    reward               | 67.21016  |\n",
      "|    std                  | 1.27      |\n",
      "|    value_loss           | 1.32e+08  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 537           |\n",
      "|    time_elapsed         | 2632          |\n",
      "|    total_timesteps      | 1099776       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2051314e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.34e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.01e+05      |\n",
      "|    n_updates            | 28370         |\n",
      "|    policy_gradient_loss | -9.93e-06     |\n",
      "|    reward               | 149.12317     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.01e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 861\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 538          |\n",
      "|    time_elapsed         | 2636         |\n",
      "|    total_timesteps      | 1101824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.161019e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 1.68e-05     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.8e+06      |\n",
      "|    n_updates            | 28380        |\n",
      "|    policy_gradient_loss | -5.75e-05    |\n",
      "|    reward               | 176.03484    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 3.6e+06      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 417         |\n",
      "|    iterations           | 539         |\n",
      "|    time_elapsed         | 2642        |\n",
      "|    total_timesteps      | 1103872     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.87928e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 3.28e-06    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.13e+06    |\n",
      "|    n_updates            | 28390       |\n",
      "|    policy_gradient_loss | -3.99e-05   |\n",
      "|    reward               | 656.5724    |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 2.26e+06    |\n",
      "-----------------------------------------\n",
      "Episode: 862\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 540           |\n",
      "|    time_elapsed         | 2648          |\n",
      "|    total_timesteps      | 1105920       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9533985e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -4.77e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.03e+07      |\n",
      "|    n_updates            | 28400         |\n",
      "|    policy_gradient_loss | -1.07e-05     |\n",
      "|    reward               | 32.38556      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.06e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 541           |\n",
      "|    time_elapsed         | 2653          |\n",
      "|    total_timesteps      | 1107968       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6307257e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.09e+08      |\n",
      "|    n_updates            | 28410         |\n",
      "|    policy_gradient_loss | -1.99e-05     |\n",
      "|    reward               | 182.11736     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.18e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 542          |\n",
      "|    time_elapsed         | 2659         |\n",
      "|    total_timesteps      | 1110016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.662472e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -5.48e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.79e+06     |\n",
      "|    n_updates            | 28420        |\n",
      "|    policy_gradient_loss | -3.92e-07    |\n",
      "|    reward               | 504.63733    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 3.59e+06     |\n",
      "------------------------------------------\n",
      "Episode: 863\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 543           |\n",
      "|    time_elapsed         | 2663          |\n",
      "|    total_timesteps      | 1112064       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1490345e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.55e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.3e+07       |\n",
      "|    n_updates            | 28430         |\n",
      "|    policy_gradient_loss | -6.41e-06     |\n",
      "|    reward               | 269.41754     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.59e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 544          |\n",
      "|    time_elapsed         | 2667         |\n",
      "|    total_timesteps      | 1114112      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.990135e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.86e+07     |\n",
      "|    n_updates            | 28440        |\n",
      "|    policy_gradient_loss | -1.46e-06    |\n",
      "|    reward               | 441.66605    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 3.72e+07     |\n",
      "------------------------------------------\n",
      "Episode: 864\n",
      "row: 5250, episode: 864\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8646287.93\n",
      "total_reward: 7646287.93\n",
      "total_cost: 319162.97\n",
      "total_trades: 290\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 545          |\n",
      "|    time_elapsed         | 2671         |\n",
      "|    total_timesteps      | 1116160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.428745e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -1.04e-05    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.12e+07     |\n",
      "|    n_updates            | 28450        |\n",
      "|    policy_gradient_loss | -1.71e-05    |\n",
      "|    reward               | -0.099909104 |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.25e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 546           |\n",
      "|    time_elapsed         | 2674          |\n",
      "|    total_timesteps      | 1118208       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6827987e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.19e+07      |\n",
      "|    n_updates            | 28460         |\n",
      "|    policy_gradient_loss | -4.02e-07     |\n",
      "|    reward               | 122.83471     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 8.38e+07      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 418            |\n",
      "|    iterations           | 547            |\n",
      "|    time_elapsed         | 2678           |\n",
      "|    total_timesteps      | 1120256        |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.13475835e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.66          |\n",
      "|    explained_variance   | -1.26e-05      |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 7.01e+05       |\n",
      "|    n_updates            | 28470          |\n",
      "|    policy_gradient_loss | -8.83e-06      |\n",
      "|    reward               | 494.64404      |\n",
      "|    std                  | 1.27           |\n",
      "|    value_loss           | 1.4e+06        |\n",
      "--------------------------------------------\n",
      "Episode: 865\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 548           |\n",
      "|    time_elapsed         | 2683          |\n",
      "|    total_timesteps      | 1122304       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4581019e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.32e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.42e+07      |\n",
      "|    n_updates            | 28480         |\n",
      "|    policy_gradient_loss | -3.93e-06     |\n",
      "|    reward               | 29.24         |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.84e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 549           |\n",
      "|    time_elapsed         | 2687          |\n",
      "|    total_timesteps      | 1124352       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6068225e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.05e+07      |\n",
      "|    n_updates            | 28490         |\n",
      "|    policy_gradient_loss | -6.56e-06     |\n",
      "|    reward               | 218.93571     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 6.1e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 550           |\n",
      "|    time_elapsed         | 2694          |\n",
      "|    total_timesteps      | 1126400       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6498961e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.45e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.43e+06      |\n",
      "|    n_updates            | 28500         |\n",
      "|    policy_gradient_loss | -4.65e-06     |\n",
      "|    reward               | 435.0581      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.86e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 866\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 551           |\n",
      "|    time_elapsed         | 2698          |\n",
      "|    total_timesteps      | 1128448       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8658466e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.28e+07      |\n",
      "|    n_updates            | 28510         |\n",
      "|    policy_gradient_loss | 7.58e-06      |\n",
      "|    reward               | 79.700294     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.57e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 552           |\n",
      "|    time_elapsed         | 2702          |\n",
      "|    total_timesteps      | 1130496       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012797097 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -8.7e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.87e+05      |\n",
      "|    n_updates            | 28520         |\n",
      "|    policy_gradient_loss | -0.000273     |\n",
      "|    reward               | 275.3481      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 5.74e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 867\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 553           |\n",
      "|    time_elapsed         | 2708          |\n",
      "|    total_timesteps      | 1132544       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012745944 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.01e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.23e+06      |\n",
      "|    n_updates            | 28530         |\n",
      "|    policy_gradient_loss | -2.3e-05      |\n",
      "|    reward               | 1.8004102     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.05e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 554           |\n",
      "|    time_elapsed         | 2711          |\n",
      "|    total_timesteps      | 1134592       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2876088e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.5e+07       |\n",
      "|    n_updates            | 28540         |\n",
      "|    policy_gradient_loss | 9.96e-05      |\n",
      "|    reward               | 73.22108      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.99e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 555           |\n",
      "|    time_elapsed         | 2716          |\n",
      "|    total_timesteps      | 1136640       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0323205e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -2.03e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.53e+05      |\n",
      "|    n_updates            | 28550         |\n",
      "|    policy_gradient_loss | -6.63e-06     |\n",
      "|    reward               | 534.6048      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 5.05e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 868\n",
      "row: 5250, episode: 868\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6345387.48\n",
      "total_reward: 5345387.48\n",
      "total_cost: 170906.06\n",
      "total_trades: 478\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 556           |\n",
      "|    time_elapsed         | 2720          |\n",
      "|    total_timesteps      | 1138688       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0057411e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 3.87e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.2e+06       |\n",
      "|    n_updates            | 28560         |\n",
      "|    policy_gradient_loss | -1.62e-05     |\n",
      "|    reward               | 82.927666     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.84e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 557           |\n",
      "|    time_elapsed         | 2725          |\n",
      "|    total_timesteps      | 1140736       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6957056e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.48e+06      |\n",
      "|    n_updates            | 28570         |\n",
      "|    policy_gradient_loss | -2.22e-05     |\n",
      "|    reward               | 248.19827     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.1e+07       |\n",
      "-------------------------------------------\n",
      "Episode: 869\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 558           |\n",
      "|    time_elapsed         | 2730          |\n",
      "|    total_timesteps      | 1142784       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2932148e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -5.96e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.48e+06      |\n",
      "|    n_updates            | 28580         |\n",
      "|    policy_gradient_loss | -1.67e-06     |\n",
      "|    reward               | 8.064705      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 6.97e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 559           |\n",
      "|    time_elapsed         | 2736          |\n",
      "|    total_timesteps      | 1144832       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1600787e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.14e+07      |\n",
      "|    n_updates            | 28590         |\n",
      "|    policy_gradient_loss | 4.71e-06      |\n",
      "|    reward               | 62.33135      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.27e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 560           |\n",
      "|    time_elapsed         | 2743          |\n",
      "|    total_timesteps      | 1146880       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5183544e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.91e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.03e+05      |\n",
      "|    n_updates            | 28600         |\n",
      "|    policy_gradient_loss | -6.78e-06     |\n",
      "|    reward               | 348.7256      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 8.06e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 870\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 561           |\n",
      "|    time_elapsed         | 2748          |\n",
      "|    total_timesteps      | 1148928       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2331445e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.62e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.99e+06      |\n",
      "|    n_updates            | 28610         |\n",
      "|    policy_gradient_loss | -1.44e-05     |\n",
      "|    reward               | 170.0978      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.2e+07       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 562           |\n",
      "|    time_elapsed         | 2753          |\n",
      "|    total_timesteps      | 1150976       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7415808e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.26e+06      |\n",
      "|    n_updates            | 28620         |\n",
      "|    policy_gradient_loss | 1.1e-06       |\n",
      "|    reward               | 291.68723     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.85e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 871\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 417           |\n",
      "|    iterations           | 563           |\n",
      "|    time_elapsed         | 2758          |\n",
      "|    total_timesteps      | 1153024       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4068868e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 3.52e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.28e+06      |\n",
      "|    n_updates            | 28630         |\n",
      "|    policy_gradient_loss | -4.92e-06     |\n",
      "|    reward               | 38.50636      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.06e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 564           |\n",
      "|    time_elapsed         | 2763          |\n",
      "|    total_timesteps      | 1155072       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.9971277e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.62e+07      |\n",
      "|    n_updates            | 28640         |\n",
      "|    policy_gradient_loss | 2.68e-06      |\n",
      "|    reward               | 260.73886     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.23e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 565          |\n",
      "|    time_elapsed         | 2767         |\n",
      "|    total_timesteps      | 1157120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.013165e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -5.36e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.55e+06     |\n",
      "|    n_updates            | 28650        |\n",
      "|    policy_gradient_loss | -2.04e-05    |\n",
      "|    reward               | 469.8623     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 7.1e+06      |\n",
      "------------------------------------------\n",
      "Episode: 872\n",
      "row: 5250, episode: 872\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6099641.73\n",
      "total_reward: 5099641.73\n",
      "total_cost: 249799.38\n",
      "total_trades: 392\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 566          |\n",
      "|    time_elapsed         | 2772         |\n",
      "|    total_timesteps      | 1159168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.536414e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -7.63e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.12e+07     |\n",
      "|    n_updates            | 28660        |\n",
      "|    policy_gradient_loss | 8.8e-06      |\n",
      "|    reward               | 102.05085    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.24e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 567          |\n",
      "|    time_elapsed         | 2775         |\n",
      "|    total_timesteps      | 1161216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.349284e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 7.75e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.3e+07      |\n",
      "|    n_updates            | 28670        |\n",
      "|    policy_gradient_loss | -1.17e-06    |\n",
      "|    reward               | 212.6447     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.6e+07      |\n",
      "------------------------------------------\n",
      "Episode: 873\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 568           |\n",
      "|    time_elapsed         | 2780          |\n",
      "|    total_timesteps      | 1163264       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2514647e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.68e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.48e+06      |\n",
      "|    n_updates            | 28680         |\n",
      "|    policy_gradient_loss | -3.06e-06     |\n",
      "|    reward               | -0.099979     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 6.97e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 569           |\n",
      "|    time_elapsed         | 2784          |\n",
      "|    total_timesteps      | 1165312       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.5401976e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.19e+07      |\n",
      "|    n_updates            | 28690         |\n",
      "|    policy_gradient_loss | 3.37e-06      |\n",
      "|    reward               | 234.47173     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.38e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 570           |\n",
      "|    time_elapsed         | 2788          |\n",
      "|    total_timesteps      | 1167360       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6612543e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -4.89e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.9e+05       |\n",
      "|    n_updates            | 28700         |\n",
      "|    policy_gradient_loss | -3.23e-06     |\n",
      "|    reward               | 619.9575      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.38e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 874\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 571          |\n",
      "|    time_elapsed         | 2793         |\n",
      "|    total_timesteps      | 1169408      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.768655e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.64e+07     |\n",
      "|    n_updates            | 28710        |\n",
      "|    policy_gradient_loss | -1.71e-07    |\n",
      "|    reward               | -7.4714794   |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 5.29e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 572           |\n",
      "|    time_elapsed         | 2798          |\n",
      "|    total_timesteps      | 1171456       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2578904e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.33e+08      |\n",
      "|    n_updates            | 28720         |\n",
      "|    policy_gradient_loss | -1.58e-07     |\n",
      "|    reward               | 144.92336     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.67e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 573           |\n",
      "|    time_elapsed         | 2804          |\n",
      "|    total_timesteps      | 1173504       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4489051e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 3.16e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.44e+06      |\n",
      "|    n_updates            | 28730         |\n",
      "|    policy_gradient_loss | -2.09e-05     |\n",
      "|    reward               | 662.4192      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.88e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 875\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 574           |\n",
      "|    time_elapsed         | 2807          |\n",
      "|    total_timesteps      | 1175552       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9035145e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.11e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.71e+07      |\n",
      "|    n_updates            | 28740         |\n",
      "|    policy_gradient_loss | -4.51e-05     |\n",
      "|    reward               | 105.97972     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.41e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 575           |\n",
      "|    time_elapsed         | 2812          |\n",
      "|    total_timesteps      | 1177600       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6694033e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.35e+06      |\n",
      "|    n_updates            | 28750         |\n",
      "|    policy_gradient_loss | 2.63e-06      |\n",
      "|    reward               | 488.99274     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.07e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 876\n",
      "row: 5250, episode: 876\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 18018160.77\n",
      "total_reward: 17018160.77\n",
      "total_cost: 350532.31\n",
      "total_trades: 485\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 576          |\n",
      "|    time_elapsed         | 2819         |\n",
      "|    total_timesteps      | 1179648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.336172e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -2.98e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.27e+07     |\n",
      "|    n_updates            | 28760        |\n",
      "|    policy_gradient_loss | -1.23e-05    |\n",
      "|    reward               | -7.3880095   |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.55e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 577          |\n",
      "|    time_elapsed         | 2823         |\n",
      "|    total_timesteps      | 1181696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.178161e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 2.38e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.36e+08     |\n",
      "|    n_updates            | 28770        |\n",
      "|    policy_gradient_loss | -1.98e-05    |\n",
      "|    reward               | 50.13271     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.73e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 578           |\n",
      "|    time_elapsed         | 2828          |\n",
      "|    total_timesteps      | 1183744       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2316334e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.23e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.33e+05      |\n",
      "|    n_updates            | 28780         |\n",
      "|    policy_gradient_loss | -6.06e-05     |\n",
      "|    reward               | 428.34573     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.66e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 877\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 579           |\n",
      "|    time_elapsed         | 2832          |\n",
      "|    total_timesteps      | 1185792       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7304585e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.23e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.86e+06      |\n",
      "|    n_updates            | 28790         |\n",
      "|    policy_gradient_loss | -6.58e-05     |\n",
      "|    reward               | 72.092445     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.57e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 580          |\n",
      "|    time_elapsed         | 2836         |\n",
      "|    total_timesteps      | 1187840      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.151234e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.01e+07     |\n",
      "|    n_updates            | 28800        |\n",
      "|    policy_gradient_loss | -9.21e-05    |\n",
      "|    reward               | 189.39204    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.01e+07     |\n",
      "------------------------------------------\n",
      "Episode: 878\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 581           |\n",
      "|    time_elapsed         | 2841          |\n",
      "|    total_timesteps      | 1189888       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2710225e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.28e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.63e+06      |\n",
      "|    n_updates            | 28810         |\n",
      "|    policy_gradient_loss | 8.79e-06      |\n",
      "|    reward               | 52.011124     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.27e+06      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 418            |\n",
      "|    iterations           | 582            |\n",
      "|    time_elapsed         | 2844           |\n",
      "|    total_timesteps      | 1191936        |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.06723746e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.66          |\n",
      "|    explained_variance   | 9.54e-07       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 3.45e+06       |\n",
      "|    n_updates            | 28820          |\n",
      "|    policy_gradient_loss | -1.16e-06      |\n",
      "|    reward               | 314.1972       |\n",
      "|    std                  | 1.27           |\n",
      "|    value_loss           | 6.9e+06        |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 583           |\n",
      "|    time_elapsed         | 2848          |\n",
      "|    total_timesteps      | 1193984       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5628175e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.43e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.67e+06      |\n",
      "|    n_updates            | 28830         |\n",
      "|    policy_gradient_loss | -3.66e-05     |\n",
      "|    reward               | 475.95563     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.33e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 879\n",
      "Episode: 880\n",
      "row: 691, episode: 880\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 690810.55\n",
      "total_reward: -309189.45\n",
      "total_cost: 9380.87\n",
      "total_trades: 18\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 584          |\n",
      "|    time_elapsed         | 2853         |\n",
      "|    total_timesteps      | 1196032      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.589107e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 2.44e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.75e+07     |\n",
      "|    n_updates            | 28840        |\n",
      "|    policy_gradient_loss | 2.44e-05     |\n",
      "|    reward               | 49.414753    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 5.5e+07      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 419         |\n",
      "|    iterations           | 585         |\n",
      "|    time_elapsed         | 2858        |\n",
      "|    total_timesteps      | 1198080     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 9.34233e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 2.98e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.57e+07    |\n",
      "|    n_updates            | 28850       |\n",
      "|    policy_gradient_loss | -1.76e-06   |\n",
      "|    reward               | 205.71634   |\n",
      "|    std                  | 1.27        |\n",
      "|    value_loss           | 3.13e+07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 586          |\n",
      "|    time_elapsed         | 2864         |\n",
      "|    total_timesteps      | 1200128      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.416238e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -4.77e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.94e+06     |\n",
      "|    n_updates            | 28860        |\n",
      "|    policy_gradient_loss | -1.51e-05    |\n",
      "|    reward               | 487.2165     |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 5.88e+06     |\n",
      "------------------------------------------\n",
      "Episode: 881\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 587           |\n",
      "|    time_elapsed         | 2868          |\n",
      "|    total_timesteps      | 1202176       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0270451e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 5.84e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2e+07         |\n",
      "|    n_updates            | 28870         |\n",
      "|    policy_gradient_loss | -1.11e-05     |\n",
      "|    reward               | 23.529615     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.01e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 588           |\n",
      "|    time_elapsed         | 2873          |\n",
      "|    total_timesteps      | 1204224       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9645086e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.1e+06       |\n",
      "|    n_updates            | 28880         |\n",
      "|    policy_gradient_loss | -3.37e-06     |\n",
      "|    reward               | 156.94312     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.82e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 882\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 589           |\n",
      "|    time_elapsed         | 2878          |\n",
      "|    total_timesteps      | 1206272       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3992907e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -4.05e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 28890         |\n",
      "|    policy_gradient_loss | -2.46e-05     |\n",
      "|    reward               | 52.988426     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.33e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 590           |\n",
      "|    time_elapsed         | 2882          |\n",
      "|    total_timesteps      | 1208320       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1345622e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.01e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.24e+06      |\n",
      "|    n_updates            | 28900         |\n",
      "|    policy_gradient_loss | -2.23e-06     |\n",
      "|    reward               | 432.30618     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.05e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 591          |\n",
      "|    time_elapsed         | 2887         |\n",
      "|    total_timesteps      | 1210368      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.063201e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 7.15e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.33e+06     |\n",
      "|    n_updates            | 28910        |\n",
      "|    policy_gradient_loss | -8.62e-06    |\n",
      "|    reward               | 1253.0997    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 1.47e+07     |\n",
      "------------------------------------------\n",
      "Episode: 883\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 592          |\n",
      "|    time_elapsed         | 2891         |\n",
      "|    total_timesteps      | 1212416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.581525e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 3.46e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.28e+07     |\n",
      "|    n_updates            | 28920        |\n",
      "|    policy_gradient_loss | -1.86e-05    |\n",
      "|    reward               | 182.42668    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 1.46e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 593           |\n",
      "|    time_elapsed         | 2895          |\n",
      "|    total_timesteps      | 1214464       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1111995e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.55e+07      |\n",
      "|    n_updates            | 28930         |\n",
      "|    policy_gradient_loss | 4.22e-06      |\n",
      "|    reward               | 351.65155     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.91e+08      |\n",
      "-------------------------------------------\n",
      "Episode: 884\n",
      "row: 5250, episode: 884\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 8564304.35\n",
      "total_reward: 7564304.35\n",
      "total_cost: 296730.41\n",
      "total_trades: 422\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 594           |\n",
      "|    time_elapsed         | 2900          |\n",
      "|    total_timesteps      | 1216512       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2089807e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -2.37e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 9.31e+06      |\n",
      "|    n_updates            | 28940         |\n",
      "|    policy_gradient_loss | -1.35e-06     |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.86e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 595           |\n",
      "|    time_elapsed         | 2904          |\n",
      "|    total_timesteps      | 1218560       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.3e+07       |\n",
      "|    n_updates            | 28950         |\n",
      "|    policy_gradient_loss | -1.13e-06     |\n",
      "|    reward               | 295.4214      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.06e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 596          |\n",
      "|    time_elapsed         | 2908         |\n",
      "|    total_timesteps      | 1220608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.462184e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -2.5e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.39e+06     |\n",
      "|    n_updates            | 28960        |\n",
      "|    policy_gradient_loss | -3.05e-06    |\n",
      "|    reward               | 514.17975    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 8.78e+06     |\n",
      "------------------------------------------\n",
      "Episode: 885\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 597           |\n",
      "|    time_elapsed         | 2913          |\n",
      "|    total_timesteps      | 1222656       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2165401e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.79e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.22e+07      |\n",
      "|    n_updates            | 28970         |\n",
      "|    policy_gradient_loss | -8.57e-06     |\n",
      "|    reward               | 50.559414     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 4.43e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 598           |\n",
      "|    time_elapsed         | 2917          |\n",
      "|    total_timesteps      | 1224704       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7395588e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.04e+07      |\n",
      "|    n_updates            | 28980         |\n",
      "|    policy_gradient_loss | -1.46e-05     |\n",
      "|    reward               | 295.64822     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 6.08e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 599           |\n",
      "|    time_elapsed         | 2923          |\n",
      "|    total_timesteps      | 1226752       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1502062e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -3.58e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.91e+06      |\n",
      "|    n_updates            | 28990         |\n",
      "|    policy_gradient_loss | 1.05e-06      |\n",
      "|    reward               | 1241.1727     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.18e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 886\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 600           |\n",
      "|    time_elapsed         | 2927          |\n",
      "|    total_timesteps      | 1228800       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.5e-06       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.47e+07      |\n",
      "|    n_updates            | 29000         |\n",
      "|    policy_gradient_loss | -1.06e-06     |\n",
      "|    reward               | 106.110535    |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.09e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 601          |\n",
      "|    time_elapsed         | 2931         |\n",
      "|    total_timesteps      | 1230848      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.738142e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.33e+07     |\n",
      "|    n_updates            | 29010        |\n",
      "|    policy_gradient_loss | -1.76e-06    |\n",
      "|    reward               | 232.19014    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.67e+07     |\n",
      "------------------------------------------\n",
      "Episode: 887\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 602           |\n",
      "|    time_elapsed         | 2936          |\n",
      "|    total_timesteps      | 1232896       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.0573955e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.96e+06      |\n",
      "|    n_updates            | 29020         |\n",
      "|    policy_gradient_loss | -1.36e-05     |\n",
      "|    reward               | 105.30936     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 5.91e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 603          |\n",
      "|    time_elapsed         | 2940         |\n",
      "|    total_timesteps      | 1234944      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.065223e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.18e+06     |\n",
      "|    n_updates            | 29030        |\n",
      "|    policy_gradient_loss | 3.51e-06     |\n",
      "|    reward               | 229.86493    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 1.24e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 604           |\n",
      "|    time_elapsed         | 2945          |\n",
      "|    total_timesteps      | 1236992       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.1013346e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 5.66e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.15e+06      |\n",
      "|    n_updates            | 29040         |\n",
      "|    policy_gradient_loss | -4.55e-07     |\n",
      "|    reward               | 414.71008     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.43e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 888\n",
      "row: 5250, episode: 888\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5599354.26\n",
      "total_reward: 4599354.26\n",
      "total_cost: 182756.35\n",
      "total_trades: 376\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 420           |\n",
      "|    iterations           | 605           |\n",
      "|    time_elapsed         | 2949          |\n",
      "|    total_timesteps      | 1239040       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0529918e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.27e-05      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.36e+07      |\n",
      "|    n_updates            | 29050         |\n",
      "|    policy_gradient_loss | -1.96e-06     |\n",
      "|    reward               | 46.80189      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 2.73e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 606           |\n",
      "|    time_elapsed         | 2956          |\n",
      "|    total_timesteps      | 1241088       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.4718035e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 5.41e+06      |\n",
      "|    n_updates            | 29060         |\n",
      "|    policy_gradient_loss | -7.69e-06     |\n",
      "|    reward               | 214.92238     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.08e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 889\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 607           |\n",
      "|    time_elapsed         | 2962          |\n",
      "|    total_timesteps      | 1243136       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1169405e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 7.03e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.71e+06      |\n",
      "|    n_updates            | 29070         |\n",
      "|    policy_gradient_loss | -3.88e-05     |\n",
      "|    reward               | 9.1807785     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 3.43e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 608           |\n",
      "|    time_elapsed         | 2967          |\n",
      "|    total_timesteps      | 1245184       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5845948e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.84e+06      |\n",
      "|    n_updates            | 29080         |\n",
      "|    policy_gradient_loss | -8.42e-06     |\n",
      "|    reward               | 43.84648      |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.57e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 609           |\n",
      "|    time_elapsed         | 2971          |\n",
      "|    total_timesteps      | 1247232       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0267093e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.41e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 6.99e+04      |\n",
      "|    n_updates            | 29090         |\n",
      "|    policy_gradient_loss | -4.42e-05     |\n",
      "|    reward               | 211.43602     |\n",
      "|    std                  | 1.27          |\n",
      "|    value_loss           | 1.4e+05       |\n",
      "-------------------------------------------\n",
      "Episode: 890\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 610          |\n",
      "|    time_elapsed         | 2976         |\n",
      "|    total_timesteps      | 1249280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.749146e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 7.27e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.39e+06     |\n",
      "|    n_updates            | 29100        |\n",
      "|    policy_gradient_loss | 1.09e-05     |\n",
      "|    reward               | 256.12387    |\n",
      "|    std                  | 1.27         |\n",
      "|    value_loss           | 2.77e+06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 419         |\n",
      "|    iterations           | 611         |\n",
      "|    time_elapsed         | 2981        |\n",
      "|    total_timesteps      | 1251328     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.61936e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 4.77e-07    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.2e+06     |\n",
      "|    n_updates            | 29110       |\n",
      "|    policy_gradient_loss | -3.57e-05   |\n",
      "|    reward               | 719.3442    |\n",
      "|    std                  | 1.28        |\n",
      "|    value_loss           | 8.4e+06     |\n",
      "-----------------------------------------\n",
      "Episode: 891\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 612           |\n",
      "|    time_elapsed         | 2986          |\n",
      "|    total_timesteps      | 1253376       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6983831e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.79e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.54e+07      |\n",
      "|    n_updates            | 29120         |\n",
      "|    policy_gradient_loss | -1.66e-05     |\n",
      "|    reward               | -0.0999004    |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3.07e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 613          |\n",
      "|    time_elapsed         | 2991         |\n",
      "|    total_timesteps      | 1255424      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.940069e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 6.43e+07     |\n",
      "|    n_updates            | 29130        |\n",
      "|    policy_gradient_loss | 5.81e-06     |\n",
      "|    reward               | 171.84482    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 1.29e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 614           |\n",
      "|    time_elapsed         | 2997          |\n",
      "|    total_timesteps      | 1257472       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8612925e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -7.63e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.5e+06       |\n",
      "|    n_updates            | 29140         |\n",
      "|    policy_gradient_loss | -5.44e-05     |\n",
      "|    reward               | 395.62222     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3e+06         |\n",
      "-------------------------------------------\n",
      "Episode: 892\n",
      "row: 5250, episode: 892\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 9302315.10\n",
      "total_reward: 8302315.10\n",
      "total_cost: 361295.50\n",
      "total_trades: 747\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 615           |\n",
      "|    time_elapsed         | 3003          |\n",
      "|    total_timesteps      | 1259520       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6207708e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -8.34e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.18e+07      |\n",
      "|    n_updates            | 29150         |\n",
      "|    policy_gradient_loss | 6.06e-05      |\n",
      "|    reward               | 145.78236     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 4.36e+07      |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| time/                   |                |\n",
      "|    fps                  | 419            |\n",
      "|    iterations           | 616            |\n",
      "|    time_elapsed         | 3008           |\n",
      "|    total_timesteps      | 1261568        |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.06636435e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.1            |\n",
      "|    entropy_loss         | -1.66          |\n",
      "|    explained_variance   | 1.79e-07       |\n",
      "|    learning_rate        | 0.00025        |\n",
      "|    loss                 | 3.16e+07       |\n",
      "|    n_updates            | 29160          |\n",
      "|    policy_gradient_loss | -5.34e-07      |\n",
      "|    reward               | 564.4208       |\n",
      "|    std                  | 1.28           |\n",
      "|    value_loss           | 6.32e+07       |\n",
      "--------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 617          |\n",
      "|    time_elapsed         | 3014         |\n",
      "|    total_timesteps      | 1263616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.604264e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -7.03e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.27e+07     |\n",
      "|    n_updates            | 29170        |\n",
      "|    policy_gradient_loss | -1.65e-07    |\n",
      "|    reward               | 1409.2004    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 2.54e+07     |\n",
      "------------------------------------------\n",
      "Episode: 893\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 618          |\n",
      "|    time_elapsed         | 3018         |\n",
      "|    total_timesteps      | 1265664      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.169974e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.6e+08      |\n",
      "|    n_updates            | 29180        |\n",
      "|    policy_gradient_loss | -1.98e-06    |\n",
      "|    reward               | 304.0528     |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 3.2e+08      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 419         |\n",
      "|    iterations           | 619         |\n",
      "|    time_elapsed         | 3023        |\n",
      "|    total_timesteps      | 1267712     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.81143e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | -2.62e-06   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.38e+06    |\n",
      "|    n_updates            | 29190       |\n",
      "|    policy_gradient_loss | -1.41e-06   |\n",
      "|    reward               | 697.7502    |\n",
      "|    std                  | 1.28        |\n",
      "|    value_loss           | 1.28e+07    |\n",
      "-----------------------------------------\n",
      "Episode: 894\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 620          |\n",
      "|    time_elapsed         | 3028         |\n",
      "|    total_timesteps      | 1269760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.344068e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | -1.43e-06    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.92e+07     |\n",
      "|    n_updates            | 29200        |\n",
      "|    policy_gradient_loss | -1.35e-06    |\n",
      "|    reward               | 6.9590898    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 9.83e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 621          |\n",
      "|    time_elapsed         | 3033         |\n",
      "|    total_timesteps      | 1271808      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.477946e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.68e+07     |\n",
      "|    n_updates            | 29210        |\n",
      "|    policy_gradient_loss | -4.8e-06     |\n",
      "|    reward               | 143.90674    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 1.14e+08     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 622           |\n",
      "|    time_elapsed         | 3039          |\n",
      "|    total_timesteps      | 1273856       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7520887e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -4.41e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.19e+06      |\n",
      "|    n_updates            | 29220         |\n",
      "|    policy_gradient_loss | -1.32e-06     |\n",
      "|    reward               | 656.4735      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 2.38e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 895\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 623           |\n",
      "|    time_elapsed         | 3043          |\n",
      "|    total_timesteps      | 1275904       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7666406e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.17e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.64e+07      |\n",
      "|    n_updates            | 29230         |\n",
      "|    policy_gradient_loss | -1.82e-06     |\n",
      "|    reward               | 45.40632      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3.29e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 624           |\n",
      "|    time_elapsed         | 3048          |\n",
      "|    total_timesteps      | 1277952       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3655746e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 8.34e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.15e+06      |\n",
      "|    n_updates            | 29240         |\n",
      "|    policy_gradient_loss | -9.44e-07     |\n",
      "|    reward               | 144.57259     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 1.63e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 896\n",
      "row: 5250, episode: 896\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3442101.13\n",
      "total_reward: 2442101.13\n",
      "total_cost: 133342.99\n",
      "total_trades: 302\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 625           |\n",
      "|    time_elapsed         | 3053          |\n",
      "|    total_timesteps      | 1280000       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9656436e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -9.89e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.27e+06      |\n",
      "|    n_updates            | 29250         |\n",
      "|    policy_gradient_loss | -2.54e-05     |\n",
      "|    reward               | 12.501877     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 2.54e+06      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 626          |\n",
      "|    time_elapsed         | 3057         |\n",
      "|    total_timesteps      | 1282048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.652285e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 4.8e+06      |\n",
      "|    n_updates            | 29260        |\n",
      "|    policy_gradient_loss | 1.37e-05     |\n",
      "|    reward               | 147.33502    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 9.6e+06      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 627          |\n",
      "|    time_elapsed         | 3061         |\n",
      "|    total_timesteps      | 1284096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.953196e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 1.13e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.49e+05     |\n",
      "|    n_updates            | 29270        |\n",
      "|    policy_gradient_loss | -8.73e-06    |\n",
      "|    reward               | 449.66772    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 1.5e+06      |\n",
      "------------------------------------------\n",
      "Episode: 897\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 628          |\n",
      "|    time_elapsed         | 3066         |\n",
      "|    total_timesteps      | 1286144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.988986e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 3.87e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.42e+07     |\n",
      "|    n_updates            | 29280        |\n",
      "|    policy_gradient_loss | 3.95e-06     |\n",
      "|    reward               | 79.47959     |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 2.83e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 629           |\n",
      "|    time_elapsed         | 3071          |\n",
      "|    total_timesteps      | 1288192       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6836642e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.98e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.11e+07      |\n",
      "|    n_updates            | 29290         |\n",
      "|    policy_gradient_loss | -8.82e-06     |\n",
      "|    reward               | 292.29218     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 2.23e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 898\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 630           |\n",
      "|    time_elapsed         | 3078          |\n",
      "|    total_timesteps      | 1290240       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7831917e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -4.17e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.92e+06      |\n",
      "|    n_updates            | 29300         |\n",
      "|    policy_gradient_loss | 3.03e-06      |\n",
      "|    reward               | 28.85554      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 7.85e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 631           |\n",
      "|    time_elapsed         | 3082          |\n",
      "|    total_timesteps      | 1292288       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.3516276e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 5.96e-08      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.58e+07      |\n",
      "|    n_updates            | 29310         |\n",
      "|    policy_gradient_loss | -8.9e-06      |\n",
      "|    reward               | 227.73111     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3.16e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 632           |\n",
      "|    time_elapsed         | 3088          |\n",
      "|    total_timesteps      | 1294336       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.9057645e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -2.26e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.79e+06      |\n",
      "|    n_updates            | 29320         |\n",
      "|    policy_gradient_loss | -7.57e-06     |\n",
      "|    reward               | 617.32404     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3.59e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 899\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 633           |\n",
      "|    time_elapsed         | 3093          |\n",
      "|    total_timesteps      | 1296384       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.1117892e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.74e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.49e+07      |\n",
      "|    n_updates            | 29330         |\n",
      "|    policy_gradient_loss | 5.62e-06      |\n",
      "|    reward               | 43.114887     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 4.99e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 634          |\n",
      "|    time_elapsed         | 3098         |\n",
      "|    total_timesteps      | 1298432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.276203e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.98e+07     |\n",
      "|    n_updates            | 29340        |\n",
      "|    policy_gradient_loss | -3.63e-06    |\n",
      "|    reward               | 192.17624    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 5.95e+07     |\n",
      "------------------------------------------\n",
      "Episode: 900\n",
      "row: 5250, episode: 900\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6600255.90\n",
      "total_reward: 5600255.90\n",
      "total_cost: 227068.50\n",
      "total_trades: 639\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 635           |\n",
      "|    time_elapsed         | 3105          |\n",
      "|    total_timesteps      | 1300480       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5310943e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.34e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.99e+06      |\n",
      "|    n_updates            | 29350         |\n",
      "|    policy_gradient_loss | -2.44e-05     |\n",
      "|    reward               | -0.09999944   |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3.99e+06      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 636           |\n",
      "|    time_elapsed         | 3109          |\n",
      "|    total_timesteps      | 1302528       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4922698e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.69e+07      |\n",
      "|    n_updates            | 29360         |\n",
      "|    policy_gradient_loss | -8.27e-06     |\n",
      "|    reward               | 250.29523     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3.37e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 637           |\n",
      "|    time_elapsed         | 3113          |\n",
      "|    total_timesteps      | 1304576       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2643653e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -7.63e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.95e+06      |\n",
      "|    n_updates            | 29370         |\n",
      "|    policy_gradient_loss | -2.88e-05     |\n",
      "|    reward               | 666.827       |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 3.89e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 901\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 638          |\n",
      "|    time_elapsed         | 3117         |\n",
      "|    total_timesteps      | 1306624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.589884e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 1.01e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3e+07        |\n",
      "|    n_updates            | 29380        |\n",
      "|    policy_gradient_loss | -6.5e-06     |\n",
      "|    reward               | 74.3012      |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 6.01e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 639           |\n",
      "|    time_elapsed         | 3122          |\n",
      "|    total_timesteps      | 1308672       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3163663e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 3.73e+07      |\n",
      "|    n_updates            | 29390         |\n",
      "|    policy_gradient_loss | 3.75e-06      |\n",
      "|    reward               | 306.51743     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 7.45e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 418           |\n",
      "|    iterations           | 640           |\n",
      "|    time_elapsed         | 3129          |\n",
      "|    total_timesteps      | 1310720       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5980913e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 5.01e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 4.43e+06      |\n",
      "|    n_updates            | 29400         |\n",
      "|    policy_gradient_loss | -6.64e-06     |\n",
      "|    reward               | 698.0104      |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 8.85e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 902\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 641           |\n",
      "|    time_elapsed         | 3132          |\n",
      "|    total_timesteps      | 1312768       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7613867e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 4.35e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.86e+07      |\n",
      "|    n_updates            | 29410         |\n",
      "|    policy_gradient_loss | 8.05e-06      |\n",
      "|    reward               | 208.22401     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 5.72e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 642           |\n",
      "|    time_elapsed         | 3136          |\n",
      "|    total_timesteps      | 1314816       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6985148e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -3.58e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.81e+06      |\n",
      "|    n_updates            | 29420         |\n",
      "|    policy_gradient_loss | -8.87e-06     |\n",
      "|    reward               | 493.11148     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 1.76e+07      |\n",
      "-------------------------------------------\n",
      "Episode: 903\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 643           |\n",
      "|    time_elapsed         | 3141          |\n",
      "|    total_timesteps      | 1316864       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3044337e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -2.15e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.49e+07      |\n",
      "|    n_updates            | 29430         |\n",
      "|    policy_gradient_loss | 2.39e-06      |\n",
      "|    reward               | 21.574226     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 2.99e+07      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 419         |\n",
      "|    iterations           | 644         |\n",
      "|    time_elapsed         | 3145        |\n",
      "|    total_timesteps      | 1318912     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.73576e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.1         |\n",
      "|    entropy_loss         | -1.66       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.34e+07    |\n",
      "|    n_updates            | 29440       |\n",
      "|    policy_gradient_loss | -1.9e-06    |\n",
      "|    reward               | 181.83986   |\n",
      "|    std                  | 1.28        |\n",
      "|    value_loss           | 6.68e+07    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 645          |\n",
      "|    time_elapsed         | 3150         |\n",
      "|    total_timesteps      | 1320960      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.509268e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 2.86e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.18e+06     |\n",
      "|    n_updates            | 29450        |\n",
      "|    policy_gradient_loss | -6.25e-05    |\n",
      "|    reward               | 455.98697    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 2.36e+06     |\n",
      "------------------------------------------\n",
      "Episode: 904\n",
      "row: 5250, episode: 904\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 7501944.05\n",
      "total_reward: 6501944.05\n",
      "total_cost: 186280.24\n",
      "total_trades: 309\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 646          |\n",
      "|    time_elapsed         | 3154         |\n",
      "|    total_timesteps      | 1323008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.185327e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 3.99e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.47e+07     |\n",
      "|    n_updates            | 29460        |\n",
      "|    policy_gradient_loss | 6.2e-05      |\n",
      "|    reward               | 351.11957    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 2.94e+07     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 647          |\n",
      "|    time_elapsed         | 3159         |\n",
      "|    total_timesteps      | 1325056      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.799827e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 2.98e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.38e+07     |\n",
      "|    n_updates            | 29470        |\n",
      "|    policy_gradient_loss | 4.09e-07     |\n",
      "|    reward               | 578.11334    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 2.76e+07     |\n",
      "------------------------------------------\n",
      "Episode: 905\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 648           |\n",
      "|    time_elapsed         | 3163          |\n",
      "|    total_timesteps      | 1327104       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9849193e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -3.58e-07     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.17e+07      |\n",
      "|    n_updates            | 29480         |\n",
      "|    policy_gradient_loss | 4.66e-10      |\n",
      "|    reward               | 23.388992     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 4.34e+07      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 649           |\n",
      "|    time_elapsed         | 3167          |\n",
      "|    total_timesteps      | 1329152       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5104888e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 7.94e+07      |\n",
      "|    n_updates            | 29490         |\n",
      "|    policy_gradient_loss | -2.14e-06     |\n",
      "|    reward               | 67.112434     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 1.59e+08      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 650           |\n",
      "|    time_elapsed         | 3173          |\n",
      "|    total_timesteps      | 1331200       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3541972e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -2.16e-05     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.59e+05      |\n",
      "|    n_updates            | 29500         |\n",
      "|    policy_gradient_loss | -0.00013      |\n",
      "|    reward               | 336.99753     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 5.19e+05      |\n",
      "-------------------------------------------\n",
      "Episode: 906\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 651          |\n",
      "|    time_elapsed         | 3177         |\n",
      "|    total_timesteps      | 1333248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.110568e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 4.71e-06     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.84e+06     |\n",
      "|    n_updates            | 29510        |\n",
      "|    policy_gradient_loss | -7.96e-05    |\n",
      "|    reward               | 48.933136    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 7.68e+06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 652          |\n",
      "|    time_elapsed         | 3182         |\n",
      "|    total_timesteps      | 1335296      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.378875e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 3.58e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 5.73e+06     |\n",
      "|    n_updates            | 29520        |\n",
      "|    policy_gradient_loss | 1.09e-05     |\n",
      "|    reward               | 176.37775    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 1.15e+07     |\n",
      "------------------------------------------\n",
      "Episode: 907\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 653           |\n",
      "|    time_elapsed         | 3188          |\n",
      "|    total_timesteps      | 1337344       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7124694e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -1.79e-06     |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 2.5e+06       |\n",
      "|    n_updates            | 29530         |\n",
      "|    policy_gradient_loss | -2.36e-07     |\n",
      "|    reward               | -0.0999516    |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 5e+06         |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 654          |\n",
      "|    time_elapsed         | 3193         |\n",
      "|    total_timesteps      | 1339392      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.788185e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 3.58e-07     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.84e+07     |\n",
      "|    n_updates            | 29540        |\n",
      "|    policy_gradient_loss | -2.93e-06    |\n",
      "|    reward               | 171.64742    |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 3.67e+07     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 655           |\n",
      "|    time_elapsed         | 3198          |\n",
      "|    total_timesteps      | 1341440       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2863893e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | -8.7e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.17e+06      |\n",
      "|    n_updates            | 29550         |\n",
      "|    policy_gradient_loss | -2.11e-06     |\n",
      "|    reward               | 377.21545     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 2.35e+06      |\n",
      "-------------------------------------------\n",
      "Episode: 908\n",
      "row: 5250, episode: 908\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 14938441.78\n",
      "total_reward: 13938441.78\n",
      "total_cost: 275564.67\n",
      "total_trades: 570\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 419           |\n",
      "|    iterations           | 656           |\n",
      "|    time_elapsed         | 3203          |\n",
      "|    total_timesteps      | 1343488       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0931703e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.1           |\n",
      "|    entropy_loss         | -1.66         |\n",
      "|    explained_variance   | 2.98e-06      |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 8.18e+06      |\n",
      "|    n_updates            | 29560         |\n",
      "|    policy_gradient_loss | -2.96e-07     |\n",
      "|    reward               | 1.4491236     |\n",
      "|    std                  | 1.28          |\n",
      "|    value_loss           | 1.64e+07      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 657          |\n",
      "|    time_elapsed         | 3207         |\n",
      "|    total_timesteps      | 1345536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.508788e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.1          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 7.2e+07      |\n",
      "|    n_updates            | 29570        |\n",
      "|    policy_gradient_loss | -2.67e-06    |\n",
      "|    reward               | 207.2997     |\n",
      "|    std                  | 1.28         |\n",
      "|    value_loss           | 1.44e+08     |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_ppo = agent.train_model(model=trained_ppo, \n",
    "                             tb_log_name='ppo',\n",
    "                             total_timesteps=total_training_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = StockTradingEnv(df = trade_data, **env_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_step = 1000\n",
    "test_result = []\n",
    "test_env.reset()\n",
    "for i in range(0,test_step):\n",
    "    observation = [test_env.state]\n",
    "    observation = np.array(observation).astype(np.float32)\n",
    "    actions, values, log_prob = ort_sess.run(None, {\"input\": observation})\n",
    "    result = test_env.step(actions)\n",
    "    test_result.append(result[1])\n",
    "    if result[2] == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2.2 Load ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"all_in_one_ppo.onnx\"\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# # Check that the predictions are the same\n",
    "# with th.no_grad():\n",
    "#     print(model.policy(th.as_tensor(observation), deterministic=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = np.zeros((1, state_space)).astype(np.float32)\n",
    "ort_sess = ort.InferenceSession(onnx_path)\n",
    "actions, values, log_prob = ort_sess.run(None, {\"input\": observation})\n",
    "print(actions, values, log_prob)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
